id,Number,sentiment,text
1465,15442,0,"Tensorflow installation error. Tensorflow installation error.

Method followed: https://www.tensorflow.org/install/install_linux#InstallingAnaconda

(Environment ubuntu 16.04, anaconda 4.3.29, Python 2.7.13, nvidia 1070 GPU)
Please help.

>>>>>
gopi@gp:~$ source activate tensorflow
(tensorflow) gopi@gp:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp27-none-linux_x86_64.whl
Collecting tensorflow-gpu==1.4.0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp27-none-linux_x86_64.whl
  Using cached https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp27-none-linux_x86_64.whl
Collecting enum34>=1.1.6 (from tensorflow-gpu==1.4.0)
  Using cached enum34-1.1.6-py2-none-any.whl
Collecting six>=1.10.0 (from tensorflow-gpu==1.4.0)
  Using cached six-1.11.0-py2.py3-none-any.whl
Collecting protobuf>=3.3.0 (from tensorflow-gpu==1.4.0)
  Using cached protobuf-3.5.0.post1-cp27-cp27mu-manylinux1_x86_64.whl
Collecting numpy>=1.12.1 (from tensorflow-gpu==1.4.0)
  Using cached numpy-1.13.3-cp27-cp27mu-manylinux1_x86_64.whl
Collecting wheel (from tensorflow-gpu==1.4.0)
  Using cached wheel-0.30.0-py2.py3-none-any.whl
Collecting backports.weakref>=1.0rc1 (from tensorflow-gpu==1.4.0)
  Using cached backports.weakref-1.0.post1-py2.py3-none-any.whl
Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow-gpu==1.4.0)
  Using cached tensorflow_tensorboard-0.4.0rc3-py2-none-any.whl
Collecting mock>=2.0.0 (from tensorflow-gpu==1.4.0)
  Using cached mock-2.0.0-py2.py3-none-any.whl
Collecting setuptools (from protobuf>=3.3.0->tensorflow-gpu==1.4.0)
  Downloading setuptools-38.2.4-py2.py3-none-any.whl (489kB)
    100% |████████████████████████████████| 491kB 30kB/s 
Collecting futures>=3.1.1; python_version < ""3.2"" (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4.0)
  Downloading futures-3.2.0-py2-none-any.whl
Collecting werkzeug>=0.11.10 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4.0)
  Downloading Werkzeug-0.13-py2.py3-none-any.whl (311kB)
    100% |████████████████████████████████| 317kB 31kB/s 
Collecting html5lib==0.9999999 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4.0)
Collecting markdown>=2.6.8 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4.0)
  Downloading Markdown-2.6.10.zip (414kB)
    100% |████████████████████████████████| 419kB 43kB/s 
Collecting bleach==1.5.0 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4.0)
  Using cached bleach-1.5.0-py2.py3-none-any.whl
Collecting funcsigs>=1; python_version < ""3.3"" (from mock>=2.0.0->tensorflow-gpu==1.4.0)
  Using cached funcsigs-1.0.2-py2.py3-none-any.whl
Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow-gpu==1.4.0)
  Using cached pbr-3.1.1-py2.py3-none-any.whl
Building wheels for collected packages: markdown
  Running setup.py bdist_wheel for markdown ... done
  Stored in directory: /home/gopi/.cache/pip/wheels/1e/5a/55/a80b200d12e234d575ad68c1528593d1ce488720b65b24e48c
Successfully built markdown
Installing collected packages: enum34, six, setuptools, protobuf, numpy, wheel, backports.weakref, futures, werkzeug, html5lib, markdown, bleach, tensorflow-tensorboard, funcsigs, pbr, mock, tensorflow-gpu
Exception:
Traceback (most recent call last):
  File ""/home/gopi/.local/lib/python2.7/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/home/gopi/.local/lib/python2.7/site-packages/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/home/gopi/.local/lib/python2.7/site-packages/pip/req/req_set.py"", line 784, in install
    **kwargs
  File ""/home/gopi/.local/lib/python2.7/site-packages/pip/req/req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""/home/gopi/.local/lib/python2.7/site-packages/pip/req/req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""/home/gopi/.local/lib/python2.7/site-packages/pip/wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/home/gopi/.local/lib/python2.7/site-packages/pip/wheel.py"", line 329, in clobber
    os.utime(destfile, (st.st_atime, st.st_mtime))
OSError: [Errno 1] Operation not permitted: '/home/gopi/anaconda2/lib/python2.7/site-packages/enum/README'
(tensorflow) gopi@gp:~$ 
"
406,4912,0,"""no module named tensorflow"" when I ""import tensorflow"" after I install tensorflow on windows successfully. I followed ..\tensorflow\contrib\cmake\README to install tensorflow on windows. Everything went well and I installed tensorflow successfully. But after I ""activate tensorflow"", and tried to ""import tensorflow"" using python, it went wrong, saying ""no module named tensorflow"". How can I fix this problem? Many thanks!
"
1474,31438,0,"Failed to load the native TensorFlow runtime.. 
C:\Users\safalabolo\Desktop\7-inversion>c:\Python36\python.exe apply_algo.py
WARNING (theano.configdefaults): g++ not available, if using conda: 
c:\Python36\lib\site-packages\theano\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory
  warnings.warn(""DeprecationWarning: there is no c++ compiler.""
WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.
WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
WARNING (theano.tensor.blas): Failed to import scipy.linalg.blas, and Theano flag blas.ldflags is empty. Falling back on slower implementations for dot(matrix, vector), dot(vector, matrix) and dot(vector, vector) (cannot import name 'NUMPY_MKL')
Using TensorFlow backend.
Traceback (most recent call last):
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 35, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 30, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Impossibile trovare il modulo specificato.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""apply_algo.py"", line 11, in <module>
    loaded_model = pickle.load(open(modelname, 'rb'))
  File ""c:\Python36\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""c:\Python36\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""c:\Python36\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""c:\Python36\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""c:\Python36\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""c:\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""c:\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 35, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 30, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Impossibile trovare il modulo specificato.

"
477,32212,0,"Raise undefined symbol in ""tensorflow.contrib.fused_conv.python.ops.fused_conv2d_bias_activation_op"" module. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): 
- TensorFlow version: 
- Python version: 
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 
- GPU model and memory: 



**Describe the problem**


I follow Anaconda compiled tensorflow conda package method(conda-build and some referenced recipes) to compile tensorflow==1.14 with tensorRT and nccl features. 


My , :
<details>



</details>

**Any other info / logs**
* use  and  docker image
* import tensorflow is ok and  will display visible gpu devices and others info
* similar behaviour can refer to: https://github.com/ContinuumIO/anaconda-issues/issues/11239 and https://github.com/tensorflow/tensorflow/issues?q=tensorflow.contrib.fused_conv.python.ops.fused_conv2d_bias_activation_op 
* pre-build wheel package will produce same error, error see https://user-images.githubusercontent.com/19144683/64264093-54077e80-cf63-11e9-9b92-83fc11a4be79.png
:

"
1449,5595,0,"Saving optimizer state (adagrad/momentum/etc.). Hey everybody,

Last week I asked this question on stackoverflow: https://stackoverflow.com/questions/40547198/saving-the-state-of-the-adagrad-algorithm-in-tensorflow . 
My problem is that I want to save the state of the optimizer (in my case the adagrad accumulators) so I can stop my learning and continue whenever I want. 

Unless I'm mistaken the state of the optimizer can't be saved (you cant pass an optimizer to a tf.train.Saver, right?). A quick (hacky?) solution for me might be is calling Optimizer.get_slot_names() and save the op of each slot. 
The next problem would be putting this op back in the slots, as I don't think there is a set_slot(name,op) at the moment. 

So my questions are: 
- Am I right that this is currently impossible? 
- Do we want to have a set_slot(name,op) function in the Optimizer class? (I am willing to help out with this)
- Do we want to be able to pass an optimizer to a Saver object?"
1183,11974,0,"tf.reshape does not accept Dimension objects for the shape parameter. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.2.1
- **Python version**: 3.6.1 (Anaconda 4.4.0 64-bit)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: GTX 780
- **Exact command to reproduce**: tf.reshape()

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

tf.reshape does not accept a list with mixed Integer and Dimension() as elements for the shape parameter. It should accept shapes that have Dimension as elements since tensor shapes consist of dimensions. Specifically, tf.tensor.shape returns a list of Dimensions, therefore using a similar object to specify a shape in tf.reshape should not cause an error.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Example code:



results in



This is rectified by casting X.shape[-1] to int before passing to tf.reshape.


A similar unrelated issue is that X.shape returns (784,) while it should return (1,784). This requires reshaping to turn the placeholder back into a 2D tensor. I haven't determined if this is a bug but it occurs when the tensor is explicitly specified as a 2D tensor so it is probably worth changing.

"
626,21670,0,"freeze_graph returns with error. I used  to freeze my graph. This program complied with bazel several minutes ago.
But this returns with error:



Here's my environment specification:
Sys:Debian 9.5
python:python 3.5.3
Tensorflow: tensorflow (1.10.0) from pip3
bazel:0.16.1
gcc:gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516

tensorflow souce code from 1.10.0 release.
"
311,18103,0,"Keras TimeDistributed wrapper around GlobalMaxPooling2D error with TPU. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0-rc1
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: Google Cloud TPU v2-8 running TF 1.7
- **Exact command to reproduce**: See below


### Describe the problem
Using a Keras TimeDistributed wrapper to wrap a Keras GlobalMaxPooling2D layer, and processing on a Google Cloud TPU results in a . The layer behaves as expected if the TPUEstimator is configured to use CPU. Error raised by the TPU failure case:


### Source code / logs
Test code minimal example keras_td_test.py:


Error on TPU, and sanitized log file:


Working as expected on CPU:
"
441,19503,0,"TOCO Quantized InceptionV3 Error: ""tensorflow/contrib/lite/kernels/pooling.cc:116 input->params.scale != output->params.scale"". ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Minor custom code for quantization but using provided InceptionV3 checkpoints and models/research/slim scripts.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: ('v1.8.0-2169-gb84878e63e', '1.8.0')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.12.0
- **GCC/Compiler version (if compiling from source)**: gcc version 7.2.0
- **CUDA/cuDNN version**: 9.1/7.1.3
- **GPU model and memory**: NVIDIA P40 (24 GB)
- **Exact command to reproduce**: I link to my script on GitHub below.

### Describe the problem
I've been trying to produce a quantized InceptionV3 model since one is not provided (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md) but it currently fails in the  demo application due to a MaxPool that has different input/output quantization ranges.

In this TensorFlow Lite announcement video, quantized InceptionV3 is shown to be ~3x faster than floating point (https://youtu.be/FAMfy7izB6A?t=8m40s), so I thought it might work ""out of the box"".
Additionally, in this Google quantization paper the accuracy is shown to be fairly high (https://arxiv.org/abs/1712.05877), which is pretty motivating. 

### Source code / logs
I have a branch of  with very minor changes to support quantization: https://github.com/parvizp/models/tree/quantize
You can run the whole script to see the training, TOCO call and call to : https://github.com/parvizp/models/blob/279e458ac99da67e405ac74bc5e4583d5111c1bb/research/slim/scripts/quantize_inception_v3_on_imagenet.sh

The error seems to stem from this MaxPool in the center of:
<img width=""850"" alt=""inception_v3 quantized maxpool-issue"" src=""https://user-images.githubusercontent.com/926261/40436998-d7f6ff38-5e79-11e8-9b1e-6e65f2f20ec9.png"">
To reach the accuracy reported in (https://arxiv.org/abs/1712.05877) should we:
- Add support for TF-Lite MaxPool kernel so it can perform requantization as needed (i.e. for this case)?
- Make TOCO nudge the ranges so all things forking from the cancat and joining have the same range? Could also make the graph re-writer emulate this with shared ranges? 

Seems to preserve more precision and easier to do the first option. I'm happy to provide a patch if you can provide some insight into which direction to pursue."
1207,22740,0,"why does tf.norm return a complex tensor?. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS 10.14
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.11.0-rc2-4-gc19e29306c 1.11.0
- **Python version**:
Python 3.6.5 :: Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:


### Describe the problem
The norm is always positive, why does it return a tensor of type tf.complex64?
"
384,23058,0,"dll load failed in tensorflow install using pip on windows 10. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.11
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip and venv
- CUDA/cuDNN version: 8 | 5.1/6
- GPU model and memory: nvidia geforce gtx 960m
- Have I written custom code: N/A
- Bazel version: N/A
- Exact command to reproduce: N/A
- Mobile device: N/A




**Describe the problem**

I did look at these issues and applied the fixes but still not working.
https://github.com/tensorflow/tensorflow/issues/10033
https://github.com/tensorflow/tensorflow/issues/5949

Then i found this https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c
turns out there were some version issues like it needed cuda 8 instead of 9. so I fixed all those things and its still not working please help


**Any other info / logs**
"
1025,27438,0,"DLL Loading Failed & Using An AMD GPU. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro x64
- TensorFlow installed from (source or binary): Unknown
- TensorFlow version: Unknown
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: None
- GPU model and memory: 4GB ATI Radeon RX 580 (MSI)



**Describe the problem**

Unable to import a DLL & Getting Tensorflow to work on an AMD GPU

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf

**Any other info / logs**

"
851,28961,0,"tensorflow compile error. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>
**grpc** **sha256** checksum was""*"" but wanted""*""
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:branch:master  05/23/2019
- Python version:2.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):0.24.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10/7.5.1
- GPU model and memory:


8g
**Describe the problem**
Downloading  **grpc** from github ,the sha256 checksum is not suit for the .bzl files.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Almost every time I compile tensorflow ,this bug will turn out.
the last few times I was lucky,the  sha code was right after  tried so .many times.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
437,11500,0,"allocate_output can result in OOM error. TF-commit: 7f008453ca1c7b

### Describe the problem
When implementing custom operations, allocating output twice (as a bug) should give at least a warning. The following code



gives a nearly un-debuggable OOM error after many iterations during runtime. Is it possible to prevent   being called with the same  twice?"
1298,7259,0,"Configure script optimization flags don't work. The default optimization switches that are set when running the configure script do not change the compiler settings.  The values are put into the bazel.rc file.  They don't get into the gcc command line.  For example if I run ./configure with all of the default answers the optimization should be ""-march=native"".  I have tested this on multiple Ubuntu 16.04 systems with don't have CUDA.

If I then build the pip package with 'bazel build --c opt -s  ...' one example compile is this:

> /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.d '-frandom-seed=bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.o' -fPIC -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/farmhash_archive/src/farmhash.cc -o bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.o)

If I explicitly set the compile options with 'bazel build --c opt --copts=-march=native -s  ...' I get the correct results:

> /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections **'-march=native'** '-std=c++0x' -MD -MF bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.d '-frandom-seed=bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.o' -fPIC -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/farmhash_archive/src/farmhash.cc -o bazel-out/local-opt/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.pic.o)

The performance is slower for the first example.  Somehow the optimizations in bazel.rc are not getting through to the compiler command line."
238,25021,1,"[TFLite, Quantization, Performance] float32 nodes faster than uint8.. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0, b'v1.12.0-6341-g8a5d48a'
- Python version: 3.5
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
I converted my TF model to two TFLite models: float32 and uint8. Then I compared them and nodes DEPTHWISE_CONV2D and SOFTMAX with float32 were faster than with uint8.

**Describe the expected behavior**
I thought, that at least conv2d with uint8 will faster than with float32.

**Code to reproduce the issue**
My code:


**Other info / logs**
Benchmark for float32:


Benchmark for uint8:


"
1331,27634,0,"Missing documentation for variable ops.. <em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: v1.13
- Doc Link:


**Describe the documentation issue**
Hi, I am looking for documentation on the ReadVariableOp and VarHandleOp. Specifically as to what are the inputs that they take and what do they produce exactly. I dug around the source code for mentions of these ops and found some calls [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L196) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L864). While this hints at the signature of these ops, I would love to look at the implementation, which probably exists in [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L37) module, but I can not find it in this public repo's master branch. Could someone point me to this?

Alternately, is there any documentation on these ops written [this](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/fused-batch-norm) way?

Thanks!
**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
531,31413,0,"//tensorflow/contrib/metrics:metric_ops_test fails with Assertion error. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04 s390x
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
python tensorflow/contrib/metrics/python/ops/metric_ops_test.py




**Describe the expected behavior**
The test should pass on s390x.
"
1184,19198,0,"__hadd() is ambiguous when EIGEN_CUDA_ARCH >= 530. ### System information
- **Have I written custom code**: I change the CUDA capabilities to 6.1 and 7.0.
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from (source or binary)**: I'm compiling the source code.
- **TensorFlow version (use command below)**: branch , 8753e2ebde6c58b56675cc19ab7ff83072824a62
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: VS 2017(v141), but v140 for CUDA host compiler
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.1
- **GPU model and memory**: 1080 Ti, Titan V
- **Exact command to reproduce**: cmake-gui, enable GPU, and change CUDA host compiler to v140

### Describe the problem
[__hadd()](https://github.com/eigenteam/eigen-git-mirror/blob/2bdd9e80c43ee491fb0cb299940995e094b1647b/Eigen/src/Core/arch/CUDA/Half.h#L212) is ambiguous when [EIGEN_CUDA_ARCH >= 530](https://github.com/eigenteam/eigen-git-mirror/blob/2bdd9e80c43ee491fb0cb299940995e094b1647b/Eigen/src/Core/arch/CUDA/Half.h#L204).

The following is where the ambiguity comes from found in VS 2017:
![image](https://user-images.githubusercontent.com/6009211/39873593-f19954bc-549d-11e8-8bb6-3c02e33c44c5.png)

### Source code / logs
 compilation fails because of this problem:


I'm confused that nobody has every post such an issue.
Nobody has ever tried changing CUDA capabilities to >=5.3?
Or is there something wrong with my environment?

It seems that this is a pure Eigen issue...
"
476,21574,0,"Tensorflow Lite, python API does not work . ### System information
- **TensorFlow version:  1.9.0**
- **Python version:  3.5**

### Describe the problem
I am try run TFlite model file with Python API (like in example: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md), but I get an error: 
**ImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKaiiS2_PKfiPfi**

### Source code / logs
My code:


Log output:
"
1329,101,0,"Neural Translation Model example fails due to missing EN tokens . The S2S Neural Translation example (tensorflow/models/rnn/translate)  runs into this error:

> tensorflow.python.platform.default._gfile.FileError: [Errno 2] No such file or directory: 'data/giga-fren.release2.ids40000.en'

The issue seems to be in prepare_wmt_data of data_utils.py. Here, instead of creating a new file for the EN tokens, the FR tokens are overwritten by EN tokens:

> data_to_token_ids(train_path + "".en"", fr_train_ids_path, fr_vocab_path)

The fix seems to be as simple as changing that line to:

> data_to_token_ids(train_path + "".en"", en_train_ids_path, en_vocab_path)
"
967,8463,0,"customize parse_op.py and recompile. Hi, my problem is that I want to hack code in the parse_op.py file， especially I want to have an option of Dict or OrderedDict for the output of parse_single_example and many other parser that return Dict. Should I submit a PR or you guys can add that in the next version ?

By installing from source , I got following errors 

"
591,32434,0,"Int32 overflow in sparse_reshape on Windows. **System information**
- OS Platform and Distribution: Windows 10 64-bit
- TensorFlow installed from: Source
- TensorFlow version: 1.14.0
- Python version: 3.6.7

**Describe the current behavior**
When a large sparse tensor is reshaped using  on Windows it fails, due to the fact that it uses  to determine whether the number of elements in the old and new SparseTensor are the same. This can fail on Windows, due to the fact that numpy converts python ints to dtype np.int32 on Windows, which will overflow for large dimensions.

**Describe the expected behavior**
 should work similarly on both Windows and Unix-based systems.

**Code to reproduce the issue**


Output:


**Other info / logs**
The issue might be resolved, by forcing the shape inputted to  to be of type np.int64.
"
1099,31684,0,"TimeDistributed does not propagate mask and prevents inner Sequential from propagating mask. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14.0, 2.0.0b
- Python version: 3.6.8

**Code to reproduce the issue**
This fails due to failed assertion:



I would expect the mask to be propagated, as is normal in . The  seems to somehow prevent the inner  from doing so.

This also fails:


**Other info / logs**

This might be related to another issue I also submitted: https://github.com/tensorflow/tensorflow/issues/31638

"
1413,2041,0,"power activation function. Hi, I am a newbie with Tensorflow. I just found there is no power activation function existing in Tensorflow? How can I implement a new activation function in TF?
"
1032,13432,0,"An exception has occurred, use %tb to see the full traceback. and low accuracy. ![tf issue](https://user-images.githubusercontent.com/22562558/31056287-282fb44e-a69d-11e7-800b-af2a8ab41f12.png)

The accuracy I get for sklearn and tensorflow is 0.428571. And after that I get a line that says: An exception has occurred, use %tb to see the full traceback.

What should I do? What is the accuracy that I am supposed to get?
"
179,18776,1,"Low Accuracy with static image on TFLite demo model. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary(PIP install)
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: Na
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I've implemented the transfer learning example for mobilenet and deployed the Android app. The classification was happening perfectly with a good accuracy for the constant stream of images. Later I modified the app to pick up a file from storage or click an image, save to storage and then classify. When this is implemented, the accuracy has dropped acutely to the range 0f 0.01 - 0.3 for any of the Flower classes or the general mobilenet model with the 1500 classes. I've implemented the 224 x 224 version of the model. Below are the steps 

- Create a basic camera app
- Take a picture and save it to storage
- The uri of the image is saved and then a drawable is created from the URI.
- This drawable is then converted to a bitmap.
- The bitmap size is transformed to 224 x 224 to match the input of the Mobile Net model
- This transformed bitmap is sent for classification to the pre-implemented class from the sample.

Link to my question on stack overflow:
https://stackoverflow.com/questions/49954439/low-accuracy-with-static-image-on-tflite-demo-model

### Source code / logs
"
1245,3631,0,"Unable to build from source for TensorFlow r0.10 C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command. I have followed this tutorial (https://github.com/samjabrahams/tensorflow-on-raspberry-pi) trying to install the latest tensorflow with quantization enabled, on raspberry pi 3. So I followed every step in it except for the tensorflow version, instead of Tensorflow 0.9 I used the latest 0.10. 

But I ran into the following error



my gcc version is 4.9.2

I appreciate any help and suggestion!
"
697,21137,0,"KNN classifier.setClassifierDataset. hi

can you help me to save and retrieve the model I've created based on KNN Classifier?
I've seen that I need to use classifier.setClassifierDataset and classifier.getClassifierDataset, but I cannot make it workj

thanks"
860,18763,0,"Multiple Classes fails in Eager Mode (""tf.keras.Model""). ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No 
- **Bazel version**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Tried on MacOS using tensorflow as well as Linux Ubuntu 16.04 using tensorflow-gpu
- **TensorFlow installed from (source or binary)**:
Installed utilizing pip
- **TensorFlow version (use command below)**:
1.7
- **Python version**: 
3.6
- **Exact command to reproduce**:


### Describe the problem
Trying to utilize multiple classes fails in tensorflow eager mode utilizing ""tf.keras.Model"". If I change ""tf.keras.Model"" to ""tfe.Network"" it works - keep in mind I am utilizing tensorflow 1.7.  The error I get running the above code results in the error below:

### Source code / logs
callcall
"
390,19458,0,"I am trying to deploy my tensorflow model(CNN) to android, when I am trying to use my .pb file( generated after training the model )in android studio it shows! . 05-22 16:23:02.608 27021-27021/com.example.bibhu.smscnn E/AndroidRuntime: FATAL EXCEPTION: main
Process: com.example.bibhu.smscnn, PID: 27021
java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.bibhu.smscnn/com.example.bibhu.smscnn.MainActivity}: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/graph11.pb'
at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2820)
at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895)
at android.app.ActivityThread.-wrap11(Unknown Source:0)
at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596)
at android.os.Handler.dispatchMessage(Handler.java:105)
at android.os.Looper.loop(Looper.java:164)
at android.app.ActivityThread.main(ActivityThread.java:6565)
at java.lang.reflect.Method.invoke(Native Method)
at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)
Caused by: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/graph11.pb'
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.(TensorFlowInferenceInterface.java:113)
at com.example.bibhu.smscnn.TensorFlowClassifier.create(TensorFlowClassifier.java:70)
at com.example.bibhu.smscnn.TensorflowIntegrationExample.loadModel(TensorflowIntegrationExample.java:35)
at com.example.bibhu.smscnn.TensorflowIntegrationExample.(TensorflowIntegrationExample.java:27)
at com.example.bibhu.smscnn.MainActivity.onCreate(MainActivity.java:21)
at android.app.Activity.performCreate(Activity.java:6975)
at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)
at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2773)
at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895) 
at android.app.ActivityThread.-wrap11(Unknown Source:0) 
at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596) 
at android.os.Handler.dispatchMessage(Handler.java:105) 
at android.os.Looper.loop(Looper.java:164) 
at android.app.ActivityThread.main(ActivityThread.java:6565) 
at java.lang.reflect.Method.invoke(Native Method) 
at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240) 
at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767) 
Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Input 1 of node embedding/embedding_lookup was passed float from inputTensor:0 incompatible with expected int32.
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:561)
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.(TensorFlowInferenceInterface.java:105)
at com.example.bibhu.smscnn.TensorFlowClassifier.create(TensorFlowClassifier.java:70) 
at com.example.bibhu.smscnn.TensorflowIntegrationExample.loadModel(TensorflowIntegrationExample.java:35) 
at com.example.bibhu.smscnn.TensorflowIntegrationExample.(TensorflowIntegrationExample.java:27) 
at com.example.bibhu.smscnn.MainActivity.onCreate(MainActivity.java:21) 
at android.app.Activity.performCreate(Activity.java:6975) 
at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214) 
at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2773) 
at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895) 
at android.app.ActivityThread.-wrap11(Unknown Source:0) 
at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596) 
at android.os.Handler.dispatchMessage(Handler.java:105) 
at android.os.Looper.loop(Looper.java:164) 
at android.app.ActivityThread.main(ActivityThread.java:6565) 
at java.lang.reflect.Method.invoke(Native Method) 
at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240) 
at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767) "
1324,25839,0,"Tensorflow save & restore without using filesystem. **System information**
- TensorFlow version (you are using): Up to date
- Are you willing to contribute it (Yes/No): Python is not my daily driver language



**Describe the feature and the current behavior/state.**
I would like to retrieve(instead of save) training information and restore it to a tensorflow session when necessary (without using file system). I want to bypass filesystem.

I have looked the followings and searched but was not successful.

https://www.tensorflow.org/guide/saved_model
https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model

Why does this feature not appear already present?




**Will this change the current api? How?**

**Who will benefit with this feature?**
Distributed system, would be an easier way to persist and handle training data

**Any Other info.**
(as a side note, not important, this Tensorflow approach using filesystem might be plausible for Python. Nonetheless, I am using Scala Tensorflow and I would like to bypass the filesystem entirely and use Cassandra DB for a distributed environment. If I know how to do it in Python. It might be a similar approach in the Scala version, if not using the main APIs, by using byte codes, etc..)"
1332,2122,0,"MacOS successful installed, fail to import. ### Environment info

Operating System:
OSX EI Capitan, 10.11.4
Installed version of CUDA and cuDNN: 
(please attach the output of ):
None

If installed from binary pip package, provide:
1. Which pip package you installed.
   tensorflow-0.8.0-py2-none-any.whl for mac
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
   ➜  examples python -c ""import tensorflow; print(tensorflow.**version**)"".
   File ""<string>"", line 1
     import tensorflow; print(tensorflow.**version**).
                                                     ^
   SyntaxError: invalid syntax
### What have you tried?
1. pip uninstall, reinstall , however, still failed.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

> > > import tensorflow as tf
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/usr/local/lib/python2.7/site-packages/tensorflow/**init**.py"", line 23, in <module>
> > >     from tensorflow.python import *
> > >   File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/**init**.py"", line 49, in <module>
> > >     from tensorflow.core.framework.graph_pb2 import *
> > >   File ""/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 16, in <module>
> > >     from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
> > >   File ""/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py"", line 16, in <module>
> > >     from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
> > >   File ""/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py"", line 16, in <module>
> > >     from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
> > >   File ""/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"", line 22, in <module>
> > >     serialized_pb=_b('\n,tensorflow/core/framework/tensor_shape.proto\x12\ntensorflow\""z\n\x10TensorShapeProto\x12-\n\x03\x64im\x18\x02 \x03(\x0b\x32 .tensorflow.TensorShapeProto.Dim\x12\x14\n\x0cunknown_rank\x18\x03 \x01(\x08\x1a!\n\x03\x44im\x12\x0c\n\x04size\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\tB/\n\x18org.tensorflow.frameworkB\x11TensorShapeProtosP\x01\x62\x06proto3')
> > > TypeError: __init__() got an unexpected keyword argument 'syntax'
"
332,5951,0,"possible bug for saver.restore not compact with previous version of tensorflow in tensorflow r-0.12. NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Ubuntu 16.04.1 LTS

Installed version of CUDA and cuDNN: 
(please attach the output of ):


If installed from binary pip package, provide:

1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc0-cp27-none-linux_x86_64.whl
2. The output from .
0.12.0-rc0

If installed from source, provide 

1. The commit hash ()
2. The output of 

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I have a model which saved by tensorflow-0.11, for example:

after update to tensorflow 0.12 version, and when I load the model by:


the error rises, log output as follows 

### What other attempted solutions have you tried?
I have no problem to restore it in tensorflow 0.11

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
499,46673,0,"Tensorflow does not work with RTX 3070 on Windows. **System information**
- Code attached below
- OS: Windows 10
- TensorFlow installed from binary ()
- TensorFlow version: tried latest stable v2.4.0-49-g85c8b2a817f 2.4.1
- Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32
- CUDA/cuDNN version: cuda_11.2.0_460.89_win10\cudnn-11.1-v8.0.5.39
- GPU drivers: 460.89
- GPU model and memory: seems to be recognized correctly by TF- GeForce RTX 3070 computeCapability: 8.6 coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s



**Describe the current behavior**
Getting error:

[tf_2.4.1_issue_on_3070.txt](https://github.com/tensorflow/tensorflow/files/5869537/tf_2.4.1_issue_on_3070.txt)


Tried also with latest  ending up with error:

[tf_nightly_issue_on_3070.txt](https://github.com/tensorflow/tensorflow/files/5869438/tf_nightly_issue_on_3070.txt)


**Describe the expected behavior**
The script was working on my old gtx 980 and CUDA 10.

**Standalone code to reproduce the issue**


DATA FILE SAMPLE: [input_data.zip](https://github.com/tensorflow/tensorflow/files/5869582/input_data.zip)
 
**Other info / logs**

Providing also path to CUDA 11.0 installation because without it getting errors like:



Full win sys PATH:





I was trying different combinations of cuda/cudnn/tensorflow just for the sake of it but actually only  comes with win nvidia GPU divers version high enough to support RTX 30xx series.
Still - there is no  build designated particularly for  yet... Maybe this is an issue...

Any idea how to make it working all together?"
929,11405,0,"Finding position of detected object. Is it possible to find the position of detected object with accuracy in given image using tensor flow?﻿
I am using label_image.py to test my trained data how can I get the position of detected object as I am getting the accuracy of detected object?

Reference:
https://stackoverflow.com/questions/44942587/save-image-of-detected-object-from-image-using-tensor-flow?noredirect=1#comment76982154_44942587"
845,15043,0,"a. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
1344,31652,0,"How to preprocess a dataset with multiple input variables for integration with Tensorflow for binary classification?. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: Tensorflow 2.0
- Python version: 3.7.1
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory:

**Describe the problem**

I am trying to setup a minimum viable neural network to train for a binary classification task. I am importing a CSV file with seven total columns, the first column is the true label, 1 or -1. I have divided the dataset into a training set and test set. Additionally, I have been able to import the dataset into python, and normalize the six variables to numbers between zero and one.

I have been spending the past few hours developing ad hoc, layers for a neural network. I have been able to get the network to feedforward, and to make binary predictions. However, I am totally at a loss for how to train the model because I cannot figure out how to update the weights. Generally, I am familiar with back-propagation, SGD, derivates, ADAM, and the perceptron learning rule. The problem is I have been unable to figure out how to implement any of these methods, so my program learns. In short, my goal is to update the weights of the neural network if the predict label and true label are not the same, so the network can learn.

I have tried using Keras because it is a high level API. [Keras](https://www.tensorflow.org/tutorials/keras/basic_classification#build_the_model ) While keras is good for getting started, it is very difficult to use Keras or Tensorflow on the datasets I generated myself. I am having trouble reformatting a neural network for the MNIST dataset to the needs of my present dataset [See Computer Vision]((https://medium.com/@brian.s.haney44/computer-vision-b39256f13fa4)).

In short, I am simply trying to find a way to develop and train a neural network on a dataset I developed. I have gone through the preprocessing and labeling, but I haven't been able load the dataset into python with Tensorflow. Instead, I am only able to load it into python through the standard library through the CSV module.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. I gathered the data and went through pre-processing procedures.
2. I found a way to load the dataset into Python.
3. I found a way to normalize the input layer.
4. I was able to complete the feedforward aspect of the network.

The problem I ran into was updating the weight when the network makes a decision contradicted by the true label.

**Any other info / logs**

This is the code I have been working from. A link to a dummy dataset can be found on my  [Github](https://github.com/Bhaney44/Deep-Neural-Network)


from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
`
However, this code is difficult to adapt to a different dataset. Indeed, I am unsure of the purpose of the  (x_train, y_train), (x_test, y_test) syntax and my dataset is size very differently than MNIST. Any help, suggestions, or criticisms would be greatly appreciated. 

With thanks,

Brian

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1212,17426,0,"tf-slim resnet-50 pretrained model get wrong results when inference. I have asked this question on stackoverflow, but no one answer. Does any can help me.
The question link to stackoverflow is [here](https://stackoverflow.com/questions/49094123/tf-slim-resnet-pretrained-model-cant-get-correct-results).
The following is the code I used to do inference.The image preprocess method following this issue [ResNet pre-processing: VGG or Inception?](https://github.com/tensorflow/models/issues/2217)

The following is result:

"
383,1311,0,"dynamic_rnn incompatible with BasicRNNCell, GRUCell, and BasicLSTMCell. ### Description

I've been playing around with  and have thus far only had success with it when using  instances. When using any other cell, a  is raised. Looking at the traceback, the culprit appears to be the calls to  at each time step, which fail when  attempts to get the shape of its input slice. This, in turn, fails because the input slices are produced from a  and have shape .
### Environment info

Operating System: Ubuntu 14.04 LTS
Installed from source at:  b88971051fbc49fa1e0b91ec1b0b60defa11697e
### Steps to reproduce

The following will attempt to call  with three different cells, catch the  that is raised, then print the exception + traceback to stdout.


### Output of above snippet


"
1428,26868,0,"What is the corresponding API for tf.nn.rnn_cell._linear in tf 2.0?. Just as the title suggests, I want to adopt an older version code to tf2.0, thanks"
1080,4013,0,"different types of GPUs. I have 4 GPUS, which are
gpu0, GTX 1080
gpu1, old TITAN X
gpu2, old TITAN X
gpu3, GTX 1080
when I run a CNN, it shows the information below, what does  mean?
Does it mean that I cannot use multiple GPUs since they are not identical(I noticed that I only have one gpu running during the training)?
What is the consequence of this info?


"
44,31826,1," TensorFlow Lite GPU on Android slower than CPU. **System information**
Android 9.0, XiaoMi9, snapdragon855
- TensorFlow installed from (source or binary):
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo
- TensorFlow version (or github SHA if from source):
dependencies {
    ...
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'
}

I run a app from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo, and it worked well, the run time of  default model(mobilenet) on GPU is half of CPU, which is normal I think. And I replace the model with a special model which structure like YOLOv2-Tiny, trained by keras and convert to tflite(Implemented by traditional conv2D and pooling), I found the speed on GPU is slower than CPU float(CPU is less than 500ms but GPU is more than 600ms),  is it normal?
is there anyone help me?
"
1468,18604,0,"No module named tensorflow.tools. ### System information
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version**: 1.3.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: Used binary
- **GCC/Compiler version (if compiling from source)**: Used binary
- **CUDA/cuDNN version**: No GPU 
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: from tensorflow import tools


### Describe the problem
I am trying to run the tensorflow to onnx converter (https://github.com/onnx/tensorflow-onnx) and for that reason I had to use some outdated versions of tf and onnx. I finally managed to get everything installed correctly, when I try to run any example I get the message that there is no module named tensorflow 

I checked this [previous issue from last year](https://github.com/tensorflow/tensorflow/issues/9778) and from what I understood, tf tools weren't supported on Windows. Is that still the case? Is there any workaround? 
"
264,27623,1,"CUDA 10 / CUDNN 7.5 performance loss (Titan V). **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 42.3
- GPU model and memory: Titan V
- GCC/Compiler version (if compiling from source): 4.8
- Nvidia Driver Version: 418.56

**Problem:** I experience consistent performance losses in TF using CUDA 10.0/CUDNN 7.5. 
Here is a minimal example:



**Results Titan V:**
Tensorflow 1.13.1 built from source, bazel 0.24.0, CUDA 10.0, CUDNN 7.5.0:

Tensorflow 1.9 built from source (same configuration besides versions), bazel 0.11.1, CUDA 9.1, CUDNN 7.1.4:

Pip wheel tensorflow_gpu-1.13.1-cp27-cp27mu-manylinux1_x86_64.whl, CUDA 10.0, CUDNN 7.5.0:

Conda (Python 3.6) linux-64/tensorflow-gpu-1.13.1-h0d30ee6_0.tar.bz2, CUDA 9.2, CUDNN 7.3.1:


Have you experienced similar performance losses? Do you know any solutions? If necessary, I can run tests on other GPU models as well.






"
1473,27897,0,"Spurious deprecation warnings. **System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): tf-nightly-2.0-preview-20190416-py2.7
- TensorFlow version (use command below): 2.0.0-dev20190416
- Python version: 2.7.16rc1
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

When using  or , a
deprecation warning is emitted instructing the user to use the same
symbol that they’re already using:

>   - “The name keras.callbacks.TensorBoard is deprecated. Please use keras.callbacks.TensorBoard instead.”
>   - “The name keras.models.Sequential is deprecated. Please use keras.models.Sequential instead.”

**Describe the expected behavior**

No deprecation warning should be emitted.

**Code to reproduce the issue**


"
782,33533,0,"Bcapak. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
109,28901,1,"[TF 2.0] Model not converging when trained with custom training loop. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.02 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary from Pypi
- TensorFlow version (use command below): 2.0 alpha0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX1070 8GB

**Describe the current behavior**

I have created a simple regression model and trained it using keras and also using a custom training loop. What I noticed is that the model only converges when using the keras fit function. If I use the custom training loop, which manually computes gradients and applies them via an optimizer, the model does not converge. The same problem occurred not only on this very simple regression task but also on a model trained on the UTK Faces data set for age regression.

**Describe the expected behavior**

I would expect both, the model trained with keras fit and the model trained with a custom training loop, to converge.

**Code to reproduce the issue**


"
346,9747,0,"'Tensor' object has no attribute 'initializer' after import from meta graph. ------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution**: Darwin Austins-MBP 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64
Mac OS X 10.12.4
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:v1.1.0-rc0-61-g1ec6ed5 1.1.0
- **Bazel version (if compiling from source)**:0.4.5
- **CUDA/cuDNN version**:None
- **GPU model and memory**:None
- **Exact command to reproduce**: Ref to Codes

### tensorflow import 
    tf.VERSION = 1.1.0
    tf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5
    tf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5
    Sanity check: array([1], dtype=int32)



### Describe the problem
After export and import a meta graph with uninitialized local variables,
You can not inittialize them with sess.run(tf. local_variables_initializer()), cause
TF do not register variable's proto function with key 'LOCAL_VARIABLES' and when 
export meta graph to protobuf, source code can not find to_proto function from repository.


### Source code / logs







As it show above, in origin graph local_variable collection is a list of **tf.Variable**
but in the new graph, is a list of **tf.Tensor**

### Work around
Add following registration in your model core
OR Ref to this [PR](https://github.com/tensorflow/tensorflow/pull/9674)

"
1424,23474,0,"how can i solve this issue on importing tensorflow in linux?. >

****System information****
- OS Platform and Distribution: (Linux Ubuntu 18.04):
- M device (e.HP-proBook) .
- Tensor-flow installed by running on terminal 
pip3 install --upgrade tensorflow-gpu
- TensorFlow version:
- Python version:3.6.6
- Installed using pip.
 -I dont have a CUDA/cuDNN version but  Intelhaswall.
- GPU model :Intel® Haswell Mobile
-memory:7.7 Gi
**the problem is :** 
I cant install and using tensor flow or numpy for object detection algorithms.
I don't know how can i install virtualenv.
I run on terminal
$sudo apt install python-pip python3-pip      
$pip3 install --upgrade tensorflow 
the tensor flow already installed 
but ,,when i run 
$python3
and import tensorflow the error was 
![screenshot from 2018-11-03 09-05-52](https://user-images.githubusercontent.com/44716923/47949282-f708f980-df48-11e8-9032-98813b6a8767.png)



for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

______________________________________________________________________

**Any other info / logs**
i dont want to use a black-screen !!! to type a command i want to clone a codes and running it from a GitHub,starting coding .. somone advice me to setup 
"
541,32627,0," Support for 32 bits architecture #32315 . Several users are still using Python32 bits and they cannot install TensorFlow. For them, pip install TensorFlow fails as no wheel matches the tags expected by their environment (to debug, pip debug --verbose shows only tags that don't math the filenames of our wheels).

There is some requests to support 32 bits, see for example #31431

This is not going to be easy as we need to also compile the C++ codebase in 32 bits mode and that would cause issues with code written assuming types have a certain bit width.

There is no change in the user visible API, just a new set of wheels to support more users.

Opening this to reference in all similar issues.

After closing my thread, i just reopen it. Google employees trying to tell me this is not the roadmap for the tool. The day i am going to care about a Google employee opinion is the day hell froze. 

If you close this issue I will keep posting daily, until issue is fixed. 

"
665,12011,0,"Quantized graph not running with commit:bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b. OS: Ubuntu 16.04 64bits
 Android Version: 7.1 (Nougat)
 NDK Version: android-ndk-r12b

commit bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b
Author: Alan Yee <alyee@ucsd.edu>
Date:   Mon Jul 24 22:46:38 2017 -0700



LOG:



Earlier this error was not getting reported.

Thanks"
287,7116,0,"Packet16q16i does not name a type. When I compile from source using the flag of AVX512 on Xeon Phi with gcc, I come across a problem as below:
ERROR: tensorflow/tensorflow/core/kernels/BUILD:895:1: C++ compilation of rule '//tensorflow/core/kernels:gather_functor' failed: gcc failed: error executing command /opt/rh/devtoolset-4/root/usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/opt/rh/devtoolset-4/root/usr/bin -B/usr/bin -Wunused-but-set-parameter ... (remaining 56 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:35:0,
                 from ./tensorflow/core/framework/numeric_types.h:25,
                 from ./tensorflow/core/framework/type_traits.h:22,
                 from ./tensorflow/core/kernels/gather_functor.h:22,
                 from tensorflow/core/kernels/gather_functor.cc:50:
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX512.h:84:11: error: 'Packet16q16i' does not name a type
   typedef Packet16q16i half;
           ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX512.h:135:11: error: 'Packet16q16i' does not name a type
   typedef Packet16q16i half;

Is it caused by some code missing? I add some code in the file PacketMathAVX512.h:
typedef struct Packet16q16i {
  __m512i val;
  operator __m512i() const { return val; }
  Packet16q16i();
  Packet16q16i(__m512i val) : val(val) {}
} Packet16q16i;

However, it is still failed with the following issue:

ERROR: tensorflow/tensorflow/core/kernels/BUILD:346:1: C++ compilation of rule '//tensorflow/core/kernels:split_lib' failed: gcc failed: error executing command 

external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:478:16: error: 'alignment' is not a member of 'Eigen::internal::unpacket_traits<Eigen::internal::Packet64q8u>'
   if(Alignment >= unpacket_traits<Packet>::alignment)
                ^
I am not sure if I did something wrong. Can someone help?

I use the command provided in [https://github.com/tensorflow/tensorflow/issues/4775](url)
bazel build --ignore_unsupported_sandboxing -c opt //tensorflow/tools/pip_package:build_pip_package  --copt ""-mavx512f"" --copt ""-mavx512cd"" --copt ""-mavx512er"" --copt ""-mavx512pf"" --copt ""-mavx2"" --copt ""-fopt-info-vec-all"" --copt ""-DEIGEN_ENABLE_AVX512"" --copt ""-DEIGEN_ENABLE_AVX2""  --verbose_failures   -j 64

The gcc version is 5.3.

BTW, I also tried to use Intel compiler to compile the code, but failed. I saw two issues are discussed, but it seems that there is no solution. "
468,1408,0,"Generalized matrix multiplication with semiring?. Would it be possible to support (generalized) matrix multiplication with non-standard semirings? 

In matrix multiplication, we have , where 

Using a different semiring practically means redefining the  and  operations. 

For example, the log semiring assumes that all numbers in the matrices are log numbers and redefines  as  (aka [logaddexp](http://docs.scipy.org/doc/numpy/reference/generated/numpy.logaddexp.html#numpy.logaddexp) or [logsumexp](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.misc.logsumexp.html)) and  as . This is useful for computing the log denominator in log-linear models, e.g., .

The [max-plus semiring](https://en.wikipedia.org/wiki/Max-plus_algebra) (aka Viterbi or tropical semiring) redefines  as  and  as . This is useful for finding the best path (assuming all matrix entries are log numbers).
"
251,30169,1,"TFLite GPU works slower than CPU. 
I run tflite in Qualcomm 660, for a test，there is just a con2d. But when i run it,
cpu costs 30ms  while  cpu costs 130ms.
I don't know why"
310,14875,0,"tf.data cannot be loaded with r1.4. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8 (jessie)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: b'v1.4.0-14-gb5df90f' 1.4.1
- **Python version**: Python 3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42)
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: gcc (Debian 4.9.2-10) 4.9.2
- **CUDA/cuDNN version**: CUDA 8, cnDNN 5.1
- **GPU model and memory**: TITAN X (Pascal), 12G
- **Exact command to reproduce**: 

### Describe the problem
After checking out r1.4 and compiling TF, tf.data cannot be loaded. Training works fine. But 'data' is listed when executing .

### Source code / logs
(tensorflow140) [13:08 user@server ~] > 
Python 3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42)
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
\>\>\> 
\>\>\> 
'1.4.1'
\>\>\> 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow.data'
\>\>\> 
['AUTO_REUSE', 'AggregationMethod', 'Assert', 'AttrValue', 'COMPILER_VERSION', 'ConditionalAccumulator', 'ConditionalAccumulatorBase', 'ConfigProto', 'DType', 'DeviceSpec', 'Dimension', 'Event', 'FIFOQueue', 'FixedLenFeature', 'FixedLenSequenceFeature', 'FixedLengthRecordReader', 'GIT_VERSION', 'GPUOptions', 'GRAPH_DEF_VERSION', 'GRAPH_DEF_VERSION_MIN_CONSUMER', 'GRAPH_DEF_VERSION_MIN_PRODUCER', 'Graph', 'GraphDef', 'GraphKeys', 'GraphOptions', 'HistogramProto', 'IdentityReader', 'IndexedSlices', 'InteractiveSession', 'LMDBReader', 'LogMessage', 'MetaGraphDef', 'NameAttrList', 'NoGradient', 'NodeDef', 'NotDifferentiable', 'OpError', 'Operation', 'OptimizerOptions', 'PaddingFIFOQueue', 'Print', 'PriorityQueue', 'QUANTIZED_DTYPES', 'QueueBase', 'RandomShuffleQueue', 'ReaderBase', 'RegisterGradient', 'RunMetadata', 'RunOptions', 'Session', 'SessionLog', 'SparseConditionalAccumulator', 'SparseFeature', 'SparseTensor', 'SparseTensorValue', 'Summary', 'SummaryMetadata', 'TFRecordReader', 'Tensor', 'TensorArray', 'TensorInfo', 'TensorShape', 'TextLineReader', 'VERSION', 'VarLenFeature', 'Variable', 'VariableScope', 'WholeFileReader', '\_\_builtins\_\_', '\_\_cached\_\_', '\_\_compiler_version\_\_', '\_\_doc\_\_', '\_\_file\_\_', '\_\_git_version\_\_', '\_\_loader\_\_', '\_\_name\_\_', '\_\_package\_\_', '\_\_path\_\_', '\_\_spec\_\_', '\_\_version\_\_', 'abs', 'accumulate_n', 'acos', 'acosh', 'add', 'add_check_numerics_ops', 'add_n', 'add_to_collection', 'all_variables', 'angle', 'app', 'arg_max', 'arg_min', 'argmax', 'argmin', 'as_dtype', 'as_string', 'asin', 'asinh', 'assert_equal', 'assert_greater', 'assert_greater_equal', 'assert_integer', 'assert_less', 'assert_less_equal', 'assert_negative', 'assert_non_negative', 'assert_non_positive', 'assert_none_equal', 'assert_positive', 'assert_proper_iterable', 'assert_rank', 'assert_rank_at_least', 'assert_rank_in', 'assert_same_float_dtype', 'assert_scalar', 'assert_type', 'assert_variables_initialized', 'assign', 'assign_add', 'assign_sub', 'atan', 'atan2', 'atanh', 'batch_to_space', 'batch_to_space_nd', 'betainc', 'bfloat16', 'bincount', 'bitcast', 'bitwise', 'bool', 'boolean_mask', 'broadcast_dynamic_shape', 'broadcast_static_shape', 'case', 'cast', 'ceil', 'check_numerics', 'cholesky', 'cholesky_solve', 'clip_by_average_norm', 'clip_by_global_norm', 'clip_by_norm', 'clip_by_value', 'colocate_with', 'compat', 'complex', 'complex128', 'complex64', 'concat', 'cond', 'confusion_matrix', 'conj', 'constant', 'constant_initializer', 'container', 'contrib', 'control_dependencies', 'convert_to_tensor', 'convert_to_tensor_or_indexed_slices', 'convert_to_tensor_or_sparse_tensor', 'cos', 'cosh', 'count_nonzero', 'count_up_to', 'create_partitioned_variables', 'cross', 'cumprod', 'cumsum', '**data**', 'decode_base64', 'decode_csv', 'decode_json_example', 'decode_raw', 'delete_session_tensor', 'depth_to_space', 'dequantize', 'deserialize_many_sparse', 'device', 'diag', 'diag_part', 'digamma', 'distributions', 'div', 'divide', 'double', 'dynamic_partition', 'dynamic_stitch', 'edit_distance', 'einsum', 'encode_base64', 'equal', 'erf', 'erfc', 'errors', 'estimator', 'exp', 'expand_dims', 'expm1', 'extract_image_patches', 'eye', 'fake_quant_with_min_max_args', 'fake_quant_with_min_max_args_gradient', 'fake_quant_with_min_max_vars', 'fake_quant_with_min_max_vars_gradient', 'fake_quant_with_min_max_vars_per_channel', 'fake_quant_with_min_max_vars_per_channel_gradient', 'feature_column', 'fft', 'fft2d', 'fft3d', 'fill', 'fixed_size_partitioner', 'flags', 'float16', 'float32', 'float64', 'floor', 'floor_div', 'floordiv', 'floormod', 'foldl', 'foldr', 'gather', 'gather_nd', 'get_collection', 'get_collection_ref', 'get_default_graph', 'get_default_session', 'get_local_variable', 'get_seed', 'get_session_handle', 'get_session_tensor', 'get_variable', 'get_variable_scope', 'gfile', 'global_norm', 'global_variables', 'global_variables_initializer', 'glorot_normal_initializer', 'glorot_uniform_initializer', 'gradients', 'graph_util', 'greater', 'greater_equal', 'group', 'half', 'hessians', 'histogram_fixed_width', 'identity', 'identity_n', 'ifft', 'ifft2d', 'ifft3d', 'igamma', 'igammac', 'imag', 'image', 'import_graph_def', 'initialize_all_tables', 'initialize_all_variables', 'initialize_local_variables', 'initialize_variables', 'initializers', 'int16', 'int32', 'int64', 'int8', 'invert_permutation', 'is_finite', 'is_inf', 'is_nan', 'is_non_decreasing', 'is_numeric_tensor', 'is_strictly_increasing', 'is_variable_initialized', 'keras', 'layers', 'lbeta', 'less', 'less_equal', 'lgamma', 'lin_space', 'linalg', 'linspace', 'load_file_system_library', 'load_op_library', 'local_variables', 'local_variables_initializer', 'log', 'log1p', 'log_sigmoid', 'logging', 'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'losses', 'make_ndarray', 'make_template', 'make_tensor_proto', 'map_fn', 'matching_files', 'matmul', 'matrix_band_part', 'matrix_determinant', 'matrix_diag', 'matrix_diag_part', 'matrix_inverse', 'matrix_set_diag', 'matrix_solve', 'matrix_solve_ls', 'matrix_transpose', 'matrix_triangular_solve', 'maximum', 'meshgrid', 'metrics', 'min_max_variable_partitioner', 'minimum', 'mod', 'model_variables', 'moving_average_variables', 'multinomial', 'multiply', 'name_scope', 'negative', 'newaxis', 'nn', 'no_op', 'no_regularizer', 'norm', 'not_equal', 'one_hot', 'ones', 'ones_initializer', 'ones_like', 'op_scope', 'orthogonal_initializer', 'pad', 'parallel_stack', 'parse_example', 'parse_single_example', 'parse_single_sequence_example', 'parse_tensor', 'placeholder', 'placeholder_with_default', 'polygamma', 'pow', 'profiler', 'py_func', 'python_io', 'pywrap_tensorflow', 'qint16', 'qint32', 'qint8', 'qr', 'quantize_v2', 'quantized_concat', 'quint16', 'quint8', 'random_crop', 'random_gamma', 'random_normal', 'random_normal_initializer', 'random_poisson', 'random_shuffle', 'random_uniform', 'random_uniform_initializer', 'range', 'rank', 'read_file', 'real', 'realdiv', 'reciprocal', 'reduce_all', 'reduce_any', 'reduce_join', 'reduce_logsumexp', 'reduce_max', 'reduce_mean', 'reduce_min', 'reduce_prod', 'reduce_sum', 'register_tensor_conversion_function', 'report_uninitialized_variables', 'required_space_to_batch_paddings', 'reset_default_graph', 'reshape', 'resource', 'resource_loader', 'reverse', 'reverse_sequence', 'reverse_v2', 'rint', 'round', 'rsqrt', 'saturate_cast', 'saved_model', 'scalar_mul', 'scan', 'scatter_add', 'scatter_div', 'scatter_mul', 'scatter_nd', 'scatter_nd_add', 'scatter_nd_sub', 'scatter_nd_update', 'scatter_sub', 'scatter_update', 'segment_max', 'segment_mean', 'segment_min', 'segment_prod', 'segment_sum', 'self_adjoint_eig', 'self_adjoint_eigvals', 'sequence_mask', 'serialize_many_sparse', 'serialize_sparse', 'serialize_tensor', 'set_random_seed', 'setdiff1d', 'sets', 'shape', 'shape_n', 'sigmoid', 'sign', 'sin', 'sinh', 'size', 'slice', 'space_to_batch', 'space_to_batch_nd', 'space_to_depth', 'sparse_add', 'sparse_concat', 'sparse_fill_empty_rows', 'sparse_mask', 'sparse_matmul', 'sparse_maximum', 'sparse_merge', 'sparse_minimum', 'sparse_placeholder', 'sparse_reduce_max', 'sparse_reduce_max_sparse', 'sparse_reduce_sum', 'sparse_reduce_sum_sparse', 'sparse_reorder', 'sparse_reset_shape', 'sparse_reshape', 'sparse_retain', 'sparse_segment_mean', 'sparse_segment_sqrt_n', 'sparse_segment_sum', 'sparse_slice', 'sparse_softmax', 'sparse_split', 'sparse_tensor_dense_matmul', 'sparse_tensor_to_dense', 'sparse_to_dense', 'sparse_to_indicator', 'sparse_transpose', 'spectral', 'split', 'sqrt', 'square', 'squared_difference', 'squeeze', 'stack', 'stop_gradient', 'strided_slice', 'string', 'string_join', 'string_split', 'string_to_hash_bucket', 'string_to_hash_bucket_fast', 'string_to_hash_bucket_strong', 'string_to_number', 'substr', 'subtract', 'summary', 'svd', 'sysconfig', 'tables_initializer', 'tan', 'tanh', 'tensordot', 'test', 'tile', 'to_bfloat16', 'to_double', 'to_float', 'to_int32', 'to_int64', 'trace', 'train', 'trainable_variables', 'transpose', 'truediv', 'truncated_normal', 'truncated_normal_initializer', 'truncatediv', 'truncatemod', 'tuple', 'uint16', 'uint8', 'uniform_unit_scaling_initializer', 'unique', 'unique_with_counts', 'unsorted_segment_max', 'unsorted_segment_sum', 'unstack', 'user_ops', 'variable_axis_size_partitioner', 'variable_op_scope', 'variable_scope', 'variables_initializer', 'variance_scaling_initializer', 'variant', 'verify_tensor_all_finite', 'where', 'while_loop', 'write_file', 'zeros', 'zeros_initializer', 'zeros_like', 'zeta']"
1313,19535,0,"TensorRT: Invalid graph after calibration -> inference graph transformation. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source.
- **TensorFlow version (use command below)**:
1.8
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.13.0
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)
- **CUDA/cuDNN version**:
9.0.176/7.0.5.15
- **GPU model and memory**:
1080 Ti
- **Exact command to reproduce**:

### Describe the problem

It seems that  may incorrectly redirect edges when transforming an INT8 calibration node into an engine node. The current implementation, for each output node, searches the first edge from this output node [1] and redirects only this edge to point from the newly created engine node [2]. If the output node has multiple outgoing edges, then only the first one is redirected, and the remaining edges still use the output node as the source. As a result, after the output node is deleted, the graph becomes invalid and the graphdef generated by  cannot be loaded (e.g., using ) due to errors like this: . (In this case,  had two edges in the original graph: to some convolution node and to ; the first edge was updated, the second one was not and caused the error.)

[1] https://github.com/tensorflow/tensorflow/blob/717aa746e7e915cba9ce36df424d05642fbe8cd7/tensorflow/contrib/tensorrt/convert/convert_nodes.cc#L2191
[2] https://github.com/tensorflow/tensorflow/blob/717aa746e7e915cba9ce36df424d05642fbe8cd7/tensorflow/contrib/tensorrt/convert/convert_nodes.cc#L2258"
1203,28442,0,"expected conv2d_input to have 4 dimensions. Hello

I'm on tensorflow and i'm trying to Machine Learning but i got this error here:
ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (24946, 50, 50)

"
200,28031,1,"java api runs much slower than Python API. i predict image with python api 
only need 15ms,
but with the same model,java api need 500+ms"
1362,2634,0,"When dose reusebale variable sync between device?. I want to validate model under training after certain iterations on train set.
The switch between train and validate is controlled by global set.
Training is on GPU:1, while validation is on CPU:0.
If share the variable between GPU and CPU using  tf.get_variable_scope().reuse_variables()
when dose the two copy sync?
Every time after GPU apply_gradients or CPU used them in seesion.run(op on CPU)?
"
647,4029,0,"ptb language modeling example broken with --use_fp16 and dropout other than 0. ### Environment info

Operating System: Ubuntu 16.04 LTS
GPU: GTX 1080
Nvidia driver: 370.23
CUDA: 8.0
cuDNN: 5
bazel: 



git commit hash: cc3153a7a0a23533d14ead34db37e4ccd7892079
### Description

When run using the  flag and either  or , the PTB language model example fails:



Modifying the model configs so that keep probability is 1 eliminates the problem, while any value other than 1 results in a call to  and, in turn, a call to , which results in a .
### What have you tried?

I have another machine running Ubuntu 14.04, CUDA 7.5, and a slightly older version of TensorFlow that does not have this issue. Details for this other set up:

Operating System: Ubuntu 14.04 LTS
GPU: Titan X
Nvidia driver: 352.39 
CUDA: 7.5
cuDNN: 5
bazel: 



git commit hash: 27eeb441bad8bcaa1bcba42a4b4ee49fb50ea0d3
"
478,19913,0,"ValueError: Could not interpret optimizer identifier (tf.keras). 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.8.0
- **Python version**: 
3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0/6.0
- **GPU model and memory**:
Nvidia
- **Exact command to reproduce**:
model.compile(optimizer=tf.keras.optimizers.Adadelta() ...)

### Describe the problem
Passing in keras optimizers into a tf.keras model causes a value error, unless they are passed as strings i.e. ""Adadelta"" instead of Adadelta( ). This prevents arguments from being passed to the optimizer. Please note that when the optimizer is imported from vanilla Keras i.e.  there is no such issue. 

### Source code / logs


    import tensorflow as tf
    from tensorflow.python.keras.optimizers import Adadelta, Adam

    model = deepshading.get_model()
    model.compile(optimizer=tf.keras.optimizers.Adadelta(rho=0.9),
                  loss=DSSIMObjective(k1=0.0001, k2=0.001, kernel_size=8),
                  metrics=[DSSIMObjective(k1=0.0001, k2=0.001, kernel_size=8)])


Returns trace-back

identifier=identifier.__class__.__name__))
Traceback (most recent call last):
  File ""C:/Users/isultan/PycharmProjects/deep-shading/run.py"", line 69, in <module>
    train()
  File ""C:/Users/isultan/PycharmProjects/deep-shading/run.py"", line 35, in train
    metrics=[DSSIMObjective(k1=0.0001, k2=0.001, kernel_size=8)])
  File ""C:\Users\isultan\AppData\Local\Continuum\miniconda3\envs\tf\lib\site-packages\keras\engine\training.py"", line 604, in compile
    self.optimizer = optimizers.get(optimizer)
  File ""C:\Users\isultan\AppData\Local\Continuum\miniconda3\envs\tf\lib\site-packages\keras\optimizers.py"", line 768, in get
    str(identifier))
ValueError: Could not interpret optimizer identifier: <tensorflow.python.keras._impl.keras.optimizers.Adadelta object at 0x000001E563508860>
"
565,17012,0,"LNK2019	unresolved external symbol __std_reverse_trivially_swappable_8  when compiling proto_text.vcxproj. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: Cmake 3.10.2, swigwin 3.0.12, Visual studio 2017, but toolset 2015 v140
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

I'm following the CMake guide as described here: https://github.com/tensorflow/tensorflow/blob/fbddebee0bf07dadfb2b15ec678291dd5730ca99/tensorflow/contrib/cmake/README.md.

Error appears after following command:


### Describe the problem
Error when compiling the proto_text.vcxproj

### Source code / logs


"
381,17138,0,"Windows installation page lists wrong cudnn version. The windows installation pages specifically asks to use cuDNN 6 - [link](https://www.tensorflow.org/install/install_windows). Then, when running tensorflow it looks specifically for cuDNN7.

    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))
ImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn

Would be great if the page modified this to specifically ask for cuDNN 7.

"
187,22889,1,"train with multi-gpu with MirroredStrategy will hang-up. ### System information
Have I written custom code: N/A
OS Platform and Distribution: CentOS Linux release 7.3.1611
TensorFlow installed from:  (pip install tf-nightly-gpu)
TensorFlow version: Tensorflow('v1.9.0-rc2-5345-g57d31aa599', '1.12.0-dev20181005')
Bazel version: N/A
GPU model and memory: Tesla P40 24G
Exact command to reproduce: N/A
Mobile device: N/A
CUDA/cuDNN version: cuda 9.0 with cudnn7.1.4

I train with tensorflow for multi-gpu with MirroredStrategy and estimator. I got the problem:
when I set the distribute mode with the following code it will got stuck after runing some training steps:

bug when I run without distribute mode like this:

It runs ok. Why? 
Is that a bug of MirroredStrategy?
"
398,28644,0,"GPU not available after installation of TF 2.0 alpha. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0-alpha0
- Python version: Python 3.6.8 Anaconda, Inc.
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: CUDA 10, cuDNN release 9.2, V9.2.148
- GPU model and memory: GTX1050M, 4GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I installed the tensorflow following the instructions:

and then instruction on installing CUDA on Ubuntu 18.04

After the above steps, I checked the installation with


When I test, however, whether TF runs on GPU, I get:


while previously (TF 1.13 with CUDA 9) it worked fine."
1227,1801,0,"Can't reshape tensor. I'm getting a strange error when trying basic reshape operations:



This gives me the following error:


"
804,10870,0," tensorflow-1.2.0  import tensorflow  Segmentation fault. hi, 
I installed tensorflow-1.2.0  in my machine, and met a segment fault as below.

linux-swfm:~/workarea/test> python
Python 2.7.13 (default, Jun 20 2017, 20:03:45) 
[GCC 4.9.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Segmentation fault

my system is : USE Linux Enterprise Server 11 SP3.
cuda sdk version is 8.0 and cudnn is 6.0.
my command to build tensorflow is below : 
     bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
"
580,31501,0,"Can't translate saved model to MLIR. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary for training, Source for translation
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.3
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.0/7.4.1
- GPU model and memory: P100


**Describe the current behavior**

I trained the official resnet imagenet model (r1) until it exported a saved_model.pb and associated variables folder

I then copied both of those into a separate directory and verified that the model was saved correctly by loading it and inferencing a few images, which returned the correct results.

I cloned tensorflow and called the following command in the tensorflow directory to translate the saved model to MLIR



However, I got the following error

> INFO: Analysed target //tensorflow/compiler/mlir:tf-mlir-translate (0 packages loaded, 0 targets configured).
> INFO: Found 1 target...
> Target //tensorflow/compiler/mlir:tf-mlir-translate up-to-date:
>   bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate
> INFO: Elapsed time: 0.571s, Critical Path: 0.00s
> INFO: 0 processes.
> INFO: Build completed successfully, 1 total action
> INFO: Running command line: bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --graphdef-to-mlir '--tf-input-arrays=input_tensor' '--tf-input-shapes=32,244,244,3' '--tf-input-data-types=DT_FLOAT' '--tf-output-arrays=ArgMax' /home/alber

_It got cut off by the edge of the terminal above_

> INFO: Build completed successfully, 1 total action
> 2019-08-09 16:59:17.399053: E tensorflow/compiler/mlir/tensorflow/utils/import_utils.cc:66] Error parsing Protobuf: /home/albert/MLIR/saved_model.pbtxt
> 2019-08-09 16:59:17.399429: E tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc:81] Graph import failed: Invalid argument: Could not parse input file

I tried converting the saved_model.pb to a .pbtxt and running the same command, but I got the same error

I've uploaded the contents of the .pbtxt file here: [http://m.uploadedit.com/bbtc/1565397330160.txt](http://m.uploadedit.com/bbtc/1565397330160.txt)
"
68,34960,1,"the model of Tensorflow2.0 lite has low accuracy, is there anything wrong?. Hello, I create new model with the program as follows:
https://github.com/tensorflow/hub/blob/master/tensorflow_hub/tools/make_image_classifier/make_image_classifier.py
and after the whole train process, the accuracy is more than 93%
However, I  run the program as follows to test the model:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/label_image.py
with the same dataset as validacation datasets. But the accuracy is very low, less than 80%.
Is there anything wrong? Thanks very much for your help.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (or github SHA if from source):
2.0.0


**Command used to run the converter or code if you’re using the Python API**

make_image_classifier --image_dir /home/ioz/bird/images/ --tfhub_module /home/tensorflow-2.0/resnet101v4/ --image_size 224 --saved_model_dir /home/ioz/bird/models/ --labels_output_file /home/ioz/bird/m
odels/class_labels.txt --tflite_output_file /home/ioz/bird/models/resnet101_20_0.001True_model.tflite --learning_rate 0.001 --train_epochs 20 --do_fine_tuning True

"
675,11200,0,"How can I cmpile the Tensorflow library to use CPU Instructions. My CPU is AMD Ryzen 5 1400
Here is my Instructions sets given by CPU-Z
	MMX (+), SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, SSE4A, x86-64, AMD-V, AES, AVX, AVX2, FMA3, SHA"
1000,13041,0,"Load model in C++ API and get ""from device: CUDA_ERROR_OUT_OF_MEMORY"" error. My model is about 2.4GB。In my inference step, I  want to load model by multi-processing method in each GPU. That means I try to make two process in one GPU and each load a model。
After I make configuration of each session done, each session get about 5GB memory, But I still meet the ""from device: CUDA_ERROR_OUT_OF_MEMORY""。I am wondering。。。 Asking for help

##  **GPU information:**
[search@qrwt01 /home/s/apps/qtfserverd/bin]$ nvidia-smi
Thu Sep 14 21:42:48 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 0000:08:00.0     Off |                    0 |
| N/A   48C    P0    61W / 149W |  11366MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 0000:09:00.0     Off |                    0 |
| N/A   32C    P0    72W / 149W |  11359MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     33056    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |
|    0     33057    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5515MiB |
|    1     33058    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |
|    1     33059    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5516MiB |
+-----------------------------------------------------------------------------+


## **Session configuration:**
 46 void* create_session(void* graph, std::string& checkpoint_path,
 47     int intra_op_threads, int inter_op_threads, std::string& device_list) {
 48     Session* session = NULL;
 49     SessionOptions sess_opts;
 50     //int NUM_THREADS = 8;
 51     if (intra_op_threads > 0) {
 52         sess_opts.config.set_intra_op_parallelism_threads(intra_op_threads);
 53     }
 54     if (inter_op_threads > 0) {
 55         sess_opts.config.set_inter_op_parallelism_threads(inter_op_threads);
 56     }
 57 
 58     sess_opts.config.set_allow_soft_placement(true);
 59     sess_opts.config.mutable_gpu_options()->set_visible_device_list(device_list);
 60     sess_opts.config.mutable_gpu_options()->set_allocator_type(""BFC"");
 61     sess_opts.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.5);
 62     sess_opts.config.mutable_gpu_options()->set_allow_growth(true);
 63     Status status = NewSession(sess_opts, &session);
 64     if (!status.ok()) {
 65         fprintf(stderr, ""Create Session Failed %s\n"", status.ToString().c_str());
 66         return NULL;
 67     }


## **Error information**
load /home/search/tensorflow/deploy_combine.model.meta graph to /gpu:1 success
2017-09-14 21:42:31.188212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:09:00.0
totalMemory: 11.17GiB freeMemory: 11.05GiB
2017-09-14 21:42:31.188260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 1, name: Tesla K80, pci bus id: 0000:09:00.0, compute capability: 3.7)
qss_switch:1, lstm_switch:1
qss_switch:1, lstm_switch:1
2017-09-14 21:42:33.826598: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.58G (1701773312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.838694: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.43G (1531596032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.893832: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.903917: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.913843: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.924008: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.935385: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.946556: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2017-09-14 21:42:33.956340: E tensorflow/stream_executor/cuda/cuda_driver.
"
391,10474,0,"No module named 'tensorflow'. Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
1020,13848,0,"can not compile android demo for x86. I tried to use the following commands to build android demo for x86
bazel build //tensorflow/examples/android:tensorflow_demo --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  --force_pic --cpu=x86_64 --config=android_x86

I always get the tensorflow_demo.apk for ARM device.
Is there anything wrong?"
848,24383,0,"Segmentation Fault running conv2d. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: CUDA 9.0 + cuDNN 7.3.1
- GPU model and memory: RTX 2070 8GB



**Running Example Convolutional Network Facing Segmentation Fault**

I installed the latest 4.10 Nvidia drivers (earlier drivers does not support new RTX cards), and downloaded and installed CUDA 9.0 (runfile) and cuDNN 7.3.1 (tarball) from Nvidia's website. I then installed tensorflow on a brand new miniconda 3 environment. I tried to run the mnist [convolutional model](https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py) from the tutorial, but gets segmentation fault. 


"
1101,8658,0,"Saving best models instead of most recent models with tf.train.Saver.. Most of the time I want to save the best models instead of the most recent models. Doing so using tf.train.Saver requires to choose when to save a model and to delete the worst model (which might not be the oldest) ""manually"".

A method to save the N best models (according to some user defined value) would be nice."
1359,23779,0,"Error on Label's datatype when enable eager execution in Colab . ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.14.1 - Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: Colab build in
- **TensorFlow version (use command below)**: 1.12.0
- **Python version**: Colab Python2
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None


### Describe the problem
When I run the simple DNN on MNIST without eager execution, it works fine. However, when I enable eager execution then fit the model, I got the error said that my  and  has to be  tensor which are originally  tensor. The error is shown as below:



Although, I can fix it by using  ,It looks like the error should be fixed in the source code.



### Source code to reproduce the issue in Colab




### Other info / logs





		"
1147,18648,0,"eager scatter_nd forward works with incorrect code. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
code, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSX 10.12.6
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.7.0-3-g024aecf414 1.7.0
- **Python version**: 
3.6.1
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
see code below


### Source code / logs



Error traceback:



### Describe the problem

This is erroneous code which runs half-way. It will still calculate the forward pass, but fail on the backward pass. This does not happen with static graph tensorflow, where you get a correct error that tensor shapes are not matching."
984,21700,0,"Numpy 15.0 support. It seems numpy newest version has comes out for a long time, but as far as tensorflow 1.10, it not support numpy 15.0 yet.
While pytorch using numpy 15.0 above, this is really awkward... I have to using 2 versions numpy"
1466,31668,0,"Models with tf.keras.layers.BatchNormalisation layers give errors when frozen, or are frozen incorrectly and cannot be loaded. This issue continues on from #31331 with a more wide ranging scope

**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 1903
- TensorFlow installed from: pip
- TensorFlow version: 1.13.1 / 1.14.0 / tf-nightly
- Python version: 3.7.3
- GPU model and memory: RTX 2080 Ti 

**Problem**

Freezing a model with tf.keras.layers.BatchNormalization layers either gives an error, or does not freeze correctly and gives an error when loading, depending on the tensorflow version.

**Method**

- Freezing: Save a model with tf.keras.layers.BatchNormalization layers using  and then freeze it using .
- Loading: Load the frozen model using tf.import_graph_def()

**Results**

*1.13.1*
- Freezing: no error
- Loading: 

*1.14.0*
- Freezing: 
- Loading: n/a

*tf-nightly  1.15.0-dev20190815*
- Freezing: no error
- Loading: 

**Code to reproduce**

Colab gist is here: https://colab.research.google.com/gist/geometrikal/da64b13d8a579bc46c005e981d9bc051/tf_31331_freezing_savedmodel.ipynb



**Workaround**

Workaround is to save the weights, clear the session, , recreate the model, restore the weights, and then freeze. https://github.com/tensorflow/tensorflow/issues/31331#issuecomment-518655879"
495,25835,0,"keras model's saving format is inconsistent (tf and h5). hi, I found the behavior is inconsistent when the keras model's saving format changed.
the keras model have two saving format(h5 and tf)

**Describe the current behavior**
the reproduce code as below:

the photo below is the behavior in my computer.
![image](https://user-images.githubusercontent.com/13925796/52948264-f2f7cf00-33b3-11e9-9ad1-fff9716911c8.png)

**Describe the expected behavior**
when we change save_format from  to , the expection(OSError) should not happen. "
784,6389,0,"[TensorFlow.org] Anchor Tag Incorrect Reference. Nothing really serious but I noticed the link on  on getting started doc body is using an anchor separated with dashes   while the corresponding  uses underscores .

For instance, the body anchor (which works on github) 
 
and the right side nav-bar  link, working normally, with underscores


I was going to submit a PR but changing to the latter breaks on github. "
446,31415,0,"tools.graph_transforms.TransformGraph has no docs or example of usage. Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

the provided wrapper function has no docs

## Description of issue (what needs changing):

provide a docs such
examples of usage:
TransformGraph( graph.as_graph_def(), [], [], ['remove_nodes(op=loss/init)']) ...

### Clear description

I wanna to use this method has a way to do specifics editions and graph redefinitions while building the model, from inside python, withou having to go to command line.

### Parameters defined

I think how to use the parameters are exactly the problem, the README.md from the repository gives bazel example, but it dosnt work as it should in the wrapper
### Usage example

this is my first try, i could not get the desirable result (strip init op from graph):


then:

 

returns True
So how to use this wrapper ? note that init op dosnt have any input output but only dependency arrows as input.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Waiting for instructions of the community about the use of this function.

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
1301,14391,0,"MonitoredTrainingSession does not initialize after restore. ### Describe the problem
 can be passed to the SessionManager through the scaffold argument in MonitoredTrainingSession. From the doc's from session manager: The  is an  that is run always after a new session was created. This does not work as expected in the below example.

### Exact command to reproduce

The first time you run this, two variables  and  will be initialized by the implied (default) initializer from the  and a checkpoint file will be written to disk for only a1; expected behavior, no errors. The second time you run this, it should load a1 from the previous checkpoint and initialize  through the  given through the scaffold. But it doesn't, instead: 

> RuntimeError: Init operations did not make model ready for local_init.  Init op: group_deps, init fn: None, error: Variables not initialized: global_step, a2

A hack that circumvents the problem by not using  is suggested here (as well as a reiteration of the expected behavior):
https://stackoverflow.com/questions/43336553/how-to-use-tf-train-monitoredtrainingsession-to-restore-only-certain-variables




### System information
- **TensorFlow version (use command below)**:
('v1.2.0-rc2-21-g12f033d', '1.2.0')
- **Python version**: 
2.7.10

"
706,18394,0,"Feature Request: Early Stopping with the tf.estimator. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Pip
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9
- **GPU model and memory**: NVIDIA K80
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I would love for there to be a SessionRunHook implementation in tensorflow that implements early stopping. Now that  is being officially deprecated, the existing way that I did early stopping (using a ValidationMonitor) is no longer an option. This seems like a super important feature to have.

The docs indicate in several places that you can simply extend a SessionRunHook to do this, and that seems reasonable. I think that having a standard way to do this would be super useful to lots of users, perhaps even directly built into TrainSpec.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

N/A"
584,27289,0,"Test case reports an exception to stdout, with stack trace, when testing under self.assertRaises(). **System information**
- Have I written custom code: Only for testing
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): From PyPI via pip3 install --upgrade tensorflow==1.11.0
- TensorFlow version: v1.11.0-0-gc19e29306c 1.11.0
- Python version: 3.5.2
- CUDA/cuDNN version: CUDA 9.0, cuDNN 7.0
- GPU model and memory: NVIDIA GeForce GTX 1080, 8GB


**Describe the current behavior**
Exception is reported to stdout, with stack trace, when testing under :



**Describe the expected behavior**
The exception must be saliently catched:



**Code to reproduce the issue**

"
325,15985,0,"Feature Request: Dense to Sparse and Dense to Sparse Tensor Ops. I think it would be helpful if there is a dense_to_sparse op in Tensorflow for ops like  that requires sparse labels. I'm not really sure where else it can be used aside from that but in case only  uses it, I think it would help if dense labels can be passed into  and do the conversion within."
1257,34580,0,"Golang tensorflow no longer builds due to bad import in saved_model.go. A recent change (Change-Id: Iefdf75ed88f54d97a0a7d210f5a42f3123205bf2) has broken an import in file tensorflow/blob/master/tensorflow/go/saved_model.go

tfpb ""github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core/framework""

"
256,29151,1,"Training time for one epoch on TF 1.12 is almost 2 times slower than TF 1.5. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04) :Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.5 and 1.12
- Keras version: 2.2.4
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:CUDA 9.2/ cuDNN 7600
- GPU model and memory: T4 GPU with 15079MB memory

**Describe the current behavior**
Training time for one epoch on TF 1.12 is almost 2 times slower than TF 1.5 when using Keras layer.
I also tried to compare the training time using pure tensorflow layers, the speed is similar for TF1.12, TF1.5 and TF1.13. But I don't understand why the same Keras version will influent training time with different TF version


**Describe the expected behavior**
Training time for one epoch on TF 1.12 should be similar to  TF 1.5

**Code to reproduce the issue**


**Other info / logs**
on TF 1.5 machine:

On TF 1.12 machine:
"
1358,11955,0,"Incorrect tfBinaryURL for installing with Anaconda on Linux. There is incorrect tfBinaryURL at tensorflow.org.

In case of Installing with Anaconda in Linux, the example shows the following command for Python 2.7.
(tensorflow)$ pip install --ignore-installed --upgrade \
 https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp34-cp34m-linux_x86_64.whl
But this is for Python 3.4 so correct command is below.
(tensorflow)$ pip install --ignore-installed --upgrade \
 https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp27-none-linux_x86_64.whl
"
82,27819,1,"Tensorflow 2.0.0 gpu Performance Issue on RTX 2060, win 10 . Hi I'm not used to write English so Please Understand my situation.
Actually My proplem is not quite sure about performance problem or just myself problem.
But No Solution appear in my circumstance include searching stackover-flow or googling.
And also in my country quite many of them is saying same issue about it.
And Yeah i do almost everything concern this problem searching googling or blogs or sites
So I decide to ask u to help me out.

My problem is I Change my GPU device NVIDIA GTX1050 TI to RTX 2060.
And i ran 1050 ti in tensorflow gpu well enough but when i change my GPU and unistall CUDA 9 before version and reinstall CUDA 10 and reinstall matching cudnn also
Cause As u know CUDA 10 is the only choice of RTX Series.
But After Changing my GPU and CUDA, cudnn, My GPU performance is too low to confuse to using it. Yeah It's working But TOO LOW PERFORMANCE!
Actually I usually jupyter notebook in anaconda env system and my sub env 1 is tensorflow 2.0.0 alpha and sub env 2 is tensorflow 1.13. 

But When i build conv model and train images,  Both tensorflow version of gpu is too low under 15% And CPU usage is almost 100%, RAM 60~70%
Usually GPU use 5 ~ 10%. 
When i use 1050 ti, GPU performance is almost 80 to 90% high

And I know tensorflow recommend docker Linux only but Even though I install All of these gpu concern program and other needed program.

So i sincerely ask tensorflow Is this Right Performance or My OS problem or some other probelm
I want to be clear Answer about it.
And Even if tensorflow 2.0 is not fit on windows 10, Why my another env tensorflow 1.13 version of GPU is also low and CPU is almost 100% right now

and Here is the image that Run tensorflow 2.0 tutorial about GPU 
And My GPU is Fine Device proof png file is upload
Thank you for reading my nonsence skill of eng writing.

![gpu1](https://user-images.githubusercontent.com/43261434/56080846-32391e00-5e41-11e9-95cb-476a0404cf2d.png)

![gpu colab과 비교 2](https://user-images.githubusercontent.com/43261434/56080864-572d9100-5e41-11e9-9e8c-271faf53645f.png)

![3d mark2](https://user-images.githubusercontent.com/43261434/56080973-a0321500-5e42-11e9-9176-ae339b5caf26.png)
![3d mark](https://user-images.githubusercontent.com/43261434/56080974-a0321500-5e42-11e9-836c-6b42dfd85cc3.png)

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): Tensorflow 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10, cuDNN 7.5(2.21.2019 for CUDA 10)
- GPU model and memory: RTX 2060 Emtec & 6G RAM(Edited)
- CPU : intel i5-8400
- Computer RAM - 8G

"
1154,15776,0,"Compiler Errors Installing Tensorflow from Source. ### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 SP1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 6.4.0
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: NVIDIA Quadro K4000

- **Exact command to reproduce**:

bazel build -c opt %BUILD_OPTS% //tensorflow/tools/pip_package:build_pip_package

### Describe the problem

I have tried compiling with MSYS2 and VS2015. I am trying to get VS2015 to work.

###Using VS2015 and  (among other options), I get the following error:



###Using VS2015 and  (among other options), I get this error:



###Any help would be appreciated. I can give you more details as well.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
199,34174,1,"Performance regression with obj. det model from 1.14 to 1.15. 2.7x slowdown on a nearly-off-the-shelf tensorflow object detection API model (differences and pipeline config below) when going from 1.14 to 1.15.  The regression is _not_ present in 2.0.

This is a fine-tuned SSD mobilenets FPN coco config, with the only changes being:

Fixed size input of 1024x1024
Single class detection
 model draws from layers 2-5 instead of 3-6 and so is a fair bit larger than one might otherwise expect.

I've attached my pipeline.config to reproduce the bug.

I'm just running natively in TF with my own inference pipeline, doing the obvious feed_dict to get data in:


[pipeline.config.txt](https://github.com/tensorflow/tensorflow/files/3833314/pipeline.config.txt)

**System information**
Running on Ubuntu 18.04, Xeon, Titan V GPU, 12GB

Comparing between TF 1.14-GPU and 1.15 with conda/pip.


CUDA 10.0
CuDNN 7.6

**Describe the current behavior**

2.7x slower in 1.15. :-)

**Describe the expected behavior**

**Code to reproduce the issue**

See attached pipeline.config (would need to specify some training/testing tfrecord)


**Other info / logs**
This issue is related to an issue on tensorflow/serving:  https://github.com/tensorflow/serving/issues/1469

"
1405,32069,0,"Export Inference TensorFlow model Option(DEFAULT/API_MODEL). <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (1.8/2.0):
- Are you willing to contribute it (Yes):



**Describe the feature and the current behavior/state.**

When exporting(freeze graph) tensorflow model(graph) only for inferencing on mobile/Embedded/IoT using [Tflite](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite)/[MNN](https://github.com/alibaba/MNN) etc lite inference framework, it is difficult to convert/implement the freezed graph efficiently sometimes. For example, when using RNN/GRU/LSTM(static or dynamic) or control flow api(operations), tensorflow will unroll the api to lower operations that make converting/implementing/optimizing on other inference framework difficult, and need much efforts for optimizing the unrolled graph.
I wonder whether the following recommendation is acceptable:
> Tensorflow provide two options when exporting(freeze graph), one is the *DEFAULT* which like the original process, the other is *API_MODEL* which do not lower the api, keep the api-operations in the freezed graph(pb). 
 This method like [convert-RNN-tflite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/convert/rnn.md), but I think the above method is better.

**Will this change the current api? How?**
Yes, add one option(DEFAULT/API_MODEL)
**Who will benefit with this feature?**
Those who using tensorflow framework to train model, then deploying the model on mobile/Embedded/IoT using Tflite/MNN etc lite inference framework.
**Any Other info.**
"
312,25656,0,"Distributed training using grpc+verbs got an assertion in verbs/rdma.cc:1557. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
Custom code using tf.estimator.DNNLinearCombinedClassifier
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): privileged ubuntu 16.04 docker 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
source built with verbs feature
bazel build --config=mkl -c opt --config=verbs //tensorflow/tools/pip_package:build_pip_package

- TensorFlow version (use command below): 1.12.0
- Python version: python3.6
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10)
- CUDA/cuDNN version: N/A
- GPU model and memory: CPU only


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

b'v1.12.0-5-g7317495' 1.12.0

**Describe the current behavior**
I deployed one chief, one PS and one worker with protocol 'grpc+verbs'. This case uses only CPU device. The master and PS works properly. The worker meets an assertion, and exits.
Switched to protocol grpc, the code works well.
The result of rping shows that RDMA configure is good.



**Describe the expected behavior**
The worker should run normally.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[worker-20190211-141600.log](https://github.com/tensorflow/tensorflow/files/2850045/worker-20190211-141600.log)

"
449,35075,0,"ValueError: Unable to save the object ListWrapper([ListWrapper([None])]) tensorflow 2.0. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10/ cudnn 7.6.5
- GPU model and memory: V100

**Describe the current behavior**
When saving a model, in my case, HRNet, i met the following error: ValueError: Unable to save the object ListWrapper([ListWrapper([None])])
The error message suggest that ""If you don't need this list checkpointed, wrap it in a tf.contrib.checkpoint.NoDependency object; it will be automatically un-wrapped and subsequently ignored."" However, tf 2.0 has no module named contri.

**Describe the expected behavior**
Save the model correctly.
**Code to reproduce the issue**
You can run the following code to reproduce this issue
https://github.com/zheLim/auto-face-parsing/blob/master/lib/model/seg_hrnet.py
"
231,9139,1,"Incorrect results when graph is split across several GPUs.. Background info:
- Custom code
- Tensorflow r1.0 installed from binaries on Windows
- CUDA 8.0, cuDNN 5.1.5
- 2 GPUs: GTX Titan X and Titan X Pascal

Problem:

I have a model that is small enough to be trained on a single GPU with 12GB memory. Training works fine and converges.

However, when I evaluate the model with a validation set that is too large to fit on one of my GPUs, TensorFlow seems to use both of my GPUs (one GTX Titan X and one Titan X Pascal). When this happens, **a large fraction of the results returned by TensorFlow are incorrect**. The returned values are not completely missing, i.e. not all zero or something like that, but so inaccurate that the validation performance is terrible. 

More specifically, my model consists of a shared initial stage, followed by a list of ~50 sub-networks that all receive input from the shared stage, but are otherwise independent. Data is split using . From the results that I get, it appears that TensorFlow moves some of the 50 sub-networks to the second GPU if the memory on the first one isn't sufficient. The moved sub-models then return incorrect results (the others are unchanged).

If I instead force evaluation to be performed on the CPU, all results are as expected. All I need to do is add  to the very top of my script. The results also look good if I reduce the size of the validation set so that it fits onto one GPU.

I am sorry for not providing a working example. I will try to create one, but it might take a while since, by nature of the problem, it needs to be a fairly large/complex model.


"
813,5191,0,"sucessfully configure, but build error with bazel. the configure sucessfully finished
but when run ""bazel build -c opt //tensorflow/tools/pip_package:build_pip_package""
it reports the error: ......./tensorflow/tensorflow/python/BUILD:1806:1: in cc_library rule //tensorflow/python:tf_session_helper: non-test target '//tensorflow/python:tf_session_helper' depends on testonly target '//tensorflow/python:construction_fails_op' and doesn't have testonly attribute set
"
139,33560,1,"BatchMatmul on GPU Wrong Result. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
-  Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): 2.0
- Python version: 3.7.3
- CUDA/cuDNN version:
- GPU model and memory:Quadro P4000 8GB

**Describe the current behavior**

batchmatmul on gpu gives wrong result.

**Describe the expected behavior**

batchmatmul should give correct result.

**Code to reproduce the issue**


**Other info / logs**
python diff.py 
tf.Tensor(
[[[[ 0.          0.          0.        ]
   [ 0.          0.          0.        ]
   [ 0.          0.          0.        ]
   ...
   [ 0.          0.          0.        ]
   [ 0.          0.          0.        ]
   [ 0.          0.          0.        ]]

  [[ 0.          0.          0.        ]
   [ 0.          0.          0.        ]
   [ 0.          0.          0.        ]
   ...
   [ 0.          0.          0.        ]
   [ 0.          0.          0.        ]
   [ 0.          0.          0.        ]]

  [[ 0.          0.          0.        ]
   [ 0.          0.          0.        ]
   [ 0.          0.          0.        ]
   ...
   [ 0.          0.          0.        ]
   [ 0.          0.          0.        ]
   [ 0.          0.          0.        ]]

  ...

  [[ 2.7437444  -2.3577256   3.3067198 ]
   [ 1.0874022  -0.7325933   2.4926019 ]
   [-2.3672104   1.6497741  -2.1795664 ]
   ...
   [-1.4023315   0.68545413 -1.9975882 ]
   [-0.44164103 -1.6766946  -0.6477224 ]
   [ 2.5612988   0.7603723   4.286979  ]]

  [[-0.9761815   1.5332515  -0.12404668]
   [ 1.871354   -0.73664904 -1.0545558 ]
   [-1.2774239   1.6571116  -2.36473   ]
   ...
   [ 2.4847538  -1.5389507   1.5169847 ]
   [ 2.2117858  -3.3593345  -0.15468699]
   [ 0.82464755 -2.0175495  -0.11506134]]

  [[ 2.8697317   0.54362947 -0.10824746]
   [ 1.6531861  -4.2324843   0.97695297]
   [ 4.3966837  -3.0250916   1.8032213 ]
   ...
   [ 4.5854487  -2.0374475  -1.1027236 ]
   [-2.7206469   0.864337   -1.3582373 ]
   [-0.98220587  0.53226703 -4.277097  ]]]], shape=(1, 480, 640, 3), dtype=float32)

"
671,33345,0,"keras optimizer `apply_gradients` arg `grads_and_vars` has wrong type in documentation. ## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/516c98da7b7d8526c153827c426c675a4ece9543/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L414

## Description of issue (what needs changing):

 is documented as list but is actually passed as .

### Clear description

This can be problematic when writing custom optimizers that iterate over the  multiple times, as in that case a  will not give the intended behaviour.

"
1276,27517,0,"ML Kit for Android fails to load valid TFLite model. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BLU Vivo XI
- TensorFlow installed from (source or binary): Source
- TensorFlow version: ('v1.13.1-0-g6612da8951', '1.13.1') 
- Python version: 2.7
- Bazel version (if compiling from source): 0.24.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: NVIDIA GeForce GTX 1050, ~5 GB

**Describe the current behavior**
I am currently attempting to use Firebase ML Kit to run a Tensorflow-Lite model that was converted to the .tflite format from Keras. I have been following [this](https://firebase.google.com/docs/ml-kit/android/use-custom-models) tutorial to host a custom model locally. 
Every attempt thus far has ended with the same error: ""ByteBuffer is not a valid flatbuffer model"" (full stack trace shown below).
The .tflite model has been created correctly; running the Interpreter (from the Python Tensorflow module) to allocate tensors and get the input and output details returned the proper information without error. 
Furthermore, the iOS version of the app (using the same model, stored locally and loaded with Firebase ML Kit) is working properly. It used a custom build of Tensorflow (following [this](https://firebase.google.com/docs/ml-kit/ios/use-custom-tflite) tutorial) that started by forking  Tensorflow 12.0.0 and then cherry-picked future commits to include the additional ops required. I did not write any custom ops myself; all of the required ops had already been added to Tensorflow at one time or another.
First, I attempted to use the same build by following [this](https://firebase.google.com/docs/ml-kit/android/use-custom-tflite) tutorial with Bazel version 0.18.0, which failed. I also attempted [this](https://heartbeat.fritz.ai/compiling-a-tensorflow-lite-build-with-custom-operations-cf6330ee30e2) tutorial to import TFLite as a module rather than publish it to my local maven repository. This had the same result.
Then, under the assumption that the latest release of Tensorflow would have all the necessary ops anyway, I built Tensorflow 1.13.1 using Bazel version 0.24.0. This failed with the same error.

Here are my questions, as succinctly as possible:
1. Is ML Kit currently unequipped to handle the most recent versions of Tensorflow-Lite? (If so, why would the iOS version still work?)
2. Is Tensorflow-Lite for Android currently not working for Tensorflow 1.13.1? I'm no expert in machine learning, so this may be nonsensical, but are there operations/features created for Tensorflow that Tensorflow-Lite is unable to handle? (And again, if this was the case, why would the iOS version still work?)
3. Are the tutorials for ML Kit missing any essential information? 
4. Has anyone else been able to get a custom model converted to .tflite and have it work with ML Kit?

**Other info / logs**
E/ModelResourceManager: Error preloading model resource
    com.google.firebase.ml.common.FirebaseMLException: Local model load failed: 
        at com.google.android.gms.internal.firebase_ml.zzpe.zza(Unknown Source:129)
        at com.google.android.gms.internal.firebase_ml.zzpe.zzlp(Unknown Source:104)
        at com.google.android.gms.internal.firebase_ml.zznx.zzf(Unknown Source:56)
        at com.google.android.gms.internal.firebase_ml.zznz.zzls(Unknown Source:7)
        at com.google.android.gms.internal.firebase_ml.zznz.call(Unknown Source:24)
        at com.google.android.gms.internal.firebase_ml.zznn.zza(Unknown Source:29)
        at com.google.android.gms.internal.firebase_ml.zzno.run(Unknown Source:2)
        at android.os.Handler.handleCallback(Handler.java:790)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at com.google.android.gms.internal.firebase_ml.zzi.dispatchMessage(Unknown Source:6)
        at android.os.Looper.loop(Looper.java:164)
        at android.os.HandlerThread.run(HandlerThread.java:65)
     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:69)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:175)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:163)
        at com.google.android.gms.internal.firebase_ml.zzpe.zzc(Unknown Source:224)
        at com.google.android.gms.internal.firebase_ml.zzpf.zzd(Unknown Source:0)
        at com.google.android.gms.internal.firebase_ml.zzpe.zzb(Unknown Source:150)
        at com.google.android.gms.internal.firebase_ml.zzpe.zza(Unknown Source:118)
        at com.google.android.gms.internal.firebase_ml.zzpe.zzlp(Unknown Source:104) 
        at com.google.android.gms.internal.firebase_ml.zznx.zzf(Unknown Source:56) 
        at com.google.android.gms.internal.firebase_ml.zznz.zzls(Unknown Source:7) 
        at com.google.android.gms.internal.firebase_ml.zznz.call(Unknown Source:24) 
        at com.google.android.gms.internal.firebase_ml.zznn.zza(Unknown Source:29) 
        at com.google.android.gms.internal.firebase_ml.zzno.run(Unknown Source:2) 
        at android.os.Handler.handleCallback(Handler.java:790) 
        at android.os.Handler.dispatchMessage(Handler.java:99) 
        at com.google.android.gms.internal.firebase_ml.zzi.dispatchMessage(Unknown Source:6) 
        at android.os.Looper.loop(Looper.java:164) 
        at android.os.HandlerThread.run(HandlerThread.java:65)
"
1189,17370,0,"Image retraining script memory problem and issue. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
      Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6-gpu/nightly-gpu
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0/7.04
- **GPU model and memory**: Tesla K80 / 11441MiB
- **Exact command to reproduce**:
python retrain_quantize.py w/ certain parameters.


### Describe the problem
I was trying the new retrain script on my own model. (In order to fully convert quantized model to tflite)
1. Different memory usage in different version.
I opened allow_growth parameter in order to trace memory usage during training.
In tf-gpu-1.6.0 :

+-----------------------------------------------------------------------------+
| Processes:                                                                                      GPU Memory |
|  GPU       PID   Type   Process name                                                    Usage      |
|======================================================== |
|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |
|    0     15440      C   python                                                                     302MiB |
+-----------------------------------------------------------------------------+

But in tf-nightly-gpu:

+-----------------------------------------------------------------------------+
| Processes:                                                                                      GPU Memory |
|  GPU       PID   Type   Process name                                                    Usage      |
|======================================================== |
|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |
|    0     15747      C   python                                                                    4152MiB |
+-----------------------------------------------------------------------------+


I was wondering what causes the tremendous difference in these two versions?

2. Per the traceback below, my retraining process could not be done in the last step. 
    Due to the feed_dict issue. I saw my process restart a session after 100 steps, could the restart 
    cause the loss of DecodeJPGInput tensor?


Thanks in advance!

### Source code / logs
"
993,4377,0,"tensorflow.contrib.learn.TensorFlowEstimator fails under sklearn.base.clone(). tensorflow.contrib.learn.TensorFlowEstimator fails under sklearn.base.clone()

This prevents the possibility to perform CV using sklearn.cross_validation.cross_val_Score over this estimator.
The clone fails due to the fact the TensorFlowEstimator.get_params() returns a key named ""params"" that is not part of the constructor input parameters.
This causes sklearn.base.clone to fail when trying to clone the estimator using it's constructor:
TensorFlowEstimator(**params)
"
1236,12998,0,"An error in  llvm/Object/SymbolicFile.h : expected ')' before 'PRIxPTR'. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  CentOS Linux release 7.2.1511 (Core)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  git hash 1e96d54d9f928c4ea4bf0564ef9900f6bd03acd5
- **Python version**:  3.6.1
- **Bazel version (if compiling from source)**: 0.5.3
- **CUDA/cuDNN version**: CUDA 8.0, cudnn : 5.1.5 
- **GPU model and memory**: Titan X (Maxwell) + Titan X (Pascal) + GTX 1080(Pascal)
- **Exact command to reproduce**: bazel build --config=opt --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package 

Compiler used :  


On compiling the source with the hash  , the compilation goes on smoothly and towards the end produces an error as follows : 


I am trying to build with XLA JIT compilation enabled. The problem comes from that. Lower down the long message, following comes as an error : 


It seems to be a problem with a header file and hence I'd be grateful for answering this bug."
1349,27478,0,"tf.print the example doesn't work with TF 2.0. <em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/print


**Describe the documentation issue**
the example doesn't work with TF 2.0


**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
907,21550,0,"Horrible. Really a waste of time. Installing all kind of s**t, winpython, anaconda, then, finally, checking that tensorflow doesnt work with 32bit systems. Why cant tensorflow write:
""Only 64 bit supported!!"""
996,21152,0,"Native TF operations drop Keras tensor metadata. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution: Ubuntu 16.04
- **Mobile device**: N/A
- **TensorFlow installed from**: from pip
- **TensorFlow version**: ('v1.9.0-0-g25c197e023', '1.9.0')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA-9.0/cuDNN-7.1.4
- **GPU model and memory**: N/A
- **Exact command to reproduce**:



### Description
The code mentioned above throws the error (see logs). As it turned out, almost all native tf operations , such padding, convolution, etc., drop tensor metadata.

I do understand that these operations can be replaced with similar Keras layers or wrapped with Lambda. However, since Keras is officially moved to tensorflow package, it would be nice to preserve tensor compatibility between tf and Keras.

### Source code / logs
Layer
"
260,28130,1,"tf.data.Dataset Performance Issue. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 / 7.4.1
- GPU model and memory: NVIDIA Titan V

- CPU Make & Model: 2x Intel Xeon E5-2620 v4 (8 cores/16 logical)
- Data Drive: Samsung SSD 960 EVO 1 TB

**Describe the current behavior**
Currently using the tf.data.Dataset API to load image pairs for super resolution. I believe based on the minimal examples I could find the method below is as optimized as I can get for my use-case. However, when I grew my path list from 550 to 7950 items it slows down over 3x. It doesn't seem like this part of the pipeline should scale so poorly since the batches themselves are the same size. And the process of mapping & batching (mostly IO) should be parallelized across the 32 CPU cores of the machine. 

Any ideas? Pertinent code below.


    lr_paths, hr_paths = ... # Flat lists of paths to LR & HR images, respectively.

    def load_image(path): return tf.image.decode_png(tf.read_file(path), 3)

    # Create the dataset
    dataset = (tf.data.Dataset.from_tensor_slices((lr_paths, hr_paths))
        .apply(tf.data.experimental.shuffle_and_repeat(count, FLAGS.max_epochs))
        .apply(tf.data.experimental.map_and_batch(lambda x, y: (load_image(x), load_image(y)), FLAGS.batch_size, num_parallel_batches=max(1, (cpu_count() - 1) // FLAGS.batch_size)))
        .apply(tf.data.experimental.prefetch_to_device('/device:GPU:0')))

    # Query for the iterator
    iterator = dataset.make_one_shot_iterator()
    im_LR_batch, im_HR_batch = iterator.get_next()

    ... # Preprocess the image batches with crop, data augmentation, type conversion"
399,34082,0,"--config=rocm build works with bazel `0.26.1` but has link errors with `0.29.1`. We are trying to figure out why the  TF build breaks (link errors) with bazel version , but works fine with .

How do we get a better understanding of what changed on the bazel side, to introduce the build errors, and how to fix them?

thanks

-------------------

/cc @whchung @parallelo @sunway513 "
1483,12501,0,"EditDistance crashes under C++ environment. ### System information
- **Windows 10**:
- **TensorFlow installed from source**:
- **TensorFlow version 1.3-rc2**:
- **Python version 3.5**:
- **VisualStudio 2017**:

### Describe the problem
I have created a C++ example with EditDistance and linked a debug version of tensorflow.dll which is created from tensorflow source.  The compiled EditDistance example crashed at **session.Run( outputs, &output_tensor );**. I have tried in this example also in the tensorflow environment it crashes also. It seems the EditDistance operator doesn't work under C++. I have searched another EditDistance examples but it seems nobody has tested it outside the python environment.

### Source code / logs


    int main( int argc, char *argv[] )


           std::vector<Tensor> output_tensor;
           ClientSession session( root );


           Tensor hypho2_ix( DT_INT64, TensorShape( { static_cast<int64_t>( 4 ), 3 } ) );
           Tensor hypho2_vals( DT_STRING, TensorShape( {static_cast<int64_t>( 4 )} ) );

           makeIndex( {""bear""}, hypho2_ix );
           makeChar ( {""bear""}, hypho2_vals );

           Tensor truth2_ix( DT_INT64, TensorShape( { static_cast<int64_t>( 5 ), 3 } ) );
           Tensor truth2_vals( DT_STRING, TensorShape( {static_cast<int64_t>( 5 )} ) );

           makeIndex( { ""beers"" }, truth2_ix );
           makeChar ( { ""beers"" }, truth2_vals );

           // Declaration of edit distance
           auto address_dist = EditDistance( root
                                   , hypho2_ix
                                   , hypho2_vals
                                   , {3,1,1}// {static_cast<int64_t>(1),3}//test_address_shape
                                   , truth2_ix
                                   , truth2_vals
                                   , {3,1,1} //ref_address_shape
                                   , EditDistance::Normalize(false) );

                                      const std::vector<Output> outputs = {address_dist};
           session.Run( outputs, &output_tensor );
     }   
        void makeIndex( const std::vector<string>& rsoStringVector, tensorflow::Tensor& roIndexTensor )
        {
           auto ix_t = roIndexTensor.matrix<int64_t>();
           std::size_t stCounter = 0;
           for( std::size_t stX = 0; stX < rsoStringVector.size() ; stX++ )
           {
              const std::string& rsString = rsoStringVector[ stX ];
              for( std::size_t stY = 0; stY < rsString.size(); stY++ )
              {
                 ix_t( stCounter, 0 ) = stX;
                 ix_t( stCounter, 1 ) = 0;
                 ix_t( stCounter, 2 ) = stY;
                 stCounter++;
              }
           }
        }

        void makeChar( const std::vector<std::string>& rsoStringVector, tensorflow::Tensor& roCharTensor )
        {
           auto vals_t = roCharTensor.vec<std::string>();

           int64_t i64Index = 0;
           for( std::size_t stX = 0; stX < rsoStringVector.size(); stX++ )
           {
              const std::string& rsString = rsoStringVector[ stX ];
              for( std::size_t stY = 0; stY < rsString.size(); stY++ )
              {
                 vals_t( i64Index++ ) = ( rsString[ stY ] );
              }
           }
        }
"
607,30846,0," mismatch in the description of BatchDot and TensorFlow's implementation. . ## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/backend/batch_dot

## Description of issue (what needs changing):
I raised [an issue on the plaidML repo](https://github.com/plaidml/plaidml/issues/358), and after some back and forth we determined the documentation for BatchDot doesn't quite match the actual implementation in the tensorflow code.

### Clear description
A BatchDot with x.shape=(1,2,6,2) and y.shape=(1,2,2,3) and axes = (3, 1)has an output shape of (1,2,6,3)) whereas by the TF definition for output shape ""A tensor with shape equal to the concatenation of x's shape (less the dimension that was summed over) and y's shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to (batch_size, 1)."" sounds like it should have an output shape of (1,2,6,2,3).

### Submit a pull request?
I am not planning to submit a PR at this time, but I may do it later"
338,34850,0," Error: slice index 0 of dimension 0 out of bounds. @ravikyram 
Hi, I am working on tensorflow 2.0  and getting an error at line 3 while running the model  

1) lstm_cell =tf.keras.layers.LSTM(units=hidden_unit)
2) lstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)
3) self._initial_state = lstm_cell.get_initial_state(128, tf.float32)

Got ERROR at line 3
ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.
may I know why I am getting this error?
"
890,34541,0,"RNN with cell  get_initial_state  and state_size incompatible. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): TF2.0
- Python version: py3.74
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

ValueError: An  was passed that is not compatible with . Received =ListWrapper([InputSpec(shape=(None, 100, 1), ndim=3)]); however  is [100]

**Describe the expected behavior**

**Code to reproduce the issue**
"
887,15263,0,"Cannot parse tensor from proto: dtype: DT_INT32 when using tf.extract_image_patches and tf.reshape. Hi,
I'm experiencing a problem when using TensorFlow to extract image patches and then reshape the output. I'm using TensorFlow 1.3.0, what am i doing wrong?

That's my code:


The error i'm getting is this:
"
1218,270,0,"Error: tensorflow/core/common_runtime/executor.cc:1052] 0x400d2bbe0 Compute status: Not found: ./checkpoints_directory/translate.ckpt-200.tempstate9246663217899500702. I have described the  error  in detail with all the output:
http://stackoverflow.com/questions/33772819/tensorflow-error-on-running-the-seq2seq-model

The other language model example is working and the library has also been built. As per comments I created the checkpoint directory , still throwing the same error: tensorflow/core/common_runtime/executor.cc:1052] 0x400d2bbe0 Compute status: Not found: ./checkpoints_directory/translate.ckpt-200.tempstate9246663217899500702
"
567,26501,0,"How to use libtensorflow-lite.a and tflite model on Raspi 3?. 1. I want to install TensorFlow Lite on the Raspi. 
I'm reading the instructions to cross compile TensorFlow Lite https://www.tensorflow.org/lite/guide/build_rpi?hl=ru, but I have no idea what to do after generating libtensorflow-lite.a .

2. Also, I have downloaded and converted mobilenet_v1_1.0_224.pb to .fb format.
But how do I use the .fb model to detect any object?
"
1042,21363,0,"tf.clip_by_global_norm() returns zero tensors if the norm is infinity. I think this should return NaN tensors instead of zero tensors to signal an error. Users expect  to return tensors unaltered or with global norm matching the threshold passed into the function. This invariant is violated by returning zero tensors.

This is caused by  which is zero for an infinite norm in [clip_ops.py](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/python/ops/clip_ops.py#L257):



Boilerplate:

- Have I written custom code: No
- OS Platform and Distribution: N/A
- TensorFlow installed from: N/A
- TensorFlow version: master
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: N/A
- Mobile device: N/A"
166,3391,1,"cudnn5  so slow. I updated cudnn from 4 to 5, and reinstall tensorflow from source.
But I found that my code run almost 6 time slower than before.
"
442,31973,0,"[TF 2.0.0-rc0] Cannot find any 2.0.0 RC0 API references.. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 and macOS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-rc0 (both GPU and CPU versions)
- Python version: 3.6.8

**Describe the current behavior**
Not a single reference to members of tensorflow can be found in IDE(PyCharm). 
However, programs run as expected.

**Describe the expected behavior**
References to members of tensorflow can be found.

<img width=""602"" alt=""Screen Shot 2019-08-26 at 17 13 30"" src=""https://user-images.githubusercontent.com/6904036/63680285-e1412980-c825-11e9-89c5-8f8abc376275.png"">

"
1161,27357,0,"-D_GLIBCXX_DEBUG compiler flag causes prediction failure. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**


- OS Platform and Distribution Linux Ubuntu 18.04
- TensorFlow installed from source: v1.13.1
- Bazel version 0.21
- GCC/Compiler version: gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04) 

**Describe the current behavior**

-D_GLIBCXX_DEBUG compiled example code gives:

Invalid argument: Must specify at least one target to fetch or execute.

Compiled without this flag:

Run session successfully
Tensor<type: float shape: [] values: 6>
output value: 6


**Code to reproduce the issue**

#include <tensorflow/core/platform/env.h>
#include <tensorflow/core/public/session.h>
#include <iostream>

int main(int argc, char **argv)
{

    tensorflow::GraphDef GraphDef;
    tensorflow::Session* Session = nullptr;
    tensorflow::Status Status;

    Status = tensorflow::ReadBinaryProto(tensorflow::Env::Default(), argv[1], &GraphDef);
    if (!Status.ok())
    {
      printf(""Error reading graph definition from %s: %s\n"", argv[1], Status.ToString().c_str());
      return false;
    }

    Session = tensorflow::NewSession(tensorflow::SessionOptions());
    if (Session == nullptr)
    {
      printf(""Could not create Tensorflow session.\n"");
      return false;
    }

    Status = Session->Create(GraphDef);
    if (!Status.ok())
    {
      printf(""Error creating graph: %s\n"", Status.ToString().c_str());
      return false;
    }

    //predict

    tensorflow::Tensor a(tensorflow::DT_FLOAT, tensorflow::TensorShape());
    a.scalar<float>()() = 3.0;

    tensorflow::Tensor b(tensorflow::DT_FLOAT, tensorflow::TensorShape());
    b.scalar<float>()() = 2.0;

    std::vector<std::pair<std::string, tensorflow::Tensor>> inputs = {
      { ""a"", a },
      { ""b"", b },
    };

    std::vector<tensorflow::Tensor> outputs;

    auto status = Session->Run(inputs, {""c""}, {}, &outputs);
    if (!status.ok()) {
      std::cerr << status.ToString() << std::endl;
      return 1;
    } else {
      std::cout << ""Run session successfully"" << std::endl;
    }

    auto output_c = outputs[0].scalar<float>();
    std::cout << outputs[0].DebugString() << std::endl;
    std::cout << ""output value: "" << output_c() << std::endl;
    Session->Close();

    return 0;
}

**Other info / logs**

The simple graph file attached - supply its path as a command line argument
[graph.pb.gz](https://github.com/tensorflow/tensorflow/files/3027851/graph.pb.gz)"
1306,9833,0,"New seq2seq interface(basic_decoder) does not support sampled softmax?. basic_decoder init function has param output_layer which must be a type of layer like Dense.
But for using sampled softmax we need some code like below, as w_t and v is needed by tf.nn.sampled_softmax_loss 
      with tf.variable_scope('output_projection'):
          self.w_t = melt.variable.get_weights_truncated('w',   
                                               [vocab_size, num_units],   
                                               stddev=FLAGS.weight_stddev)   
          #weights  
          self.w = tf.transpose(self.w_t)    
          #biases  
          self.v = melt.variable.get_weights_truncated('v',     
                                              [vocab_size], 
                                              stddev=FLAGS.weight_stddev)   
for old seq2seq interface, we can just pass output_function like 
      def output_fn(output):  
          return tf.nn.xw_plus_b(output, self.w, self.v )  

How can we do sampled softmax with seq2seq new internface ?"
1046,5587,0,"iOS No OpKernel to support TruncatedNormal. I have a NN similar to this one: http://stackoverflow.com/a/38576462/828184 and my iOS project is based on the simple contrib example.

I write out the graph.pb file like so (and replace it with the original iOS one):


But XCode crashes on execution with this error:

> RunModelViewController.mm: Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'TruncatedNormal' with these attrs.  Registered kernels:
>   <no registered kernels>
> 	 [[Node: OutputLayer/truncated_normal/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0](OutputLayer/truncated_normal/shape)]]

I guess it's because of the usage of . Is there an alternative to that call or am I doing something wrong?

So far I got a minimalistic multiplication graph working on iOS but no trained NN."
873,12205,0,"BUG: TypeError in DNNClassifier.eval() when using same name for feature in feature_engineering_fn. ### Describe the problem

If we use the  same key to replace a feature, tensorflow might throw TypeError when evaluating:

eg:


When  is , it is a mutable object. Hence the bug is caused by  method which runs  again, see [code](https://github.com/facaiy/tensorflow/blob/c7b80d51da4fb6d51ea54a0bdf2601afa379d60c/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L1165).

I'll open a PR later.
"
42,23634,1,"Incorrect loss calculation after v1.10: fitting tf.Dataset in Keras.. In 1.12, we can pass  into our Keras models. For instance, I'm modelling a binary classification with high class imbalance. 


where  is changed from 1 to 10000, with no affect on the loss.
Create 



But when I fit the model using Keras, no matter what weight is passed, it seems ignored.

E.g.


Interestingly, if I pass  to the , then I get the message that both  and  are provided. So why doesn't the loss change?

The loss is incredibly small and it doesn't change with weight.

Epoch 1/30
2018-11-09 16:24:10.605651: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 461/9375 [>.............................] - ETA: 2:01:50 - loss: 53.4499 - binary_accuracy: 0.8183 - binary_crossentropy: 0.4290 - mean_absolute_error: 0.3171


## Sidenote
When I used numpy arrays with  class as a generator, using , the loss was also not changing properly. 


### Takeaways

Clearly, the weight is not passed to the loss function correctly. Even if I use  the loss differs in tensor flow > 1.10. Additionally, workers must be set to 0 i.e.  for the generator to work which is incredibly slow during training."
100,29910,1,"Android performance: CPU affinity. **System information**
- Mobile device: **Xiaomi A2 (8 core)**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **master** at [6cf83ea] (2019/06/17)

**Describe the current behavior**

The  that we run on the device performs very differently when we choose CPU affinity. E.g. average inference time (32 bit, CPU only) for our custom .tflite model is 172ms for taskset=ff, 143ms for taskset=f0, and 281ms taskset=0f. We used num_threads=8, i.e. much more threads that we have cores available. When we increase num_threads=12, inference time further decreases. The change of min times is even more impressive: 125ms for ff, 114ms for f0, and 211ms for 0f. And 105ms for f0 with 12 threads, i.e. 3 threads per CPU. Further increasing num_threads does not deliver visible improvements.

Note that we have no control on the actual number of threads used by interpreter->Invoke(), and their CPU affinity. 

**Describe the expected behavior**

I would expect the _interpreter_ to choose the optimal threading model. The [document](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#reducing-variance-between-runs-on-android) reads:

> Reducing variance between runs on Android.
> When running benchmarks on these phones there can be significant variance between different runs of the benchmark.

While benchmarks are nice, our real necessity is to run TFLite optimally in our app. But we cannot control thread CPU affinity or the _interpreter_?

But even if we could setup the _interpreter_ threading configuration beyond the generic _""Set the number of threads available to the interpreter""_,  across multiple devices, choosing the optimal taskset is beyond the capabilities of most development teams. This cannot be done by analyzing the cpuinfo: on our A2 development phone, all 8 cores are declared [almost](https://github.com/tensorflow/tensorflow/issues/29910#issuecomment-503589179) equal:


Maybe I am missing something, but I see here no clue that the performance of **f0** will be so much different from **0f**.

The dynamic optimization should be performed once in a while, because the load on the device may change, either in the same process, or because of other processes/apps running along our app.

It could be useful to persist these findings, so that next time interpreter starts, it could have a reasonable starting point. I am not sure if this info is relevant for specific .tflite model, or for any model. In our experiments, the we only used similar FCNN networks, and their performance was effected by taskset just the same."
620,23918,0,"tf.dynamic_partition may cause NaN loss when use it with multi gpus and it performs normally with single gpu. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **_no_**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _**Ubuntu 16.04.5 LTS**_
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**_no_**
- TensorFlow installed from (source or binary):**_binary_**
- TensorFlow version (use command below):**_1.12.0_**
- Python version: **_Python 3.5.2_**
- Bazel version (if compiling from source): _**None**_
- GCC/Compiler version (if compiling from source):  **_5.4.0_**
- CUDA/cuDNN version: **_cuda-9.0.176/cudnn-7.2.1_**
- GPU model and memory: **_2 same GTX Titan X (Pascal), 12GB_** 


I just use the offical docker image of the tensorflow, the tag is 

**Describe the problem**

**The api  may cause NaN loss when use it with multi gpus and it is normal with single gpu.**

The code to reproduce the problem:


When I use two gpus, I set 





When I use one gpus, I set 
Then I get


I think the implementation of the  api  is really terrible, 
I also reported that this api may cuase memory leak under certain situation
https://github.com/tensorflow/tensorflow/issues/22464

It also cost me lots of time to find that it is not suitable with mulit gpus.....
I think there may other potential issues about the  .
And I don't figure out why I still use it......

"
480,31318,0,"InvalidArgumentError: Cannot assign a device for operation embedding_1. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I have written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS version: **Fedora 29.5.1.18 (also tested Ubuntu 18.10)**
- TensorFlow installed from (source or binary): **tensorflow/tensorflow:latest-gpu-py3-jupyter**
- TensorFlow version (use command below): **1.14.0**
- Python version: **3.6.8**
- CUDA/cuDNN version: **10.0.130**
- GPU model and memory: **GeForce RTX 2080 ti, 11 Gb**

**Describe the current behavior**
I'm using keras. I try to to fit model that contains **Embedding** layer. When I call  I get an error: 

_InvalidArgumentError: Cannot assign a device for operation embedding/embeddings/Initializer/random_uniform/sub: Could not satisfy explicit device specification '' because the node {{colocation_node embedding/embeddings/Initializer/random_uniform/sub}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
Identity: GPU CPU XLA_CPU XLA_GPU 
Const: GPU CPU XLA_CPU XLA_GPU 
ResourceSparseApplyRMSProp: CPU 
RandomUniform: GPU CPU XLA_CPU XLA_GPU 
ReadVariableOp: GPU CPU XLA_CPU XLA_GPU 
Sub: GPU CPU XLA_CPU XLA_GPU 
Add: GPU CPU XLA_CPU XLA_GPU 
Mul: GPU CPU XLA_CPU XLA_GPU 
VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU 
VarHandleOp: GPU CPU XLA_CPU XLA_GPU 
AssignVariableOp: GPU CPU XLA_CPU XLA_GPU 
ResourceGather: GPU CPU XLA_CPU XLA_GPU_ 

ResourceSparseApplyRMSProp looks strange for me.

After getting this error I cannot fit new (simplified) model, because I get this error again. I get this error even I run 

**Describe the expected behavior**
Model fits without any problems, like the same model without Embedding layer (no _dest_input_).
**Code to reproduce the issue**



**Other info / logs**
Some times I get this error on simplified model:


Probably, the issue occuring depends on CPU usage.

I'm attaching full log.

[gpu_error.txt](https://github.com/tensorflow/tensorflow/files/3464780/gpu_error.txt)
"
326,18111,0,"importing tensorflow.contrib produces a warning. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows and Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**:
- **Exact command to reproduce**:
python
import tensorflow.contrib

### Describe the problem
Using tensorflow.contrib produces a warning in 1.7. The warning is

WARNING:tensorflow:From ...\envs\tf-1.7\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives."
1001,29806,0,"TensorFlow Lite: undefined reference to `flatbuffers::ClassicLocale::instance_'. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.13
- Python version: 3.5
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I was following the [instructions](https://www.tensorflow.org/lite/guide/build_rpi) to crosscompile TensorFlow Lite for my Raspberry Pi. I got the error messages showing the undefined reference to flatbuffers. 
I believe all the dependencies for Lite are downloaded by the  script, but I do not have Bazel installed. Is Bazel necessary for compiling TensorFlow Lite?


**Any other info / logs**
tflite::ops::custom::while_kernel::Init(TfLiteContext*, char const*, unsigned int)':
while.cc:(.text+0x1648): undefined reference to tflite::ops::custom::audio_spectrogram::Init(TfLiteContext*, char const*, unsigned int)':
audio_spectrogram.cc:(.text+0xe0c): undefined reference to tflite::ops::custom::detection_postprocess::Init(TfLiteContext*, char const*, unsigned int)':
detection_postprocess.cc:(.text+0x211c): undefined reference to flexbuffers::Reference::AsInt64() const':
detection_postprocess.cc:(.text._ZNK11flexbuffers9Reference7AsInt64Ev[_ZNK11flexbuffers9Reference7AsInt64Ev]+0x264): undefined reference to tflite::ops::custom::if_kernel::Init(TfLiteContext*, char const*, unsigned int)':
if.cc:(.text+0xf8c): undefined reference to flatbuffers::ClassicLocale::instance_' follow
collect2: error: ld returned 1 exit status
tensorflow/lite/tools/make/Makefile:267: recipe for target '/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/minimal' failed
make: *** [/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/minimal] Error 1
make: *** Waiting for unfinished jobs....
`
"
1191,28001,0,"Tensorflow (cpu) vs concurrent.futures: Exception . I am using Tensorflow v2.0.0-alpha on MacBook (CPU only). Python version: 3.7.2.

I am testing a rl agent on multiple environments. For this I am using concurrent.futures.ProcessPoolExecutor.map function. Roughly around the time when first processes are done, exception occurs:


"
165,25405,1,"FP16 profermance on GTX1080. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows10
- TensorFlow installed from (source or binary):
Binary(PIP install)
- TensorFlow version (use command below):
1.12.0
- Python version:
3.6.2
- CUDA/cuDNN version:
CUDA 9.0 / cuDNN 7.3.1
- GPU model and memory:
GeForce GTX 1080, 8GB

**Describe the current behavior**
I try to evaluate the accuracy of fp16 precision on GTX 1080 GPU. As we known, GTX1080 doesn't have
tensor core to supporting FP16 calc, but my test reuslt shows that inference with fp16 is faster than fp32 by 20%-30%, could you give an explanation? 

Here is my operation step:
1. train a model using fp32 precision and save the checkpoint.
2. in order to inference with fp16, load the checkpoint and convert each param to fp16 type

3. rewrite the network and pass the fp16 params to initialize each layer, then run the network.


I got a approximate accuracy from fp16 reuslts, and its faster than fp32 by 20%-30%. In order to verify the speed of fp16, I construct a simple net with several dense layers, which are initialzed with fp16 random values, and run the forward procedure only. But fp16 is slower than fp32. ...

**Question**
1. Why FP16 is faster on a Pascal GTX GPU, what hardware units were used?
2. Is there any better way to inference with fp16 using a pretrained fp32 model?
3. If the TensorRT is the normal method to inference with fp16?
4. If using RTX 2080 with tensor core, we will get a dramatic performance in fp16?
"
796,34812,0,"The checkpoint generated by tensorflow2.0 training does not have a meta file. How should it be converted to a pb file?. When T1, the meta file is converted to a pb file, and then the pb file is converted to a tflite file. Now T2 does not generate a meta file, what should I do? Can anyone help me?"
676,19343,0,"AttributeError: module 'tensorflow.contrib' has no attribute 'distribute'. When I include mirroring strategy using following code -

# Build the Estimator
distribution = tf.contrib.distribute.MirroredStrategy()
config = tf.estimator.RunConfig(train_distribute=distribution)
model = tf.estimator.Estimator(model_fn,config=config)

I get the error - AttributeError: module 'tensorflow.contrib' has no attribute 'distribute'

I tried this with both Tensorflow 1.7 and 1.8 - I get the same issue. I am using Python 3.6 https://colab.research.google.com/ .
"
1371,2242,0,"Silent fail when network is big. ### Steps to reproduce
1. Build a 10 layer hidden network with 2400 neurons in each layer 
2. Have data with correlation
3. Accuracy will be 1 / n_classes_outputs for each training step. 
### What have you tried?
1. If you will have less neurons and layers, everything works

I presume that there is a issue with memory and setting weights, but a error message would be nice.
"
746,26854,0,Tensorflow 2.0: where is tf.contrib.layers.layer_norm?. Cannot find tf.contrib.layers.layer_norm in TF 2.0
1464,3574,0,"Numerical differences between self.test_session and regular sessions?. I'm trying to use the test suite to create tests that verify various aspects of my model. Strangely, I'm getting numerical differences between the exact same model run on the exact same machine if I use  inside  versus if I just run the same piece of code in a regular session. Most models work fine, it's just that certain flavors that use dropout show big discrepancies, usually after 4 or 5 steps of training. Is this expected behavior? If so what's the cause of the discrepancy and can it be removed?
"
268,29675,1,"Adam/Adagrad gives different derivatives when initializing globaly and localy. 
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (2.0):
- Python version:(3.6)


**Describe the current behavior**
Adam and Adagrad, both gives different losses, when initialized once (globally) and initialized every time train function is called. 




**Describe the expected behavior**
We expect same loss from both train and train2. ( Is it because, some internal parameters are changing inside the optimizer, because SGD optimizer works fine in both the cases).


**Code to reproduce the issue**
The plot of derivatives are provided in the image . Full code to reproduce is in the .ipynb file.
![tf](https://user-images.githubusercontent.com/10637096/59318400-9151cc80-8ce4-11e9-9b65-1712a86d2218.png)

"
1089,18225,0,"Distributed TensorFlow: All works use the same GPU device on host with 4 GPUs. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution **: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip2.7 install
- **TensorFlow version (use command below)**: 
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
tf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514
- **Python version**: python2.7
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**:  cuda_8.0.61_375.26_linux.run/cudnn-8.0-linux-x64-v6.0.tgz
- **GPU model and memory**: TITAN Xp, 12189MiB
- **Exact command to reproduce**:
- **Cuda libs**:
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7

### Describe the problem
All the 4 works use the same GPU device(GPU:0) when running a distributed training on a host with 4 GPUs.

### Source code / logs
**The distributed runner:**


**Main python script to use the runner:**


**Shell script to trigger the main script:**
date  +""%Y%m%d"""
598,8820,0,"Feature Request: Accelerate TensorFlow core on FPGA - How?. Consider the two following hardware scenarios:
1) Linux running on x86 w/ FPGA fabric connected via PCIe
2) Linux running on Arm A53 with AXI i/f to FPGA fabric (Think Xilinx Zynq)

How could the tensorflow core be accelerated for these scenarios? 
FPGA vendors do offer OpenCL binaries for running OpenCL APIs to parallelize computations.

Forgive me, I am a bit ignorant, still learning in this area, but I am intrigued by the future possibility of this, and would love to help in anyway I can."
248,26068,1,"Incorrect predictions while exporting keras model to android. I trained the Keras model on State Farm Distracted Driver Dataset and exported it to **.pb** graph file with the code given below:



 The problem is when I run that exported model on android, it always predicts wrong classes. The same code below predicts the correct labels when given .pb file of model trained on MNIST Dataset from [source.](https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android/blob/master/mnistandroid/app/src/main/assets/opt_mnist_convnet-keras.pb).


**ModelClassifier.java** class is given below:


**Note that model input shape is [100,100,1]**

Please tell me where I am making mistake. I have tried training on different number of classes but the outputs are completely different. 




"
389,11638,0,"Probably wrong implementation for tf.layers.max_pooling1d when data_format='channels_first'. In function call of class _Pooling1D,  when the input data_format='channels_first', it should transform input tensor from '**N,C,H**' to '**N,C,H,W**' (batch_size, channels, height, width), meaning that we should expand dimension on the last dimension. 

However, in the code we use inputs = array_ops.expand_dims(inputs, 1), expanding on the second dimension and transforming from '**N,C,H**' to '**N,1,C,H**'. Then the pool_shape and strides are looking at the third dimension, which is not consistant with our expand_dims(inputs, 1) used before.

I think the code should be changed to inputs = array_ops.expand_dims(inputs, -1) and return array_ops.squeeze(outputs, -1). Using **-1** will expand and squeeze on the last dimension, transforming from '**N,C,H**' to **'**N,C,H,1**', and then doing pool_shape and strides on the third dimension.

### Source Code
------------------------


"
1377,20510,0,"Memory Leak. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
Anaconda
- **TensorFlow version (use command below)**:
b'unknown' 1.8.0
CuDNN - None
GPU-Integrated
### Describe the problem
Memory Leak when running my code 

### Source code / logs















  

  





   






 
 



 
  
 
  











    
 
  
 











  





  
  
           





 



  

  

  
 
  
 
      

  












 






"
1216,14072,0,"Feature request: support convert python object to tensor automatical, and can back to object in py_func or other mechod.. I want to use python object as tensor in Tensorflow and can convert it back to object when useing tf.py_func method,  to  support using other python package."
1416,11542,0,"Tensorflow feed_dict issue. I am new to tensorflow. I understand that we need to create the tensorflow graph and then call sess.run() to get the values I want.

However, I am confused by how the feed_dict works. 
For example:
Will 1 be the same as 2?

I don't know that if tensorflow receives the same feed_dict in cost and in tensorflow graph computing accuracy already computes cost, do it go over the neural net again to evaluate the value, or it will return the value computed without going through the net again?

Also, from [Hvass-Labs/TensorFlow-Tutorials/TensorFlow Tutorial #02 Convolutional Neural Network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb), in function plot_conv_weights(weights, input_channel=0)

Since training weights require we fill the placeholder X and Y with values, but here I saw no feed_dict.

So how exactly feed_dict works?"
1067,17730,0,"freeze_graph.py of tensorflow1.5 without ""--restore_op_name"" function. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0/7.1
- **GPU model and memory**:navidia GTX 1080Ti
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I use freeze_graph.py of tensorflow1.5 to convert model,but i want to use ""--restore_op_name"".
Is there this function for this version or any other method recommended?
Thanks!

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

My convert command as following:
python freeze_graph.py 
--input_graph=D:\mywork\Re3\demo\simple_as_binary.pb --input_binary=True --input_checkpoint=D:\mywork\Re3\demo\simple.ckpt.data-00000-of-00001 --output_node_names=re3/fc_output/add,re3/lstm1/rnn/while/Exit_3,re3/lstm1/rnn/while/Exit_4,re3/lstm2/rnn/while/Exit_3,re3/lstm2/rnn/while/Exit_4 --restore_op_name=re3/conv1/W_conv,re3/conv1/b_conv,re3/conv1_skip/W_conv,re3/conv1_skip/b_conv,re3/conv1_skip/prelu,re3/conv2/W_conv,re3/conv2/b_conv,re3/conv2_skip/W_conv,re3/conv2_skip/b_conv,re3/conv2_skip/prelu,re3/conv3/W_conv,re3/conv3/b_conv,re3/conv4/W_conv,re3/conv4/b_conv,re3/conv5/W_conv,re3/conv5/b_conv,re3/conv5_skip/W_conv,re3/conv5_skip/b_conv,re3/conv5_skip/prelu,re3/fc6/W_fc,re3/fc6/b_fc,re3/fc_output/W_fc,re3/fc_output/b_fc,re3/lstm1/rnn/LSTM/block_input/biases,re3/lstm1/rnn/LSTM/block_input/weights,re3/lstm1/rnn/LSTM/forget_gate/biases,re3/lstm1/rnn/LSTM/forget_gate/weights,re3/lstm1/rnn/LSTM/input_gate/biases,re3/lstm1/rnn/LSTM/input_gate/weights,re3/lstm1/rnn/LSTM/output_gate/biases,re3/lstm1/rnn/LSTM/output_gate/weights,      re3/lstm2/rnn/LSTM/block_input/biases,re3/lstm2/rnn/LSTM/block_input/weights,re3/lstm2/rnn/LSTM/forget_gate/biases,re3/lstm2/rnn/LSTM/forget_gate/weights,re3/lstm2/rnn/LSTM/input_gate/biases,re3/lstm2/rnn/LSTM/input_gate/weights,re3/lstm2/rnn/LSTM/output_gate/biases,re3/lstm2/rnn/LSTM/output_gate/weights 
--output_graph=frozen_model.pb"
1300,21866,0,"tensorflow 1.10.1 requires numpy 1.14.5?. I have successfully built tensorflow  1.10.1 from source but when  I installed the whl with pip3 it downloaded and installed numpy-1.14.5 and removed numpy-1.15.1. 

After tensorflow was installed I manually uninstalled numpy-1.14.5 and reinstalled numpy-1.15.1, pip gave a warning that tensorflow-1.10.1 requires numpy <= 1.14.5 but went ahead anyway. Afterwards I ran some tests on tensorflow and it works just fine with numpy 1.15.1 so this requirement appears to be unnecessary. **Edited:** there is no complaint about incompatibility with numpy-1.15.1 for tensorflow-1.9

Is there a file I can edit to get rid of this requirement? I am ok with compiling again.

OS Ubuntu 16.04.5 LTS 64 bits , python3.5 (cuda-9.2)
"
1051,6863,0,"Protobuf import issue. I am trying to run tensorboard but I am getting the following import error on mac osx;



I have tried reinstalling setuptools and distribute. This is coming up as an error in python 3.5 even though my tensorflow is installed in 2.7; I.e. 

"
1027,23520,0,"Install from pip, run test and results in ""illegal instruction"". 
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9 AMD 64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Intel Laptop
- TensorFlow installed from (source or binary): pip per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)
- TensorFlow version: 1.11.0
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: venv per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip) 
- Bazel version (if compiling from source):  N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- set up venv and activate
- pip install tensorflow per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)
- verify install per [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)
- import of tensorflow causes ""illegal instruction"" raised."
807,35195,0,"Tensorflow not detecting/recognizing GPU. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: PC
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 2.0.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip/conda (both, neither work)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7
- GPU model and memory: NVIDIA GeForce 940M



I'm using an NVIDIA GeForce 940M, and have followed the instructions to install Tensorflow GPU exactly as given [here](https://www.tensorflow.org/install/gpu):

    # Add NVIDIA package repositories
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
    sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
    sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
    sudo apt-get update
    wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
    sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
    sudo apt-get update
    
    # Install NVIDIA driver
    sudo apt-get install --no-install-recommends nvidia-driver-418
    # Reboot. Check that GPUs are visible using the command: nvidia-smi
    
    # Install development and runtime libraries (~4GB)
    sudo apt-get install --no-install-recommends \
        cuda-10-0 \
        libcudnn7=7.6.2.24-1+cuda10.0  \
        libcudnn7-dev=7.6.2.24-1+cuda10.0
    # Install TensorRT. Requires that libcudnn7 is installed above.
    sudo apt-get install -y --no-install-recommends libnvinfer5=5.1.5-1+cuda10.0 \
        libnvinfer-dev=5.1.5-1+cuda10.0

This is the output of :

    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |===============================+======================+======================|
    |   0  GeForce 940M        On   | 00000000:01:00.0 Off |                  N/A |
    | N/A   52C    P0    N/A /  N/A |    238MiB /  4046MiB |      6%      Default |
    +-------------------------------+----------------------+----------------------+
                                                                                   
    +-----------------------------------------------------------------------------+
    | Processes:                                                       GPU Memory |
    |  GPU       PID   Type   Process name                             Usage      |
    |=============================================================================|
    |    0      1027      G   /usr/lib/xorg/Xorg                            24MiB |
    |    0      1184      G   /usr/bin/gnome-shell                          46MiB |
    |    0      1388      G   /usr/lib/xorg/Xorg                           110MiB |
    |    0      1555      G   /usr/bin/gnome-shell                          52MiB |
    +-----------------------------------------------------------------------------+

and :

    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2018 NVIDIA Corporation
    Built on Sat_Aug_25_21:08:01_CDT_2018
    Cuda compilation tools, release 10.0, V10.0.130

and I've installed  using both conda and pip (neither work).

The output of

    from tensorflow.python.client import device_lib
    print(device_lib.list_local_devices())

is:

    [name: ""/device:CPU:0""
    device_type: ""CPU""
    memory_limit: 268435456
    locality {
    }
    incarnation: 9340164754758349370
    , name: ""/device:XLA_CPU:0""
    device_type: ""XLA_CPU""
    memory_limit: 17179869184
    locality {
    }
    incarnation: 3967057350071782501
    physical_device_desc: ""device: XLA_CPU device""
    ]

As seen, tensorflow does not recognize the GPU. What do I do?





"
317,19971,0,"Pd turns tflite to report all kinds of errors. Have you done the test?. Download the network model

MobileNet_v1_1.0_224_quant

https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md
Turn tflite error

bazel run toco -- 
--input_file=/Users/dchealth/Desktop/mobilenet_v1_1.0_224_quant/mobilenet_v1_1.0_224_quant_frozen.pb 
--output_file=/Users/dchealth/Desktop/mobilenet_v1_1.0_224_quant/mobilenetnew.tflite 
--input_format=TENSORFLOW_GRAPHDEF 
--inference_type=FLOAT 
--output_format=TFLITE 
--input_shapes=1,224,224,3 
--input_arrays=input 
--output_arrays=MobilenetV1/Predictions/Reshape_1
--inference_type=QUANTIZED_UINT8
--std_values=128
--mean_values=128
------------------------

System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):mac os 10.13.5

TensorFlow installed from (source or binary):pip

TensorFlow version (use command below):'1.8.0

Python version: 3.6.4

Bazel version (if compiling from source):
Build label: 0.14.0-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jun 1 14:26:58 2018 (1527863218)
Build timestamp: 1527863218
Build timestamp as int: 1527863218

GCC/Compiler version (if compiling from source):no

CUDA/cuDNN version:no

GPU model and memory:no

Exact command to reproduce:no

End of run log:

./tensorflow/contrib/lite/builtin_op_data.h:122:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
./tensorflow/contrib/lite/builtin_op_data.h:125:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
./tensorflow/contrib/lite/builtin_op_data.h:161:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
./tensorflow/contrib/lite/builtin_op_data.h:164:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
./tensorflow/contrib/lite/builtin_op_data.h:203:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
5 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/model.cc:
In file included from tensorflow/contrib/lite/model.cc:25:
./tensorflow/contrib/lite/builtin_op_data.h:122:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
./tensorflow/contrib/lite/builtin_op_data.h:125:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
./tensorflow/contrib/lite/builtin_op_data.h:161:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
./tensorflow/contrib/lite/builtin_op_data.h:164:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
./tensorflow/contrib/lite/builtin_op_data.h:203:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]
typedef struct {
^
5 warnings generated.
INFO: From Linking external/protobuf_archive/libprotobuf.a:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/external/protobuf_archive/libprotobuf.a(gzip_stream.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/external/protobuf_archive/libprotobuf.a(error_listener.o) has no symbols
INFO: From Linking external/protobuf_archive/libprotobuf_lite.a:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/external/protobuf_archive/libprotobuf_lite.a(arenastring.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/external/protobuf_archive/libprotobuf_lite.a(atomicops_internals_x86_msvc.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/external/protobuf_archive/libprotobuf_lite.a(io_win32.o) has no symbols
INFO: From Compiling tensorflow/core/lib/strings/numbers.cc:
tensorflow/core/lib/strings/numbers.cc:394:34: warning: format specifies type 'unsigned long ' but the argument has type 'uint64_t ' (aka 'unsigned long long ') [-Wformat]
if (sscanf(s.c_str(), ""%lx%c"", &result, &junk) == 1) {
~~~ ^~~~~~~
%llx
1 warning generated.
INFO: From Linking tensorflow/core/liblib_internal_impl.a:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/liblib_internal_impl.a(android_armv7a_cpu_utils_helper.o) has no symbols
INFO: From Compiling tensorflow/core/util/command_line_flags.cc:
tensorflow/core/util/command_line_flags.cc:73:37: warning: format specifies type 'long ' but the argument has type 'int64_t ' (aka 'long long ') [-Wformat]
if (sscanf(arg.data(), ""%ld%c"", &parsed_int64, &extra) != 1) {
~~~ ^~~~~~~~~~~~~
%lld
1 warning generated.
INFO: From Compiling tensorflow/core/util/strided_slice_op.cc:
tensorflow/core/util/strided_slice_op.cc:270:33: warning: lambda capture 'i' is not used [-Wunused-lambda-capture]
auto canonical = [stride_i, i, dim_i, masks, valid_range](int64 x, int c) {
^
1 warning generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/propagate_default_min_max.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/propagate_default_min_max.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/dump_graphviz.cc:
In file included from tensorflow/contrib/lite/toco/dump_graphviz.cc:15:
In file included from ./tensorflow/contrib/lite/toco/dump_graphviz.h:20:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_stack.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_stack.cc:17:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/propagate_activation_function_into_constants.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/propagate_activation_function_into_constants.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_merge.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_merge.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_slice_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_slice_attributes.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/propagate_array_data_types.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/propagate_array_data_types.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/tooling_util.cc:
In file included from tensorflow/contrib/lite/toco/tooling_util.cc:15:
In file included from ./tensorflow/contrib/lite/toco/tooling_util.h:31:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_binary.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_binary.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/reorder_elementwise_unary.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_addn_to_add.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_addn_to_add.cc:15:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_fake_quant.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_fake_quant.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_slice.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_slice.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
tensorflow/contrib/lite/toco/graph_transformations/reorder_reshape_transpose.cc:51:6: warning: unused function 'Filter' [-Wunused-function]
void Filter(std::vector vec, int value) {
^
3 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_strided_slice.cc:18:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_tensorflow_assert.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_tensorflow_assert.cc:19:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/experimental_shuffle_fc_weights.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/experimental_shuffle_fc_weights.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/fuse_binary_into_following_affine.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/fuse_binary_into_following_affine.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/drop_im2col_arrays.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/drop_im2col_arrays.cc:15:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/fuse_binary_into_preceding_affine.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/fuse_binary_into_preceding_affine.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_quantized_activation_func.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_quantized_activation_func.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_multiply_by_zero.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_multiply_by_zero.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_fill.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_fill.cc:17:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_shape_or_rank.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_shape_or_rank.cc:15:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_concatenation_input.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_concatenation_input.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/identify_l2_pool.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/identify_l2_pool.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_squeeze_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_squeeze_attributes.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_range.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_range.cc:15:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_concatenation.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_concatenation.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/merge_reshape_into_preceding_transpose.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/merge_reshape_into_preceding_transpose.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_binary.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_binary.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/unroll_batch_matmul.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/unroll_batch_matmul.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_to_space_nd_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_to_space_nd_attributes.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_reshape.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_reshape.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:15:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/identify_lstm_split_inputs.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/identify_lstm_split_inputs.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_stack_to_reshape.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_stack_to_reshape.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/make_initial_dequantize_operator.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/make_initial_dequantize_operator.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_reshape.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_reshape.cc:17:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_passthrough.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_passthrough.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/quantization_util.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/quantization_util.cc:17:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_strided_slice_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_strided_slice_attributes.cc:15:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_transpose.cc:17:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_final_dequantize_op.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_final_dequantize_op.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:23:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:17:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/lstm_utils.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/lstm_utils.cc:15:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/lstm_utils.h:22:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_reorder_axes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_reorder_axes.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/unpartition_embedding_lookup.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/unpartition_embedding_lookup.cc:19:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_unary.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_unary.cc:23:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_mean_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_mean_attributes.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/read_fake_quant_min_max.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/read_fake_quant_min_max.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/create_im2col_arrays.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/create_im2col_arrays.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_concat.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_concat.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/dequantize.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/dequantize.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/drop_fake_quant.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/drop_fake_quant.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:18:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_reshape_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_reshape_attributes.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/convert_pure_conv_to_depthwise.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/convert_pure_conv_to_depthwise.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/identify_l2_normalization.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/identify_l2_normalization.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/identify_lstm_merge_inputs.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/identify_lstm_merge_inputs.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/identify_prelu.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/identify_prelu.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/identify_relu1.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/identify_relu1.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_unused_op.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_unused_op.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/unfuse_activation_functions.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/unfuse_activation_functions.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/fuse_activation_functions.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/fuse_activation_functions.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/propagate_fake_quant_num_bits.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/propagate_fake_quant_num_bits.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_pad_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_pad_attributes.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_fake_quant.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_fake_quant.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/identify_lstm.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/identify_lstm.cc:19:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_gather.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_gather.cc:17:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_transpose_to_reshape.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/convert_trivial_transpose_to_reshape.cc:17:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_space_to_batch_nd_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_space_to_batch_nd_attributes.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_matmul.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_matmul.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_padv2_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_padv2_attributes.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/convert_squeeze_to_reshape.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/convert_squeeze_to_reshape.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_concatenation.cc:22:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_tile.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_tile.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_tensorflow_identity.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_tensorflow_identity.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/ensure_bias_vectors.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/ensure_bias_vectors.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:24:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_transpose_attributes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_transpose_attributes.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:18:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/convert_expanddims_to_reshape.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/convert_expanddims_to_reshape.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/convert_reorder_axes.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/convert_reorder_axes.cc:21:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_quantized_min_max.cc:
In file included from tensorflow/contrib/lite/toco/graph_transformations/remove_trivial_quantized_min_max.cc:20:
In file included from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/tensorflow_graph_matching/cluster.cc:
In file included from tensorflow/contrib/lite/toco/tensorflow_graph_matching/cluster.cc:15:
In file included from ./tensorflow/contrib/lite/toco/tensorflow_graph_matching/cluster.h:21:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_cluster.cc:
In file included from tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_cluster.cc:15:
In file included from ./tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_cluster.h:22:
In file included from ./tensorflow/contrib/lite/toco/tensorflow_graph_matching/cluster.h:21:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_svdf.cc:
In file included from tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_svdf.cc:15:
In file included from ./tensorflow/contrib/lite/toco/tensorflow_graph_matching/resolve_svdf.h:21:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/tflite/types.cc:
In file included from tensorflow/contrib/lite/toco/tflite/types.cc:15:
In file included from ./tensorflow/contrib/lite/toco/tflite/types.h:19:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/tflite/import.cc:
In file included from tensorflow/contrib/lite/toco/tflite/import.cc:15:
In file included from ./tensorflow/contrib/lite/toco/tflite/import.h:19:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
tensorflow/contrib/lite/toco/tflite/import.cc:168:6: warning: unused function 'Verify' [-Wunused-function]
bool Verify(const void buf, size_t len) {
^
3 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/tflite/export.cc:
In file included from tensorflow/contrib/lite/toco/tflite/export.cc:15:
In file included from ./tensorflow/contrib/lite/toco/tflite/export.h:18:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/tflite/operator.cc:
In file included from tensorflow/contrib/lite/toco/tflite/operator.cc:15:
In file included from ./tensorflow/contrib/lite/toco/tflite/operator.h:20:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/core/kernels/priority_queue.cc:
In file included from tensorflow/core/kernels/priority_queue.cc:25:
In file included from ./tensorflow/core/kernels/priority_queue.h:27:
./tensorflow/core/kernels/typed_queue.h:83:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::deque& sq) {
^
./tensorflow/core/kernels/typed_queue.h:91:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::vector& sq) {
^
2 warnings generated.
INFO: From Compiling tensorflow/core/kernels/fifo_queue.cc:
In file included from tensorflow/core/kernels/fifo_queue.cc:26:
In file included from ./tensorflow/core/kernels/fifo_queue.h:26:
./tensorflow/core/kernels/typed_queue.h:91:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::vector& sq) {
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/padding_fifo_queue.cc:
In file included from tensorflow/core/kernels/padding_fifo_queue.cc:26:
In file included from ./tensorflow/core/kernels/padding_fifo_queue.h:27:
In file included from ./tensorflow/core/kernels/fifo_queue.h:26:
./tensorflow/core/kernels/typed_queue.h:91:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::vector& sq) {
^
1 warning generated.
INFO: From Compiling tensorflow/core/framework/reader_base.cc:
tensorflow/core/framework/reader_base.cc:205:17: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
context, [this, context, &n, &work](const QueueInterface::Tuple& tuple) {
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/initializable_lookup_table.cc:
In file included from tensorflow/core/kernels/initializable_lookup_table.cc:16:
./tensorflow/core/kernels/initializable_lookup_table.h:54:10: warning: 'ExportValues' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
Status ExportValues(OpKernelContext context) {
^
./tensorflow/core/framework/lookup_interface.h:74:18: note: overridden virtual function is here
virtual Status ExportValues(OpKernelContext ctx) = 0;
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/lookup_util.cc:
In file included from tensorflow/core/kernels/lookup_util.cc:16:
In file included from ./tensorflow/core/kernels/lookup_util.h:21:
./tensorflow/core/kernels/initializable_lookup_table.h:54:10: warning: 'ExportValues' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
Status ExportValues(OpKernelContext context) {
^
./tensorflow/core/framework/lookup_interface.h:74:18: note: overridden virtual function is here
virtual Status ExportValues(OpKernelContext ctx) = 0;
^
1 warning generated.
INFO: From Linking tensorflow/core/kernels/libfused_batch_norm_util_gpu.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libfused_batch_norm_util_gpu.lo(fused_batch_norm_op.cu.o) has no symbols
warning: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: warning for library: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libfused_batch_norm_util_gpu.lo the table of contents is empty (no object file members in the library define global symbols)
INFO: From Compiling tensorflow/core/kernels/boosted_trees/prediction_ops.cc:
tensorflow/core/kernels/boosted_trees/prediction_ops.cc:115:41: warning: lambda capture 'batch_size' is not used [-Wunused-lambda-capture]
&output_node_ids, batch_size,
^
tensorflow/core/kernels/boosted_trees/prediction_ops.cc:225:21: warning: lambda capture 'batch_size' is not used [-Wunused-lambda-capture]
batch_size, latest_tree](int32 start, int32 end) {
^
2 warnings generated.
INFO: From Compiling tensorflow/core/kernels/collective_ops.cc:
tensorflow/core/kernels/collective_ops.cc:143:28: warning: lambda capture 'col_exec' is not used [-Wunused-lambda-capture]
auto actual_done = [c, col_exec, done](const Status& s) {
^
tensorflow/core/kernels/collective_ops.cc:197:28: warning: lambda capture 'col_exec' is not used [-Wunused-lambda-capture]
auto actual_done = [c, col_exec, done](const Status& s) {
^
tensorflow/core/kernels/collective_ops.cc:247:28: warning: lambda capture 'col_exec' is not used [-Wunused-lambda-capture]
auto actual_done = [c, col_exec, done](const Status& s) {
^
3 warnings generated.
INFO: From Linking tensorflow/core/kernels/libself_adjoint_eig_v2_op.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libself_adjoint_eig_v2_op.lo(self_adjoint_eig_v2_op_gpu.o) has no symbols
INFO: From Linking tensorflow/core/kernels/libcudnn_rnn_kernels.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libcudnn_rnn_kernels.lo(cudnn_rnn_ops.o) has no symbols
warning: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: warning for library: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libcudnn_rnn_kernels.lo the table of contents is empty (no object file members in the library define global symbols)
INFO: From Compiling tensorflow/contrib/tensorboard/db/summary_db_writer.cc:
tensorflow/contrib/tensorboard/db/summary_db_writer.cc:812:22: warning: private field 'meta_' is not used [-Wunused-private-field]
RunMetadata* const meta_;
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/sdca_internal.cc:
tensorflow/core/kernels/sdca_internal.cc:49:9: warning: suggest braces around initialization of subobject [-Wmissing-braces]
Eigen::IndexPair(1, 0)};
^~~~~~~~~~~~~~~~~~~~~~~~~~~
{ }
tensorflow/core/kernels/sdca_internal.cc:210:11: warning: suggest braces around initialization of subobject [-Wmissing-braces]
Eigen::IndexPair(1, 1)};
^~~~~~~~~~~~~~~~~~~~~~~~~~~
{ }
2 warnings generated.
INFO: From Linking tensorflow/core/kernels/libgather_functor.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libgather_functor.lo(gather_functor.o) has no symbols
warning: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: warning for library: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libgather_functor.lo the table of contents is empty (no object file members in the library define global symbols)
INFO: From Compiling tensorflow/core/kernels/adjust_saturation_op.cc:
tensorflow/core/kernels/adjust_saturation_op.cc:196:12: warning: lambda capture 'channel_count' is not used [-Wunused-lambda-capture]
[channel_count, &input_data, &output_data, scale_h](
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/adjust_hue_op.cc:
tensorflow/core/kernels/adjust_hue_op.cc:219:12: warning: lambda capture 'channel_count' is not used [-Wunused-lambda-capture]
[channel_count, &input_data, &output_data, delta_h](
^
1 warning generated.
INFO: From Linking tensorflow/core/kernels/libconcat_lib.a:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libconcat_lib.a(concat_lib_gpu.o) has no symbols
INFO: From Linking tensorflow/core/kernels/libscatter_functor.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libscatter_functor.lo(scatter_functor.o) has no symbols
warning: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: warning for library: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libscatter_functor.lo the table of contents is empty (no object file members in the library define global symbols)
INFO: From Linking tensorflow/core/kernels/libscatter_nd_op.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libscatter_nd_op.lo(scatter_nd_op_cpu_impl_0.o) has no symbols
INFO: From Compiling tensorflow/core/common_runtime/collective_rma_local.cc:
In file included from tensorflow/core/common_runtime/collective_rma_local.cc:15:
./tensorflow/core/common_runtime/collective_rma_local.h:37:8: warning: 'StartAbort' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
void StartAbort(const Status& s);
^
./tensorflow/core/framework/collective.h:305:16: note: overridden virtual function is here
virtual void StartAbort(const Status& s) = 0;
^
tensorflow/core/common_runtime/collective_rma_local.cc:40:13: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
key, [this, to_tensor, to_device_ctx, to_device, to_alloc_attr, done](
^
2 warnings generated.
INFO: From Compiling tensorflow/core/common_runtime/executor.cc:
tensorflow/core/common_runtime/executor.cc:2082:19: warning: calling function 'ActivateNodes' requires holding mutex 'output_frame->mu' exclusively [-Wthread-safety-precise]
output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready);
^
tensorflow/core/common_runtime/executor.cc:2082:19: note: found near match 'input_frame->mu'
tensorflow/core/common_runtime/executor.cc:2147:21: warning: calling function 'ActivateNodes' requires holding mutex 'output_frame->mu' exclusively [-Wthread-safety-precise]
output_frame->ActivateNodes(item, is_dead, output_iter, outputs, ready);
^
tensorflow/core/common_runtime/executor.cc:2147:21: note: found near match 'input_frame->mu'
2 warnings generated.
INFO: From Compiling tensorflow/core/common_runtime/ring_reducer.cc:
In file included from tensorflow/core/common_runtime/ring_reducer.cc:17:
./tensorflow/core/common_runtime/collective_rma_local.h:37:8: warning: 'StartAbort' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
void StartAbort(const Status& s);
^
./tensorflow/core/framework/collective.h:305:16: note: overridden virtual function is here
virtual void StartAbort(const Status& s) = 0;
^
tensorflow/core/common_runtime/ring_reducer.cc:166:19: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
output_, [this, &note, &status](const Status& s) {
^
2 warnings generated.
INFO: From Compiling tensorflow/core/common_runtime/accumulate_n_optimizer.cc:
tensorflow/core/common_runtime/accumulate_n_optimizer.cc:77:31: warning: lambda capture 'g' is not used [-Wunused-lambda-capture]
auto base_make_node = [n, g, &n_attrs](const string& op,
^
tensorflow/core/common_runtime/accumulate_n_optimizer.cc:89:30: warning: lambda capture 'n_attrs' is not used [-Wunused-lambda-capture]
auto make_node = [n, g, &n_attrs, &base_make_node](string op) {
^
2 warnings generated.
INFO: From Compiling tensorflow/core/common_runtime/broadcaster.cc:
In file included from tensorflow/core/common_runtime/broadcaster.cc:17:
./tensorflow/core/common_runtime/collective_rma_local.h:37:8: warning: 'StartAbort' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
void StartAbort(const Status& s);
^
./tensorflow/core/framework/collective.h:305:16: note: overridden virtual function is here
virtual void StartAbort(const Status& s) = 0;
^
tensorflow/core/common_runtime/broadcaster.cc:148:25: warning: lambda capture 'recv_from_rank' is not used [-Wunused-lambda-capture]
[this, recv_from_rank, &mu, &note](const Status& s) {
^
tensorflow/core/common_runtime/broadcaster.cc:166:18: warning: lambda capture 'target_rank' is not used [-Wunused-lambda-capture]
[this, target_rank, &mu, &pending_count, &all_done](const Status& s) {
^
3 warnings generated.
INFO: From Compiling tensorflow/core/common_runtime/parallel_concat_optimizer.cc:
tensorflow/core/common_runtime/parallel_concat_optimizer.cc:53:33: warning: lambda capture 'g' is not used [-Wunused-lambda-capture]
auto base_make_node = [n, g, &n_attrs](const string& op,
^
tensorflow/core/common_runtime/parallel_concat_optimizer.cc:63:32: warning: lambda capture 'n_attrs' is not used [-Wunused-lambda-capture]
auto make_node = [n, g, &n_attrs, &base_make_node](string op) {
^
2 warnings generated.
INFO: From Compiling tensorflow/core/common_runtime/collective_executor_mgr.cc:
In file included from tensorflow/core/common_runtime/collective_executor_mgr.cc:19:
./tensorflow/core/common_runtime/collective_rma_local.h:37:8: warning: 'StartAbort' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
void StartAbort(const Status& s);
^
./tensorflow/core/framework/collective.h:305:16: note: overridden virtual function is here
virtual void StartAbort(const Status& s) = 0;
^
1 warning generated.
INFO: From Compiling tensorflow/core/common_runtime/function.cc:
tensorflow/core/common_runtime/function.cc:911:12: warning: lambda capture 'item' is not used [-Wunused-lambda-capture]
[item, frame, exec_args](DoneCallback done,
^
tensorflow/core/common_runtime/function.cc:911:18: warning: lambda capture 'frame' is not used [-Wunused-lambda-capture]
[item, frame, exec_args](DoneCallback done,
^
2 warnings generated.
INFO: From Compiling tensorflow/core/common_runtime/collective_param_resolver_local.cc:
tensorflow/core/common_runtime/collective_param_resolver_local.cc:534:47: warning: lambda capture 'gr' is not used [-Wunused-lambda-capture]
irec->init_waiters.push_back([this, gr, cp, done](InstanceRec* irec) {
^
tensorflow/core/common_runtime/collective_param_resolver_local.cc:534:51: warning: lambda capture 'cp' is not used [-Wunused-lambda-capture]
irec->init_waiters.push_back([this, gr, cp, done](InstanceRec* irec) {
^
tensorflow/core/common_runtime/collective_param_resolver_local.cc:669:50: warning: reading variable 'source_rank' requires holding mutex 'ir->out_mu' [-Wthread-safety-precise]
source_rank = ir->source_rank;
^
tensorflow/core/common_runtime/collective_param_resolver_local.cc:669:50: note: found near match 'irec->out_mu'
tensorflow/core/common_runtime/collective_param_resolver_local.cc:662:29: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
[this, ir, device, cp, done](InstanceRec* irec) {
^
4 warnings generated.
INFO: From Linking tensorflow/core/libcore_cpu_impl.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/libcore_cpu_impl.lo(mkl_cpu_allocator.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/libcore_cpu_impl.lo(mkl_layout_pass.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/libcore_cpu_impl.lo(mkl_tfconversion_pass.o) has no symbols
INFO: From Compiling tensorflow/core/common_runtime/gpu/process_state.cc:
tensorflow/core/common_runtime/gpu/process_state.cc:56:6: warning: unused function 'useCudaMallocAllocator' [-Wunused-function]
bool useCudaMallocAllocator() {
^
tensorflow/core/common_runtime/gpu/process_state.cc:62:6: warning: unused function 'useCudaMemoryGuardAllocator' [-Wunused-function]
bool useCudaMemoryGuardAllocator() {
^
2 warnings generated.
INFO: From Linking tensorflow/core/libgpu_runtime_impl.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/libgpu_runtime_impl.lo(gpu_device.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/libgpu_runtime_impl.lo(gpu_device_factory.o) has no symbols
INFO: From Compiling tensorflow/contrib/lite/toco/tensorflow_util.cc:
In file included from tensorflow/contrib/lite/toco/tensorflow_util.cc:15:
In file included from ./tensorflow/contrib/lite/toco/tensorflow_util.h:21:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/export_tensorflow.cc:
In file included from tensorflow/contrib/lite/toco/export_tensorflow.cc:25:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
tensorflow/contrib/lite/toco/export_tensorflow.cc:1731:6: warning: unused function 'ConvertSparseToDenseOperator' [-Wunused-function]
void ConvertSparseToDenseOperator(const Model& model,
^
3 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:
In file included from tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:23:
In file included from ./tensorflow/contrib/lite/toco/allocate_transient_arrays.h:18:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/toco_tooling.cc:
In file included from tensorflow/contrib/lite/toco/toco_tooling.cc:15:
In file included from ./tensorflow/contrib/lite/toco/toco_tooling.h:21:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/contrib/lite/toco/import_tensorflow.cc:
In file included from tensorflow/contrib/lite/toco/import_tensorflow.cc:15:
In file included from ./tensorflow/contrib/lite/toco/import_tensorflow.h:20:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/core/grappler/optimizers/remapper.cc:
In file included from tensorflow/core/grappler/optimizers/remapper.cc:16:
./tensorflow/core/grappler/optimizers/remapper.h:42:26: warning: private field 'opt_level' is not used [-Wunused-private-field]
RewriterConfig::Toggle opt_level;
^
1 warning generated.
INFO: From Compiling tensorflow/core/grappler/optimizers/shape_optimizer.cc:
In file included from tensorflow/core/grappler/optimizers/shape_optimizer.cc:16:
./tensorflow/core/grappler/optimizers/shape_optimizer.h:48:26: warning: private field 'opt_level' is not used [-Wunused-private-field]
RewriterConfig::Toggle opt_level;
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/data/parallel_map_dataset_op.cc:
tensorflow/core/kernels/data/parallel_map_dataset_op.cc:343:24: warning: lambda capture 'result_index' is not used [-Wunused-lambda-capture]
[result, result_index](Status ret_status) {
^
1 warning generated.
INFO: From Linking tensorflow/core/libsycl_runtime.a:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/libsycl_runtime.a(sycl_allocator.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/libsycl_runtime.a(sycl_device.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/libsycl_runtime.a(sycl_device_context.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/libsycl_runtime.a(sycl_device_factory.o) has no symbols
warning: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: warning for library: bazel-out/darwin-opt/bin/tensorflow/core/libsycl_runtime.a the table of contents is empty (no object file members in the library define global symbols)
INFO: From Compiling tensorflow/core/kernels/fifo_queue_op.cc:
In file included from tensorflow/core/kernels/fifo_queue_op.cc:26:
In file included from ./tensorflow/core/kernels/fifo_queue.h:26:
./tensorflow/core/kernels/typed_queue.h:91:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::vector& sq) {
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/random_shuffle_queue_op.cc:
In file included from tensorflow/core/kernels/random_shuffle_queue_op.cc:28:
./tensorflow/core/kernels/typed_queue.h:83:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::deque& sq) {
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/padding_fifo_queue_op.cc:
In file included from tensorflow/core/kernels/padding_fifo_queue_op.cc:27:
In file included from ./tensorflow/core/kernels/padding_fifo_queue.h:27:
In file included from ./tensorflow/core/kernels/fifo_queue.h:26:
./tensorflow/core/kernels/typed_queue.h:91:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::vector& sq) {
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/tensor_array_ops.cc:
tensorflow/core/kernels/tensor_array_ops.cc:330:21: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
auto creator = [this, key, tensor_array, array_size, marked_size,
^
tensorflow/core/kernels/tensor_array_ops.cc:332:21: warning: lambda capture 'output_handle' is not used [-Wunused-lambda-capture]
output_handle](TensorArray** ret) -> Status {
^
2 warnings generated.
INFO: From Compiling tensorflow/core/kernels/priority_queue_op.cc:
In file included from tensorflow/core/kernels/priority_queue_op.cc:25:
In file included from ./tensorflow/core/kernels/priority_queue.h:27:
./tensorflow/core/kernels/typed_queue.h:83:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::deque& sq) {
^
./tensorflow/core/kernels/typed_queue.h:91:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::vector& sq) {
^
2 warnings generated.
INFO: From Compiling tensorflow/core/kernels/barrier_ops.cc:
tensorflow/core/kernels/barrier_ops.cc:512:33: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
ComputeAsync(ctx, barrier, this, callback, barrier {
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertManyEigen::half' requested here
barrier->TryInsertMany(keys, component_index_, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOpEigen::half::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOpEigen::half::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertManytensorflow::bfloat16' requested here
barrier->TryInsertMany(keys, component_index_, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOptensorflow::bfloat16::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOptensorflow::bfloat16::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany' requested here
barrier->TryInsertMany(keys, component_index_, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany' requested here
barrier->TryInsertMany(keys, component_index_, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany<std::_1::complex >' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp<std::__1::complex >::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp<std::__1::complex >::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany<std::_1::complex >' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp<std::__1::complex >::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp<std::_1::complex >::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertMany<std::_1::basic_string >' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp<std::__1::basic_string >::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOp<std::1::basic_string >::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertManytensorflow::ResourceHandle' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOptensorflow::ResourceHandle::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOptensorflow::ResourceHandle::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
tensorflow/core/kernels/barrier_ops.cc:183:31: warning: lambda capture 'component_index' is not used [-Wunused-lambda-capture]
this, ctx, callback, component_index {
^
tensorflow/core/kernels/barrier_ops.cc:552:14: note: in instantiation of function template specialization 'tensorflow::barrier::Barrier::TryInsertManytensorflow::Variant' requested here
barrier->TryInsertMany(*keys, component_index, values, ctx, callback);
^
tensorflow/core/kernels/barrier_ops.cc:526:12: note: in instantiation of member function 'tensorflow::barrier::InsertManyOptensorflow::Variant::ComputeAsync' requested here
explicit InsertManyOp(OpKernelConstruction context)
^
tensorflow/core/kernels/barrier_ops.cc:565:19: note: in instantiation of member function 'tensorflow::barrier::InsertManyOptensorflow::Variant::InsertManyOp' requested here
TF_CALL_ALL_TYPES(REGISTER_INSERTMANY);
^
In file included from tensorflow/core/kernels/barrier_ops.cc:28:
In file included from ./tensorflow/core/kernels/priority_queue.h:27:
./tensorflow/core/kernels/typed_queue.h:83:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::deque& sq) {
^
./tensorflow/core/kernels/typed_queue.h:91:7: warning: unused function 'SizeOf' [-Wunused-function]
int64 SizeOf(const std::vector& sq) {
^
19 warnings generated.
INFO: From Compiling tensorflow/core/kernels/lookup_table_init_op.cc:
In file included from tensorflow/core/kernels/lookup_table_init_op.cc:17:
In file included from ./tensorflow/core/kernels/lookup_table_init_op.h:19:
./tensorflow/core/kernels/initializable_lookup_table.h:54:10: warning: 'ExportValues' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
Status ExportValues(OpKernelContext context) {
^
./tensorflow/core/framework/lookup_interface.h:74:18: note: overridden virtual function is here
virtual Status ExportValues(OpKernelContext ctx) = 0;
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/generate_vocab_remapping_op.cc:
In file included from tensorflow/core/kernels/generate_vocab_remapping_op.cc:23:
In file included from ./tensorflow/core/kernels/lookup_table_init_op.h:19:
./tensorflow/core/kernels/initializable_lookup_table.h:54:10: warning: 'ExportValues' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
Status ExportValues(OpKernelContext context) {
^
./tensorflow/core/framework/lookup_interface.h:74:18: note: overridden virtual function is here
virtual Status ExportValues(OpKernelContext ctx) = 0;
^
1 warning generated.
INFO: From Compiling tensorflow/core/kernels/lookup_table_op.cc:
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
In file included from ./tensorflow/core/kernels/lookup_table_op.h:25:
In file included from ./tensorflow/core/kernels/lookup_util.h:21:
./tensorflow/core/kernels/initializable_lookup_table.h:54:10: warning: 'ExportValues' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
Status ExportValues(OpKernelContext* context) {
^
./tensorflow/core/framework/lookup_interface.h:74:18: note: overridden virtual function is here
virtual Status ExportValues(OpKernelContext* ctx) = 0;
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle_' requires holding mutex 'mu_' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle_.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::__1::basic_string, double>, std::__1::basic_string, double>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:815:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::_1::basic_string, double>, std::1::basic_string, double>::LookupTableOp' requested here
REGISTER_KERNEL(string, double);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::__1::basic_string, float>, std::__1::basic_string, float>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:816:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::_1::basic_string, float>, std::1::basic_string, float>::LookupTableOp' requested here
REGISTER_KERNEL(string, float);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::__1::basic_string, int>, std::__1::basic_string, int>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:817:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::_1::basic_string, int>, std::1::basic_string, int>::LookupTableOp' requested here
REGISTER_KERNEL(string, int32);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::__1::basic_string, long long>, std::__1::basic_string, long long>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:818:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::1::basic_string, long long>, std::1::basic_string, long long>::LookupTableOp' requested here
REGISTER_KERNEL(string, int64);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<long long, std::1::basic_string >, long long, std::1::basic_string >::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:819:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<long long, std::1::basic_string >, long long, std::1::basic_string >::LookupTableOp' requested here
REGISTER_KERNEL(int64, string);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<long long, long long>, long long, long long>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:820:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<long long, long long>, long long, long long>::LookupTableOp' requested here
REGISTER_KERNEL(int64, int64);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<long long, float>, long long, float>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:821:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<long long, float>, long long, float>::LookupTableOp' requested here
REGISTER_KERNEL(int64, float);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::__1::basic_string, std::__1::basic_string >, std::__1::basic_string, std::__1::basic_string >::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:822:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::__1::basic_string, std::__1::basic_string >, std::_1::basic_string, std::1::basic_string >::LookupTableOp' requested here
REGISTER_KERNEL(string, string);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::__1::basic_string, bool>, std::1::basic_string, bool>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:823:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<std::1::basic_string, bool>, std::1::basic_string, bool>::LookupTableOp' requested here
REGISTER_KERNEL(string, bool);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<int, int>, int, int>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:824:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::HashTable<int, int>, int, int>::LookupTableOp' requested here
REGISTER_KERNEL(int32, int32);
^
tensorflow/core/kernels/lookup_table_op.cc:805:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::HashTable<key_dtype, value_dtype>, key_dtype, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<std::__1::basic_string, float>, std::__1::basic_string, float>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:845:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<std::_1::basic_string, float>, std::1::basic_string, float>::LookupTableOp' requested here
REGISTER_KERNEL(string, float);
^
tensorflow/core/kernels/lookup_table_op.cc:835:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfScalars<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<std::__1::basic_string, long long>, std::__1::basic_string, long long>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:846:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<std::_1::basic_string, long long>, std::1::basic_string, long long>::LookupTableOp' requested here
REGISTER_KERNEL(string, int64);
^
tensorflow/core/kernels/lookup_table_op.cc:835:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfScalars<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<long long, std::__1::basic_string >, long long, std::__1::basic_string >::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:847:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<long long, std::1::basic_string >, long long, std::1::basic_string >::LookupTableOp' requested here
REGISTER_KERNEL(int64, string);
^
tensorflow/core/kernels/lookup_table_op.cc:835:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfScalars<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<std::1::basic_string, bool>, std::1::basic_string, bool>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:848:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<std::1::basic_string, bool>, std::1::basic_string, bool>::LookupTableOp' requested here
REGISTER_KERNEL(string, bool);
^
tensorflow/core/kernels/lookup_table_op.cc:835:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfScalars<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<long long, float>, long long, float>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:849:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<long long, float>, long long, float>::LookupTableOp' requested here
REGISTER_KERNEL(int64, float);
^
tensorflow/core/kernels/lookup_table_op.cc:835:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfScalars<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<long long, tensorflow::Variant>, long long, tensorflow::Variant>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:850:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfScalars<long long, tensorflow::Variant>, long long, tensorflow::Variant>::LookupTableOp' requested here
REGISTER_KERNEL(int64, Variant);
^
tensorflow/core/kernels/lookup_table_op.cc:835:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfScalars<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfTensors<std::__1::basic_string, float>, std::__1::basic_string, float>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:871:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfTensors<std::_1::basic_string, float>, std::1::basic_string, float>::LookupTableOp' requested here
REGISTER_KERNEL(string, float);
^
tensorflow/core/kernels/lookup_table_op.cc:861:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfTensors<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfTensors<std::__1::basic_string, long long>, std::__1::basic_string, long long>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:872:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfTensors<std::_1::basic_string, long long>, std::1::basic_string, long long>::LookupTableOp' requested here
REGISTER_KERNEL(string, int64);
^
tensorflow/core/kernels/lookup_table_op.cc:861:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfTensors<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfTensors<long long, std::_1::basic_string >, long long, std::1::basic_string >::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:873:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfTensors<long long, std::1::basic_string >, long long, std::1::basic_string >::LookupTableOp' requested here
REGISTER_KERNEL(int64, string);
^
tensorflow/core/kernels/lookup_table_op.cc:861:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfTensors<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfTensors<std::1::basic_string, bool>, std::1::basic_string, bool>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:874:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableHashTableOfTensors<std::1::basic_string, bool>, std::1::basic_string, bool>::LookupTableOp' requested here
REGISTER_KERNEL(string, bool);
^
tensorflow/core/kernels/lookup_table_op.cc:861:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableHashTableOfTensors<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, long long>, long long, long long>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:895:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, long long>, long long, long long>::LookupTableOp' requested here
REGISTER_KERNEL(int64, int64);
^
tensorflow/core/kernels/lookup_table_op.cc:885:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableDenseHashTable<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, float>, long long, float>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:896:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, float>, long long, float>::LookupTableOp' requested here
REGISTER_KERNEL(int64, float);
^
tensorflow/core/kernels/lookup_table_op.cc:885:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableDenseHashTable<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, double>, long long, double>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:897:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, double>, long long, double>::LookupTableOp' requested here
REGISTER_KERNEL(int64, double);
^
tensorflow/core/kernels/lookup_table_op.cc:885:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableDenseHashTable<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<std::__1::basic_string, float>, std::__1::basic_string, float>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:898:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<std::_1::basic_string, float>, std::1::basic_string, float>::LookupTableOp' requested here
REGISTER_KERNEL(string, float);
^
tensorflow/core/kernels/lookup_table_op.cc:885:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableDenseHashTable<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<std::__1::basic_string, bool>, std::1::basic_string, bool>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:899:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<std::1::basic_string, bool>, std::1::basic_string, bool>::LookupTableOp' requested here
REGISTER_KERNEL(string, bool);
^
tensorflow/core/kernels/lookup_table_op.cc:885:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableDenseHashTable<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, bool>, long long, bool>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:900:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, bool>, long long, bool>::LookupTableOp' requested here
REGISTER_KERNEL(int64, bool);
^
tensorflow/core/kernels/lookup_table_op.cc:885:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableDenseHashTable<key_dtype, value_dtype>, 
^
In file included from tensorflow/core/kernels/lookup_table_op.cc:16:
./tensorflow/core/kernels/lookup_table_op.h:68:39: warning: reading variable 'table_handle' requires holding mutex 'mu' [-Wthread-safety-analysis]
container->MemoryUsed() + table_handle.AllocatedBytes());
^
./tensorflow/core/kernels/lookup_table_op.h:42:12: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, tensorflow::Variant>, long long, tensorflow::Variant>::Compute' requested here
explicit LookupTableOp(OpKernelConstruction* ctx)
^
tensorflow/core/kernels/lookup_table_op.cc:901:1: note: in instantiation of member function 'tensorflow::LookupTableOp<tensorflow::lookup::MutableDenseHashTable<long long, tensorflow::Variant>, long long, tensorflow::Variant>::LookupTableOp' requested here
REGISTER_KERNEL(int64, Variant);
^
tensorflow/core/kernels/lookup_table_op.cc:885:7: note: expanded from macro 'REGISTER_KERNEL'
LookupTableOp<lookup::MutableDenseHashTable<key_dtype, value_dtype>, 
^
28 warnings generated.
[1,781 / 2,012] 4 actions running
Compiling tensorflow/core/kernels/sparse_tensors_map_ops.cc; 13s local
INFO: From Compiling tensorflow/core/kernels/deep_conv2d.cc:
tensorflow/core/kernels/deep_conv2d.cc:1005:30: warning: lambda capture 'tile_rows' is not used [-Wunused-lambda-capture]
out_depth, tile_rows, tile_cols, out_tile_rows, out_tile_cols,
^
tensorflow/core/kernels/deep_conv2d.cc:1154:26: note: in instantiation of member function 'tensorflow::functor::DeepConv2D<Eigen::ThreadPoolDevice, float>::operator()' requested here
template struct functor::DeepConv2D<CPUDevice, float>;
^
tensorflow/core/kernels/deep_conv2d.cc:1005:41: warning: lambda capture 'tile_cols' is not used [-Wunused-lambda-capture]
out_depth, tile_rows, tile_cols, out_tile_rows, out_tile_cols,
^
tensorflow/core/kernels/deep_conv2d.cc:436:55: warning: lambda capture 'out_depth' is not used [-Wunused-lambda-capture]
&num_filters_transform, &in_depth, &out_depth,
^
tensorflow/core/kernels/deep_conv2d.cc:974:5: note: in instantiation of member function 'tensorflow::TransformFilters::operator()' requested here
TransformFilters()(ctx, args, transform.get(), filter_shards_row,
^
tensorflow/core/kernels/deep_conv2d.cc:535:20: warning: lambda capture 'tile_spatial_size' is not used [-Wunused-lambda-capture]
&tile_spatial_size, &in_depth, &out_depth, &filter_shards_row,
^
tensorflow/core/kernels/deep_conv2d.cc:979:5: note: in instantiation of member function 'tensorflow::PackFilters::operator()' requested here
PackFilters()(ctx, args, tile_spatial_size, filter_shards_row,
^
4 warnings generated.
INFO: From Linking tensorflow/core/kernels/libpooling_ops.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libpooling_ops.lo(cudnn_pooling_gpu.o) has no symbols
INFO: From Compiling tensorflow/core/kernels/topk_op.cc:
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, long long>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, long long>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, long long>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, int>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, int>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, int>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned short>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, unsigned short>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, unsigned short>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, short>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, short>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, short>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, unsigned char>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, unsigned char>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, unsigned char>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, signed char>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, signed char>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, signed char>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, Eigen::half>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, Eigen::half>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, Eigen::half>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, tensorflow::bfloat16>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, tensorflow::bfloat16>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, tensorflow::bfloat16>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, float>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, float>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, float>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
tensorflow/core/kernels/topk_op.cc:137:28: warning: lambda capture 'context' is not used [-Wunused-lambda-capture]
auto SortIndices = [&, context](int start_batch, int limit_batch) {
^
tensorflow/core/kernels/topk_op.cc:90:49: note: in instantiation of member function 'tensorflow::functor::TopKFunctor<Eigen::ThreadPoolDevice, double>::Compute' requested here
Status s = functor::TopKFunctor<Device, T>::Compute(
^
tensorflow/core/kernels/topk_op.cc:42:12: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, double>::Compute' requested here
explicit TopK(OpKernelConstruction* context) : OpKernel(context) {
^
tensorflow/core/kernels/topk_op.cc:241:27: note: in instantiation of member function 'tensorflow::TopK<Eigen::ThreadPoolDevice, double>::TopK' requested here
TF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);
^
10 warnings generated.
INFO: From Linking tensorflow/core/kernels/libconv_ops.lo:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/darwin-opt/bin/tensorflow/core/kernels/libconv_ops.lo(conv_ops_using_gemm.o) has no symbols
INFO: From Compiling tensorflow/contrib/lite/toco/toco.cc:
In file included from tensorflow/contrib/lite/toco/toco.cc:20:
In file included from ./tensorflow/contrib/lite/toco/model.h:26:
In file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:
In file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:
external/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
external/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]
c128 = _mm_set1_epi8 (128);
~~~~~~~~~~~~~ ^~~
2 warnings generated.
INFO: From Compiling tensorflow/core/kernels/batch_kernels.cc:
tensorflow/core/kernels/batch_kernels.cc:979:10: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
[this](UnbatchGradResource** r) {
^
1 warning generated.
Target //tensorflow/contrib/lite/toco:toco up-to-date:
bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 2814.103s, Critical Path: 164.73s
INFO: 2000 processes, local.
INFO: Build completed successfully, 2012 total actions
INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/Users/dchealth/Desktop/mobilenet/frozen_graph.pb' '--output_file=/Users/dchealth/Desktop/mobilenet/mobilenetnew.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--inference_type=FLOAT' '--output_format=TFLITE' '--input_type=FLOAT' '--input_shapes=1,224,224,3' '--input_arrays=input' '--output_arrays=MobilenetV1/PredINFO: Build completed successfully, 2012 total actions
2018-06-13 10:34:42.340286: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:251] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.
2018-06-13 10:34:42.460038: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 416 operators, 583 arrays (0 quantized)
2018-06-13 10:34:42.470369: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 416 operators, 583 arrays (0 quantized)
2018-06-13 10:34:42.507758: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 89 arrays (0 quantized)
2018-06-13 10:34:42.508374: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 89 arrays (0 quantized)
2018-06-13 10:34:42.508942: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.
2018-06-13 10:34:42.509181: I tensorflow/contrib/lite/toco/toco_tooling.cc:373] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).
DCHealthdeMac-mini:toco dchealth$ --inference_type=QUANTIZED_UINT8
-bash: --inference_type=QUANTIZED_UINT8: command not found
DCHealthdeMac-mini:toco dchealth$ --std_values=128
-bash: --std_values=128: command not found
DCHealthdeMac-mini:toco dchealth$ --mean_values=128。"
979,22412,0,"Tensor2Tensor Intro tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
windows 10, x64
- **TensorFlow installed from (source or binary)**:
 pip (pip install --upgrade tensorflow)
- **TensorFlow version (use command below)**:
1.10.0
- **Python version**:
3.6
- **GPU model and memory**:
not using GPU, 12GB memory
Mobile device: N/A
Bazel version : N/A
CUDA/cuDNN version : N/A

### Describe the problem 
I'm trying to make the Tensor2Tensor intro: English to German translation with a pre-trained model work locally. I have downloaded the checkpoints to the right directory, however it's returning this error:
tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested

As you can see the checkpoints are in the proper directory:
![image](https://user-images.githubusercontent.com/24248699/45819470-ac0e8c00-bce4-11e8-85bf-fccc4e2e7c22.png)

I have tried redownloading the checkpoints many times, I can't seem to get the English to German translation working with the provided checkpoints.
@lukaszkaiser 

- **Exact command to reproduce**:


`

### Source code / logs
>>> def translate(inputs):
...     encoded_inputs = encode(inputs)
...     print(encoded_inputs)
...     with tfe.restore_variables_on_create(ckpt_path):
...         model_output = translate_model.infer(encoded_inputs)[""outputs""]
...         print(model_output)
...     return decode(model_output)
...
>>> inputs = ""The animal didn't cross the street because it was too tired""
>>> outputs = translate(inputs)
{'inputs': <tf.Tensor: id=202, shape=(1, 15, 1), dtype=int32, numpy=
array([[[   28],
        [ 4705],
        [ 6253],
        [   83],
        [   62],
        [ 3444],
        [    4],
        [ 3825],
        [  244],
        [   40],
        [   53],
        [  362],
        [19285],
        [   85],
        [    1]]])>}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 4, in translate
  File ""C:\Users\david.salsbach\AppData\Local\Programs\Python\Python36\lib\contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""C:\Users\david.salsbach\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\eager\python\saver.py"", line 91, in restore_variables_on_create
    ckpt_var_cache[k] = reader.get_tensor(k)
  File ""C:\Users\david.salsbach\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 348, in get_tensor
    status)
  File ""C:\Users\david.salsbach\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested
>>> print(""Inputs: %s"" % inputs)
Inputs: The animal didn't cross the street because it was too tired
>>> print(""Outputs: %s"" % outputs)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'outputs' is not defined
"
1023,28856,0,"TFLiteConverterV2 has no attribute 'from_saved_model'. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): **v1.12.0-9492-g2c319fb415 2.0.0-alpha0**
- Python version: **3.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **9.0/7.1.4**
- GPU model and memory:

**Describe the current behavior**
When running , an error occurs as described in the title.

**Describe the expected behavior**
The TF lite converter should work properly.

**Code to reproduce the issue**
Just follow the sample code in [here](https://www.tensorflow.org/lite/r2/convert/python_api)
"
1231,2199,0,"Tensorflow GRU error when trying to concatenate activations to outputs. I have been trying to grab the activations of the GRU cell layer in the following manner



Later I try to grab the activations in the following way



However this fails in the  step with the following error

Traceback (most recent call last):
  File ""myautoencoder.py"", line 29, in <module>
    outputs_activations, outputs_activations = rnn.rnn(cell=cell, inputs=x, initial_state=initial_state, sequence_length=s)
  File ""xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 141, in rnn
    zero_output, state, call_cell)
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 265, in _rnn_step
    _maybe_copy_some_through)
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1157, in cond
    res_f = context_f.BuildCondBranch(fn2)
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1073, in BuildCondBranch
    r = fn()
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 247, in _maybe_copy_some_through
    lambda: _copy_some_through(new_output, new_state))
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1157, in cond
    res_f = context_f.BuildCondBranch(fn2)
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1073, in BuildCondBranch
    r = fn()
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 247, in <lambda>
    lambda: _copy_some_through(new_output, new_state))
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 236, in _copy_some_through
    return (math_ops.select(copy_cond, zero_output, new_output),
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 1396, in select
    name=name)
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2156, in create_op
    set_shapes_for_outputs(ret)
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1612, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 1411, in _SelectShape
    t_e_shape = t_shape.merge_with(e_shape)
  File ""/xxx/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 554, in merge_with
    (self, other))
ValueError: Shapes (32, 33) and (32, 132) are not compatible

My batch size is 32 and number of neurons is 33, 132 = 33*4 which makes sense. What I dont understand is why is there a lingering (32,33) tensor? Thank you.
"
1235,27112,0,"Cannot export Keras model TypeError: ('Not JSON Serializable:', b'\n...'). Versions:
Mac OS 10.14.3
TF 2.0.0-dev20190319




It is curious that the preprocessing steps can serialize fine, but something goes wrong either with the definition/building of the , or with the postprocessing steps...
(I apologize in advance if this is user error and not a bug-- it seems related to issues [here] (https://github.com/tensorflow/tensorflow/issues/19303) and [here](https://github.com/keras-team/keras/issues/9342) both of which have seemingly hacky solutions.



It is also curious to me that the constants used for unstandardizing the output do not show up as  of the second model.  Is this expected behavior?  "
998,6711,0,"Android demo accuracy. ### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I searched #1269 #504 

### Environment info
Mac OS for build and Android version 5 to run .apk demo.   

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I followed the steps mentioned in #1269 and could able to run the example successfully, but the accuracy of the result is very low and often wrong. I have trained my systems on 25 different daily used products like soap, soup, noodles, etc. 
Where as when i run the same example using following script it give me very high accuracy (approx. 90-95%)



The only difference i see here is that the model file used in the android demo is stripped because it does not support DecodeJpeg, whereas in the above code its the actually generated unstripped model. Is there any specific reason or somewhere i am wrong here? 

### What other attempted solutions have you tried?
Yes, i the above script and it gives me quite high accuracy result. 
"
789,2883,0,"iOS example DecodeJpeg issue with Image Retraining model. ### Environment info

Operating System: iOS
### Steps to reproduce
1. Follow the contrib/makefile/README to install the tensorflow iOS core lib
2. Create my own model with the Image Retraining tutorial
3. Run the iOS example, error is logged.
### Logs or other output that would be helpful


### Related to
#2754 except that I want to use the pd file generated from the Image Retraining tutorial
"
1179,21588,0," ./tensorflow/contrib/lite/build_rpi_lib.sh /bin/sh: 1: [[: not found make: arm-linux-gnueabihf-: Command not found. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04 64bit
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:  NO devices
- **TensorFlow installed from (source or binary)**:  pip install tensorflow-gpu==1.9
- **TensorFlow version (use command below)**:  1.9 gpu
- **Python version**:   python2.7
- **Bazel version (if compiling from source)**: NO Bazel
- **GCC/Compiler version (if compiling from source)**:   gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**:  cuda9.0 cudnn7.0
- **GPU model and memory**: gtx1070
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

### Describe the problem
get help from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/rpi.md

my work Dir is :  /home/icare/yqli/tensorflow

I can successfully run the  :     
./tensorflow/contrib/lite/download_dependencies.sh
chmod 777 ./tensorflow/contrib/lite/build_rpi_lib.sh

but when I run the  :   
./tensorflow/contrib/lite/build_rpi_lib.sh

erros occurs ：
icare@icare-5F:tensorflow$ ./tensorflow/contrib/lite/build_rpi_lib.sh
+ set -e
+++ dirname ./tensorflow/contrib/lite/build_rpi_lib.sh
++ cd ./tensorflow/contrib/lite
++ pwd
+ SCRIPT_DIR=/home/icare/yqli/tensorflow/tensorflow/contrib/lite
+ cd /home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../..
+ CC_PREFIX=arm-linux-gnueabihf-
+ make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7
/bin/sh: 1: [[: not found
/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/bin/rpi_armv7/benchmark_model
arm-linux-gnueabihf- g++ --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/allocation.cc -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/allocation.o
arm-linux-gnueabihf- g++ --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/arena_planner.cc -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/arena_planner.o
make: arm-linux-gnueabihf-: Command not found
arm-linux-gnueabihf- gcc --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/context.c -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/context.o
make: arm-linux-gnueabihf-: Command not found
tensorflow/contrib/lite/Makefile:203: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/allocation.o' failed
make: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/allocation.o] Error 127
make: *** Waiting for unfinished jobs....
tensorflow/contrib/lite/Makefile:203: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/arena_planner.o' failed
make: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/arena_planner.o] Error 127
make: arm-linux-gnueabihf-: Command not found
tensorflow/contrib/lite/Makefile:207: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/context.o' failed
make: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/context.o] Error 127

It seems that the “arm-linux-gnueabihf- g++” can not be found,

icare@icare-5F:tensorflow$ arm-linux-gnueabihf- g++
arm-linux-gnueabihf-: command not found

But it can find ""arm-linux-gnueabihf-g++""
icare@icare-5F:tensorflow$ arm-linux-gnueabihf-g++
arm-linux-gnueabihf-g++: fatal error: no input files
compilation terminated.

why  there is a space between the ""arm-linux-gnueabihf-"" and ""g++"",
how to solve the problem?

I have never modified the build_rpi_lib.sh and teh Makefile   
                                                                                                         

thanks for your time"
439,26421,0,"Syntax Error in Udacity example code. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0
- GPU model and memory: nVIDIA GTX 1050 Ti


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
A syntax error is raised.

**Describe the expected behavior**
The example code should work without modifications.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
The first code block on:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb

gives an error when it tries to execute:



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
491,44361,0,"tensorflow 2.3.1 issue causing python crash on macOS upon index update(PyCharm). <em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution macOS Catalina 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from binary
- TensorFlow version v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: 
2. TF 2.0: 


**Describe the current behavior**

A problem occurs upon index update conducted by PyCharm which happens at PyCharm startup. It goes away by uninstalling tensorflow, however when I reinstall, the same problem recurs.

**Describe the expected behavior**

Python shouldn't crash

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Just open pycharm after installing tf 2.3.1

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

"
1228,24064,0,"can't use tensorflow-gpu=1.12. I use anaconda and I both have cuda8 and cuda10 with the right cudnn. 
I tried 3.6.2 and 3.6.6. None of them works. 
Not only in the windows desk top. The tensorflow can't work on  linux cluster.

(py36) C:\Users\GUANGYUAN>python
Python 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\GUANGYUAN\AppData\Local\conda\conda\envs\py36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
249,4566,1,"Allow user to control amount of GPU memory consumed. Environment:
~tensorflow 0.10 
~NVIDIA K80 GPU server. 
~ubuntu 14.04

I do not change the code which is download from this repository.
When I run ptb_word_lm.py, it takes up all my GPU memory (from device gpu:0 to device gpu:15). Is it a bug? How could I fix it?

Thanks a lot in advance!
Swind
"
1109,12427,0,"Error when retraining with retrain.py. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: original one
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.10
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: latest,  before retraining, 
- **Python version**: Python 3.5.3 |Anaconda custom (64-bit)| (default, Mar  6 2017, 11:58:13) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
- **Bazel version (if compiling from source)**: not used
- **CUDA/cuDNN version**: not used
- **GPU model and memory**: not used
- **Exact command to reproduce**:

I know it's Ubuntu 16.10, but mb the problem in something different (that you may already know)?

### Describe the problem
Error when retraining mobilenet

### Source code / logs


"
935,796,0,"Problem running Convolutional.py. I am trying to run the convolutional.py through the tensorflow virtualenv i am getting an error as follows 
Traceback (most recent call last):
  File ""convolutional.py"", line 290, in <module>
    tf.app.run()
  File ""/home/akshay/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""convolutional.py"", line 121, in main
    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')
  File ""convolutional.py"", line 56, in maybe_download
    os.mkdir(WORK_DIRECTORY)
OSError: [Errno 13] Permission denied: 'data'

I am getting this error even when i am running as super user.
"
1343,19480,0,"Manually placing operations in eager execution raises FailedPreconditionError.. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: tensorflow 1.6
- **Python version**: python3.6
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**:  NVIDIA Corporation GV100GL [Tesla V100 SXM2 16GB] (rev a1)
- **Exact command to reproduce**: 



### Describe the problem
I have a model that I believe was not automatically being placed onto an available GPU.  

I then placed this part of the computation inside a with_device() block.  This schematically looks like:

 

The error is thrown during the loss calculation step.  

### Source code / logs


2018-05-22 13:57:11.963021: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-22 13:57:12.324280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:04:00.0
totalMemory: 15.78GiB freeMemory: 15.36GiB
2018-05-22 13:57:12.324584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-05-22 13:57:12.623548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14878 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:04:00.0, compute capability: 7.0)
Traceback (most recent call last):
  File ""tf_registration_continuous.py"", line 619, in <module>
    cProfile.run('main()', file)
  File ""/share/software/user/open/python/3.6.1/lib/python3.6/cProfile.py"", line 16, in run
    return _pyprofile._Utils(Profile).run(statement, filename, sort)
  File ""/share/software/user/open/python/3.6.1/lib/python3.6/profile.py"", line 55, in run
    prof.run(statement)
  File ""/share/software/user/open/python/3.6.1/lib/python3.6/cProfile.py"", line 95, in run
    return self.runctx(cmd, dict, dict)
  File ""/share/software/user/open/python/3.6.1/lib/python3.6/cProfile.py"", line 100, in runctx
    exec(cmd, globals, locals)
  File ""<string>"", line 1, in <module>
  File ""tf_registration_continuous.py"", line 615, in main
    accuracy ,runtime, final_loss = run_registration(directory='output/' + hparams_run.name, hparams=hparams_run, dataset_path = dataset_path, save_figs=False)
  File ""tf_registration_continuous.py"", line 525, in run_registration
    rm.register(save_summaries = save_figs, make_animation=save_figs)
  File ""tf_registration_continuous.py"", line 190, in register
    num_images_to_optimize_params= self.hparams.num_images_to_optimize_params
  File ""tf_registration_continuous.py"", line 105, in single_registration_step
    _ = self.eif.warp(scale, num_images_for_loss_calculation)
  File ""/home/groups/bmacint/Ultrasound/timing/elastic_image_field.py"", line 118, in warp
    field_image.initialize_translation()
  File ""/home/groups/bmacint/Ultrasound/timing/field_image.py"", line 87, in initialize_translation
    self.translation_warp_points = tf.tile(self.translation[tf.newaxis, tf.newaxis, :],
  File ""/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 828, in _SliceHelperVar
    return _slice_helper(var._AsTensor(), slice_spec, var)
  File ""/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 741, in _AsTensor
    return self.value()
  File ""/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 572, in value
    return self._read_variable_op()
  File ""/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 655, in _read_variable_op
    self._dtype)
  File ""/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 304, in read_variable_op
    attrs=_attrs, ctx=_ctx, name=name)
  File ""/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 2, in raise_from
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable 152 from Container: eager-execution-0/. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:ReadVariableOp]

"
1317,29332,0,"[TF 2.0 API Docs] tf.image.adjust_saturation. ## URL(s) with the issue:

https://www.tensorflow.org/versions/master/api_docs/python/tf/image/adjust_saturation
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py

## Description of issue (what needs changing):

### Raises listed and defined

Raises are not listed

### Usage example

No usage example has been provided

### Submit a pull request?

Yes
https://github.com/tensorflow/tensorflow/pull/29333"
1278,12388,0,"Why my tensorflow-gpu runs only on cpu? Ubuntu. ### System information
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from **: 
- **TensorFlow version**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Python version**:  Python 3.5.2
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61


My tensorflow-gpu runs only on cpu, how to fix it?
I've already tried to set 
  But it didn't work.

Then I tried to use this code:

The output is following:


Then nothing. It doesn't output the map.
I've also tried to install it with virtualenv and run the program in virtualenv or reinstall, still not working. 
Why is this happening? How can I fix it? 



"
703,7685,0,"Image problems in Tensorflow documentation at tensorflow.org, Error 404. I'm not sure I should put the issue here. But when I clicked ""Issue tracker"" on tensorflow.org, it redirects me here.

I realized that the directory structure for TF documentation has changed a lot for TF 1.0 on http://www.tensorflow.org. Now there are problems for web pages with images, like https://www.tensorflow.org/api_docs/python/tf/segment_max?hl=bn. A 404 error is reported while loading the images."
223,2983,1,"Multiple CPU usage ineffective: CPU utilization only 200% on a 8 core VM. This is the same as #583. Opening a new issue since CPU utilization is still low. I built tensorflow with  flag and ran . TensorFlow is recognizing num of cores as 8, yet still only 200% of CPU is used during training.
### Environment info

Operating System:
14.04.1-Ubuntu x86_64 GNU/Linux

Installed version of CUDA and cuDNN: 
No CUDA or cuDNN installed.

If installed from sources, provide the commit hash:
Commit hash:


### Steps to reproduce
1. Enable logging of  and .
2. Configure, build tensorflow with AVX2 with this command
   
3. Build wheel, install package and run .
"
1097,9381,0,"Mac Gpu Link not working?. Hello,

I was trying to install Tensorflow for Mac GPU Python 3 one from github and I found that it is a broken URL Giving me an HTTP 404 Error.

The URL is - 

Please Fix the Url.


Best,
Daksh"
416,9115,0,"Documentation link issue. NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.
under this link 
https://www.tensorflow.org/api_docs/python/tf/contrib/deprecated/scalar_summary

the webpage is not found 

https://www.tensorflow.org/code/tensorflow/contrib/deprecated/__init__.py


### You must complete this information or else your issue will be closed
- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:
- *TensorFlow installed from (source or binary)?*:
- *TensorFlow version*:
- *Bazel version (if compiling from source)*:
- *CUDA/cuDNN version*:
- *GPU Model and Memory*:
- *Exact command to reproduce*:

### Describe the problem clearly

### Source Code / Logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem
"
1364,19303,0,"Model checkpoint issue: serialization error for tf.string. TF version: 1.8
Installation: tensorflow/tensorflow:latest-gpu
OS details: 
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.4 LTS
Release:	16.04
Codename:	xenial

AWS instance: p2.xlarge
GPU details:


Custom code:


"
216,31136,1,"Inconsistent gradient. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.4.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Colab GPU

**Describe the current behavior**
I have a very simple Keras model and I want to compute the gradient of the different layers using TensorFlow. I start by creating the computational graph in the first cell of a Jupyter notebook. Here is the code of the computational graph:

    import tensorflow as tf
    import tensorflow.keras as keras
    import tensorflow.keras.backend as K
    import numpy as np
    from tensorflow.keras.layers import Dense, Input, Layer
    from tensorflow.keras.models import Model
    input_tensor = Input(shape=(20,), name=""input"")
    print(input_tensor.name)
    hidden = Dense(100, activation='relu')(input_tensor)
    out1 = Dense(10, activation='relu', name=""out1"")(hidden)
    model = Model(inputs=input_tensor, outputs=[out1])
    grad = []
    for i in range(4):
       grad.append(tf.gradients(out1, model.trainable_weights[i]))
    model.compile(loss={""out1"": ""mse""}, 
    optimizer=tf.train.AdamOptimizer(learning_rate=0.001))

    np.random.seed(0)
    X = np.random.random((3, 20)).astype(np.float32)
    Y = np.random.random((3, 10)).astype(np.float32)
    model.fit(x={'input' : X}, y={'out1' : Y}, batch_size=1, epochs=10)

Then each time I run the tf.gradients operator, I get a different gradient vector (the gradient changes). This result is not reasonable. Where is the problem in my code?

And here is the code for the created Session:

    with tf.Session() as sess:
       sess.run(tf.global_variables_initializer())
       out_grad = sess.run(grad, feed_dict={'input:0':X})
       print(out_grad)

**Describe the expected behavior**
The same gradient each time I run the 

"
438,33818,0,"KMeans module not found in TensorFlow 2. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.13.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): pip package manager
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: /
- GPU model and memory: /

**Describe the current behavior**
I am following this TensorFlow documentation https://www.tensorflow.org/api_docs/python/tf/compat/v1/estimator/experimental/KMeans

When running the example I am getting the following error message:


**Describe the expected behavior**
I expect a example which is possible to execute

**Code to reproduce the issue**


**Other info / logs**
I also tried to run it with the nightly version of TensorFlow 2, but I get the same results
"
818,28423,0,"tf.contrib.summary.create_file_writer 'name' parameter is not recognized. While creating summary writer in eager execution, name parameter is not recognized properly, hence all the graphs are getting associated with single (default) name.

Attaching [screenshot](https://drive.google.com/open?id=1bwOFOA631vtBvejoXGu9Xf6oVTXtqnVI) of Tensorboard for reference

Tensorflow version: 1.13"
96,15057,1,"gpu is slower than cpu. Have I written custom code：No
OS Platform and Distribution: win10
TensorFlow installed from: pip install tensorflow-gpu or pip install tensorflow
TensorFlow version: 1.4 and 1.3
Bazel version: None
CUDA/cuDNN version: cuda8.0 and cudnn 6.0
GPU model and memory: GTX1050 2GB
Exact command to reproduce: no

I have trained a cnn+lstm mode, and use the model to predict one image, however, I find the GPU is slowly than CPU. I have test the tensorflow-gpu 1.4, tensorflow-gpu 1.3， tensorflow 1.4 and tensorflow 1.4
the code is 

tensorflow-gpu 1.4 log

tensorflow-gpu 1.3 log

tensorflow 1.4 log

tensorflow 1.3 log

As can be seen from the log, tensorflow1.4 slower than 1.3 #14942, and gpu mode slower than cpu. If needed, I can provide models and test images"
708,1688,0,"A small bug in tutorials/mnist/mnist_with_summaries.py. Please fix the bug in **Line 85** of file _tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py_
the code 
 
should be changed to 
,
otherwise there will be an error 
"
354,23603,0,"Is tensorflow supported in python 2.7?. 

**System information**
- OS Platform and Distribution: **Linux Ubuntu 16.04**
- Python version: **2.7-64bits**
- Installed using pip: **pip version 9.0.3**
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 9.1

**Describe the problem**
Hi:
Im trying to install tensorflow with pip with this command:

and the output says:


Is my python version compatible with tensorflow? I see that are instructions here [https://www.tensorflow.org/install/pip?lang=python2](url) to install tensorflow for python 2.7 but i have the same result.
I want to use Python 2.7 because the other modules I need for my scripts are installed in python 2.7. I have seen in other posts and issues that tensorflow is not compatible with python 2.7, is that true? How can I solve this? Should I try to install the other modules in python 3.5.2?
Thanks for help.
"
1031,1797,0,"No way to re-enter previous name/variable-scoped context. As far as I can tell, there is no way to re-enter the same name_scope or variable_scope (once closed) without the name being ""unique-ified.""

The [example in the docs](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#Graph.name_scope) shows how to re-enter the original scope from within a nested scope - but not when context is closed.

Perhaps this could be solved by enabling access to the scope reference as  ?

Current hack-y workaround is to manually specify variable name as 
"
1379,430,0,"scalar_summary does not support int32 datatypes. 
"
106,26331,1,"tf.gradients returns incorrect results if used multiple times on CudnnLSTM. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.12.0
- Python version: 3.6.7
- Bazel version (if compiling from source): unknown
- GCC/Compiler version (if compiling from source): unknown
- CUDA/cuDNN version:  10.0 / 7.4
- GPU model and memory: TI 1080, 12GB RAM

I found some cases where fetching multiple gradient computations for the parameters of a CudnnLSTM seems to result in incorrect results. For example, the following code:


Shows the two gradient computation are far from equal, even though they are requesting exactly the same value. On my machine I saw:



which seems far too large for the difference to be a rounding issue.

Using  will fix the issue.

The issue also goes away if we use  , but seems to remain for other, non-trivial cases. For example, if we had done   should be twice as large as , but it will be way off."
1345,28545,0,"problem with installation. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC Yosemite 10.10.5 (14F27)
- TensorFlow installed from (source or binary):source
- TensorFlow version: 1.13.1
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?: pip


- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 4 GB 1600 MHz DDR3

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Although tensorflow is installed and pip install tensorflow shows that it is already satisfied, but I cannot import tensorflow library

File ""/Users/botaduisenbay/Desktop/tester.py"", line 9, in <module>
    import tensorflow as tf

  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): no suitable image found.  Did find:
	/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: truncated mach-o error: segment __LINKEDIT extends to 365926664 which is past end of file 154042368


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
141,21271,1,"tf.nn.softmax_cross_entropy_with_logits_v2 returns wrong value with soft labels. ### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Linux CentOS 7.4
- **Mobile device if the issue happens on mobile device**: N/A
- **TensorFlow installed from**: binary
- **TensorFlow version**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: Python 3.6.3
- **Bazel version**: N/A
- **GCC/Compiler version**: N/A
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**:  GeForce 940MX, 4GB
- **Exact command to reproduce**: save the codes below as a .py file, and run it with command-line something like .

### Describe the problem
I use soft labels (for example, [0.2, 0.8] instead of [0, 1]) in a CNN model, in which I use  for loss computing. But when I trained the model, the loss became +inf in 10 steps, so I debugged the codes and found that the problem was caused by .
So I implemented the softmax and cross_entropy process separately, then the returned value seemed to make sense. 

### Source code / logs


**Output:**
"
177,15697,1,"Very slow tf.transpose on CPU (compared to numpy). ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tested on Linux Ubuntu 16.04 and Mac OS
- **TensorFlow installed from (source or binary)**: Both affected
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**:  3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: 

### Describe the problem
Tensorflow transpose is 10000 slower than numpy transpose on my example.

### Source code / logs
"
1112,22636,0,"Query regarding 1.11.0 release. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: Binary
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Two questions:
- Are the wheels uploaded on PyPI for Windows made using bazel or cmake?
- The wheels uploaded on PyPI for macOS claim that they target 10.11, however, the function at https://github.com/tensorflow/tensorflow/blob/v1.11.0/tensorflow/core/platform/posix/env_time.cc#L31 uses , which wasn't available till 10.12 was released. So, how was it built? 🤔 "
452,2256,0,"missing build_pip_package runfiles tensorflow external. -bash-4.1$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
Fri May 6 18:41:53 UTC 2016 : === Using tmpdir: /tmp/tmp.HLtyx30grJ
cp: cannot stat bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/external': No such file or directory
### Environment info

Operating System:

redhat-release-server-6Server-6.7.0.3.el6.x86_64
epel-release-6-8.noarch
scl python27 devtoolset-2 enable
numpy 1.11.0

Installed version of CUDA and cuDNN: 

No CUDA -- configured without GPU

If installed from sources, provide the commit hash:

f8eb1d70a7ea7dc2cd5e1eddde389395f88a6be9
### Steps to reproduce

git clone https://github.com/bazelbuild/bazel.git
cd bazel/
./compile.sh

--git 1.7.1
git clone --recursive https://github.com/tensorflow/tensorflow
bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
### What have you tried?
1. With previously installed protobuf from source 3.0.0b2- thought that was the problem so
2. Tried on another box with no previously installed protobuf
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
WARNING: /home/ebice/.cache/bazel/_bazel_ebice/e10106cd0aa7dd3e05f3af5f70558af6/external/grpc/WORKSPACE:1: Workspace name in /home/ebice/.cache/bazel/_bazel_ebice/e10106cd0aa7dd3e05f3af5f70558af6/external/grpc/WORKSPACE (@__main__) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions.
WARNING: /home/ebice/.cache/bazel/_bazel_ebice/e10106cd0aa7dd3e05f3af5f70558af6/external/re2/WORKSPACE:1: Workspace name in /home/ebice/.cache/bazel/_bazel_ebice/e10106cd0aa7dd3e05f3af5f70558af6/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.
INFO: Loading complete.  Analyzing...
"
357,390,0,"parse_example can be _much_ faster than parse_single_example. FYI I am working with Example protos for model input, and I am learning that use  to parse a (shuffled) batch of serialized examples is _much_ faster than using  prior to batching. For my particular dataset, using parse_single_example allows me to create s with batch size 128 at about 100/min; batching the serialized Example protos and then using parse_example is running at around 3000/min.

You may want to update the [documentation](http://www.tensorflow.org/how_tos/reading_data/index.html#file-formats) to suggest using  everywhere, as is suggested when using [sparse input data](http://www.tensorflow.org/how_tos/reading_data/index.html#sparse-input-data).
"
436,16688,0,"how to assign the GPU device using C++?. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
1314,24053,0,"Why not using TensorFixedSize on most const TTypes?. Hi im not too sure if this is a bug but why in TTypes.h, most of the tensor types that are 'Const' are not using TensorFixedSize? only a few are.

Apologies if this was not a bug."
1327,1453,0,"Multidimensional RNN. Hi all,

This is a feature request for [Multidimensional LSTM](http://people.idsia.ch/~juergen/nips2009.pdf). Is there any plan to support this in tensorflow?

I might be able to help with implementation if there is such a plan.

Cheers,
"
1113,20343,0,"Please, invest into profiler documentation. After trying to figure out how to use tensorflow profiler to test my model I am giving up. Current   does not only lack details but is actually quite confusing. "
1177,31665,0,"tf.math ops do not work on MirroredVariables. **System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.5
- TensorFlow installed from (source or binary): binary (tf-nightly)
- TensorFlow version (use command below): 1.15.0-dev20190729
- Python version: 3.7.4

**Describe the current behavior**
When I am using the  with multiple replicas,  ops do not work on  when inside a cross-replica scope.

**Describe the expected behavior**
The [MirroredVariable class](https://github.com/tensorflow/tensorflow/blob/e19c354920c3b246dda6598229210a582caaa1a9/tensorflow/python/distribute/values.py#L782) is a subclass of the [DistributedDelegate class](https://github.com/tensorflow/tensorflow/blob/e19c354920c3b246dda6598229210a582caaa1a9/tensorflow/python/distribute/values.py#L375), which if I understand correctly, means that  are supposed to act like regular tensors that you can perform ops on. This works fine with most standard ops, like multiplication, division, subtraction, etc. However, if you try to use any  ops, you get .

**Code to reproduce the issue**

"
1263,31664,0,"Building tflite-with-select-ops.aar library in tf v1.12. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14, 1.13, 1.12
- Python version: 2.7, 3.6
- Bazel version (if compiling from source): 0.28, 0.16


**Describe the problem**
Before getting into the problem I need a little intro.
I'm trying to run a tflite model on android. The first little issue that I met was during  file conversion. One of my ops wasn't supported by tf lite builtins and so I have to export my model using the **SELECT_TF_OPS** tag as bescribed in the [official guide](https://www.tensorflow.org/lite/guide/ops_select#converting_the_model) .
No big deal. **The model was converted using tf v1.14**.
After the successful conversion I needed only to build (from source) my custom select-ops friendly library that should be imported in my project. So I configured the bazel WORKSPACE as explained in [this guide](https://www.tensorflow.org/lite/guide/android#build_tensorflow_lite_locally).
I've chosen ANDROID SDK 23 (also tried 29) and NDK 18.
Then I've built the library using the bazel (v0.28 i guess) build arguments described [here](https://www.tensorflow.org/lite/guide/ops_select#android_aar).
The build took a **long time**, I imported it into my project and I get this Error:
**This was the first real problem**


No bad, I surfed the web and found [this fix](https://github.com/googlecodelabs/tensorflow-for-poets-2/issues/26). They say to downgrade tf back to 1.12.
Honestly I wasn't feeling really well on this, but I have to try.
Changed Gradle version to 4.1 and Plugin Version to 3.0.0.
_Before the fix I decided to try the previous aar build procedure on tf v1.13. But still this  appear._
Ok let's move on **tf v1.12**.
Now **Bazel is v0.16**. The WORKSPACE now was ANDROID SDK 23 and NDK 15.
**Here the second issue will come**, when building with select ops in tf 1.12 the bazel command [given by the official guide](https://www.tensorflow.org/lite/guide/ops_select#android_aar) is not working. Cause _//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops_ cannot be resolved.
So I tried this, taking the idea from [here](https://www.tensorflow.org/lite/guide/ops_select#c):


 
 
 



The build was **really fast**. _Don't feel good about this._
Then, imported the model into android studio, run and again : 


I really don't know what I am getting wrong.

What is this error and why does it keeps coming?

Is my bazel build command correct (for building my tflite select-ops friendly aar library in tf v1.12)? 

Should I use ANDROID NDK 17 as [described here](https://www.tensorflow.org/lite/guide/android#install_bazel_and_android_prerequisites)?

I can't find anything that could help me on the web.
Lastly : 
Should I convert the  model to  in tf v1.12?

I will really appreciate any help.
Sorry for bad english I'm not native."
1410,17987,0,"graph.pb.h missing in cc tutorial. In tensorflow > cc > tutorials >>graph.pb.h , the code includes the header file  . However in the path it specifies ( there is no such file.

Where is this header file located? I could not find it inside the tensorflow master branch."
208,31628,1,"fft2d on CPUs. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
It seems like there is a performance issue when running fft2d on data with dimension > 2 (CPU only).
When running fft2d on an array of (40,40,256,256), the function correctly computes fft2d on the inner most 2 dimensions as expected. But it only uses 1 CPU core which results in poor performance.


**Describe the expected behavior**
Because it is equivalent to simultaneously computing 40x40 fft2d, I would expect parallel computation across multiple CPU cores to boost the performance.

**Code to reproduce the issue**
a=tf.cast(tf.Variable(tf.random.uniform([40,40,256,256],dtype=tf.float32)), dtype=tf.complex64)
b=tf.signal.fft2d(a)

Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
836,3544,0,"Should *.pb.h files be generated in cross-compile cases?. A few days ago, I have tried to import the tensorflow project directly into Android Studio on Windows and use Gradle to build the project. Header files with suffix "".pb.h"" are not generated. Google says bazel should be used to build the project. Now I use bazel on Linux to build it again with CPU=arm64-v8a using the following command

bazel build //tensorflow/examples/android:tensorflow_native_libs --crosstool_top=//external:android/crosstool --cpu=$CPU --host_crosstool_top=@bazel_tools//tools/cpp:toolchain

as in the issue #3444 .

However header files with suffix "".pb.h"" are still not generated. Android Studio cannot find the include file and hence the build procedure cannot be continued. Is there any method to use tensorflow C++ library in Android Studio?
"
131,18187,1,"Adam optimizer with decaying learning rate. I tried to implement the Adam optimizer with different beta1 and beta2 to observe the decaying learning rate changes using:


To track the changes in learning rate, I printed the  variable of the object in the session:


But for all iterations, I get the same value (the initialized 0.001 value).

I couldn't find how the parameters are updated in this class.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
"
633,34526,0,"Xxx. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):



**Provide the text output from tflite_convert**



Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
1108,26879,0,"TF 2.0: tf.stack can cause a segmentation fault. **System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- Mobile device: N/A
- TensorFlow installed from: binary
- TensorFlow version: 2.0.0-alpha0, 2.0.0-dev20190319
- Python version: 3.6.7
- Bazel version: N/A
- GCC/Compiler version: N/A
- CUDA/cuDNN version: 10.1 / 7.4.2
- GPU model and memory: GeForce GTX 1080 Ti (11 GB)

**Describe the current behavior**
The Python interpreter crashes with SIGSEGV (Segmentation Fault); according to gdb the fault occurs in .

**Describe the expected behavior**
No segmentation fault.
Ideally a stacked tensor returned (I was adapting code I developed interactively in eager execution mode, where it worked, in a Jupyter notebook for addition to a Keras based model), or an error that the argument cannot be a tensor (this is the TF1 behavior):
 

**Code to reproduce the issue**
"
931,19367,0,"Abnormal feature function. #### Expected behavior
Supposedly, When I click ""Save Feature"" the image should be saved. instead of connecting to the ""Sharing Features"" that are not working and do not contain any data / images.

#### Actual behavior
When I click ""Save Feature"", the App takes me to the ""Sharing Feature"" which is not working and does not contain any data / images.

#### How to reproduce
1. Instal App [here](https://play.google.com/store/apps/details?id=com.deerslab.stylized1)
2. Run the app
3. Choose the effect you like
4. Click ""SAVE feature"", you will be taken to ""Share Feature"" which does not contain data and can not be shared.

***
***

* Browser : Redmi Note 5A
* System Operating : 7.1.0 Nougat
* App Version : 1.02

#### Recording Of The Bug
https://youtu.be/MXN-WgNp17Q

#### Proof Of Work Done
http://www.github.com/rezamusic881"
1119,28555,0,"Building TensorFlow Lite Micro for TARGET=bluepill fails. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version: b02f70947d
- Python version: 3.6
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): 0.25.1
- GCC/Compiler version (if compiling from source): arm-none-eabi-g++ 7.3.1
- CUDA/cuDNN version: NA
- GPU model and memory: NA


**Describe the problem**
When building Tensorflow Lite Micro for the bluepill target the build fails on the file tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc with the following message:

In file included from ./tensorflow/lite/kernels/internal/common.h:49:0,
        from tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc:18:
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In constructor 'gemmlowp::Mutex::Mutex()':
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:70:13: error: 'pthread_mutex_init' was not declared in this scope
    Mutex() { pthread_mutex_init(&m, NULL); }
                   ^~~~~~~~~~~~~~~~~~
             pthread_mutex_t
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In destructor 'gemmlowp::Mutex::~Mutex()':
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:71:14: error: 'pthread_mutex_destroy' was not declared in this scope
   ~Mutex() { pthread_mutex_destroy(&m); }
              ^~~~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:71:14: note: suggested alternative: 'pthread_mutexattr_t'
   ~Mutex() { pthread_mutex_destroy(&m); }
              ^~~~~~~~~~~~~~~~~~~~~
              pthread_mutexattr_t
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In member function 'void gemmlowp::Mutex::Lock()':
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:73:17: error: 'pthread_mutex_lock' was not declared in this scope
   void Lock() { pthread_mutex_lock(&m); }
                 ^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:73:17: note: suggested alternative: 'pthread_mutex_t'
   void Lock() { pthread_mutex_lock(&m); }
                 ^~~~~~~~~~~~~~~~~~
                 pthread_mutex_t
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In member function 'void gemmlowp::Mutex::Unlock()':
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:74:19: error: 'pthread_mutex_unlock' was not declared in this scope
   void Unlock() { pthread_mutex_unlock(&m); }
                   ^~~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:74:19: note: suggested alternative: 'pthread_mutex_t'
   void Unlock() { pthread_mutex_unlock(&m); }
                   ^~~~~~~~~~~~~~~~~~~~
                   pthread_mutex_t
tensorflow/lite/experimental/micro/tools/make/Makefile:209: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o' failed
make: *** [tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o] Error 1


**Provide the exact sequence of commands / steps that you executed before running into the problem**
make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=bluepill test


**Any other info / logs**
Reverting commit a52f5b54e8 fixes the build issue."
1304,30533,0,"[TF2.0] trainable=True on tf.keras.Embedding result in ""constant folding failed: Invalid argument: Unsupported type: 21"". <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1
- Python version: 3.7.3
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: CUDA 10.0.130_411.31 / cuDNN 7.5.1.10
- GPU model and memory: GTX 1060 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
When enabling training on Embedding layer using keras subclassed model, an error is raised during model.fit():



This has a huge impact on training speed, that scales with the amount of data, in my full code, each epoch take more than 1 minute to complete(dataset of 869 tokenized strings, for train an encoder/decoder model), the code for reproduce this error dont have a noticeable impact because there is only one sample, already tokenized.

The error only is showed when an Embedding layer with trainable=True, is followed by an LSTM/GRU layer

**Describe the expected behavior**
LSTM/GRU supporting zero masking, and having no impact of training speed

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

What the minimal code example above show in the console for me



Edit 01: i think is somewhat ralated to [this](https://github.com/tensorflow/tensorflow/issues/29525#issuecomment-509154149) on https://github.com/tensorflow/tensorflow/issues/29525"
839,30864,0,"How can I get h5 file to tflite file. I tried to do with the documentation but it is giving me an error. 

This is the error I am getting

ValueError                                Traceback (most recent call last)
<ipython-input-27-d2fc0cb4c75c> in <module>
----> 1 coverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)

/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in from_keras_model_file(cls, model_file, input_arrays, input_shapes, output_arrays, custom_objects)
    741 
    742       frozen_func = _convert_to_constants.convert_variables_to_constants_v2(
--> 743           concrete_func)
    744       _set_tensor_shapes(frozen_func.inputs, input_shapes)
    745       return cls(frozen_func.graph.as_graph_def(), frozen_func.inputs,

/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func)
    164         input_name = get_name(map_name_to_node[input_name].input[0])
    165       if map_name_to_node[input_name].op != ""Placeholder"":
--> 166         raise ValueError(""Cannot find the Placeholder op that is an input ""
    167                          ""to the ReadVariableOp."")
    168       # Build a map of Placeholder ops that are inputs to ReadVariableOps to the

ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.

model = keras.Sequential()
model.add(keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=Combined.shape[1]))
model.add(keras.layers.SpatialDropout1D(0.2))
model.add(keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(keras.layers.Dense(11, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
epochs = 40
batch_size = 64
history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])


Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 50, 100)           40000     
_________________________________________________________________
dropout (Dropout)            (None, 50, 100)           0         
_________________________________________________________________
lstm (LSTM)                  (None, 100)               80400     
_________________________________________________________________
dense (Dense)                (None, 11)                1111      
=================================================================
"
1461,28043,0,"Weird behavior using tf.Variable in a tf.data.experimental.map_and_batch callback function. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): 1.13.1 from conda
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Code to reproduce the issue**



**Describe the current behavior**

If using normal  and , one get



**Describe the expected behavior**

If using , one get changing results like:



There should be always only one and non-duplicate non-zeros per-line.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Not sure if this is related to #27507 . On a [Colab with tf-nightly](https://colab.research.google.com/drive/1DzTvOJRXYUk0CSjvGa11ewmjTP7VJ1kF), #27507 seems fixed but this issue still reproduces."
785,5220,0,"go. NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of ):

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from .

If installed from source, provide 
1. The commit hash ()
2. The output of 
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
"
940,27471,0,"Linking of _pywrap_tensorflow_internal.so fails with no obvious cause. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 18.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not mobile device, laptop
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: source
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 6.5.0
- CUDA/cuDNN version: cuda 10.0 / cudnn 7
- GPU model and memory: GeForce GT 650M

**Describe the problem**

Steps to reproduce:



Output from building, building fails during linking:

tf.distributionstfp.distributionstf.distributionstfp.distributionstf.distributionstfp.distributionstf.distributionstfp.distributionstf.contrib.distributionstfp.distributionstf.distributionstfp.distributionstf.contrib.distributionstfp.distributionstf.contrib.distributionstfp.distributionstf.contrib.distributionstfp.distributionstf.contrib.distributionstfp.distributionstf.contrib.distributionstfp.distributions

So it seems that linking of  fails. However I see no way of diagnosing why that linking failed. The setup should be pretty straight-forward, so I am really wondering why it fails.

Any and all help is very appreciated. Thanks in advance."
337,31077,0,"Downgrade tensorflow 1.14 to tensorflow 1.13. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0/7.4
- GPU model and memory: Nivida GeForce 840m 3 Go



**Describe the current behaviour**
I training a Resnet Model ( CNN) that detects an eye region with landmarks on real-time using Tensorflow library 
For the first time, I have used Tensorflow library ( 1.13.1), the model gives sometimes results and for others times not for the webcam ( and for video from my dataset ).
I have upgrade Tensorflow to 1.14.1 recently and I have tested my webcam and I don't get any results contrary for a video the same result does not change when I upgrade the version of Tensorflow.
I have a little doubt that the problem may be can related to the version of Tensorflow.
My question is :

+ Can I downgrade the version of Tensorflow 1.14.1 to 1.13.1? ( all packages )




"
753,29862,0,"[TF 2.0] Converting keras model to estimator ignores data-types. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
ProductName:	Mac OS X
ProductVersion:	10.14.2
BuildVersion:	18C54
- TensorFlow installed from (source or binary): pip install ...
- TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0
- Python version: 3.6.8

**Describe the current behavior**
When converting Keras model to estimator it converts all integer inputs to floats or doubles.
This will result in data-type errors in the converted estimator.  

I am not entirely sure if this is a bug or ""feature"" because this function is indeed called in the conversion: https://github.com/tensorflow/estimator/blob/c956dd32561bac645a1cd870d3c8cfe8e9fe969b/tensorflow_estimator/python/estimator/keras.py#L62

However, this is blocking using integer-inputs in the converted estimator.

**Describe the expected behavior**
Input-data-types are preserved when converting keras-model to estimator.

**Code to reproduce the issue**
Small example to reproduce the issue:



Because input is converted to float, last line results in 

"
55,13939,1,"Quantization make graph slower during inference.. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: using TF source code (GPU build), can provide docker to reproduce environment conditions if necessary
- **TensorFlow version**: using r1.3 branch, version 1.3.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.6.1
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: GeForce GTX 1080 Ti, 11170 MB
- **Exact command to reproduce**: 
/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --in_graph=/quantization/VGG16/frozen_model.pb   --outputs=""Validation_segmentation/Validation/decoder/Softmax"" --out_graph=/quantization/VGG16/optimized_model.pb   --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""384,1248,3"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes'


### Describe the problem
Hi, I compressed a graph using transform_graph tool but the resulting graph is actually slower during inference. I am compressing a graph similar to the one presented in this article: https://arxiv.org/pdf/1612.07695.pdf, which has VGG16 as an encoder in input and a classification decoder with a Softmax in output. Inference uses same python script for both graph (original and quantized) and make an average of 100 inferences. Original graph takes ~0.1s for inference, quantized graph takes 70s! If I perform quantization without quantize_nodes, inference takes ~0.3s.

I understand that this quantization is still in a work in progress and maybe was more aimed at improving inference on mobile devices, but I'm surprised that it is actually so much slower, so that's why I'm logging it as a bug here. (I posted this on stackoverflow but didn't get any answer...)

The graph takes ~500Mb, let me know if I should attach it to this ticket (or include an external link?)

### Source code / logs
[quantization_logs.txt](https://github.com/tensorflow/tensorflow/files/1409825/quantization_logs.txt)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1409824/tf_env.txt)
[inference.py.txt](https://github.com/tensorflow/tensorflow/files/1409864/inference.py.txt)

"
1448,23718,0," Failed to load the native TensorFlow runtime.. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): using ""pip install tensorflow""
- TensorFlow version:
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:No GPU



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Installed tensorflow using pip install
When i tried to load import tensorflow as tf, it throws a log

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

--------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: dlopen(/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
  Referenced from: /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)
  Expected in: /usr/lib/libSystem.B.dylib
 in /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-3-41389fad42b5> in <module>()
----> 1 import tensorflow as tf

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/pritee/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/pritee/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
  Referenced from: /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)
  Expected in: /usr/lib/libSystem.B.dylib
 in /Users/pritee/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
1197,31414,0,"ImportError: DLL load failed: %1 is not a valid Win32 application.. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Windows 10):
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.13.1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: conda 
- CUDA/cuDNN version:  10.0.130/ 7.6.0
- GPU model and memory:  Running on laptop to demo a conda environment build so not using GPU, 16 GB memory 

**Describe the problem**

After building my conda environment using the environment.yml from a known working system, I encounter a ""ImportError: DLL load failed: %1 is not a valid Win32 application."" issue which occurs when the script attempts to import pywrap_tensorflow (see traceback below). This is strange because both the origin computer (from which the environment was copied) and the source computer are 64-bit and tensorflow is certainly not a 32-bit application. The error seems to be coming from the imp module. Has anyone else encountered a similar issue? 


**Provide the exact sequence of commands / steps that you executed before running into the problem**

conda env create -n <envname> -f  <Environment.yml>
python <main.py>


**Any other info / logs**

Traceback (most recent call last):
  File ""hyperparam_grid_search.py"", line 10, in <module>
    import tensorflow as tf
  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg).pywrap_tensorflow_internal import *
ImportError: Traceback (most recent call last):ib\site-packages\tens  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *s\tens  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()l', fp, pathna  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper_module
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\mmusil\.conda\envs\AutoTune5\lib\imp.py"", line 343,  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\imp.py"", line 243, in load_moduled(spec)
    return load_dynamic(name, filename, file)id Win32 application.
  File ""C:\Users\mmusil\.conda\envs\AutoTune5\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)ive TensorFlow runtime.
ImportError: DLL load failed: %1 is not a valid Win32 application.

"
571,15880,0,"Allow full deallocation of GPU memory. When using the TF C++ library inside an application that also uses GPUs for other tasks (not implemented in TF), it would be useful to be able to deallocate all the GPU memory TF has allocated once the session is closed, and no further TF calls are expected for the time being. gpu_options.allow_growth keeps TF's allocated pool small, but it still can grow to several GB. Even after the session is deleted, the pool doesn't shrink. To free it up, the whole application must be restarted, if I'm not mistaken.

Being able to destroy the [ProcessState](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.cc) singleton seems to solve it without breaking anything. However, its destructor is protected. Alternatively, getting the Allocator for each GPU from ProcessState and manually destroying them does the trick, but renders TF unusable for all future operations because ProcessState still thinks the Allocators exist and doesn't recreate them when they are required again.

I think making the ProcessState destructor public (or adding a public method to invoke similar code) would be the best solution, but maybe I'm missing an obvious solution that already exists?"
712,17014,0,"Importing graph with tf.contrib.resampler.resampler fails. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSx High Sierra
- **TensorFlow installed from (source or binary)**:
pip install
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.5.4
- **CUDA/cuDNN version**:
CPU
- **Bazel version (if compiling from source)**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
See below


### Describe the problem
Importing a graph def with a   op fails iff  is not imported first.

Execute:


Then, in a new interpreter (where the load(..) function is defined), execute:


This give the error message:
"
790,31928,0,"[tf.estimator] Training with tf.estimator + tf.keras and tf.keras only yields inconsistent results. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): 
- Python version: 
- Bazel version (if compiling from source): **n/a**
- GCC/Compiler version (if compiling from source): **n/a**
- CUDA/cuDNN version: **n/a**
- GPU model and memory: **none**

**Describe the current behavior**

Following the official guides ([[1]](https://www.tensorflow.org/guide/estimators#creating_estimators_from_keras_models), [[2]](https://www.tensorflow.org/beta/guide/migration_guide#using_a_custom_model_fn)), I was training [PSENet](https://github.com/sdll/psenet/tree/master/psenet) for text detection. Even though train metrics did improve to almost perfect levels and the loss remained stable and low after a while, the inference I got was gibberish.

PSENet works as follows: running the image through the feature pyramid network to obtain segmentation maps, it then applies a custom algorithm to extract bboxes. The images below show the segmentation maps, with yellow regions corresponding to the predicted text, and purple to everything else.

After 186 attempts to make it work on the AI Platform and $400 of GSoC credits, I realized that the problem was deeper than the implementation details, and decided to overfit on a single sample, using the  implementation of FPN from [](https://github.com/qubvel/segmentation_models) by @qubvel. I have tweaked his implementation for PSENet and ran into the same problems with , so it seems that  is indeed the culprit.

For this sample image

![sample image](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/input.png)
 
and one of the labels

![sample label](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/label.png)

after 300 epochs, the same loss and optimizer, the predicted labels

- with a pure  implementation:

![](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/training-with-pure-keras.png)

- with the  model converted using :

![](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/training-with-estimator-and-conversion.png)

- with the  model used in the  model function:

![](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/training-with-estimator-and-no-conversion.png)

I have tried the -in- setup on 10 000 images for 30-50 epochs, and the results are much worse than this, which is itself not perfect.

**Describe the expected behavior**

1. TensorFlow documentation should state clearly the preferred way to use  models inside , given the knowledge that  is built on  and thus the expectation that interops is seamless.

2. The discrepancy between training with  and  +  should be minimal or non-existent

**Code to reproduce the issue**

The minimal failing example with the code and data is [here](https://github.com/sdll/tf.estimator-failing-example).

**Other info / logs**

https://github.com/tensorflow/tensorflow/issues/25670 is a related issue. Similar findings are documented [here](https://stackoverflow.com/questions/54910215/tensorflow-estimator-fails-to-converge-on-model-converted-from-keras-when-using).

I can also confirm that training with  took longer than with pure , in alignment with [other reports](https://stackoverflow.com/questions/56930892/tf-estimator-vs-tf-keras-speed-disparity) of this behavior.  


The model itself is sensible, and the following example shows that it does generalize well. For this input (not in the original data):

![IMAGE 2019-08-23 17:13:30](https://user-images.githubusercontent.com/17913919/63598985-549a2f80-c5c9-11e9-9cfe-2db8e72694f9.jpg)

- the output from [another implementation](https://github.com/liuheng92/tensorflow_PSENet) written in tf.slim is as follows:

![IMAGE 2019-08-23 17:14:20](https://user-images.githubusercontent.com/17913919/63599060-72679480-c5c9-11e9-84e1-0b95a9f12edf.jpg)

- the output from the [Pytorch implementation](https://github.com/whai362/PSENet) by the original authors is this:

![IMAGE 2019-08-23 17:16:54](https://user-images.githubusercontent.com/17913919/63599211-cecab400-c5c9-11e9-95e7-e8b48a5cc2e3.jpg)"
105,28381,1,"TPU Performance Regression 1.12 -> 1.13. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 & 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0 & 1.13.1
- Python version: 3.6.8
- CUDA/cuDNN version: N/A TPU
- GPU model and memory: N/A TPU

**Describe the current behavior**

Upgrading from Tensorflow 1.12.0 to 1.13.1 shows a large performance regression. Training goes from 2100 examples/s & 8.2 global steps/s to 860 examples/s & 3.4 global steps/s.

**Describe the expected behavior**

I would expect no performance penalty or improved performance for upgrading a library.

**Other info / logs**
Here are the outputs of  for the base case & regression case respectively.
![Base Profile](https://user-images.githubusercontent.com/2245080/57163873-35bc2700-6da7-11e9-856a-677a6b021731.png)
![ Regression Profile](https://user-images.githubusercontent.com/2245080/57163877-3785ea80-6da7-11e9-87de-e05dd121be30.png)

For reference this profile was captured during training on MobileNet but we have also observed this on other convolutional architectures. We also use quantization."
975,19513,0,"Cannot use lookup table in dataset when using  Distribute MirroredStrategy. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

tensorflow-gpu 1.8.0

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

![image](https://user-images.githubusercontent.com/12636388/40460717-b398fd3c-5f3a-11e8-946c-85f8c2516951.png)
![image](https://user-images.githubusercontent.com/12636388/40460728-becca0fa-5f3a-11e8-932f-b0cb5b3ed8dd.png)

I use a lookup table in dataset map function, for mapping string labels to digits.
When I use this dataset feed to estimator using MirroredStrategy,
it gives this error.

If using eager mode, it may support this feature,  but estimator is not supported in eager mode

So how can I still keeping lookup table while using MirroredStrategy?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


"
693,20826,0,"TOCO (TensorFlow 1.9.0) -- 'TocoConverter' has no attribute 'from_keras_model_file'. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04 on Windows 10 VirtualBox
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: [Git] 'v1.9.0-0-g25c197e023'; [Version] '1.9.0'
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: 


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I'm trying to convert a keras model to the TensorFlow Lite format via TOCO. For some reason, though, the keras conversion functions seem to be missing from the TocoConverter. I've also tried using the command line feature and the same thing happens when I use the command-line version of TOCO. Is there something wrong? I just followed the [Python API tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md) and the [command-line examples.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md)
### Source code / logs
Traceback (most recent call last):
  File ""modelconv.py"", line 3, in <module>
    converter = tf.contrib.lite.TocoConverter.from_keras_model_file(""model.h5"")
AttributeError: type object 'TocoConverter' has no attribute 'from_keras_model_file'

"
734,33786,0,"Cannot import tf.keras.engine. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14 and 2.0 (gpu)
- Python version: 3.6.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/6.7.4
- GPU model and memory: RTX 2060 6GB

After switching my code to tf.keras I cannot import tf.keras.engine:
with Keras:




with tf.keras:





"
353,35413,0,"vgg19.preprocess_input doesn't work in TF2.1 autograph mode. **System information**

* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
* TensorFlow installed from (source or binary): binary
* TensorFlow version (use command below): v2.1.0-rc1-58-g9837ece 2.1.0-rc2 (python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"")
* Python version: Python 3.6.8
* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2
* GPU model and memory: Tesla V100-SXM2-16GB

**Describe the current behavior**
Running 

inside @tf.function results in exception:


**Describe the expected behavior**
Should work the same as in tf1.x
https://github.com/olegmyrk/SPADE-Tensorflow/blob/develop/vgg19_keras.py#L15

**Code to reproduce the issue**

https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L631


https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L776


https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L784

This is the line that fails:

https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/vgg19_keras.py#L15

I also tried to patch the code for preprocess_input myself
https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/vgg19_keras.py#L62
It somewhat works but judging by the scale of the VGG loss some input normalization is off.

**Other info / logs**
"
933,36,0,"No module named tensorflow.python.platform. when running 
python tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py 

get the following error: 
Traceback (most recent call last):
  File ""tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py"", line 14, in <module>
    import tensorflow.python.platform
ImportError: No module named tensorflow.python.platform
"
1148,23547,0,"How to make output sequence length equal to the input sequence length in seq2seq model?. I am trying to formulate a resource mapping problem using seq2seq model where the input sequence length varies and the output sequence length must be equal to the input sequence length.
From the manual of seq2deq in tensorflow, it seems that the output sequence length is determined by the end token or the maximal time step. So how can I make output length exactly equal to input length when building the seq2seq model?"
1088,30580,0,"TPU has XLA compilation issue on TF 1.14. I am getting an issue with using XLA on the cloud TPU on tensorflow version 1.14

**System information**
- Using Google's cloud TPU with Tensorflow 1.14


[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3379601/tf_env.txt)
System info (sanity check log message removed, full  attached above):


**Describe the current behavior**

When I run my model (which is a partial implementation of Transformer) using XLA, I get an error message.

The error message is the following:

tf.compat.v1.data.get_output_types(dataset)tf.compat.v1.data.get_output_shapes(dataset)tf.compat.v1.data.get_output_types(iterator)tf.compat.v1.data.get_output_shapes(iterator)tf.compat.v1.data.get_output_classes(iterator)

**Describe the expected behavior**

I expect there to be no error message!


**Code to reproduce the issue**
Reproduce with 

[issue.py.zip](https://github.com/tensorflow/tensorflow/files/3379603/issue.py.zip)
issue.py is the following (also zipped above):


**Other info / logs**

The issue can actually be avoided if the  file is modified slightly:

change

to

on lines 

Note that this fix works on the Google cloud TPU, may not work on other machines?

The issue was also reproduced on multiple tf versions:

: Was able to reproduce the issue
: Was able to reproduce the issue
: Was able to reproduce the issue
: Was able to reproduce the issue
: Was able to reproduce the issue
: No module named 'tensorflow.contrib' import error

All of these tf versions were run on vanilla tf docker images. The fix above where I divide the numbers by 4 also fixes the issue on all those versions as well.

tf version 2 and higher was not tested because of significant API changes, for this I need to spend some time creating a new python file with new API calls to get the issue reproduced

Images:

Issue reproduced using issue.py:
<img width=""1081"" alt=""tpu_err"" src=""https://user-images.githubusercontent.com/44978436/61009735-92e4d380-a328-11e9-9701-aed9d921ecde.png"">

Issue avoided by dividing variables by 4  (expected behavior):
<img width=""1430"" alt=""tf_no_issue"" src=""https://user-images.githubusercontent.com/44978436/61009751-9f692c00-a328-11e9-9e2c-9c2417211360.png"">

tf.compat.v1.data.get_output_types(dataset)tf.compat.v1.data.get_output_shapes(dataset)tf.compat.v1.data.get_output_types(iterator)tf.compat.v1.data.get_output_shapes(iterator)tf.compat.v1.data.get_output_classes(iterator)"
960,33016,0,"Xla and memory allocation. - TensorFlow version 1.14
- Python version: 2.7

I am defining this kind of graph:

    product_list = [(5, 6), ...] # list of pairs of integers
    data = tf.compat.v1.placeholder(dtype=tf.float64,
                                           name='input_tensor', shape=[1, 1000])
    x_i = tf.gather(data, [i[0] for i in product_list], axis=1)
    x_j = tf.gather(data, [i[1] for i in product_list], axis=1)
    res = x_i * x_j

    res = tf.concat([data, res], axis=-1)

and using  to compile an .
Then calling the compiled function seems to take a lot of time.
So my question is : is there memory allocation at every call?
If yes, how can I avoid it by telling Tensorflow to pre-allocate the memory (as all the shapes are known in compile time) 
For example are there any commands that result in a new memory allocation at every call? (,  etc..)

"
1294,7910,0,"How tensorflow handles complex gradient ?. Let **z** is a complex variable, **C(z)** is its conjugation.
In complex analysis theory, the derivative of **C(z)** w.r.t **z** don't exist. But in tesnsorflow, we can calculate **dC(z)/dz** and the result is just **1**.
Here is an example:
>x = tf.placeholder('complex64',(2,2))
y = tf.reduce_sum(tf.conj(x))
z = tf.gradients(y,x)
sess = tf.Session()
X = np.random.rand(2,2)+1.j*np.random.rand(2,2)
X = X.astype('complex64')
Z = sess.run(z,{x:X})[0]

The input **X** is
>[[ 0.17014372+0.71475762j  0.57455420+0.00144318j]
    [0.57871044+0.61303568j  0.48074263+0.7623235j ]]

and the result **Z** is
>[[ 1.-0.j  1.-0.j]
    [1.-0.j  1.-0.j]]
       
I don't understand why the gradient is set to be **1**?
And I want to know **how tensorflow handles the complex gradients in general**."
1175,28920,0,Limit of Classes (label_map.txt or label_map.pbtxt). Is there any limit in label map classes used in the training config file?
1011,29276,0,"[TF 2.0 API Docs] tf.histogram_fixed_width. ## URL(s) with the issue:

https://www.tensorflow.org/versions/master/api_docs/python/tf/histogram_fixed_width

## Description of issue (what needs changing):

### Raises listed and defined

Raises are not defined and listed

### Submit a pull request?

No"
189,3952,1,"Poor results with tensorflow DNNClassifier and cross_val_score. I am using python 3.5, tensorflow 0.10 and its DNNClassifier. If I perform a single training and testing stage, as below, the test result is decent: accuracy = 0.9333

import tensorflow as tf
from tensorflow.contrib import learn
from sklearn.cross_validation import cross_val_score, ShuffleSplit, train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn import datasets, cross_validation

iris = datasets.load_iris()

feature_columns = learn.infer_real_valued_columns_from_input(iris.data)

x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.20, random_state = 20)

model = learn.DNNClassifier(hidden_units=[5], 
                             n_classes=3, 
                             feature_columns=feature_columns, 
                            )

model.fit(x_train, y_train, steps=1000)
predicted = model.predict(x_test)

print('Accuracy on test set: %f' % accuracy_score(y_test, predicted))
If I use sklearn's cross_val_score, then the final results is much poorer, about 0.33 accuracy:

model = learn.DNNClassifier(hidden_units=[5], 
                             n_classes=3, 
                             feature_columns=feature_columns, 
                            )

scores = cross_val_score(estimator=model, 
                         X=iris.data, 
                         y=iris.target, 
                         scoring = 'accuracy',
                         cv=5,
                         fit_params={'steps': 1000},
                        )

print(scores)
print(np.mean(scores))
The scores ad their mean are:

[ 0.          0.33333333  1.          0.33333333  0.        ]
0.333333333333
What's wrong with my code in cross validation estimation?
"
25,9777,1,"tf.nn.embedding_lookup  poor performance. In my programe, I use  f.nn.embedding_lookup as follow:

embedding = tf.get_variable(""embedding"", [200000, 128], tf.float32, initializer=tf.        random_normal_initializer(stddev=0.1), trainable=True,partitioner=tf.fixed_size_partitioner(10))

word_embedding = tf.nn.embedding_lookup(embedding, query_tensor)
while(1):
    sess.run(word_embedding )


when I start 30 worker，each worker qps is 3000.
but when I test tf.nn.embedding_lookup_sparse, each worker qps is 110000

how to solve this problem,any suggestion welcome"
1226,29273,0,"[TF 2.0 API Docs] tf.keras.metrics. ## URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics

## Description of issue (what needs changing):
the link to the source code links to 404 page


### Correct links
https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/metrics.py


### Submit a pull request?
no
"
36,30791,1,"Use XLA to Tacotron2 is slower than without XLA. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:V9.0.176 / 7
- GPU model and memory: P40

**Describe the current behavior**
Before using XLA, the [Tacotron 2](https://github.com/Rayhane-mamah/Tacotron-2) runs about 0.5s, however after using XLA, it increases to about 0.7s which is much slower.

I use XLA like above. Any thoughts would be appreciated. Thanks."
1261,15291,0,"Dockerfile.devel-gpu: infinite prompt loop. 

Is it because the  branch doesn't support CUDA 9.0?
https://github.com/tensorflow/tensorflow/blob/abd5375ba8d373045321d1eebdb4501c36ab0ccd/tensorflow/tools/docker/Dockerfile.devel-gpu#L74-L76

@gunan"
160,23348,1,"CPU memory slowly filling, during inference, cannot seem to find the reason. **System information**
- We have made a specific adaptation of a inference script, that reads in tensorflow records, pushes the data through the network and then stores the result.
- Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): default Ubuntu repositories
- TensorFlow version (use command below): 1.3
- Python version: 2.7
- CUDA/cuDNN version: Cuda 8.0 CuDNN 6.0
- GPU model and memory: NVIDIA P5000 - 16GB

**Describe the current behavior**

The script runs slow but fine. However, CPU memory of the system is slowly clogging up and system will fail.

**Describe the expected behavior**

We expected the code to run through all data parts without issues.

**Code to reproduce the issue**



There are no errors reported. Any help welcome."
722,32836,0,"Detected cudnn out-of-bounds write in conv scratch buffer! This is likely a cudnn bug. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): both source and binary
- TensorFlow version (use command below): 1.14.0
- Python version:3.7
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: cuda10, cuda10.1,  7.6.0, 7.6.2
- GPU model and memory: 2080Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


2019-09-26 17:28:07.044771: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.044827: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.044863: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.044874: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.044881: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.044889: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.044896: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.044907: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.065798: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.065836: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.065866: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.065877: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.065885: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.065892: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.065899: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.065909: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.082404: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.082436: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.082464: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.082474: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.082482: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.082490: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.082497: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.082506: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.109990: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.110017: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.110045: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.110055: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.110064: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.110071: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.110078: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.110088: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.144289: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.144361: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.144396: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.144406: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.144414: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.144422: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.144429: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.144440: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.161422: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.161467: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.161501: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.161512: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.161520: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.161529: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.161536: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.161547: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.176122: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.176175: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.176206: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.176217: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.176226: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.176234: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.176241: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.176253: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.191145: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.191200: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.191232: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.191243: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.191251: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.191259: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.191266: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.191279: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.207762: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.207808: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.207856: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.207867: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.207874: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.207881: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.207889: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.207901: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.225787: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:157] Detected cudnn out-of-bounds write in conv input/output buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2019-09-26 17:28:07.225859: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:166] Redzone mismatch in LHS redzone of buffer 0x7fd3e5090200 at offset 0; expected ffffffffffffffff but was 3b0387303ca4a820.
2019-09-26 17:28:07.225900: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:167] HloInstruction %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}""
2019-09-26 17:28:07.225934: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:112] Device: GeForce RTX 2080 Ti
2019-09-26 17:28:07.225948: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:113] Platform: Compute Capability 7.5
2019-09-26 17:28:07.225966: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:114] Driver: 10010 (418.87.0)
2019-09-26 17:28:07.225981: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:115] Runtime: <undefined>
2019-09-26 17:28:07.226003: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:122] cudnn version: 7.6.2
2019-09-26 17:28:07.226796: E tensorflow/compiler/xla/service/gpu/cudnn_conv_algorithm_picker.cc:525] Internal: All algorithms tried for convolution %custom-call = (f32[5,5,2,64]{1,0,2,3}, u8[0]{0}) custom-call(f32[155,2,64,64]{3,2,1,0} %arg8.9, f32[155,64,60,60]{3,2,1,0} %select.42), window={size=5x5}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_type=""Conv2DBackpropFilter"" op_name=""HICO_1/gradients/HICO_0/resnet_v1_50_3/conv1_sp/Conv2D_grad/Conv2DBackpropFilter""}, backend_config=""{\""convResultScale\"":1}"" failed.  Falling back to default algorithm.
"
1169,21228,0,"Cloud MLEngine: Check failed: DeviceNameUtils::ParseFullName(new_base, &parsed_name). ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian GNU/Linux 9
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary (from https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.8.0/tensorflow-1.8.0-cp35-none-linux_armv7l.whl)
- **TensorFlow version (use command below)**: v1.8.0-1g0d1f389b6c 1.8.0
- **Python version**: 3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: (Running on gcloud)
date +%m_%d_%Y_%H_%M_%S

Thank you."
547,7480,0,"preprocessor definition clash with glog. CHECK macros from  leak out into  headers which clash with users of glog.

One path is through :

This one is easy to fix by moving method implementation to allocator.cc.

Another is through .


This one is more work to fix because  is used all over the code, but it does not seem to be necessary for .
"
1204,30537,0,"Colocation error with SparseApply* ops and ResourceVariables. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0/7.4.1
- GPU model and memory: GeForce GTX 980 Ti

**Describe the current behavior**

TensorFlow attempts to colocate ops with incompatible devices (when the ops are related to , e.g. , operations), which results in an error.  This only occurs when using ResourceVariables (not RefVariables).

**Describe the expected behavior**

There should be no error, and ops should be assigned to their appropriate devices (CPU vs GPU).

**Code to reproduce the issue**


**Other info / logs**
Colocation debug info:
"
250,15320,1,"Multi-core CPU performance dropped for MKL TF build. ### System information

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS (64-bit)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: Tensorflow r1.4
- **Python version**: Python version: 2.7.12
- **Bazel version (if compiling from source)**: Bazel release 0.7.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
- **CUDA/cuDNN version**: no CUDA
- **GPU model and memory**: no GPU, but i7-6850K with 32Gb ddr4
- **Exact command to reproduce**: run the script below

Tested on two machines:
1) i7-6850K with 32Gb ddr4
2) two Xeon x5650 with 24Gb ddr3

### Describe the problem
When I build Tensorflow with MKL it dropped CPU performance in a strange way. Performance of individual core is much higher, but for multicore is much worse.
It's a big epic bottleneck for my project and I can't solve it by myself. I will appreciate any help!

1) TF installation from sources with MKL support
> Tensorflow r1.4 installed from source. Configured with jemalloc as malloc support and other configure settings ignored.
> $ bazel build --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package
> $ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
> $ pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl

**Run tests:** 
one core: 0.03s
all cores: 0.12s

2) TF installation with pip
> $ pip install tensorflow
> (tensorflow-1.4.1-cp27-cp27mu-manylinux1_x86_64.whl installed)

**Run tests:**
one core: 0.16s
all cores: 0.03s


### Source code / logs

"
898,7998,0,"Strange messages shown up when define a tensorflow variable. When I try to define a variable, the following messages show. 
However, these messages will only show when I define the first variable.  


Python 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> bias = tf.Variable(tf.random_normal([1]))
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
"
132,34062,1,"BatchNorm generates NaN moving_variance on GPU with fused set to True for some inputs. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: the one on Colab


**Describe the current behavior**

I have an input of shape (1, 1, 1, num_channels), and I run it through a tf.keras.layers.BatchNormalization in training mode.

When running on CPU (fused or not) or GPU (not fused), the batch norm has the expected moving_variance of [0.99, 0.99 ...]

When running on GPU with fused=True, the moving_variance is [nan, nan, ...]

**Describe the expected behavior**

The moving variance should not be nan

**Code to reproduce the issue**
Check this [gist](https://colab.research.google.com/gist/jnd77/38ae530b17ee84e7b3907e3010c8529a/untitled0.ipynb)

**Other info / logs**
It works fine on CPU.
"
914,31645,0,"i try to run a python code using tensorflow and Mask RCNN but i get that error. check failed status == cudnn_status_success (7 vs. 0)failed to set cudnn stream

i am using 

tensorflow  1.14.0
cuda  10.0"
394,4512,0,"Indexing the last N-1 elements of  a 1D tensor raises a ""stop condition"" error.. ### Environment info

Operating System:
OSX 10.11.6
1. The output from .

0.10.0rc0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I'm trying to select the last N-1 element of a 1D tensor with N elements, for the array [0,1,2,3,4] I'm trying to extract [1,2,3,4]


### Logs or other output that would be helpful

  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 313, in _SliceHelper
    if s.stop is not None and s.stop < 0:
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 529, in __nonzero__
    raise TypeError(""Using a  as a Python  is not allowed. ""
TypeError: Using a  as a Python  is not allowed. Use  instead of  to test if a tensor is defined, and use the logical TensorFlow ops to test the value of a tensor.
(If logs are large, please upload as attachment or provide link).
"
79,30372,1,"Poor Feature/Example serialization performance. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): unknown 2.0.0-beta1
- Python version: 3.7.2
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: GeForce GTX Titan X, 12GB

**Describe the current behavior**
The rate of serializing examples is unreasonably slow. In particular, performance is slow for s that are a sequence of values. I have a dataset with numerous fields, including two that are represented by short, fixed length, 1-dimensional s. When excluding those two features, serialization happens in a slow but manageable amount of time. Adding either of those two features causes it to take many times as long.

Notably, when mapping a  to a function that performs serialization, increasing the number of threads does not significantly impact performance and the CPU remains mostly idle. I do see marked performance improvements and higher CPU usage when parallelizing other mapped functions, so it could be that there is some kind of global bottleneck for this operation.

**Describe the expected behavior**
Records should be serialized in a reasonable amount of time.

**Code to reproduce the issue**
The following code serializes 10000 records in about 6s on a particular machine. Note that if I replace the mapped function with one that simply returns a constant, it takes less than 1s to complete, so the serialization is the problem.

    def make_example_1(*data_list):
        feature_dict = {
            'a':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[0]])),
            'b':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[1]])),
            'c':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[2]])),
            'd':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[3]])),
            'e':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[4]])),
            'f':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[5]])),
            'g':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[6]])),
            'h':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[7]])),
            'i':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[8]])),
            'j':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[9]]))}
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
        return example.SerializeToString()
    
    def make_example_1_wrap(*input):
        data = input[0]
        return tf.py_function(
            make_example_1,
            (
                data['a'],data['b'],data['c'],data['d'],data['e'],
                data['f'],data['g'],data['h'],data['i'],data['j']),
            Tout=tf.string)
        
    def feature_test_1(num_threads=None):
        source = {
            'a':tf.constant(0),
            'b':tf.constant(1),
            'c':tf.constant(2),
            'd':tf.constant(3),
            'e':tf.constant(4),
            'f':tf.constant(5),
            'g':tf.constant(6),
            'h':tf.constant(7),
            'i':tf.constant(8),
            'j':tf.constant(9)}
        ds = tf.data.Dataset.from_tensors(source).repeat(10000)
        ds = ds.map(make_example_1_wrap, num_threads)
        it = iter(ds)
        for x in it:
            pass

The following example uses about the same input data size as the previous one, but uses 2 features of length 5 instead of 10 features of length 1. The execution time increases to 17s, highlighting the problem with sequences as s.

    def make_example_2(*data_list):
        feature_dict = {
            'a':tf.train.Feature(int64_list=tf.train.Int64List(value=data_list[0])),
            'b':tf.train.Feature(int64_list=tf.train.Int64List(value=data_list[1]))}
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
        return example.SerializeToString()
    
    def make_example_2_wrap(*input):
        data = input[0]
        return tf.py_function(
            make_example_2,
            (data['a'], data['b']),
            Tout=tf.string)
    
    def feature_test_2(num_threads=None):
        x = {
            'a':tf.constant([0,1,2,3,4]),
            'b':tf.constant([5,6,7,8,9])}
        ds = tf.data.Dataset.from_tensors(x).repeat(10000)
        ds = ds.map(make_example_2_wrap, num_threads)
        it = iter(ds)
        for x in it:
            pass

**Other info / logs**
A possibly related performance issue is mentioned by many users in #16933, although that issue was closed over a year ago due to inactivity."
1264,30064,0,"Deploy .pb frozen model in Android mobile. Hello,
I have worked on iris detection ( in the human eye ) in the real-time project using deep learning. So I have used a CNN model based on facial landmarks detection using Tensorflow GPU version with regression predict and I have modified the number of points to get only the iris region, 
I have now .ckpt files trained the model with 146 000 steps and I have tried to freeze the model but I'm not sure that the process of the freeze is correct or no.
I get a .pb file with size 28 Mo.
I want to deploy and use this .pb file into an Android application to detect the iris region by drawing the landmarks on it.
How can I use a .pb file for Android application?
How can I extract the prediction from .pb file in the Android application?
Thanks
"
426,15400,0,"setuptools error on upgrading to 1.4.1. - **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6 (normal install, NO virtualenv, NO anaconda, ...)
- **Exact command to reproduce**: 

When upgrading TF 1.4.0 to 1.4.1 using the command above, I get the error message:


I have been able to solve the problem with the following command:


After this, running pip3 install --upgrade Tensorflow runs successfully."
340,24220,0,"No Device Mapped with Tensorflow-gpu - Ubuntu 18.04. ## System information
- OS Platform and Distribution: Ubuntu 18.04.1 LTS x86_64
- TensorFlow installed from (source or binary): Miniconda (used cmd: 'conda install -c anaconda tensorflow-gpu')
- TensorFlow version: Tensorflow-gpu (Tried both 1.12.0 and 1.9.0, and got the same result)
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: Conda
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): GCC 7.3.0
- CUDA/cuDNN version: 9.0
- GPU model and memory: As below.

## CPU and GPU info
CPU: Intel i7-8750H (12) @ 4.100GHz
GPU: NVIDIA GeForce GTX 1070 Mobile
GPU: Intel Device 3e9b


## Describe the problem

It looks like in the 'Device Mapping' section as below, no GPU is linked, though no error msg noted in code. Strangely though, the GPU name/mode has been detected. Would you please help?

Details as below.

Python 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> import os
>>> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
>>> tf.Session(config=tf.ConfigProto(log_device_placement=True))
2018-12-08 00:44:56.363230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-12-08 00:44:56.363327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-08 00:44:56.363349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-12-08 00:44:56.363370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-12-08 00:44:56.363659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6738 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1
2018-12-08 00:44:56.371266: I tensorflow/core/common_runtime/direct_session.cc:288] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1

<tensorflow.python.client.session.Session object at 0x7f693a136ef0>




## Provide the exact sequence of commands / steps that you executed before running into the problem
 Pretty much followed this post:
https://mc.ai/install-conda-tensorflow-gpu-and-keras-on-ubuntu-18-04/



## Any other info / logs

## GPU Driver Info 

Sat Dec  8 00:56:11 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.57                 Driver Version: 410.57                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 107...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   46C    P3    24W /  N/A |    936MiB /  8119MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1142      G   /usr/lib/xorg/Xorg                           535MiB |
|    0      1313      G   /usr/bin/gnome-shell                         217MiB |
|    0      1941      G   ...quest-channel-token=4040395960093113594   181MiB |
+-----------------------------------------------------------------------------+


## cuDNN ver

cudnn-9.0-linux-x64-v7.1.tgz 


## Full info for CUDA

$ ./deviceQuery
./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""GeForce GTX 1070 with Max-Q Design""
  CUDA Driver Version / Runtime Version          10.0 / 9.0
  CUDA Capability Major/Minor version number:    6.1
  Total amount of global memory:                 8120 MBytes (8513978368 bytes)
  (16) Multiprocessors, (128) CUDA Cores/MP:     2048 CUDA Cores
  GPU Max Clock rate:                            1266 MHz (1.27 GHz)
  Memory Clock rate:                             4004 Mhz
  Memory Bus Width:                              256-bit
  L2 Cache Size:                                 2097152 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.0, CUDA Runtime Version = 9.0, NumDevs = 1
Result = PASS


## Check if cards are recognised on the bus:
$ lspci | grep -i nvidia
01:00.0 VGA compatible controller: NVIDIA Corporation GP104M [GeForce GTX 1070 Mobile] (rev a1)
01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)


## Check Drivers:
$ dmesg | grep NVRM
[    3.286908] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  410.57  Tue Sep 18 23:25:09 CDT 2018 (using threaded interrupts)


Thanks very much in advance!

"
858,23456,0,"Tensorflow softmax is not updating, docu doesn't help?. Please be so kind to delete this issue for me, thank you in advance."
1103,28398,0,"Passing tf.data.Dataset to model.predict raises ""ValueError: The `batch_size` argument must not be specified when using dataset as an input."". **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: 19.04
- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-alpha0 
- TensorFlow version (use command below): 2.0.0-alpha
- Python version: 3.7.3

**Describe the current behavior**
Running simple classification example with Keras interface
**Describe the expected behavior**
Predict results of fitted model with tf.data.Dataset using model.predict

**Code to reproduce the issue**


**Other info / logs**
batch_sizebatch_size"
544,32085,0,"Documentation for ./configure environment variables. I'm looking for documentation with all of the  environment variables that can be set to make  unattended. All I can find are various github issues or random articles using them, but is there a definitive official list somewhere?"
767,30498,0,"No `output_shape` after tf.keras layer/model build() and call. Is it intended?. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary.
- TensorFlow version (use command below):  and 
- Python version: 3.6.8
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: without and with GPU (P100)

**Describe the current behavior**

Hi, all. When I tried to use , the output shapes were printed as **multiple**. After few tries, I realized that the  of keras layers/models are not determined even after the layer/model was built and called. Here are short examples.

1. Keras layer


2. Keras model


**Describe the expected behavior**
I think keras layer/model should have , but they aren't. Please see if it's intended. I've just started to migrate from TF 1.x to TF 2.0 and to use keras APIs, so I might be wrong when using keras API.

**Code to reproduce the issue**
Described above.

**Other info / logs**
"
32,22080,1,"The performance of FusedBatchNormGradGrad is very low. When I test the performance of GAN model (https://github.com/tensorflow/models/tree/master/research/gan), I find to use FusedBatchNorm can boost the performance by 20% to 30% (https://github.com/tensorflow/models/issues/5206).

However, when use gan_loss with gradient_penalty, there is no performance improvement with FusedBatchNorm.  I find the root cause is FusedBatchNormGradGrad is very low performance.

I propose to set fused_batch_norm to false on (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/losses/python/losses_impl.py#L367) to use BatchNormGradGrad to replace FusedBatchNormGradGrad. There is about 20% performance improvement with this change. 
Change

to 

   
"
500,18354,0,"LookupError: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: b'v1.3.0-rc1-3011-gd86448938' 1.3.0
- **Python version**: Python 3.6.4 
-Bazel version: N/A
-CUDA/cuDNN version: N/A
-GPU model and memory: N/A
- **Exact command to reproduce**:



I'm not sure if this is a bug or feature request. "
185,24423,1,"Difference in performance of Python and cpp libraries:. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.8
- Python version:3.5.0
- Bazel version (if compiling from source):1.19
- GCC/Compiler version (if compiling from source):msvc 2015
- CUDA/cuDNN version:No
- GPU model and memory:No


Issue:
The issue is related to the performance of tensorflow on Cpp and python
I have installed tensorflow(1.8) on both windows and python (binary)
I am using tensorflow to predict the score of any image using a model as .pb files
But the time taken by tensorflow graph function in python is about 50ms and 
that of cpp is about 500ms.
I had assumed that cpp would be more faster but the results were opposite.
What can i do to increase the performace of the tensorflow in winodows.
I am using the pre-built binary in windows build using cmake release mode.
"
801,21500,0,"Memory error with tensorflow GPU. I have NVIDIA GPU Titan V installed on my remote machine and I am trying to do simple multi class lassification using CNN



 


here my csv_data zip file is 65MB .








I am getting memory error at 


I think it is trying to load all the data in memory which is obviously a bad practice. How can I optimize this code to load it in the memory efficiently.

Can anyone please point me to a detailed example or tutorial? 

Thanks"
1004,23300,0,"AttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant'. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA
- TensorFlow installed from (source or binary):
From pip install
- TensorFlow version (use command below):
1.11.0
- Python version:
3.6.6
- Bazel version (if compiling from source):
NA
- GCC/Compiler version (if compiling from source):
NA
- CUDA/cuDNN version:
???
- GPU model and memory:
NA

**Describe the current behavior**
After installing 1.11.0 from pip, whenever I do , I get the error in title.  If I downgrade to 1.10.0, the installation appears to be successful, or at least the import command works.

**Describe the expected behavior**
1.11.0 should work out of the box when doing a pip install

**Code to reproduce the issue**
All you need is  to see the problem

**Other info / logs**
NA"
1059,18266,0,"Cache lockfile already exists. Using the train_and_evaluate method from estimator API and cache policy on filesystem, I get an error because the evaluation starts before that all cache is written on the filesystem. So when the train runs the second time, after the evaluation, I get the error because it finds the lock file. 
If the cache is written before the first evaluation I don't get the error.

Here a snippet runnable on colab 


and here the output


"
729,15941,0,"Import Error: No module named '_pywrap_tensorflow'. On running the following command: import tensorflow I get an error:




I have the following system features:

windows 64 bit
python 3.5.0 64 bit
Nvidia computing toolkit/CUDA/v8.0./(the cuDNN version 6.0)
all of them are added to my path location also which is: Python\Python35\Scripts
i have tensorflow in Python\Python35\Lib\site-packages\tensorflow
I even have a _pywrap_tensorflow.so file and pywrap_tensorflow.py"
1093,18995,0,"compiling c++ code with TFLite library gives error with cstring. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: N/A (using only source files)
- **TensorFlow version (use command below)**: N/A
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: g++ 5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm trying to compile custom c++ code with command


and it gives some error that some functions in cstring cannot be found.

When I delete line  from flatbuffers/base.h, errors from cstring disappear but others remain. Including cstring in flatbuffers.h also gives same errors.
I'm using flatbuffers cloned from git google flatbuffers repository. Is this can be a problem?


I've made c++ tensorflow lite library with command

with lite/BUILD including



Thanks.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

#### C++ Code


#### Log
"
239,19390,1,"depthwise_conv2d_native too slow. ***edit***: Simplified the example, added system info

According to this [thread](https://github.com/tensorflow/tensorflow/pull/17961) Tensorflow now uses the CuDNN accelerations of group convolutions for depthwise_conv2d_native. Thank you for working on this! However, I am having a hard time reproducing any gains from the accelerated version. Both the native and accelerated versions of depthwise_conv2d_native is about 3-4 times slower than doing a full (dense) convolution.

In the example below, a dense 3x3 convolution with 64-in and 64-out channels should do about 16 times more multiplications compared to a group convolution with the same dimensions and 16 groups (64x3x3x64 vs 16x4x3x3x4). The latter can be implemented with depthwise_conv2d_native with channel_multiplier of 4 followed by a sum. So I expect a fully amortized depthwise_conv2d_native to be 16 times faster than a dense convolution, and yet, it is about 4 times slower. It is also substantially slower than naive slice/convolve/concat implementation of group convolution


### System information
- **Have I written custom code**: no
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from (source)**: from May 15, 2018 commit 1521eeb676383417b33ad55ad73b152bd5b046ca
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.1, 7.1.3
- **GPU model and memory**: GeForce GTX 1080ti (tf compiled for compute capability 6.1) and Volta (V100, tf compiled for compute capability 7.0)
- **Exact command to reproduce**: see code below


Here are my results on GTX 1080ti:

Here is the code I used to test performance:


I also tested with NHWC with similar results. Let me know if I am doing something wrong or whether you can replicate my perf results. Thanks!
"
654,34185,0,"TF-TRT optimized graph get wrong results when using batch size <=7. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.2
- GPU model and memory:Tesla T4 with 15001MiB memory

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
When using TF-TRT optimized graph to do inference, it only works as expected when batch size >=7(I tested batch size from 1-16). And the result is completely wrong when batch size <7
**Describe the expected behavior**
Small batch size should also give me the correct result since my original un-optimized graph can handle small batch size.
**Code to reproduce the issue**
Setup according to  https://github.com/tensorflow/tensorrt/tree/master/tftrt/examples/object_detection:

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
My frozen graph can be download at https://drive.google.com/open?id=1s_laMhRw8GC6I9brQjn6apKTfhWe734J

"
519,23010,0,"TensorFlow Lite installation on Raspberry Pi. Hi,

I've tried stackoverflow but thus far I've not had a reply - so feel free to close off is you feel this is unnecessary.

[https://stackoverflow.com/questions/52804669/tensor-flow-lite-raspberry-pi-installation](url)

I'm just looking from some clarity. I've build TF Lite on my RPi3 and the static lib's been created without issue.

At the moment python is returning the error:

> Traceback (most recent call last): 
>    File ""label_image.py"", line 23, in
>       <module> import tensorflow as tf

What I'm uncertain of is if it pip install TF then how can I know/be certain that I will get/be using TF Lite as apposed to the full version?

Also are there any stats re: TF Lite on the Pi vs full version for image classification?

Thanks & Regards

Mark
"
726,74,0,"Cannot run the android example on Android 5.1.1. The android example APK can be successfully compiled and installed (except that adb -g option is not recognized), however, at launch it shows ""Failed"" without showing any camera view.

Device is a Nexus 6 with Android 5.1.1.

This is what I have in WORKSPACE



logcat information


"
322,1459,0,"partial_run segfault. Running partial_run twice results in a segfault. By running twice I mean: setting up placeholder, ops and completing one run through the graph and then trying to do another run.
### Environment info

Operating System: Linux. 
Version: 0.7.1
Hash: 028d0b4
### Steps to reproduce


### What have you tried?
1. Changing execution target to CPU instead of GPU (same problem)
2. Reusing the same handle for the second run results in a  error because the input has already been fed
### Logs or other output that would be helpful

Core dump is 1.7 GB. Can attach if useful
"
692,35159,0,"Cannot successfully serialize and restore model using either 'tf' or 'h5' format. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux on Google Cloud instance
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.1.0.rc1
- Python version: 3.7.3

**Describe the current behavior**

I've attempted to save a TF regression model in 'h5' format as follows:

    model_mean.save(HITTER_MODEL_DIR + 'hitter_model_mean.h5', save_format='h5')

however this fails:



I then attempt to save as 'tf' format:

    model_mean.save(HITTER_MODEL_DIR + 'hitter_model_mean', save_format='tf')

which succeeds, however, attempting to load the model fails:


"
57,11894,1,"Slow Hessian. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have written custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.4.0-47-generic #68-Ubuntu and Also macOS Sierra 10.12.3 (CPU only) 
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: cuda/8.0 and cudnn/5.1
- **GPU model and memory**: 12GB, memory:100GB
- **Exact command to reproduce**: tf.hessians(ys,xs)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
The command tf.hessians(ys, xs) is used to add nodes to the graph in order to compute hessians of ys with respect to xs where both ys and xs are list of tensors. Currently computing hessian with respect to a vector is not possible. The trick is to unstack the input tensor (x) into a list of one dimensional tensors (xs) and compute hessian with respect to each of them separately then stack them together. Tensorflow gets stuck in the phase of graph construction even when the dimension of x (length of list xs) is about one hundred. In the implementation of Hessian in gradients_impl.py the second derivative is implemented as the derivative of partial derivative with respect to each member of xs. I guess the slowness of graph construction is due to this line:

_hess = [gradients(_gradient, x, **kwargs)[0] for _gradient in _gradients]

which may add several unnecessary intermediate nodes with overlapping functionality to the graph due to the for loop. Is there any way other than looping over input dimensions that efficiently constructs the graph in a reasonable time?

### Source code / logs
This source code can simulate the problem:
import tensorflow as tf
import numpy as np
in_dimension = 256
x = tf.placeholder(tf.float32, shape=(1, in_dimension))
x_list = tf.unstack(x, axis=1)
xx= tf.stack(x_list, axis=1)
y = tf.pow(xx,3)
hess = tf.hessians(y, x_list)
sess = tf.Session()
print(sess.run(hess, feed_dict={x : np.random.normal(0, 1, size=(1, in_dimension))}))
"
191,28020,1,"tf.control_dependencies does execute nodes in the right order. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.10.0-rc1-19-g656e7a2b34,  1.10.0
- Python version: 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
with tf.control_dependiencies does not execute the dependant nodes before the nodes in the with block

**Describe the expected behavior**
the nodes in the with block are executed after the dependant nodes

**Code to reproduce the issue**
import tensorflow as tf
import numpy as np

x = tf.Variable(initial_value = 0.0, name = ""x"", dtype = tf.float32)
y = tf.Variable(initial_value = 0.0, name = ""y"", dtype = tf.float32)
zero = tf.constant(value = 0.0, dtype = np.float32, name = ""zero"")
op1 = tf.assign(x, zero)
with tf.control_dependencies([op1]):
    op2 = tf.assign(y, x)
with tf.control_dependencies([op2]):
    tf.assign_add(y, 1, name = ""assign_add"")

sess = tf.Session()
sess.run(""assign_add"")
print(""y"", sess.run(""y:0""))

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
376,24616,0,"Tensorflow 1.10. Hi . I have unstatement import issue while using Pycharm community edition. I’m using this command 
import tensorflow as tf 
Python : 3.6.0
Tensorflow : 1.10.1
"
162,35124,1,"Poor performance of model.fit and/or model.predict in TF 2.1.0-rc1. **System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Kubuntu 18.04, kernel 5.0
- Mobile device: Not verified on mobile devices
- TensorFlow installed from: binary via 
- TensorFlow version: , however affected are also , , ,  and .
- Python version: 3.6.9
- CUDA version: 10.1 for TF 2.1.0-rc0; 10.0 for the earlier versions of TF
- cuDNN version: 7
- GPU model and memory: Nvidia GeForce GTX 1050 Ti (4GB)
- CPU model: AMD Ryzen 7 1700

**Describe the current behavior**

The code attached below works about 10x slower with TF  and  than with . This code was posted earlier by @ipsec in the issue #33030 in connection with memory leaks in TF . Now, in  the memory leaks seem fixed, but the execution of the code in question exhibits poor speed performance. Said that, the performance was even worse in  and pre-releases , but the about 10x slowdown in  compared to  is significant.

Here I provide some more results obtained on my device:

* With TF , one ""episode"", i.e. one iteration of the  loop in the below code, takes ca. 8.5-9.5 seconds in the graph execution mode and ca. 7-7.5 seconds with the eager execution enabled. The memory consumption of the Python process associated with this code is ca. 600 MB.

* With TF from  to  I obtain very similar results.

* With TF from  to  one episode takes ca. 120 seconds. There is also a memory leak. The memory usage after the 1st episode is ca 2 GB and is growing ca 1.5 GB per episode.

* For TF  and  the memory usage is ca. 800 MB and doesn't grow significantly from episode to episode, so the memory leak is fixed. Note that this memory usage is higher than in  but the difference is plausible. Nevertheless, for  and  I have measured the execution time to be ca. 75 seconds per episode, so the code execution is still very slow compared to .

**Describe the expected behavior**

The code attached below should be executed in  comparably fast to . The difference in the execution time shouldn't be 10-fold.

**Code to reproduce the issue**

This code has already been posted by @ipsec in the issue #33030. Here I only add time measuremets, change verbosity details and reduce the default number of episodes.




"
604,26136,0,"Estimator.train doesn't respect the RunConfig keep_checkpoint_max limit. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
UBUNTU 16.04.5 LTS (Xenial Xerus)
- TensorFlow installed from (source or binary):
pip version 1.12 tensorflow-gpu
- TensorFlow version (use command below):
1.12.0
- Python version:
3.5

- CUDA/cuDNN version:
9.0
- GPU model and memory:
Tesla V100 16G

**Describe the current behavior**
Running Estimator training as follows, should only keep 1 checkpoint file version (latest)
instead all checkpoints are kept.

session_config = tf.ConfigProto()
session_config.allow_soft_placement = True

run_conf = RunConfig(
            model_dir=out_dir,
            save_summary_steps=400,
            save_checkpoints_steps=5000,
            keep_checkpoint_max=1,
            session_config=session_config,
            tf_random_seed=None, 
            keep_checkpoint_every_n_hours=None,
            log_step_count_steps=10000, 
            train_distribute=None, 
            device_fn=None,
            protocol=None,
            eval_distribute=None,
            experimental_distribute=None
        )


estimator = tf.estimator.Estimator(
            model_fn=model.build_model_fn,
            config=run_config,
            params=params
        )



**Describe the expected behavior**
Only keep_checkpoint_max checkpoints should be saved




"
424,34287,0,"Cannot dlopen some GPU libraries - Tensorflow2.0 . <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  (using pip install tensorflow-gpu)
- TensorFlow version: 2.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: V100 , memory_limit: 17179869184




**Describe the problem**
I am using tensorflow2.0. The version of cuda installed on my system is cuda 10.0.
I installed tensorflow-gpu. My machine has a gpu-device available and I am trying to get tensorflow-gpu to work.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf
tf.test.is_gpu_available()


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Output:
2019-11-14 22:27:38.319853: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-14 22:27:38.327036: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2019-11-14 22:27:38.328593: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560728b482f0 executing computations on platform Host. Devices:
2019-11-14 22:27:38.328624: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-11-14 22:27:38.330948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-11-14 22:27:40.529184: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560728c428e0 executing computations on platform CUDA. Devices:
2019-11-14 22:27:40.529229: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0
2019-11-14 22:27:40.529242: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla V100-PCIE-16GB, Compute Capability 7.0
2019-11-14 22:27:40.531923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:86:00.0
2019-11-14 22:27:40.532794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:d8:00.0
2019-11-14 22:27:40.532945: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs
2019-11-14 22:27:40.533020: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs
2019-11-14 22:27:40.533090: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs
2019-11-14 22:27:40.533159: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs
2019-11-14 22:27:40.533228: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs
2019-11-14 22:27:40.533294: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs
2019-11-14 22:27:40.533364: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/singularity/3.2.0/libexec:/usr/local/singularity/3.2.0/lib:/usr/local/squashfs/4.3.0.21/lib64:/usr/local/torque-releases/torque-6.1.2-el7/lib:/.singularity.d/libs
2019-11-14 22:27:40.533381: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2019-11-14 22:27:40.533434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-14 22:27:40.533451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 
2019-11-14 22:27:40.533463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y 
2019-11-14 22:27:40.533474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N 
False
"
759,22677,0,"Tensorflow serving does not work for TF 1.11.0 . 
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No
- **TensorFlow installed from (source or binary)**:Binary with gpu supported
- **TensorFlow version (use command below)**:1.11.0
- **Python version**:python 27
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**: ec2 p2.16xlarge 16 NVIDIA K80 GPUs
- **Exact command to reproduce**:


### Describe the problem
Hi all, I already opened this issue in tensorflow-serving but I think this might be a better place to open this issue. Please let know if I need to close the previous one.
I used the code below to test whether tensor-serving works with tensorflow. Here are some combinations and results:
TF 1.10.0 TF-Serving 1.10.0 Success
TF 1.11.0 TF-Serving 1.10.0 Failed
TF 1.11.0 TF-Serving 1.11.0rc1 Failed

### Source code / logs


Log
"
20,548,1,"Latest dev release actually slower than 0.5. After updating TensorFlow to the most recent source yesterday (I'm at b1cabed4e60015602dacd66ea39d419db50c3e1b), I've noticed that while GPU utilization frequently appears much higher in nividia-smi than in prior releases, my actual code is much slower. Some sequence to sequence models I was training began taking 3-4 times as long per step, despite GPU utilization hovering between 60 and 99%, which is much higher than I have observed in the past. As I have code for benchmarking fully connected feedforward networks on MNIST in various frameworks, I dusted that off and, again, slower. Previously, training a network with three hidden layers of 2,048 rectified linear units + dropout (input + hidden) took 1.78 seconds per epoch (averaged over 10 epochs)  when trained using vanilla SGD with momentum and a minibatch size of 256. That is now up to 65.2 seconds. This holds across different combinations of hidden layer sizes and minibatch sizes. On the other hand, convolutional net performance does not seem to be affected as when I run Soumith's convolutional net benchmarks, I get numbers close to what he originally reported using the same test setup.

So to summarize, I've been recompiling TensorFlow regularly (every few days since its release) and after the most recent compile noticed a quite substantial performance hit for vanilla fully connected feedforward and recurrent architectures, but not for convolutional networks. This is all with TensorFlow running on a Titan X with no other processes running and using the most recent versions of CUDA, cuDNN (well, I have v3 installed, not the release candidate for v4), cuBLAS, etc.
"
415,18038,0,"static graph vs dynamic graph. since dynamic graph has so many advantages, why not tf use dynamic graph from the bey beginning? Is static graph has any advantages over the dynamic?"
465,30447,0,"kissfft compile errors due to old archived version of kissfft. external ""kissfft"" fails to compile on Windows 10

**System Information**
Windows 10 v1903 on desktop computer 
Building Latest git pull of Tensorflow (7/6/2019)
MSys64
Python 3.6.8 :: Anaconda, Inc.
CUDA 10.1
cuDNN 7
Bazel 0.24.1
MS Visual Studio 2019 Community
NVIDIA GeForce GTX 850M
TF_CUDA_COMPUTE_CAPABILITIES=""5.0""

Build Command:
bazel build --config=opt --config=v2 --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

Problem:
When building Tensorflow from source, I get the following errors:

c:\users\lawrence\_bazel_lawrence\jc3cdzjy\execroot\org_tensorflow\external\kissfft\kiss_fft.h(60): error C2061: syntax error: identifier 'int16_t'
...

The problem is line 46 of kiss_fft.h:
""#include <sys/types.h>""
which is an attempt to define int16_t. This doesn't work in Windows, and is depricated in unix. The correct approach is:
""#include <stdint.h>""

NOTE: The author of kissfft has fixed this in recent distributions (see https://github.com/mborgerding/kissfft/blob/master/kiss_fft.h) so updating the tensorflow external archive with a more recent version of kissfft would resolve this issue.
"
1450,30376,0,"Tflite for microcontrollers : micro speech example with more keywords (yes, no, up, down ..) fails. **System information**
- Have I written custom code : No
- OS Platform and Distribution : Linux Ubuntu 16.04 host machine with TensorFlow docker image
- Mobile device : SparkFun Edge Development Board
- TensorFlow installed from (source or binary): Source
- TensorFlow version : 1.14.0
- Python version: Python 2.7.15+
- Bazel version : 0.27 (apt repository)
- GCC/Compiler version : 7.4.0
- GPU model and memory: Nvidia GTX 1070

**Describe the current behavior**

I am trying to extend the [Micro Speech example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech) by adding more keywords such as 'yes, no, stop, go, up, down' etc. I followed the instructions for  training and converting the model to tflite format from the [Micro Speech GitHub page](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model). However, when I add my newly trained and converted 'tiny_conv_micro_features_model_data.cc' file to my project, compiling and running it on the Sparkfun Microcontroller gives the following error :

 '_Bad input tensor parameters in model_' . 

**More detail and analysis on the above problem**

I successfully compiled and ran the Micro Speech example on SparkFun Edge Development Board using [AI on a microcontroller with TensorFlow Lite and SparkFun Edge](https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#0) tutorial.

Next, I trained the model for recognizing more (""yes,no,up,down"")  keywords giving the following commands:

> bazel run -c opt --copt=-mavx2 --copt=-mfma --verbose_failures --host_force_python=PY2 tensorflow/examples/speech_commands:train -- \
--model_architecture=tiny_conv --window_stride=20 --preprocess=micro \
--wanted_words=""yes,no,up,down"" --silence_percentage=25 --unknown_percentage=25 --quantize=1

Then I added the newly created 'tiny_conv_micro_features_model_data.cc' file to the [micro_speech_test project](https://drive.google.com/file/d/1cawEQAkqquK_SO4crReDYqf_v7yAwOY8/view) in order to verify that the newly trained model works correctly. Compiling the project and running the binary gives the following error: 

![image](https://user-images.githubusercontent.com/36483321/60624860-310df200-9d9b-11e9-8fef-83cab23fc777.png)

This shows that interpreter requires an input tensor of shape: (1, 49, 43, 1), but the interpreter receives tensor of shape (1, 49, 40, 1) from the newly trained model. 

If the model is trained on input tensor of shape  (1, 49, 43, 1) as per the the [ spectrogram ](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#calculating-the-input-to-the-neural-network) of (49x43) pixels, how does the model interpreter of the micro speech project receives wrong shaped input tensor? 

Is it because we select input shape as (1, 49, 40, 1), while converting '.pb' file to '.tfilte' file by running the bazel command:

> bazel run tensorflow/lite/toco:toco -- \
--input_file=/tmp/tiny_conv.pb --output_file=/tmp/tiny_conv.tflite \
--input_shapes=1,49,40,1 --input_arrays=Reshape_1 --output_arrays='labels_softmax' \
--inference_type=QUANTIZED_UINT8 --mean_values=0 --std_values=9.8077

as per the instruction on [GitHub page](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model) If this is the reason why is the input shape been selected as (1,49,40,1) and not (1,49,43,1).

I also tried the above command by changing the input_shapes argument as --input_shapes=1,49,43,1. But this gives the following error:

![image](https://user-images.githubusercontent.com/36483321/60626163-c8c10f80-9d9e-11e9-9bff-b49f9a92cedb.png)


To verify that whether this error occurs due to adding more keywords to the wanted words argument (such as 'yes, no, up, down'), I trained the model only for 'yes,no' keywords. Still the same error and problem persisted as above.

I have not made any changes to the TensorFlow or micro speech example source files for the above experimentation. I just replaced the pre-existing 'tiny_conv_micro_features_model_data.cc' file by the new 'tiny_conv_micro_features_model_data.cc' file of the newly trained model for verifying my work.

Kindly guide me to solve my above problem.

Thanks."
1234,9539,0,"load a checkpoint and use it to create a new graph. **system information:**
I am using the latest Tensorflow code on Ubuntu 16.04

**problem:**
because the tensorflow SSD can't directly output the final bounding box that i want, so i want to use the orginal checkpoint to create my graph. But i failed, i really wish someone could help me!!! Thanks!!

**error:**
i get the error:


**Source code**
"
554,25077,0,"No code examples for rewriting existing graph using tf.quantization.fake_quant_with_min_max_vars for quantization-aware training. <em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12.0
- Doc Link:


**Describe the documentation issue**
It would be great if code examples or a tutorial can be written up to help with manually inserting fake quant nodes into an existing TensorFlow graph. The current rewriter provided in contrib/quantize doesn't seem to be able to handle complex graphs well.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
880,25799,0,"Error importing TensorFlow nightly gpu (docker). **System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: Docker
- TensorFlow version: nightly-gpu
- Python version: 2
- CUDA version: Driver Version: 410.79 CUDA Version: 10.0
- GPU model and memory: Nvidia Tesla P100 16280MiB

(similar issue when installed from pip, not in a container with cuDNN v7 not sure if this would affect the Docker container)

**Describe the problem**
Using  image with the following code

yields

as output

Using  image works perfectly

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Fresh installation of ubuntu 18.04  
installed nvidia drivers and cuda through  
verified by building code samples and running the  executable as described [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#running-binaries)

installed docker as described [here](https://docs.docker.com/install/linux/docker-ce/ubuntu/) and nvidia-docker as described [here](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0))


same issue when using version installed from pip (), not in a container
cuDNN v7, installation verified by running [mnistCUDNN](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#verify) example


**Any other info / logs**
Running through gdb (not in a container, using pip version as above) gives the following

"
843,2801,0,"Error building from source:  ""/usr/bin/env: python2.7: No such file or directory"". I'm building tensorflow from source but I don't know why python2.7 is required. Does anyone know whether I can change the crosstool_wrapper_driver_is_not_gcc file?

ERROR: ~/.cache/bazel/_bazel_czeng/211970d9a1065a07eafbeb7dcc542b10/external/protobuf/BUILD:331:1: Linking of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /csproject/dygroup2/czeng/.cache/bazel/_bazel_czeng/211970d9a1065a07eafbeb7dcc542b10/execroot/tensorflow && \
  exec env - \
  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/external/protobuf/protoc bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/external/protobuf/libprotoc_lib.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.
/usr/bin/env: python2.7: No such file or directory
"
964,23930,0,"Failed to load the native TensorFlow runtime.. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>


Hi!
I worked with Tensorflow on without GPU. No I'm trying to run it on a device with following specs.
 I have get the follow error message, how I can solve it?

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 Home
- Mobile device : ThinkPad X1 Yoga 1st, Intel(R)Core(TM)i7-6500U CPU@2.50GHz 2.59GHz
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.9.0
- Python version: 3.7.0

**Describe the problem**

ImportError: Traceback (most recent call last):
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\y_pas\python\Anaconda3\envs\tensor\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 指定されたモジュールが見つかりません。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.



"
820,23726,0,"Win 10: Failed to load native runtime - DLL load failed. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 1.12.0
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?:
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: Geforce 1050 Ti 8gb



**Describe the problem**

I am unable to import tensorflow. It gives the error below.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Updated my GPU drivers
2. Installed CUDA 10.0
3. Downloaded the cuDNN tool
4. Added both CUDA and cuDNN bin to PATH
5. pip installed tensorflow-gpu
6. Tried to import

7. did pip uninstall tensorflow-gpu
8. tried pip install tensorflow-gpu==1.10.0 (same error)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Error Shown:


"
678,30406,0,"how to use tensorflow::tensor in c++. 
python code:
 input_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 256, 512, 3], name='input_tensor')
 phase_tensor = tf.constant(False, tf.bool)

c++ code:
tensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 256, 512, 3}));
    tensorflow::Tensor phase(tensorflow::DT_BOOL, tensorflow::TensorShape());
    phase.scalar<bool>()() = false;

this convertion is right?
who can help me?


"
1370,11462,0,"Retrieving LLVM IR from AOT tfcompile. Is it possible to get LLVM intermediate representation (.ll) files from tfcompile instead of object code? If so, how can it be done? Alternatively, can one get source code instead of object code from tfcompile?"
1123,25776,0,"convert yolov3 model to tflite. not able to convert yolov3 model to tflite format

converted the weights file to pb format

then

./bazel-bin/tensorflow/lite/toco/toco --input_file=yolov3_finale_manga109.pb --output_file=foo.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_arrays=input --input_shapes=1,406,406,3 --output_arrays=output --allow_nonexistent_arrays

got error

2019-02-15 17:23:48.560625: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2044 operators, 2743 arrays (0 quantized)
2019-02-15 17:23:48.601097: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 0 operators, 2 arrays (0 quantized)
2019-02-15 17:23:48.601134: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:136] Model is empty!!!
2019-02-15 17:23:48.601217: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 0 operators, 2 arrays (0 quantized)
2019-02-15 17:23:48.601228: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:136] Model is empty!!!
2019-02-15 17:23:48.601237: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 0 operators, 2 arrays (0 quantized)
2019-02-15 17:23:48.601243: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:136] Model is empty!!!
2019-02-15 17:23:48.601249: W tensorflow/lite/toco/tooling_util.cc:1263] Fixing constant output array output by inserting a copy. This is not optimal.
2019-02-15 17:23:48.601260: F ./tensorflow/lite/toco/model.h:2134] Check failed: has_shape() 
"
780,31241,0,"Error when changing transformer hyperparameters. ## URL(s) with the issue:
https://www.tensorflow.org/beta/tutorials/text/transformer#set_hyperparameters

## Description of issue (what needs changing):
I've been working with the 'Transformer model for language understanding' notebook on my own dataset. I got it to work with the default hyperparameters. The tutorial explains that I can create a Transformer XL by adjusting the hyperparameters to those that are used in the paper. I changed them to the suggested values, and I am now getting a  when I try to train.

I think that this means that some object is not configured properly (not using the hyper parameter variables), but I can't figure out where it's happening. I tried restarting the runtime and running everything again, but it didn't help.

## System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have made small adjustments to the provided code in the transformer notebook to accommodate my own data. I also changed the values of the hyper parameters of the model.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  I am running the notebook in a Colab GPU runtime; Linux Ubuntu 18.04.2
- TensorFlow installed from (source or binary): I'm not exactly sure, I install it using this command, provided with the notebook 
- TensorFlow version (use command below): 
- Python version: 3.6.8
- CUDA version: CUDA: 10.0.130
- GPU model and memory: I'm not sure how to get this info, but it's a Colab GPU runtime.

## Code snippets
I changed the output encoder from a  to a :

I changed the  function to use a single argument instead of two:

And I changed the hyperparameter values:
"
832,6471,0,"how to custom learning rate decay policy ?. as it shown in the learning rate deay **[API doc](https://www.tensorflow.org/api_docs/python/train/decaying_the_learning_rate)** , there are only 4 kinds of policies. but in caffe  policies are as follows:  
 
1.   //    - fixed: always return base_lr.  
2.   //    - step: return base_lr * gamma ^ (floor(iter / step))  
3.   //    - exp: return base_lr * gamma ^ iter  
4.   //    - inv: return base_lr * (1 + gamma * iter) ^ (- power)  
5.   //    - multistep: similar to step but it allows non uniform steps defined by  
6.   //      stepvalue  
7.   //    - poly: the effective learning rate follows a polynomial decay, to be  
8.   //      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)  
9.   //    - sigmoid: the effective learning rate follows a sigmod decay  

in issue #2922 they mentioNed on write a warpper for the existing method, but they did not implemented anything new ?  for example the **inv**  policy in caffe.

so I wonder if there anyone who can help me out how to custom learning rate decay policy by myself, any suggestions will be sincerely gratitude. 

"
621,18934,0,"Patch Request: Move CROSSTOOL_nvcc.tpl to c++14. CUDA9.0 is already supporting C++14 now:
https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp14

"
538,32320,0,"keras model.evaluate() progress bar WAY too long by default. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): just testing this https://www.tensorflow.org/tutorials/keras/basic_classification
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 fully updated
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 
- Python version: 3.7.4
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 1660 Ti, 6 GB

**Describe the current behavior**
Just running a basic image classifier with Keras. Import data, create model, train, evaluate.
I run  in the Command Prompt.
 prints out an insanely long progress bar at the end. It's many, MANY pages long, with Command Prompt already maximized (so one page is already a lot of characters). I have to scroll WAAAY UP to see the previous output.

**Describe the expected behavior**
I know I could turn off verbosity, but I would expect sane defaults for the progress bars printed by TF/Keras. And with  that thing is so huge, it's useless.

**Code to reproduce the issue**

"
1221,18082,0,"Android speech recognition sample averaging wrong values. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: none
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  not related to this question
- **TensorFlow installed from (source or binary)**: not related to this question
- **TensorFlow version (use command below)**: not related to this question
- **Python version**: not related to this question
- **Bazel version (if compiling from source)**: not related to this question
- **GCC/Compiler version (if compiling from source)**: not related to this question
- **CUDA/cuDNN version**: not related to this question
- **GPU model and memory**: not related to this question
- **Exact command to reproduce**: not related to this question

### Describe the problem &  Source code / logs
Please refer to this question and comments on stackoverflow: https://stackoverflow.com/questions/49269555/logging-and-deque-operation-problems-in-tensorflow-android-speech-recognition-sa

In brief, previousResults.addLast should be revised. Otherwise, it will take wrong values to average. 
"
1282,17619,0,"undefined symbol error for _dataset_ops.so on rasp pi. 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:



- **TensorFlow installed from (source or binary)**: 

installed from nightly wheel 

- **TensorFlow version (use command below)**:



- **Python version**: 



- **CUDA/cuDNN version**:  N/A   running cpu on pi

Have I written custom code NA

Bazel version NA

GPU model and memory NA

- **Exact command to reproduce**:



### Describe the problem

Import of slim fails with .so error (as described above). doubt it's actually slim, it was just the first import to reference 

"
970,11957,0,"cudnn failed with GTX960. I am trying to use conv2d (for cnn) with GPU, and there are one GTX 960 and Quadro 600 on my computer. I set up the  to use GTX 960, but I get



### System information
- I Just using Haskell's binding, but I don't think that is key to this problem, and the libtensorflow.so is the gpu one.
- system

- Using libtensorflow.so download from official site
- tensorflow 1.0
- Using Haskell's binding
- CUDA-8.0 & cuDNN-5.1

- gpu:

"
656,15655,0,"tf.layers.conv3d with ""channels_first"" does not accept batch dimension to be None. code to reproduce:



traceback


The error source appears [here][1] and can be simply fixed by adding



however you might suggest a deeper fix?


[1]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L181-L185"
694,20509,0,"Cannot use tf.metrics with MirroredStrategy. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: using nvidia containter: https://docs.nvidia.com/deeplearning/dgx/tensorflow-release-notes/rel_18.06.html#rel_18.06 
- **TensorFlow version (use command below)**:1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: nvidia tesla v100
- **Exact command to reproduce**: N/A



### Describe the problem
I'm training a network using a custom tf.estimator. To monitor its training i used several metrics from tf. metrics: accuracy, auc, true positives, ... When running this on a single GPU, This works as expected, however when using  i get an exception. Apparently all metrics suffer from the same bug (tested by iteratively leaving the metric out).

### Source code / logs
I tried to extract the relevant part from my code:


This results in the following error trace:

"
1182,35316,0,"Where is the pip package for TF 1.15?. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu (or similar)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: 1.15
- Python version: python3
- Installed using virtualenv? pip? conda?: virtualenv and pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**
Version 1.15 of TF does not seem to exist on pip, but the documentation [states that it does](https://www.tensorflow.org/install/pip?lang=python3#3.-install-the-tensorflow-pip-package)

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Version 1.15 is required for einsum gradients.
"
259,24034,1,"the performance of tensorflow distributed . I have trained a speech recognition network using tensorflow both on multi-gpu and distributed. there is some question that i have been puzzled a long time. that's my [code,](https://github.com/dingevin/distributed-training) when i'm training i faced two questions:
1. **the accuracy of distributed not equal to multi-gpu**. in multi-gpu version, i use 2 gpu and training 8000 steps, the accuracy can reach 32%; however the accuracy only can reach 28% at distributed version(i have 2 ps 2 worker, and each worker use 2 gpu), each worker training 4000 steps, and the learning rate was same. it seems the parameters not share.
multi-gpu: (when training 4000 steps， loss is reduce to about 5.0)

distributed ( training 2000 steps， the loss was reduce to 5.5， equal to 2000 steps at multi-gpu )

2. **the speed of distributed version was so slow**. the bandwidth is 125Mb/s, is not the bottleneck.
i execute the command as follow on every worker:

and another worker:


was my code wrong? or i misunderstand distributed? can somebody give me some advice. thanks.

"
1192,3603,0,"RC 0.10 3X Slower than 0.9 and Error Compiling From Source Under Certain Conditions. This issue is a follow-up to [this posting](http://stackoverflow.com/questions/38704095/tensorflow-rc-0-10-3x-slower-than-0-9) on stack overflow. As noted, the 0.10 version is ~3x slower than version 0.9 for a particular training script I am using. The same slow-down is observed when using the pip version with CuDNN4 or with a version compiled from source and CuDNN 5.1.

I'm not sure what information about the training script would help. It is a fairly complex script that is training a network for object detection using methods similar to YOLO and SSD. It won't be possible to post the entire set of code, but if there is a way to isolate the source of the delay I may be able to modify it and post that portion.

While trying to get insight into the source of the slow-down I also noted that I wasn't able to compile from source using CuDNN4 as noted in the SO posting. This is a secondary issue, but may also warrant investigation.
"
460,32708,0,"module 'tensorflow' has no attribute 'enable_eager_execution'. I installed *tensorflow-gpu==2.0.0rc1* from pip

"
576,31051,0,"caching cuDNN CNN kernel choices. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes):



**Describe the feature and the current behavior/state.**

Pytorch as a functionality to cache the chosen cuDNN kernels when the input dimensions are identical to previous calls:

torch.backend.cudnn.benchmark = True
This improves throughput of a network by 30~40% (again, when the input dimensions of the network do not change).

Here is where the caching is implemented in Aten:
https://github.com/pytorch/pytorch/blob/358fb51e773b8ad509ec270caee5ec1c51d82f38/aten/src/ATen/native/cudnn/Conv.cpp#L340

Do you have any plans to add similar functionality? If not, I would love to send a PR.

**Will this change the current api? How?**

Like pytorch, it would require setting a flag.

**Who will benefit with this feature?**

All forward passes that are performed with fixed input sizes and types

**Any Other info.**
"
315,35018,0,"Can't convert Albert to tflite by missing IdentityN support. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
- TensorFlow installed from (source or binary): colab
- TensorFlow version (or github SHA if from source): 1.15


**Provide the text output from tflite_convert**



Also, please include a link to a GraphDef or the model if possible.
-> I am using ""bert-for-tf2"" to load albert_base.




Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1375,21402,0,"Features: train.saver  namescoped-restoring. Hello:

Scenario of need:
Joe the ML-engineer wants a model that predict multiple thing out of a single img (lets say object detection & some environment values: hour of day, type of surroundings: inside/outside/forest/stadium/etc).
So Joe train a model to get its object detected.
Then Joe train a model to get its environment data.
And now, Joe want a model that does both in a single prediction.
But Joe may want to have a clean **modulated model** (for scalable/maintainable purposes)

Scenario of usage:
Joe create its model of object detection using the same function he used for first creating it, but in a name scope.
Joe then use the  to restore his checkpoint, adding the name scope as parameter to indicate that his variables are now contained in a name_scope thus their name has changed.
Then the same for the environment model.


This feature need was inspired by this issue: 
[make-predictions-with-an-old-model-without-losing-the-current-model](https://stackoverflow.com/questions/51654259/make-predictions-with-an-old-model-without-losing-the-current-model/51654678?noredirect=1#comment90301946_51654678)

Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A
Mobile device: N/A"
1394,35133,0,"Installing tensorflow go failed. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): n/a
- TensorFlow version: n/a
- Python version: n/a
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**
Followed https://www.tensorflow.org/install/lang_go
but got this:
go get github.com/tensorflow/tensorflow/tensorflow/go
package github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core: cannot find package ""github.com/tensorflow/tensorflow/tensorflow/go/genop/internal/proto/github.com/tensorflow/tensorflow/tensorflow/go/core"" in any of:


go get -d github.com/tensorflow/tensorflow/tensorflow/go
yielded the same result.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1320,26151,0,"Tensorflow 1.13: please do not deprecate important functions!!!. According to Tensorflow 1.13, many important functions will be deprecated, such as:
tf.layers.dense()
tf.layers.dropout()
tf.layers.flatten()
tf.layers.batch_normalization()
...
Since these functions are very widely used, and also very helpful for building complicted models, would you please keep them in the future versions?"
881,26595,0,"Issues with running compiled versions of tensorflow for different machines and CFLAGS. This is a bug  related to https://bugs.gentoo.org/show_bug.cgi?id=679714.

Here is the system information:
**### System information**
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**2019-03-12 05:23:07.606918: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.**

Actual system is an IvyBridge non AVX2 CPU. GCC version 8.3.0. No LTO. Simple use flags. 

How did this happen? I compiled tensor flow on my build machine. The build machine has loads of RAM big fans and is suited for compling stuff. I wanted to run tensorflow on my notebook, which was a bit older and more limited. I tried building tensor flow from the gentoo ebuild directly, with specific CPU flags for the CPU that I was going to run tensorflow on. I then ran into the gentoo bug 679714. This is where the libtensorflow_framework.so is missing the symbol __cpu_model (the flags I used on the build machine were **-O2 -march=ivybridge -mcx16 -msahf -mno-movbe -maes -mpclmul -mpopcnt -mno-abm -mno-lwp -mno-fma -mno-fma4 -mno-xop -mno-bmi -mno-bmi2 -mno-tbm -mavx -mno-avx2 -msse4.1 -msse4.2 -mno-lzcnt -mno-rtm -mno-hle -mrdrnd -mf16c -mfsgsbase -mno-rdseed -mno-prfchw -mno-adx -mfxsr -mxsave -mxsaveopt -mtune=ivybridge -minline-all-stringops -finline-functions -fmerge-all-constants**). I changed the CFLAGS to ""-### O2"" only and tensorflow ran fine on the build machine. I tried to run a python script (python versions 3.6 and 2.7) found was told that the CPU didn't support AVX2.



"
1092,29734,0,"Autograph failed for function contains Non-ASCII character comment in TF 2.0.0-beta0. **System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta0
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 1080 8GB

**Describe the current behavior**

When using non-ASCII character in the function with @tf.function, autograph failed.

> W0613 17:25:35.704982  5776 ag_logging.py:145] Entity <function train at 0x0000000003C96F28> could not be transformed and will be executed as-is. Please report 
this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, ) and attach the full output. Cause: (unicode
 error) 'utf-8' codec can't decode byte 0x93 in position 1: invalid start byte (tmp9j9bce4r.py, line 6)

Temporary file is created by Shift JIS (Japanese Windows default) character code.

**Describe the expected behavior**

No warning and building a graph successfully.

**Code to reproduce the issue**


"
713,1304,0,"Training a model using GPUs across different machines. Hi, if/how can TensorFlow's distributed runtime be used for training a model using GPU resources across machines in a cluster?
"
878,28327,0,"Max retries error with BigtableClient. For reference, pertains to [](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/bigtable/python/ops/bigtable_api.py) and .

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 'v1.13.1-0-g6612da8951' 1.13.1
- Python version: 3.6.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

Dataset constructed by way of  raises error that is primarily the following:



This occurs both in graph and eager mode; occurs on a machine where the conventional bigtable python client apis function properly (appropriately credentialed); persists after alternatively activating a service account with the appropriate credentials via .

**Describe the expected behavior**

Iterator yields training examples from Dataset when iterated over without error.

**Code to reproduce the issue**



**Other info / logs**

"
88,15554,1,"Tensorflow Lite exhibits longer inference time when build with Android NN API on Google Pixel 1. 

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
'v1.3.0-rc1-6055-gfdf34a8', '1.4.0'
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.8.0
- **GCC/Compiler version (if compiling from source)**:
GCC 5.4.0-6ubuntu1~16.04.5
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
When enabling the NN API usage (edit interpreter.cc) one would expect that due to the HW acceleration, inferece times would be shorter. However, exactly the opposite happens. I tried this using the included demo app, using mobilenet_quant_v1_224.tflite and a custom (not quantized) model that I converted to tflite. In the mobilenet case, inference takes around 80ms without NNAPI, and ~100ms with NNAPI . My custom model, which sadly I cannot share , takes 40ms without , and ~90ms with NNAPI.

Please note that I have also tested this with FAST_SINGLE_ANSWER  and SUSTAINED_SPEED  NNAPI preference settings. There was no significant change in inference times.

My Pixel Build number is: OPM1.171019.011

### Source code / logs
--- a/tensorflow/contrib/lite/interpreter.cc
+++ b/tensorflow/contrib/lite/interpreter.cc
@@ -51,7 +51,7 @@ Interpreter::Interpreter(ErrorReporter* error_reporter)
   tensors_.reserve(kSlotsToReserve);
   nodes_and_registration_.reserve(kSlotsToReserve);
   next_allocate_node_id_ = 0;
-  UseNNAPI(false);
+  UseNNAPI(true);
"
1470,28761,0,"Can't set an initial state for the Bidirectional LSTM Layer of tf.keras 2.0 under eager execution mode. **System information**
- Have I written custom code: No
- OS Platform and Distribution: MacOS 10.14.4 
- TensorFlow installed from: pip install tensorflow==2.0.0-alpha0 
- TensorFlow version: v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.7.3

**Describe the current behavior**
- Get an Error when setting an initial state for the Bidirectional LSTM Layer of tf.keras 2.0 under eager execution mode

**Describe the expected behavior**
- The code should work with eager execution mode.

**Code to reproduce the issue**


**Other info / logs**
The Error detail：
"
1467,4442,0,"predict function loads only the first batch from the input_fn. I have a regular  function. 
When using  everything is OK and the model is evaluated on the whole dataset. But calling  only returns the predicted values of the first batch (The size of  is equal to  parameter). 
I'm using TensorFlow 0.10 and test it on Ubuntu (GPU) and MAC OSX (CPU).
Is it a bug in the predict function?
"
932,24850,0,"Too many LSTM implementations (6). **System information**
- TensorFlow version: v1.8
- Doc Link: [tf.contrib.rnn.LSTMCell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/LSTMCell), [tf.contrib.rnn.LSTMBlockCell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/LSTMBlockCell), [tf.contrib.rnn.LSTMBlockFusedCell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/LSTMBlockFusedCell), [tf.nn.rnn_cell.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell), [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [tf.contrib.cudnn_rnn.CudnnLSTM](https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTM), [tf.contrib.cudnn_rnn.CudnnLSTMSaveable](https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTMSaveable)

Ignoring any fancy versions like Grid LSTM cells and LSTM cells somehow combined with convolutions there are **6!** different LSTM implementations in tensorflow (see documentation links above). Some are better documented than others, some claim to be faster than others (there seems to be the order  tf.contrib.rnn.LSTMCell < tf.contrib.rnn.LSTMBlockCell < tf.contrib.rnn.LSTMBlockFusedCell but then why keep the slower implementations instead of just replacing the slower ones with faster implementations?), all have slightly different APIs.

What I am missing is a guide that tells me:
* What is the fastest implementation on what hardware (I might be willing to deal with poor documentation if the result is at least fast, but having to deal with poor documentation just to find out that the performance is bad sucks).
* What is the stable implementation which I can expect to be maintained for a longer time period.
* What implementations are just included for historic reasons or to maintain backwards compatibility (and therefor should be avoided when starting a new project).
* Maybe there are good reasons (that I am not aware of) to keep multiple implementations in parallel because they all have different tradeoffs. In this case I would like to know more what the differences are between the implementations. This can just be a table with pros and cons.

The state of affairs is probably similar for GRU cells and simple RNN cells. The situation with the LSTM cells is just exemplary ..."
695,303,0,"Typo in variable name in Variables Getting Started Doc. Example code begins by defining the counter variable as ""var"" but later on adds to an undefined variable called ""state""
"
578,32796,0,"When does tensorflow lite support 3d cnn?. 
I trained 3d cnn network on tensorflow, now I want to port to Android phone, but browsed the document and found that tensorflow-lite does not support 3d cnn operation, when will it support it?"
1110,25067,0,"Latest commit in deprecation.py breaks R TensorFlow client . <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 28
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly (r1.13)
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA


Hi,

the commit 

https://github.com/tensorflow/tensorflow/commit/b97727bc3c7a9216670f361b639a60ed516917e0#diff-fa5fb3b8d9f512ad269f7eb67903ec2b

breaks the R TensorFlow client which uses embedded Python from R (see https://github.com/rstudio/tfdatasets/issues/17)

Specifically, the line



triggers the error



on our side. For us, in one specific example, instead of a list with at least 4 items,  contains a list of 2 tuples, each of which are of length 6.

Unfortunately, this will error and stop execution every time a new deprecation warning is issued.

Current workaround on our side is https://github.com/rstudio/tensorflow/pull/287, however this comes at the cost of totally disabling deprecation warnings.

Is there anything you could do to make this change compatible?

Thanks,
Sigrid

"
67,1168,1,"Program using GPU and CPU mix got slower in  Tensorflow 0.7. I have a Titan X, 
On 0.6, code running well and fast 5 sec for 10 epoch.
I upgrade to 0.7, code running so slow 85 sec for 10 epoch and no matter python2.7 or python3.4.
GPU always on and detected so no CPU is used (unless it has to : ""allow_soft_placement=True"").
With nvidia-smi, i checked that the GPU was 40 % used in 0.6 version. And it is 97% used in 0.7 version.
So, the GPU is more used for a worst training time ?
"
1485,10196,0,"tf.contrib.ffmpeg.decode_audio causes kernel crash w/ multi-threading. - **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow version (use command below)**: v1.1.0 - gpu
- **CUDA/cuDNN version**: 8.0
- **Exact command to reproduce**:

I have noticed that attempting to run the 'tf.contrib.ffmpeg.decode_audio' function with multiple threads causes the kernel to crash. This occurs when trying to create batches of data from audio binaries. 

The code underlying 'tf.contrib.ffmpeg.decode_audio' appears to a basic reference to functions outside of tensorflow so I am unsure there is a solution inside of the tensorflow domain. Nonetheless I wanted to bring this up in case someone had a solution. This is probably not a bug report and more of a low priority feature request. 

The code below will run without error when **num_threads=1** for tf.train.batch but the kernel will crash for **num_threads=2** or more. 


graph = tf.Graph()

with graph.as_default():

    
    batch_size=2

    queue = tf.train.slice_input_producer([paths, labels], 
                                          num_epochs=2, shuffle=True, capacity=32)
    
    
    audio_binary = tf.read_file(queue[0])
    signal = tf.contrib.ffmpeg.decode_audio(audio_binary, file_format='mp3', 
                                        samples_per_second=22500,  
                                        channel_count=1)[:450000,0]
    y_ = tf.one_hot(queue[1], 16, dtype=tf.float32)


    batch_sig, batch_y_ = tf.train.batch([signal, y_], batch_size=batch_size, 
                                         shapes=[(450000,), (16,)], 
                                         num_threads=1, capacity=64)
    
    
    with tf.Session(config=tf.ConfigProto(operation_timeout_in_ms=500)) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())

        
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        
        
        for i in range(2):
            print(tf.reduce_max(batch_sig, axis=1).eval())
            
        coord.request_stop()
        coord.join(threads)            
        `


"
12,32276,1,"Memory keeps increasing with GradientTape. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0.130
- GPU model and memory: GeForce RTX 208, memory 10G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
I create a subclass model with a encoder layer and decoder layer. Encoder is to call tf bidirectional LSTM and decoder is just a dense layer. I train the model with GradientTape, looping to pass data with generator. The GPU memory keep increasing.  

**Describe the expected behavior**
The GPU memory usage should be stable after the 1st epoch.

**Code to reproduce the issue**
I minimize my code and save it to https://github.com/ChenYang-ChenYang/tf-memory-increase
You can just run train.py to reproduce.

The memory increase from 700M at epoch 1 to 1080M at epoch 6 in the sample project. My actual project have much more data and more complex model. The memory increase to 10G after some epochs and OOM, so I can't continue to train.

**Other info / logs**
If I change the sample project to use model.compile() and then fit(), instead of using GradientTape, the memory is stable.  But my actual project is complex, including seq2seq addons, multiple outputs and losses, so it is very difficult to change to use model.compile() and fit(). I have to use GradientTape.
It seems the memory is re-created and not released in each epoch or step.
By the way, I searched most of the tf memory issues, like https://github.com/tensorflow/tensorflow/issues/32052, https://github.com/tensorflow/tensorflow/issues/19385, https://github.com/tensorflow/tensorflow/issues/19671, none of them are the same or the solution doesn't work. The most common solution to release memory cache is to call tf.set_random_seed(1) in tf version 1.x. I tried to call tf.random.set_seed(1) in tf 2.0 but it didn't work. 

"
1083,25311,0,"tflite_convert makes no difference to converted model after changing detection_postprocess.cc. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (or github SHA if from source): **r1.12**


**Describe the issue**
I am converting SSD object detection model trained through object detection api. The only difference in my custom model and other pre-trained model is that the **detection_boxes** have 6 output coords instead of 4 box coords, I am also predicting the center coordinates of the bounding box to make the model more robust. It is performing great in frozen_graph.pb but as I convert it into .tflite format by using tflite_convert, it only gives 4 box coords whereas I need 6 box coords. But, eventually I figured out that I need to change the [detection_postprocess.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc), but changes made in this file are not effecting the model, it seems like tflite_convert is ignoring this file while converting the model.

**Steps to reproduce the issue**
- tensorflow build from source and installed in a virtualenv
- make some changes in the [detection_postprocess.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc)
- uninstall tensorflow from virtualenv
- again build tensorflow using bazel and install it using pip in same virtualenv
- run following command to convert model:
 

**More information**
- tflite_graph.pb is obtained by using [export_tflite_ssd_graph.py](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py)


How to make sure that changes made in detection_postprocess.cc will effect the converted model using tflite_convert?"
573,21830,0,"Canned TensorForest Tracking. Implementation steps for https://github.com/tensorflow/community/blob/master/rfcs/20180626-tensor-forest.md


- [x] classification only and inference only  the tf/core/kernerl part https://github.com/tensorflow/tensorflow/pull/21803
- [ ] classification only and inference only the tf/estimator python part https://github.com/tensorflow/estimator/pull/9
- [ ] classification only and with training https://github.com/tensorflow/tensorflow/pull/23889
- [ ] end2end classification estimator with tests and docs.
- [ ] regression support 
"
363,20268,0,"HI what is the difference between examples and java folder's demo. Can anyone tell me which tensorflow lite demo is good,cuz i met problem with both
1.https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app
and the problem is Plugin with id 'com.android.application' not found.
2.https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android
and this is more weird ,BUILD\android-profile 
thx
as 3.0 
"
138,35095,1,"MirroredStrategy compared to OneDeviceStrategy slower . **System information**
- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH
- TensorFlow installed from: binary
- TensorFlow version: 2.1.0rc0-1
- Keras version: 2.2.4-tf
- Python version: 3.8
- GPU model and memory: 2x GTX 1080 Ti 11GB""`

**Describe the current behavior**
executing Tensorflow's MNIST handwriting example with MirroredStrategy using two GPUs is slower (286 sec) than OneDeviceStrategy (197 sec) 

**Describe the expected behavior**
using MirroredStrategy should be faster or at least fast as OneDeviceStrategy

**Cod e to reproduce the issue**
hype rparameters were tuned for MirroredStrategy using Keras Tuner




**Other info / logs**
**OneDeviceStrategy**:
> ..
> 12000/12000 [==============================] - 39s 3ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 0.0385 - val_accuracy: 0.9892
> elapsed: 197.177

**MirroredStrategy**:
> ...
> 12000/12000 [==============================] - 56s 5ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.0390 - val_accuracy: 0.9884
> elapsed: 286.224"
1297,33222,0,"TF 2.0: Can't authenticate with google storage in colab. I'm trying to use the TPU in colab so I have to authenticate to my google storage account to feed the data (as I understood from past tutorials on using TPUs on colab). When I'm trying to authenticate I get the following error:



The code I run is:


The following link contains the code to reproduce the error https://colab.research.google.com/drive/1LQ_SuPoetIUBTBhohVbFJnatkyKNb1G9
"
636,35411,0,"My GPU doesn't support CUDA. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): (I've use this) pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl
- TensorFlow version: 1.12.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: GeForce GTX 745


When I want to run test code I get error ""ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'""

I know that it is caused by lack of CUDA, but my GPU doesn't support it. What can I do with it?




"
558,24044,0,"Secondary algorithm not provided error when not using cuDNN autotune. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0 / 7.1.4
- GPU model and memory: Nvidia Tesla K80, Quadro M1200

**Describe the current behavior**
I get the following error message when I set TF_CUDNN_USE_AUTOTUNE=0:
 2018-11-22 10:10:01.227780: E tensorflow/stream_executor/cuda/cuda_dnn.cc:82] The primary convolution algorithm failed memory allocation, while a secondary algorithm is not provided.

**Describe the expected behavior**
I would expect a secondary algorithm that does not require any workspace to kick in should the scratch allocation for the primary algorithm fail even when I am not using autotune.

**Other info / logs**
I traced back the error message to when it was [first introduced](https://github.com/tensorflow/tensorflow/blob/466eb299f0ce20cf929b9e06d3d3c16959360c59/tensorflow/stream_executor/cuda/cuda_dnn.cc#L706) in the code base to revise the logic.

It seems to me that years of refactoring have disabled the [fallback mechanism](https://github.com/tensorflow/tensorflow/blob/466eb299f0ce20cf929b9e06d3d3c16959360c59/tensorflow/stream_executor/cuda/cuda_dnn.cc#L792) when both the primary and secondary algorithms are the default ones.
"
926,33150,0,"TF2.0: Translation model: Error when restoring the saved model: Unresolved object in checkpoint (root).optimizer.iter: attributes. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs Mojave 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below):' 2.0.0-beta1'
- Python version: 3.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

I am trying to restore the checkpoints and predict on different sentences NMT Attention Model. While restoring the checkpoints and predicting, I am getting gibberish results with warning below:



Below is the additional warnings that I am getting and the results:


The warning at the very end says:

'A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used...' what does it mean?
**Describe the expected behavior**
Checkpoints should be restored properly
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1476,31103,0,"pip install tensorflow-lite PLEASE!. I'm finding very, very difficult-to-understand information online for how to install TF-lite.  Most of it involves cross-compilation and 10+ hours of waiting.  Tensorflow installation is easy.  Could you please make it so we can install TF-lite by just typing ""pip install tensorflow-lite?""

Thanks!"
684,35187,0,"How to solve RuntimeError in Tensorflow 2.0?. I want to run a TF1.X programm in TF2.0 on a GPU (RTX 2080 TI) in Anaconda, so I changed the code a bit, but I'll get an error in the last line below.



The error message is 



It would be awesome if someone could help me."
629,1038,0,"sequence_loss_by_example()  type error for rnn/ptb. Hi 

I downloaded 0.6.0 for python 2.7 and I tried running the rnn/ptb example but got a type error:

$ python ptb_word_lm.py --data_path=/this/path/ --model small

I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8
Traceback (most recent call last):
  File ""ptb_word_lm.py"", line 303, in <module>
    tf.app.run()
  File ""/Library/Python/2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""ptb_word_lm.py"", line 280, in main
    m = PTBModel(is_training=True, config=config)
  File ""ptb_word_lm.py"", line 135, in __init__
    [tf.ones([batch_size \* num_steps])])
TypeError: sequence_loss_by_example() takes at least 4 arguments (3 given)

It seems that this sequence_loss_by_example() method is imported from seq2seq which imports it from tensorflow.python.ops.seq2seq. Beyond that I'm not really sure how this happened.

Thanks
"
75,19611,1,"performance bug in Conv3d_transpose causes it to be a factor >100 slower than equivalent computations. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This only concerns a nativ tensorflow function
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tested on Arch Linux, OS X and Windows 8
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5, 1.7 & 1.8 (cpu)
- **Python version**: 2.7.14 and 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CPU-only issue
- **GPU model and memory**: CPU-only issue
- **Exact command to reproduce**: tf.conv3d_transpose

### Describe the problem
tf.conv3d_transpose is a factor >100 slower than other operations doing the same or an equivalent computation when working with tensorflow on cpu. This was previously reported in Issue #10535 and #7610 , but none have provided a minimal example and both were closed due to inactivity.

### Source code / logs
Minimal example that shows how much slower conv3d_transposed is compared to einsum doing the same computation and to the conv3d forward computation:

"
624,33463,0,"Undefined symbols for architecture arm64:   ""NewGpuDelegate(GpuDelegateOptions const*). System information

    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone X.s, 13.1.3
    TensorFlow installed from (source or binary): source
    TensorFlow version (use command below): master branch cloned on 2019-10-8
    Python version: 3.7.4
    Bazel version (if compiling from source):  0.26.1
    GCC/Compiler version (if compiling from source): NA
    CUDA/cuDNN version: NA
    GPU model and memory: NA

Describe the current behavior:
It fails to build the library in Xcode.
Undefined symbols for architecture arm64:   ""NewGpuDelegate(GpuDelegateOptions const*)

Describe the expected behavior:
I want to build my C++ library with the ability to run it with GPU in an iOS app

Code to reproduce the issue

Other info / logs
I changed the extension of the file having this line of code: 
_delegate = NewGpuDelegate(&options);_
from cpp to mm, as otherwise I couldn't include header files for metal(which are written in obj-c).
I tested the tensorflow ios/camera example and it builds and runs fine. But using the same .framework lib gives me the same undefined symbol error.

Bazel Command for building the tensorflow lib:
**bazel build -c opt --cpu ios_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=hidden --linkopt -s --strip always --cxxopt=-std=c++14 :tensorflow_lite_gpu_framework --apple_platform_type=ios**

Any help would be appreciated!"
219,31654,1,"Running tensorflow on GPU is far slower than on CPU. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1809 & Windows Server 2016
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-beta1, as well as tensorflow-gpu, compared to tensorflow & tensorflow==2.0.0-beta1
- Python version: 3.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: Cuda 10 & cuDNN 7.6.2 for cuda 10.0

### Current behavior:
Im getting a 50%+ performance loss with GPU!
In the below example, the CPU version is even training way faster on a bigger model with slightly bigger epochs.
![time](https://user-images.githubusercontent.com/12736950/63144316-9daf1a00-bff2-11e9-9b39-036fd9291008.png)

Im training on 2 different systems:
My server, without a GPU:

Intel Core i5 6500T (4x@2.5 ghz) Notebook processor
Network attached storage for training data and output
16 Gb DDR 4 Ram
My Desktop PC:

Intel core i7 3770k (4x3,5-4 ghz)
Nvidia GTX 970 @4GB
32gb ddr3 ram
Training Data on local SSD, output to NAS

Now interestingly 3000 epochs, 100000 records each, takes roughly 3h on the server using TF 1.14
The same on my Desktop with GPU takes 8h with TF 2.0
It sits with full Video Ram but at 3% graphical processor use.
The CPU is sometimes at 30% use with tensorflow GPU but 100%  at any time with any CPU build.  
The harddisk is utilized with a whopping 0%.
![system utilisation](https://user-images.githubusercontent.com/12736950/63122624-7f6ffc80-bfa7-11e9-81ee-2b76e4bc5d99.png)

### Expected behavior:
Tensorflow-GPU trains faster than Tensorflow CPU

### Code to reproduce the issue:
My model is a fairly simple keras sequential lstm:

I am having lots of test data which does not fit in memory so I use interleaved datasets:
"
587,24474,0,"How to build from source with hdfs support on. No option in configure?. **System information**
- OS Platform and Distribution: Mac OS Mojave 10.14.1 (18B75)
- TensorFlow installed from (source or binary): source
- TensorFlow version: branch r1.12
- Python version: 3.6.6 
- Installed using virtualenv? pip? conda?: building is in a virtualenv created by 
- Bazel version (if compiling from source): from source. 0.15.0
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)
- CUDA/cuDNN version:  no cuda, build only for cpu
- GPU model and memory: Intel Core i7, 16 GB

**Describe the problem**
When I reading threcord in my local hdfs, I encounter error below:


I fallowed [this guide](https://www.tensorflow.org/deploy/hadoop) to set hadoop pathes, and I'm sure my  is in my . (I build hadoop natively for mac)

I'm not sure if my tensorflow, which is installed via brew, is hdfs enabled. So I just want to build from source with hdfs enable, but when I run , there is no option for hdfs?  My questions are:

1/ is the tensorflow shipped via brew hdfs enabled?
2/ how to build from source with hdfs turn on?"
265,6119,1,"Different prediction result for tf.learn QuickStart?. Hello,

Today I upgrade tensorflow package and read the [tutorials from the beginning](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/tflearn/index.md).And find the result of classifier prediction is different, it is . I remember that the old version tf run out . So what's the problem?

The OS is OSX EI Capitan, and tensoflow is 0.12.0-rc0

python -c ""import tensorflow; print(tensorflow.__version__)""tf.train.Saver(write_version=tf.train.SaverDef.V2)tf.train.Saver(write_version=tf.train.SaverDef.V2)
"
197,33146,1,"tensorflow (cpu only) with mkl is much slower than without mkl! . <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.8
- Python version: 2.7
- Bazel version (if compiling from source): 0.10.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
I compile tensorflow 1.8 with mkl, and I use c api session to predict wide and deep model.
The Parallel Performance is very slow, avg reponse time is 40 microsecond in 60 qps, cpu load is only 200%

export KMP_BLOCKTIME=0
export KMP_AFFINITY=granularity=fine,verbose,compact,1,0
export OMP_NUM_THREADS=64
export KMP_SETTINGS=1
export MKLDNN_VERBOSE=1

in the opposite， without mkl, The Parallel Performance is ok, avg reponse time is 9 microsecond in 60 qps. I don't know why.Maybe I use wrong!

**Describe the expected behavior**

**Code to reproduce the issue**

    SessionOptions session_options;
    ConfigProto& cp = session_options.config;
    cp.set_intra_op_parallelism_threads(64);

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1452,8145,0,"tensorflow r1.0 does not report training speed in the log any more. I used tensorflow r0.10 before with contrib.learn.
When I set 
tf.logging.set_verbosity(tf.logging.INFO)

after imports, I would get the following info from the training log:

INFO:tensorflow:Results after 10 steps (0.185 sec/batch): loss = 0.0644001, auc = 0.73555, accuracy/threshold_0.500000_mean = 0.988108

But with tensorflow r1.0, no such info in the training log any more.
Why is it removed? It is such a useful information.
or do I need to set something else in the script?

Thanks for help.

"
513,15045,0,"Updated Tensor flow, incompatible function parameters. (Float32>int). Hi, 

### System information
Windows 10
TensorFlow installed using pip. 
I am using CDU 
Python 3.6.3 (using Spyder)
TensorFlow version: 1.4.0

Have I written custom code: No
OS Platform and Distribution:
TensorFlow installed from
Bazel version:NA
CUDA/cuDNN version:NA
GPU model and memory: using cpu.
Exact command to reproduce: 
activate myenvrionment
spyder 


This code was written on 0.x tensorFlow, it could be an update issue, but I can't find the outdated function. 



# Spyder execution log:
runfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')
Using TensorFlow backend.
Traceback (most recent call last):

  File ""<ipython-input-3-d1b60b53383b>"", line 1, in <module>
    runfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')

  File ""C:\Users\PC\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 710, in runfile
    execfile(filename, namespace)

  File ""C:\Users\PC\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 101, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py"", line 197, in <module>
    model = fcn_model((100, 100, 1), 2, weights=None)

  File ""E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py"", line 162, in fcn_model
    crop1 = Lambda(crop, name='crop1')([upsample1, score_conv11])

  File ""C:\Users\PC\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 603, in __call__
    output = self.call(inputs, **kwargs)

  File ""C:\Users\PC\Anaconda3\lib\site-packages\keras\layers\core.py"", line 651, in call
    return self.function(inputs, **arguments)

  File ""E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py"", line 36, in crop
    cropped = Cropping2D(cropping=(crop_h_dims, crop_w_dims))(tensors[1])

  File ""C:\Users\PC\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 603, in __call__
    output = self.call(inputs, **kwargs)

  File ""C:\Users\PC\Anaconda3\lib\site-packages\keras\layers\convolutional.py"", line 1874, in call
    self.cropping[1][0]: -self.cropping[1][1],

  File ""C:\Users\PC\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 538, in _SliceHelper
    name=name)

  File ""C:\Users\PC\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 706, in strided_slice
    shrink_axis_mask=shrink_axis_mask)

  File ""C:\Users\PC\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 5429, in strided_slice
    name=name)

  File ""C:\Users\PC\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 609, in _apply_op_helper
    param_name=input_name)

  **File ""C:\Users\PC\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 60, in _SatisfiesTypeConstraint
    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))

**TypeError: Value passed to parameter 'begin' has DataType float32 not in list of allowed values: int32, int64****


Please help."
1200,21718,0,"Error importing tensorflow.  Unless you are using bazel, you should not try to import tensorflow from its source directory; please exit the tensorflow source tree, and relaunch your python interpreter from there.. Traceback (most recent call last):
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Python Files\csv_to_tfrecord.py"", line 16, in <module>
    import tensorflow as tf
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there."
653,5462,0,"dist_test local k8s cluster setup is broken, use kubernetes/minikube. **Feature Request/Documentation Request**

local k8s cluster dist_test broken, use [kubernetes/minikube](https://github.com/kubernetes/minikube) instead


As of commit 21a7ae05e04f4f060938db08015cb47896970dd1, the dockerfile for setting up a local cluster for dist_test is broken.  This dockerfile imports start_local_k8s_service.sh and other scripts that were deleted with this merge.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/local/Dockerfile#L18


Instead of this dockerfile, this local cluster setup should be switched over to Minikube, the officially supported method for non-k8s-development local clusters.

http://kubernetes.io/docs/getting-started-guides/minikube/

As a maintainer for minikube, I would happily create a PR with instructions and a tutorial on how to run dist_test locally.  If this sounds good to you, please assign this issue to me.      
"
877,5255,0,"Error compiling Raspberry Pi label_image. Hi there,
I'm trying to build Temsorflow on a Raspberry pi 3 running Raspbian Jessie and kernel 4.3.
I followed the instructions on: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile 
When I compile the label_image example, as described in:
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples
I receive the following compilation errors with:  make -f tensorflow/contrib/pi_examples/label_image/Makefile

gcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads -I/home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads/eigen-latest/ -I/home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto/ -I/home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto_text/ -c tensorflow/contrib/pi_examples/label_image/label_image.cc -o /home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o
In file included from ./tensorflow/core/framework/tensor.h:19:0,
                 from tensorflow/contrib/pi_examples/label_image/label_image.cc:32:
./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:4:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such ffile or directory
 #include ""unsupported/Eigen/CXX11/Tensor""
                                                                    ^
compilation terminated.

Is there anyway to fix it?
Thanks.
"
856,8263,0,"Documentation incorrect for RNN tutorial?. Is it possible the documentation is incorrect on https://www.tensorflow.org/tutorials/recurrent ?



It seems the correct output should be:


Since you want to process the words in the same sequence correct?

E.g.
[[The, quick, brown]
 [fox, jumped, over]]

words[:,0] == [The, fox]

Whereas what you want is [The, quick, brown]  == words[0,:]

Please correct me if I'm wrong, thanks."
1385,30267,0,"How should i use tf.image.draw_bounding_box() when each batch have different bbox_num ?. The documents show that bboxes' shape is [batch, num_bounding_boxes, 4]. This means every batch have same number of bbox. 
But in detect, each image may have different number of bboxes. Who can tell me how should i do, thanks !
BTW, I use tf.gather_nd() to get bboxes, is there any method to get dimension-aligned bboxes? 


**System information**
- TensorFlow version (you are using): Tensorflow 1.13"
835,20635,0,"Unexpected result when load new model versions. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7.0
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**:  4.8.5
- **CUDA/cuDNN version**: 5.1.10
- **GPU model and memory**: p40 etc
- **Exact command to reproduce**: 

### Describe the problem
Using tensorflow serving with hdfs and serving automatic loading new versions based on 1 second interval. Due to the load of hdfs, variables.index file may appeared later than saved_model.pb.

So, the unexpected things appeard, you can see the log in the next few lines.

### Source code / logs

when predict serving returns :

Attempting to use uninitialized value conv1d_1_1/kernel
       [[Node: conv1d_1_1/kernel/read = Identity[T=DT_FLOAT, _class=[""loc:@conv1d_1_1/kernel""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](conv1d_1_1/kernel)]]
       [[Node: dense_1_1/BiasAdd/_31 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_178_dense_1_1/BiasAdd"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

when load new version, serving print :

2018-06-25 16:56:40.483957: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:165] The specified SavedModel has no variables; no checkpoints were restored.

### Some thoughts

It's hard to say who is wrong, but should tensorflow consider and prevent this inconsistency?

"
9,15375,1,"Performance  problem TF VS Keras. Hello , 
I just got huge difference in results using Keras (Back-end TensorFlow) and TensorFlow. I want to know if the difference in performances is normal .

The keras model produces a loss of 0.2


The TensorFlow normal model produces 0.7 :



PS : the code above , is inspired from : [ https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d](url)
Can anybody help me, please, to undersand , if it's normal or not ? does Keras, uses different tensorflow parameters than the default parameters of tensorflow ?
Thanks in advance.
Toetoe."
1254,4578,0,"Default weights_initializer for tf.contrib.layers.convolution2d should be xavier_initializer_conv2d. Hi, 

I notice that in [tf.contrib.layers.convolution2d](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L354), [tf.contrib.layers.convolution2d_in_plane](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L469), [tf.contrib.layers.convolution2d_transpose](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L573), [tf.contrib.layers.separable_convolution2d](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1097), the default weights_initializer is tf.contrib.layers.xavier_initializer, rather than tf.contrib.layers.xavier_initializer_conv2d. Is it better to use tf.contrib.layers.xavier_initializer_conv2d?

Thanks!
"
313,3681,0,"Segmentation Fault When Importing Tensorflow. GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System: CentOS 6.8/RedHat 6.8

Installed version of CUDA and cuDNN: 
(please attach the output of ):
CUDA 7.5 + cuDNN v4

If installed from binary pip package, provide:
1. Which pip package you installed.
2. The output from .

If installed from source, provide 
1. The commit hash ()
2. The output of 
### Steps to reproduce
1. Install tensorflow by using anaconda (conda command)
2. Install glibc ""locally"" and add the path /glibc-2.14/lib to LD_LIBRARY_PATH
3. Open python and use the command ""import tensorflow as tf"", and then you will get the message ""Segmentation Fault"". No other messages.
### What have you tried?
1. Use gdb to see what causes the segmentation fault.
### Logs or other output that would be helpful

gdb messages:
[New Thread 0x7f653bf9f700 (LWP 71825)]
[New Thread 0x7f653b59e700 (LWP 71826)]
[New Thread 0x7f6538b9d700 (LWP 71827)]
[New Thread 0x7f653619c700 (LWP 71828)]
[New Thread 0x7f653379b700 (LWP 71829)]
[New Thread 0x7f6530d9a700 (LWP 71830)]
[New Thread 0x7f652e399700 (LWP 71831)]
[New Thread 0x7f652b998700 (LWP 71832)]
[New Thread 0x7f6528f97700 (LWP 71833)]
[New Thread 0x7f6526596700 (LWP 71834)]
[New Thread 0x7f6523b95700 (LWP 71835)]
[New Thread 0x7f6521194700 (LWP 71836)]
[New Thread 0x7f651e793700 (LWP 71837)]
[New Thread 0x7f651bd92700 (LWP 71838)]
[New Thread 0x7f6519391700 (LWP 71839)]
[New Thread 0x7f6516990700 (LWP 71840)]
[New Thread 0x7f6513f8f700 (LWP 71841)]
[New Thread 0x7f651158e700 (LWP 71842)]
[New Thread 0x7f650eb8d700 (LWP 71843)]
[New Thread 0x7f650c18c700 (LWP 71844)]
[New Thread 0x7f650978b700 (LWP 71845)]
[New Thread 0x7f6506d8a700 (LWP 71846)]
[New Thread 0x7f6504389700 (LWP 71847)]
[New Thread 0x7f6501988700 (LWP 71848)]
[New Thread 0x7f64fef87700 (LWP 71849)]
[New Thread 0x7f64fc586700 (LWP 71850)]
[New Thread 0x7f64f9b85700 (LWP 71851)]
[New Thread 0x7f64f5184700 (LWP 71852)]
[New Thread 0x7f64f4783700 (LWP 71853)]
[New Thread 0x7f64f1d82700 (LWP 71854)]
[New Thread 0x7f64ef381700 (LWP 71855)]
[New Thread 0x7f64ec980700 (LWP 71856)]
[New Thread 0x7f64e9f7f700 (LWP 71857)]
[New Thread 0x7f64e757e700 (LWP 71858)]
[New Thread 0x7f64e4b7d700 (LWP 71859)]
[New Thread 0x7f64e217c700 (LWP 71860)]
[New Thread 0x7f64df77b700 (LWP 71861)]
[New Thread 0x7f64dcd7a700 (LWP 71862)]
[New Thread 0x7f64da379700 (LWP 71863)]
[New Thread 0x7f64d7978700 (LWP 71864)]
[New Thread 0x7f64d4f77700 (LWP 71865)]
[New Thread 0x7f64d2576700 (LWP 71866)]
[New Thread 0x7f64cfb75700 (LWP 71867)]
[New Thread 0x7f64cd174700 (LWP 71868)]
[New Thread 0x7f64ca773700 (LWP 71869)]
[New Thread 0x7f64c7d72700 (LWP 71870)]
[New Thread 0x7f64c5371700 (LWP 71871)]
[New Thread 0x7f64c2970700 (LWP 71872)]
[New Thread 0x7f64bff6f700 (LWP 71873)]
[New Thread 0x7f64bd56e700 (LWP 71874)]
[New Thread 0x7f64bab6d700 (LWP 71875)]
[New Thread 0x7f64b816c700 (LWP 71876)]
[New Thread 0x7f64b576b700 (LWP 71877)]
[New Thread 0x7f64b2d6a700 (LWP 71878)]
[New Thread 0x7f64b0369700 (LWP 71879)]

Program received signal SIGSEGV, Segmentation fault.
__pthread_init_static_tls (map=0x0) at allocatestack.c:1196
1196        init_one_static_tls (list_entry (runp, struct pthread, list), map);
from /home/abc/Libraries/glibc-2.14/install/lib/libpthread.so.0

Because they are clusters, it is hard (or nearly impossible in the near future) to upgrade the OSs. I also have no root privilege.
At first, I got an error telling that I don't have glibc_2.14.so because RedHat/CentOS 6.8 only comes with glibc 2.12. Therefore, I locally compiled the glibc and added it to LD_LIBRARY_PATH. After then, the glibc_2.14 error disappeared and was replaced by another error ""Segmentation Fault"" (no other messages). Something like this:
import tensorflow as tf
Segmentation Fault
(tensorflow) bash-4.1$

I tried to use gdb to see what causes this error, and it turns out it is ""libpthread.so.0"".

Any ideas?

Thanks.
"
92,32454,1,"Calling tf.function from tf.py_function in dataset.map hangs.. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS or Windows.
- TensorFlow installed from (source or binary): binary.
- TensorFlow version (use command below):2.0.0b1/rc0/rc1
- Python version: 3.6.
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla K80

**Describe the current behavior**
Calling tf.function from tf.py_function in dataset.map hangs the program.
By removing tf.function decorator or enable run_functions_eagerly, program runs as expected.

**Describe the expected behavior**
Calling tf.function from tf.py_function does not hang the program.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


"
1132,30530,0,"Request for ComplexAbs and RFFT operations in tf.lite for Tensorflow 2.0. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.0.0-dev20190709


**Provide the text output from tflite_convert**



Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**
Full traceback:


I'm trying to convert a model with tf.lite and running into this error. There is another issue https://github.com/tensorflow/tensorflow/issues/27303 that requests the RFFT operator as well, but it seems to be for Tensorflow 1.x. There is a commit https://github.com/tensorflow/tensorflow/commit/c77e7e56de56c624116cf9eea340b4f96f032c85#diff-ed4b7d597384e8e4b1210b7558a16640 that whitelists the RFFT operation, however my conversion fails. Is this only implemented in Tensorflow 1.x right now?

I'm converting my model using this code:

I've tried using  and  but nothing changes."
49,3676,1,"BatchNorm on GPU become very slow in latest TF. Nightly built, python2 gpu, cuda 7.5, cudnn 4.0.7, archlinux. TitanX.
I run [batch_norm_benchmark.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/batch_norm_benchmark.py) and got the following output:



Although  in a loop might not be an accurate way to benchmark, I think the performance regression exists because running the same script with an earlier binary built (not sure which commit it is) is 10x-20x faster.
"
148,21287,1,"Possible bug in dynamic_rnn when training on TPU for iterations_per_loop > 1. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Cloud Platform (Linux Debian)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA (TPU training)
- **GPU model and memory**: NA (TPU training)
- **Exact command to reproduce**:

### Describe the problem
Training an RNN (constructed with ) on TPU gives largely different loss values for  and . The loss when training on TPU with  is very close to the loss when training on CPU, but the loss for  case is orders of magnitude different.

See below for the code to reproduce this issue. I also tested it with BasicRNNCell (instead of GRU) and observed the same issue. For easier debugging, I have made the runs deterministic (all the random ops are seeded, repeated runs produce the exact same values).

Note that if I replace my model_fn with a simple linear model containing only matrix multiplication (instead of ) the loss for any value of  will be the same which is as expected. So I suspect there is a bug in using  with TPU. 

### Source code / logs


### Output (multiple runs):
**CPU Run:**


**TPU Runs:**
iterations_per_loop=1
Run1:

Run2:


iterations_per_loop=100
Run1:

Run2:
"
939,23145,0,"Could not initialize a memory descriptor when using softmax layer. I have both CPU and GPU version installed by Miniconda, each with a unique environment. While GPU version works fine, the CPU version seems to throw an error when I try to add a softmax layer after a convolution layer.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Manjaro 4.14.74
- TensorFlow installed from (source or binary): binary from Miniconda
- TensorFlow version (use command below): 1.11.0
- Python version: Python 3.6.6 :: Anaconda, Inc.
- CUDA/cuDNN version: CPU version, no CUDA/cuDNN
- Bazel version: N/A
- GPU model and memory: N/A
- Mobile device: N/A
- Exact command to reproduce: python code.py

**Describe the current behavior**

Run the test code, the program throws AbortedError, info is:



**Describe the expected behavior**

The program should finish with no error.

**Code to reproduce the issue**



**Other info / logs**

* I set up the environment by 

* Traceback is:



* GPU version works fine.

* If i set axis to 0, 1 or 2, the program finishes with no error, but with it set to  -1 or 3, the error occurs.

* If the softmax layer is added after a dense layer, it also works fine.

* I've also tested on another server with CentOS 7 and a Quadro P2000, the problem still occurs. (GPU version works fine while CPU version not)

* This code still not work:
"
488,799,0,"Got error when initialize bidirectional rnn with LSTM cell . I want to build a bi-rnn model with tensorflow with LSTM cell, when i try to initialize bidirectional_rnn,
it gives: 


"
682,17201,0,"Tensorflow build incorrectly complained about Bazel version. When I built Tensorflow, it complained my bazel was 0.4.5, asked me to upgrade to bazel 0.5.4 or above.
So I upgraded to the newest bazel 0.10.1. Then when I built Tensorflow again, it still complained:

Current Bazel version is 0.10.1, expected at least 0.5.4

So Tensorflow thinks 0.1 is less than 0.5, it did not treat that as version 10 v.s. 5.
Please fix.

Thank you!
Jan

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
992,26209,0,"TensorFlow 1.13.1: ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS
- TensorFlow installed from (source or binary): Binary (pip3 install -U tensorflow-gpu
- TensorFlow version: current version from pip (maybe version 1.13.1)
- Python version: Python 3.5.2
- Installed using virtualenv? pip? conda?: virtualenv environment with pip
- CUDA/cuDNN version: Cuda 9.0 
- GPU model and memory: Titan Xp



**Describe the problem**
I tried to install tensorflow-gpu follow the instruction at [link](https://www.tensorflow.org/install/gpu) and pip install -U tensorflow gpu. But when I check it  appear a bug as follow: 

> ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory

Meanwhile, I also tried installing pytorch and succeeded. (Figure below)
![screenshot from 2019-02-28 20-31-09](https://user-images.githubusercontent.com/28798474/53569943-e0417f00-3b97-11e9-8a31-7c23a0f2b481.png)
"
1412,8098,0,"ValueError: setting an array element with a sequence. (tf 1.0.0's invalid use of np). Using  I get the following error.


This is a minimal example that should replicate this. I came across this problem also using several higher level abstractions to tf but not until I got to barebones  I believed that this is a bug and not just an error on my part.

"
548,20368,0,"Using new tensorflow op for matrix exponential in a c++ library that already uses tensorflow as third party. ### System information
- **Have I written custom code**: I'm using ZeroOut CPU versionfrom https://github.com/MatteoRagni/tf.ZeroOut.gpu
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS
- **TensorFlow installed from**: source
- **TensorFlow version** : 1.8.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0,  cmake 3.5.1
- **CUDA/cuDNN version**: release 9.0, V9.0.176
- **GPU model and memory**: GeForce GT 730/PCIe/SSE2
- **Exact command to reproduce**: N/A


Hallo to everyone! That's my first time asking a question in tensorflow. I will try my best to formulate my question properly.

My plan is: 

 - implement a new tensorflow GPU op for the matrix exponential,using Eigen unsupported MatrixFunctions or the already existing tensorflow matrix exponential op
 - add the gradient
 - use the new op in a c++ library, which already uses tensorflow as third party.

I have started from the basics, and I realized that I don't know how to use my custom operation in c++. I registered the ZeroOut op for cpu from tensorflow c++ tutorial as in https://github.com/MatteoRagni/tf.ZeroOut.gpu but now I don't know how to use that in my c++ code. 

I tried to add the ZeroOut.so file to my lib as shared library, but it didn't work. Maybe I'm doing something wrong? My CMakeList.txt is attached. And including  ZeroOut.cpp in my c++ files hasn't make any difference until now.
I looked in tensorflow documentation, stackoverflow and the internet but I couln't find an answer to my questions. Hopefully I didn't miss anything.

Can you help me? Maybe giving an example of the required CMakeList.txt, even if not related to mine?

Speaking about my general plan, I would also like to have some advices from more experienced programmers. I know tensorflow has a matrix exponential op, but as far as I know it doesn't work for GPU (see #15465) and has no gradient implementation. Should I add this features to the existing op rather than registering a new one? And what about using Eigen unsupported MatrixFunctions in a new user op?

### Source code / logs
Here is my CMakeList.txt, which also creates the whole library I'm working with:

    cmake_minimum_required(VERSION 2.8)
    project(Project1)

    set(CMAKE_BUILD_TYPE ""Release"") # Debug Release
    set(CMAKE_CXX_FLAGS_RELEASE ""$ENV{CXXFLAGS} -std=c++14 -O3 -Wall                 -fopenmp"")
    SET(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)

    #-------------path of 3rd party libraries-------------
    # special libs.
    find_package(Boost COMPONENTS filesystem iostreams regex)
    find_package(FFTW)
    find_package(NLopt)
    find_package(HDF5 COMPONENTS CXX)

    set(EXTERN_LIB_ROOT ${PROJECT_SOURCE_DIR}/3rd-party)

    set(TENSORFLOW_ROOT /.../tensorflow)
    set(TF_INCLUDE_DIRS ""${TENSORFLOW_ROOT}"" ""${TENSORFLOW_ROOT}/bazel-  genfiles"" ""${TENSORFLOW_ROOT}/bazel-tensorflow/external/protobuf_archive/src"")

    # lib dirs.
    set(LUA_LIBRARIES ""${EXTERN_LIB_ROOT}/lua/liblua53.so"") #5.3.4
    set(LINENOISE_LIBRARIES ""${EXTERN_LIB_ROOT}/linenoise-ng/build/liblinenoise.so"")
    set(YACAS_LIBRARIES ""${EXTERN_LIB_ROOT}/yacas/cyacas/libyacas/build/libyacas.so"")


    set(TF_LIBRARIES ${TENSORFLOW_ROOT}/bazel-bin/tensorflow/libtensorflow_cc.so
        ${TENSORFLOW_ROOT}/tensorflow/core/user_ops/tf.ZeroOut.gpu-master/zero_out.so) 
    #-------------ssl headers-------------
    include_directories(${PROJECT_SOURCE_DIR}/src
        ${EXTERN_LIB_ROOT}/eigen
        ${EXTERN_LIB_ROOT}/gnuplot-iostream
        ${EXTERN_LIB_ROOT}/
        ${EXTERN_LIB_ROOT}/linenoise-ng/include
        ${EXTERN_LIB_ROOT}/yacas/cyacas/libyacas/include
        ${EXTERN_LIB_ROOT}/lua/src
        ${NLOPT_INCLUDE_DIRS}
        ${FFTW_INCLUDES}
        ${TF_INCLUDE_DIRS}
        ${Boost_INCLUDE_DIRS}
        ${HDF5_INCLUDE_DIRS}
        ${TENSORFLOW_ROOT}) 

    option(BUILD_SHARED_LIBS ""build shared library"" ON)
    set(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)

    #-------------ssl kernel lib-------------
    file(GLOB_RECURSE _src_list
        LIST_DIRECTORIES false
        RELATIVE ""${CMAKE_CURRENT_SOURCE_DIR}"" ""${PROJECT_SOURCE_DIR}/src/*.h"" ""${PROJECT_SOURCE_DIR}/src/*.cpp"" """")

    add_library(ssl SHARED ${_src_list})

    set(SSL_LIBRARIES ${TF_LIBRARIES} ${LUA_LIBRARIES} ${Boost_LIBRARIES}     ${NLOPT_LIBRARIES} ${FFTW_LIBRARIES} ${LINENOISE_LIBRARIES} ${YACAS_LIBRARIES} ${HDF5_CXX_LIBRARIES}) #${TF_LIBRARIES}

    target_link_libraries(ssl ${SSL_LIBRARIES} dl)

    add_executable(Project1 main.cpp)
    target_link_libraries(Project1 ssl)
"
6,31243,1,"Model loaded via tf.keras.load_model is very slow. OS: Ubuntu 18.04
Python 3.6.8
Tensorflow version 2.0.0b1 (GPU)
GPU: Titan RTX
CUDA version 10.0

Issue: current script is working fast


After finishing this script I launch next one, which work very slow (30-40 sec).


the last two lines of first script output

Second script for some reason doesn't output the loading of libcudnn.so.7
"
62,24191,1,"tensorflow for golang did not use GPU to speed up. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.5.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
cuda-9.2
cudnn-7.2.1
- GPU model and memory:
GeForce GTX 1080 Ti 11GB

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
session Run cost 4s on GPU:

session Run cost 4s on CPU:


**Describe the expected behavior**
running on GPU should be much faster than on CPU

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
908,13711,0,"--config=mkl leads to libmklml_intel.so: cannot open shared object file: No such file or directory. I compiled passing --config=mkl to bazel, it compiles fine and i get the .whl file, i install it with pip correctly, but when i launch a python session and type : import tensorflow as tf i get:
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory

Any ideas?
"
1423,16985,0,"undefined symbol: PyUnicodeUCS4_FromString   . I tried to install tensorflow cpu version using python 2.7 on Ubuntu16.04 under virtualenv. but when I want to import tensorflow, the error is :

`(My_python2) yuan@ubuntu:~$ python
Python 2.7.13 (default, Feb 13 2018, 14:17:11) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyUnicodeUCS4_FromString


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Please help! Many thanks"
1019,12765,0,"Three questions about ChiefSessionCreator?. * There are two parameters of [ChiefSessionCreator][1]:  and ,
what difference between them?

* And ChiefSessionCreator has a parameter [scaffold][2] which has a parameter , does it mean ChiefSessionCreator uses scaffold.saver to save and restore variables?

* And scaffold also has a parameter , and it can be used like 
init_fn=[assign_from_checkpoint_fn][3], assign_from_checkpoint_fn will new a saver with the parameter  of assign_from_checkpoint_fn. So here comes another question, ChiefSessionCreator will use scaffold.saver to restore variables or init_fn?

To answer the three questions above, you may need read the source code, but I am not capable to do it, so I need your help. Thanks.

  [1]: https://www.tensorflow.org/versions/master/api_docs/python/tf/train/ChiefSessionCreator
  [2]: https://www.tensorflow.org/versions/master/api_docs/python/tf/train/Scaffold
  [3]: https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/framework/assign_from_checkpoint_fn"
319,107,0,"Truncated backprop docs are confusing. The docs (http://tensorflow.org/tutorials/recurrent/index.md) imply that to truncate backprop, you feed blocks of fixed (time) length () to .  

Does this mean that if I have sentences of length 100 and I want to truncate by 20 time steps, I would send tensors of shape ?

If I'm right and you are supposed to feed blocks of fixed length inputs in an inner loop, don't I need to initialize the initial state from the previous block for the forward pass to be right.  But if I do use the final hidden state of the previous block as the next initial state, won't the autograd backpropagate all the way through.  In other words, looping over blocks of intervals and then within each in block while connecting the previous block's hidden state to the next block's initial state seems equivalent to just looping over time in the first place.

Am I missing something?
"
696,511,0,"Large RNN graphs, incredibly slow graph creation and step time.... When I build a large seq2seq RNN graph (i.e., 3000 timesteps encoder, and 100 time steps decoder), both the graph construction and step time is insanely slow.

For example, using approx 3000 timesteps of stacked 4 layers of GRUs (in a seq2seq model w/ attention), we get something like this:

**\* graph creation time ***
create_encoder graph time 77.442740
create_decoder graph time 7.298214
create_loss graph time 0.934293
create_optimizer graph time 349.426908
create_model graph time 489.119399

create_optimizer is the part where i call something like this (pulled from the tutorial):
  def create_optimizer(self):
    start_time = time.time()



**\* step time ***
step_time: 142.356251001

When I run nvidia-smi while the graph is computing , the utilization is something like 12% ... definitely something wrong...

In my personal framework (not TF), creating a graph of equal size takes approx 2 secs (as opposed to TF almost 8 mins) and the step time is less 4 seconds (as opposed to TF 2 mins).... something really weird is going on.

I wasn't able to profile / debug it too much (are there public tools for that? i cant find anything on tensorboard). also, is there a command i can run to confirm how many weights i have (and the dimensions and the device location of the variables).

I've confirmed with the tensorboard and log_device_placement=True, and seems like most of the graph is in the GPU (except for embedding layers).

If you want a link to my graph, send me an email.

Thanks All!
"
515,12871,0,"About Deterministic Behaviour of GPU implementation of tensorflow. - **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian
- **TensorFlow version (use command below)**:('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce GTX 950


Dear experts,

I have just written a python code that implements a CNN with tensorflow. Despite using tf.set_random_seed(0) throughout my code, I get different results for each different run using a GPU, while this is not happening when switching to CPU. I read different threads about stochastic behaviour of GPUs and I could find that the stochastic behaviour is happening when using AdamOptimizer. My code now uses a CPU just for the optimizer, and a GPU for all the others operations, so now the time performance of the GPU is 25% better than CPU despite the 700% of the full GPU implementation. My question is: is this avoidable? Should I really give up in having a deterministic code when using GPUs? How can I tune my hyperparameters and use a full GPU code (some forums say to make an average of different runs.. but this kills the time advantage of using GPUs).

Thanks in advance for all your kind support and for the wonderful job
"
409,14306,0,"Segmentation fault when using bidirectional_dynamic_rnn + orthogonal_initializer. ### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Ubuntu 14.04 LTS (kernel: 3.16.0-77-generic)
- **TensorFlow installed from**: source
- **TensorFlow version**: 1.4.0rc1 (ae04712e3b74bc85445e12c90e375f980a907e2d) 
- **Python version**:  2.7.10
- **Bazel version**: 0.7.0
- **GCC/Compiler version**: 4.8.5
- **CUDA/cuDNN version**: 8.0/6.0.21
- **GPU model and memory**: Maxwell Titan X with 12 GiB memory
- **Exact command to reproduce**: see https://gist.github.com/nryant/57f7810e333fa94379cad201eeff24c8


### Describe the problem
I recently upgraded from 1.4.0-rc0 to 1.4.0-rc1 and found that a number of my model architectures fail to compile. After a bit of a detective work, I tracked the issue down to the use of bidirectional_dynamic_rnn in conjunction with a VariableScope in which initializer=orthogonal_initializer(). See, for example, the test program at

    https://gist.github.com/nryant/57f7810e333fa94379cad201eeff24c8

Running this will result in a segfault with 1.4.0-rc1, but not with earlier versions. The problem is specific to the combination of orthogonal_initializer and bidirectional_dynamic_rnn and does not replicate with other initializers (e.g., uniform_unit_scaling_initializer) or dynamic_rnn. Nor does the choice of RNN cell appear to matter.
"
146,13603,1,"SVD on GPU is slower than SVD on CPU. OS:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS release 7.4.1708
- **TensorFlow installed from (source or binary)**: From source
- **Python version**: 2.7.13
- **Bazel version**: 0.6.1
- **CUDA/cuDNN version**: CUDA 8.0/cuDNN 6.0.21
- **GPU model and memory**: GeForce GTX 950M, memory 4GB

output of 


output of 


### Describe the problem

SVD on GPU is slower than SVD on CPU

### Source code / logs

file main.py


## run on GPU



## run on CPU

"
690,12444,0,"The sample code on PROGRAMMER'S GUIDE has a problem. Hello,

There is a problem on the PROGRAMMER'S GUIDE.
https://www.tensorflow.org/programmers_guide/variables#sharing_variables

In the second code snippet of the chapter ""Sharing variables""


### Problem
The shape doesn't match.

### Solution
Change the third line to


And the code after this also have the problem."
124,29948,1,"TF2.0beta distribute.MirroredStrategy hangs causing 100% GPU. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I run the code provided in the tutorial https://www.tensorflow.org/beta/tutorials/distribute/keras

== check python ===================================================
python version: 3.6.7
python branch: 
python build version: ('default', 'Feb 28 2019 09:07:38')
python compiler version: GCC 7.3.0
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #1 SMP Thu Nov 29 14:49:43 UTC 2018
os release version: 3.10.0-957.1.3.el7.x86_64
os platform: Linux-3.10.0-957.1.3.el7.x86_64-x86_64-with-centos-7.6.1810-Core
linux distribution: ('CentOS Linux', '7.6.1810', 'Core')
linux os distribution: ('centos', '7.6.1810', 'Core')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='monod33.mbb.ki.se', release='3.10.0-957.1.3.el7.x86_64', version='#1 SMP Thu Nov 29 14:49:43 UTC 2018', machine='x86_64', processor='x86_64')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                    1.16.4              
protobuf                 3.8.0               
tensorflow-datasets      1.0.2               
tensorflow-gpu           2.0.0b1             
tensorflow-metadata      0.13.0              

== check for virtualenv =========================================
False


- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
binary (pip)
- TensorFlow version (use command below):
- Python version:
2.0.0b1 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
cuda 10.0/cuDNN 7
- GPU model and memory:
4x Nvidia 1080Ti 11GB (MSI GeForce GTX) 

**Describe the current behavior**
- Training on one GPUs works fine
- All GPUs are recognized
- When I select multiple GPUs example:  the processing hang and the GPUs are stuck at 100%

code output hangs at:

nvidia-smi output
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.93       Driver Version: 410.93       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:1A:00.0 Off |                  N/A |
| 29%   38C    P2    73W / 250W |  10883MiB / 11178MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:1B:00.0 Off |                  N/A |
| 29%   35C    P2    73W / 250W |  10883MiB / 11178MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  Off  | 00000000:88:00.0 Off |                  N/A |
| 29%   28C    P8     8W / 250W |    157MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  Off  | 00000000:89:00.0 Off |                  N/A |
| 29%   26C    P8     8W / 250W |    157MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      |
|=============================================================================||    0    195955      C   ...processing/anaconda3/envs/tf/bin/python 10873MiB |
|    1    195955      C   ...processing/anaconda3/envs/tf/bin/python 10873MiB ||    2    195955      C   ...processing/anaconda3/envs/tf/bin/python   147MiB |
|    3    195955      C   ...processing/anaconda3/envs/tf/bin/python   147MiB |+-----------------------------------------------------------------------------+

**Describe the expected behavior**
Run the training in parallel

**Code to reproduce the issue**
Code provided in the tutorial: https://www.tensorflow.org/beta/tutorials/distribute/keras for TF2.0 Beta
**Other info / logs**
output when running the code


Thanks for the help!"
203,26500,1,"LSTM recurrent kernel ( hidden state ) makes Host to GPU transaction for every sequence, which looks quite inefficient.. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): build from source
- TensorFlow version (use command below): v1.13.0-7-g9ca8321 1.12.0
- Python version: Python 3.6.1 :: Anaconda 4.4.0 (64-bit)
- Bazel version (if compiling from source): Build label: 0.19.2
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 (GCC)
- CUDA/cuDNN version: 
I got cuda version by checking nvcc verison. it is Cuda compilation tools, release 10.0, V10.0.130.
cudnn 7.5.0
- GPU model and memory: v100

I am evaluating the inference performance of 2-level stacked LSTM model. 
As you could see in the code below, my model has 2 LSTM cell and one time-distributed (FC) layer.
Because i want to make close investigation of how inference works, i utilize google timeline tool.

**Describe the current behavior**
Whenever new sequence starts, the recurrent kernel ( hidden state of lstm cell ) makes Host to GPU data transaction. Because this data transfer is initiated by ReadVariableOp which is connected to matmul op of a lstm hidden state, i am pretty sure that a data is the weight of the hidden state.
What i am curious is that why the weight has to move from H2D? if right, where is the D2H transaction? Doesn't hidden state be renewed every new sequence?

**Describe the expected behavior**
I think there should be two kinds of correct behavior.
1. For every sequence of lstm, GPU renew the weight, and this is moved to Host memory (D2H transaction - which doesn't exist at the timeline result). And then the weight goes to GPU again at the next sequence. 
2. No Transaction. the hidden state is renewed and reused only inside the GPU memory. In this case, i could see the H2D transaction (which is observed at the timeline result)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


"
47,34906,1,"tensorflow gpu inference  only one thread is busy.  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
  **5708 root      20   0 21.518g 1.820g 794224 S 82.0  0.7   6:38.31 server**
  5722 root      20   0 21.518g 1.820g 794224 S  8.0  0.7   0:36.46 server
  5747 root      20   0 21.518g 1.820g 794224 S  8.0  0.7   0:36.61 server
  5712 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.35 server
  5714 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.49 server
  5716 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.57 server
  5718 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.36 server
  5721 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.52 server
  5723 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.42 server
  5724 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.47 server
  5725 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.60 server
  5726 root      20   0 21.518g 1.820g 794224 R  7.7  0.7   0:36.48 server
  5728 root      20   0 21.518g 1.820g 794224 R  7.7  0.7   0:36.35 server
  5730 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.46 server
  5731 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.37 server
  5734 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.42 server
  5736 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.43 server
  5737 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.35 server
  5741 root      20   0 21.518g 1.820g 794224 R  7.7  0.7   0:36.58 server

tensorflow gpu inference  only one thread is busy, and the inference is so slow ,and the gpu useage is low, something bug?

tensorflow gpu version is 1.13.1"
309,28941,0,"Can't use padded_batch on dataset with distributed strategy make_dataset_iterator. I can't use padded_batch when I'm trying to create a distributed iterator using the following code.



It throws this exception:

batchmap_and_batch

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0/CuDNN 7.3.1
- GPU model and memory: Nvidia Titan V
"
745,4735,0,"Resource exhausted error in the middle of training. I train the inception v1 (slim) model on my own data set. 269 classes total.
The max training step is 60435 and batch size is 256 as below
**--max_number_of_steps=60435
--batch_size=256**
The model runs under GPU mode and I have 4 Tian X GPUs, with each has 12G GPU memory.
The Resource exhausted error happen after at least 46831 trainning steps, since i can see the last check point file is model.ckpt-46831.

I do not know why the issue happen in the middle, but not at very beginning of the training process.

The error log report by Tensor Fow is as below:

//other lines above.
I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 11.16GiB
I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: 
**Limit:                 12049707828
InUse:                 11984328960
MaxInUse:              12038083584
NumAllocs:               248036306
MaxAllocSize:           2831155200**

W tensorflow/core/common_runtime/bfc_allocator.cc:274] *************************************************************************************************_xx
W tensorflow/core/common_runtime/bfc_allocator.cc:275] *_Ran out of memory trying to allocate 39.81MiB.**  See logs for memory state.
W tensorflow/core/framework/op_kernel.cc:968] Resource exhausted: OOM when allocating tensor with shape[256,832,7,7]
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors.ResourceExhaustedError'>, OOM when allocating tensor with shape[256,112,112,64]
     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]
     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference', defined at:
  File ""train_image_classifier.py"", line 585, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""train_image_classifier.py"", line 482, in main
    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/deployment/model_deploy.py"", line 195, in create_clones
    outputs = model_fn(_args, *_kwargs)
  File ""train_image_classifier.py"", line 466, in clone_fn
    logits, end_points = network_fn(images)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/nets_factory.py"", line 103, in network_fn
    return func(images, num_classes, is_training=is_training)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py"", line 288, in inception_v1
    net, end_points = inception_v1_base(inputs, scope=scope)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py"", line 61, in inception_v1_base
    net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(_args, *_current_args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 445, in convolution2d
    outputs = normalizer_fn(outputs, *_normalizer_params)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(_args, **current_args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 250, in batch_norm
    mean, variance = nn.moments(inputs, axis, shift=shift)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 835, in moments
    y, axes, shift=shift, keep_dims=keep_dims, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 762, in sufficient_statistics
    v_ss = math_ops.squared_difference(x, shift)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2347, in squared_difference
    result = _op_def_lib.apply_op(""SquaredDifference"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2386, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,112,112,64]
     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]
     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Traceback (most recent call last):
  File ""train_image_classifier.py"", line 585, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""train_image_classifier.py"", line 581, in main
    sync_optimizer=optimizer if FLAGS.sync_replicas else None)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 781, in train
    raise
  File ""/usr/lib/python2.7/contextlib.py"", line 35, in **exit**
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 969, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 797, in stop
    stop_grace_period_secs=self._stop_grace_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 386, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 296, in stop_on_exception
    yield
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 481, in run
    self.run_loop()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 999, in run_loop
    self._sv.global_step])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 717, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 915, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 985, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[256,112,112,64]
     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]
     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference', defined at:
  File ""train_image_classifier.py"", line 585, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""train_image_classifier.py"", line 482, in main
    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/deployment/model_deploy.py"", line 195, in create_clones
    outputs = model_fn(_args, *_kwargs)
  File ""train_image_classifier.py"", line 466, in clone_fn
    logits, end_points = network_fn(images)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/nets_factory.py"", line 103, in network_fn
    return func(images, num_classes, is_training=is_training)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py"", line 288, in inception_v1
    net, end_points = inception_v1_base(inputs, scope=scope)
  File ""/home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/models/slim/nets/inception_v1.py"", line 61, in inception_v1_base
    net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(_args, *_current_args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 445, in convolution2d
    outputs = normalizer_fn(outputs, *_normalizer_params)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(_args, **current_args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 250, in batch_norm
    mean, variance = nn.moments(inputs, axis, shift=shift)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 835, in moments
    y, axes, shift=shift, keep_dims=keep_dims, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 762, in sufficient_statistics
    v_ss = math_ops.squared_difference(x, shift)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2347, in squared_difference
    result = _op_def_lib.apply_op(""SquaredDifference"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2386, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,112,112,64]
     [[Node: InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/moments/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](InceptionV1/InceptionV1/Conv2d_1a_7x7/Conv2D, InceptionV1/InceptionV1/Conv2d_1a_7x7/BatchNorm/Add)]]
     [[Node: InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool/_1231 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3156_InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
"
510,18736,0,"tensorflow/core/framework/allocator.cc:101] Allocation of X exceeds 10% of system memory. #18735. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:

### Describe the problem
My server has 32gb of RAM, but tensorflow uses only 10% of that memory.
It returns the message:
<<tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.>>

Is it possible to increase this percentage?

### Source code / logs
### SOURCE CODE:
import keras
from keras import regularizers
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, add
from keras.models import Model
from keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape
from keras.regularizers import l2
from keras.utils import np_utils
from keras.callbacks import TensorBoard
from sklearn.model_selection import train_test_split
import numpy as np
import h5py
import time

file_train = 'features/files_train_test/train_inceptionv3_doc2vec.npy'
file_test = 'features/files_train_test/test_inceptionv3_doc2vec.npy'
x_train = np.load(file_train)
x_test = np.load(file_test)

print('shape train: ',x_train.shape,'shape test: ', x_test.shape)
print('size train: ', len(x_train), 'size test: ', len(x_test))
print('prod train: ',np.prod(x_train.shape[1:]), 'prod test: ', np.prod(x_test.shape[1:]))

x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

print('shape train: ',x_train.shape,'shape test: ', x_test.shape)

epochs = 10
batch_size = 32
input_size = x_train.shape[1]
output_size = x_train.shape[1]
hidden_size = x_train.shape[1]
file_name = 'features/files_reduce/sparse/autoencoder_inceptionv3_doc2vec_'

x = Input(shape=(input_size,))
h = Dense(hidden_size, activation='relu', activity_regularizer=regularizers.l1(10e-5))(x)
r = Dense(output_size, activation='sigmoid')(h)
autoencoder = Model(inputs=x, outputs=r)
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()
autoencoder.fit(x_train
                ,x_train
                ,batch_size=batch_size
                ,epochs=epochs
                ,verbose=1
                ,validation_data=(x_test, x_test))

autoencoder.save(file_name+name_full)
print('Save encode' + file_name+name_full)

===============================================================================
OUTPUT: 

Using TensorFlow backend.
shape train:  (21248, 49452) shape test:  (5313, 49452)
size train:  21248 size test:  5313
prod train:  49452 prod test:  49452
shape train:  (21248, 49452) shape test:  (5313, 49452)

Layer (type)                      | Output Shape              | Param    
- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -
input_1 (InputLayer)         | (None, 49452)             | 0         
dense_1 (Dense)              | (None, 49452)             | 2445549756
dense_2 (Dense)              | (None, 49452)             | 2445549756
- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -

Total params: 4,891,099,512
Trainable params: 4,891,099,512
Non-trainable params: 0

Train on 21248 samples, validate on 5313 samples
Epoch 1/10
2018-04-20 11:22:08.069764: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.
2018-04-20 11:22:12.089390: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory."
1440,30474,0,"[TF2.0] Bug allowing misuse of the batch dimension of a convolution layer.  rightfully complains about the following minimal example with .

 however happily runs it and prints .



As [per discussion](https://groups.google.com/a/tensorflow.org/forum/#!topic/testing/txsgcR3cubQ) this seems to be a bug in TF 2.0."
110,28390,1,"different results on different machine. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10(10.0 17134  10.0 15063)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: unclear
- TensorFlow installed from (source or binary): anaconda navigator
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): spyder 3.3.1
- CUDA/cuDNN version: CPU
- GPU model and memory: CPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
different results on different machine
**Describe the expected behavior**
same results on different machine
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import numpy as np
import csv
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.utils import shuffle
from batch import Dataset




def generate(sample_size,mean,cov,diff,regression):
    num_classes = 2
    samples_per_class = int(sample_size/2)
    np.random.seed(1)
    X0 = np.random.multivariate_normal(mean,cov,samples_per_class)
    Y0 = np.zeros(samples_per_class)
    
    for ci,d in enumerate(diff):
        X1 = np.random.multivariate_normal(mean+d,cov,samples_per_class)
        Y1 = (ci+1)*np.ones(samples_per_class)
        
        X0 = np.concatenate((X0,X1))
        Y0 = np.concatenate((Y0,Y1))
    
    if regression==False:
        class_ind = [Y0==class_number for class_number in range(num_classes)]
        Y0 = np.asarray(np.hstack(class_ind),dtype=np.float32)
        
    X,Y = shuffle(X0,Y0,random_state=1)
    
    return X,Y



np.random.seed(10)
num_classes = 2
mean = np.random.randn(num_classes)
cov = np.eye(num_classes)
X,Y = generate(100,mean,cov,[3.0],True)
colors = ['r' if l==0 else 'b' for l in Y[:]]
plt.scatter(X[:,0],X[:,1],c=colors)
plt.xlabel('Scaled age (in yrs)')
plt.ylabel('Tumor size (in cm)')
plt.show()
lab_dim =1


path = 'F:\\实验室代码\\python\\autoencoder\\testData\\线性逻辑回归训练集.csv'
f = open(path ,'w',newline='',encoding='utf-8') 
writer = csv.writer(f)
for item in X :
   writer.writerow(item)  
f.close()

input_dim = X.shape[1]
lab_dim = 1
input_features = tf.placeholder(tf.float32,[None,input_dim])
input_labels = tf.placeholder(tf.float32,[None,lab_dim])
W = tf.Variable(tf.random_normal([input_dim,lab_dim],seed=1),name='weight')
b = tf.Variable(tf.zeros([lab_dim]),name='bias')

output = tf.nn.sigmoid(tf.matmul(input_features,W)+b)
cross_entropy = -(input_labels*tf.log(output)+(1-input_labels)*tf.log(1-output))
ser = tf.square(input_labels-output)

loss = tf.reduce_mean(cross_entropy)
err = tf.reduce_mean(ser)
optimizer = tf.train.AdamOptimizer(0.04)
train = optimizer.minimize(loss)

maxEpochs = 51
minibatchSize = 25


path = 'F:\\实验室代码\\python\\autoencoder\\testData\\线性逻辑回归优化前权重.csv'
f_1 = open(path ,'w',newline='',encoding='utf-8') 
writer_1 = csv.writer(f_1)

path = 'F:\\实验室代码\\python\\autoencoder\\testData\\线性逻辑回归优化后权重.csv'
f_2 = open(path ,'w',newline='',encoding='utf-8') 
writer_2 = csv.writer(f_2)

path = 'F:\\实验室代码\\python\\autoencoder\\testData\\所有epoch的index.csv'
f_3 = open(path ,'w',newline='',encoding='utf-8') 
writer_3 = csv.writer(f_3)



with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    w_1 = sess.run(W)  #优化前的权重
    for item in w_1 :
        writer_1.writerow(item)  
    f_1.close()

    index_all_epoch = []  #所有epoch
    for epoch in range(maxEpochs):
        sumerr=0
        index = Dataset(np.arange(0, 100))  #index为下标数组，从0到n_examples-1的整数
        index_whole = np.array([]) #一次epoch 
        seed = epoch
        for i in range(np.int32(len(Y)/minibatchSize)):
            
            tempindex = index.next_batch(minibatchSize,seed)  #将下标数组打乱，取出前面的下标
            
            index_whole = np.append(index_whole,tempindex)
            
            x1 = X[tempindex]
            y1 = np.reshape(Y[tempindex],[-1,1])
            tf.reshape(y1,[-1,1])
            _,lossval,outputval,errval = sess.run([train,loss,output,err],feed_dict = {input_features:x1,input_labels:y1})
            sumerr = sumerr + errval
        print('Epoch:','%04d' %(epoch+1),'cost=','{:.9f}'.format(lossval),'err=',sumerr/np.int32(len(Y)/minibatchSize))
        index_all_epoch.append(index_whole)
    
    w_2 = sess.run(W)  #优化后的权重
    for item in w_2 :
        writer_2.writerow(item)  
    f_2.close()
    
    for item in index_all_epoch:#所有epoch
        writer_3.writerow(item)
    f_3.close()
        
        

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
180,26460,1,"GPU Device Selector in TensorFlow 2.0. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Happy to help as much as I can!

**Describe the feature and the current behavior/state.**
TensorFlow 1.x support specifying GPU devices to use:


There's no comparable API in TensorFlow 2.0.  The closest option is to use the  environment variable.  Unfortunately,  prevents processes from doing  from/to devices not owned by the process.  There's a significant performance degradation when NCCL is used with P2P communication disabled.

The ask is to add an API to TensorFlow 2.0 to enable device selection.

**Will this change the current api? How?**
Yes, will introduce an API to select GPU devices to use.

**Who will benefit with this feature?**
Users of [Horovod](http://horovod.ai).

**Any Other info.**
cc @azaks2 @alextp @jaingaurav @guptapriya "
1348,27260,0,"tensorflow.keras model.fit_generator not working with use multiprocessing. I am using Ubuntu 18 with conda and python 3.6 with tf 1.20 GPU over a gtx 1070 and 2080ti

When I am using this code 

    model.fit_generator(i, epochs=1, workers=16,
                            use_multiprocessing=False, max_queue_size=16,
                            verbose=1)

It works fine

When I change use_multiprocessing to True, and run, it does nothing, the cpu is not utilized by python at all, and nothing shows on the monitor. It used to work fine in the past. 

What am I missing here? "
651,4361,0,"Update tf.contrib.layers.batch_norm() docs. _Tensorflow version that I use : 0.10 (pip package)_

---

I took heavy use of _[tf.contrib.layers.batch_norm()](https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100)_ the last weeks. 

After facing some problems on how to use it correctly, I figured out that there are many devs out there who are confused as well, such as here:
- https://github.com/tensorflow/tensorflow/issues/1122
- http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow

I would suggest to do following improvements to make it more clear:

**1) Update example in doc-string:**

The example tells in case we use _update_collections_ on its defaults, we have to include this:



But this is actually not working or deprecated, as it throws errors. Instead, we have to do some tiny changes. I would suggest to update the docs as follows:



As a side question, why do we apply it to the _total_loss_, and not to the train_op directly, as described in the doc-string text. Added a dependency to _total_loss_ works, but grouping it with the _train_op_ would make the example more clear in my opinion, because we do batch-statistic updates only during training.

**2) _UPDATE_OPS_ in combination with reuse varscope:**

This is related to the question above. Let's say we have a model with which reuses an convolutional encoder (and also its batch-norm-layers) several times. Even when we reuse these layers, the update operation for the batch-statistics is added to _UPDATE_OPS_ nevertheless. Personally, I'm not sure if this is a bug, or if this is really what should be done?
Or is it required to filter the update-ops after collecting them with , so that each one is executed just once?

To sum this up: Am I wrong that lines 213-215 should not be executed when reuse=True? So changing it to:



In my case, I'm using a Conv-LSTM-Conv_tp architecture, where I reuse the Conv/Conv_tp for each timestep. When I increase the number of timesteps in the LSTM, the number of update-ops increases in proportionally, while the number of model-parameters stays constant because they get reused. Currently, I'm getting 420 update-ops when calling . As the performance feels super slow when I use batch-norm, I guess this high number of update-ops cannot be right.

**3) Handling of _is_training_ parameter:**

I have seen a lot of examples people doing something like this in their code to handle the _is_training_ parameter:



As far as I know, this was really required in the past, because is_training was just a Boolean. But since the param can be a Bool-Tensor as well, this is not required anymore. Since many devs are still ding this workaound, added a comment to the doc-string that this is not required anymore could be helpful.

**4) Usage on Multi-GPU configuration**

a) When I optimize my code for multi-GPU systems (as in the CIFAR10 example) the number of update-ops increases with the factor of num_gpus (might be related to 2) ).

b) When I use _tf.contrib.batch_norm()_ within a multi-GPU system, I get an error like this:



Hence, to we have to wrap evey _batch_norm()_ call with _tf.device(""/cpu:0"")_? I guess this might have bad impacts on performance, right?

Thanks!

_PS: Sorry in case this question would fits better to StackOverflow. As it is a combination of suggested improvements and questions. Just let me know..._
"
1414,20660,0,"Attempting to use uninitialized value Variable. Hi all,

I had Import a graph with go:



use the model:



but get error: 


It haven't init in NewSession? how to initialize variable?

env:

OS: mac 10.11.6
python version: python2.7
tensorflow install base on https://www.tensorflow.org/install/install_go
tensorflow-go version: v1.9.0-rc2
go version: go1.9.2 darwin/amd64"
657,33655,0,"Exception in thread ""main"" java.lang.IllegalStateException: Table not initialized.. When training model in tensorflow using tf.contrib.lookup.HashTable and tf.contrib.lookup.KeyValueTensorInitializer, and then save model as .pb, it works when load .pb and predict in tensorflow,
but when importGraphDef in java and predict, an error occurred as below:

2019-10-24 07:24:29.753767: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
2019-10-24 07:24:29.753768: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
2019-10-24 07:24:29.753784: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
2019-10-24 07:24:29.753840: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
2019-10-24 07:24:29.753852: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
2019-10-24 07:24:29.753865: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
2019-10-24 07:24:29.753852: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
2019-10-24 07:24:29.753905: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
2019-10-24 07:24:29.753905: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
2019-10-24 07:24:29.753933: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.
Exception in thread ""main"" java.lang.IllegalStateException: Table not initialized."
224,33708,1,"non_max_suppression GPU version is 3x slower than CPU version in  TF 1.15. <em>
</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.15
- Python version:3.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10
- GPU model and memory:K80

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
non_max_suppression GPU version is 3x slower than CPU version.
**Describe the expected behavior**
GPU version is expected to be faster ( or at the least same ) than CPU version.

**Code to reproduce the issue

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
34,29762,1,"Accuracy Discrepancy Between Eager and Graph Mode Tensorflow Alpha 2.0.0. **System information**
- Custom and Stock Code
- TensorFlow installed from binary:
- [tf_env.txt](https://github.com/tensorflow/tensorflow/files/3288005/tf_env.txt)
- CUDA/cuDNN version: 
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018
Cuda compilation tools, release 10.0, V10.0.130
- GPU model and memory:
  - Name: NVIDIA GeForce GTX 1060 with Max-Q Design
  - Memory: 8124 MB

**Describe the current behavior**
There is a large accuracy difference between running tensorflow in Eager versus Graph mode 
(i.e. using @tf.function and not using @tf.function) for the same model, number of batches and epochs on the same data.

Three different tests are run to measure performance on both a GPU and CPU if
available. Each test uses tensorflow alpha 2.0, and trains using the MNIST dataset provided/bundled by tensorflow. Each test runs for 1 epoch comprised of 1000 mini-batches. A mini-batch contains 32 samples. All tests use Adam optimizer, and sparse categorical loss.

Test descriptions:
- Test 1: Uses the Keras 'compile' and 'fit' function
- Test 2: Uses tf.GradientTape and does not use the @tf.function decorator
- Test 3: Uses tf.GradientTape and the @tf.function decorator. 
    The @tf.function decorator is applied to the function training 
    on MNIST mini-batches

**Describe the expected behavior**
I would expect roughly the same training accuracy after 1000 steps for each test. When using the @tf.function decorator a much greater accuracy is achieved for the same model type, number of epochs, number of batches, and dataset compared to a training run not using @tf.function. 

**Code to reproduce the issue**


**Other info / logs**
[perf_test_tf_2_0.log](https://github.com/tensorflow/tensorflow/files/3287965/perf_test_tf_2_0.log)

"
400,19743,0,"error：Library objects cannot exceed 65,536，win10 + TensorFlowGPU . **System information:**
· Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No

· OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Win10-x64

· TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow.git

· TensorFlow version (use command below):  r1.4, r1.5, r1.6  r1.7 and r1.8,  command : 
v1.8's command : git checkout -b v1.8 -f origin/r1.8
v1.7's command : git checkout -b v1.7 -f origin/r1.7
v1.6's command : git checkout -b v1.6 -f origin/r1.6
and so on include v1.4 and v1.5

· Python version:  Anaconda3 - python3.6

· Bazel version (if compiling from source):  I used CMAKE 3.11.1

· GCC/Compiler version (if compiling from source):  Visual Studio 2015( VS15' MSBuild)

· CUDA/cuDNN version:  CUDA9.0,   cudnn-9.0-win10-7.1

· GPU model and memory: GTX-860m with 2Gb Memory



Exact command to reproduce: 
""
D:\ProgramData\tensorflow\tensorflow\contrib\cmake\build> cmake .. -A x64 -DCMAKE_BUILD_TYPE=Debug -DSWIG_EXECUTABLE=D:/soft/TensorflowSoft/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/ProgramData/Anaconda3/python.exe -DPYTHON_LIBRARIES=D:/ProgramData/Anaconda3/libs/python36.lib -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=""D:\soft\TensorflowSoft\cudnn"" -G ""Visual Studio 14 2015""

D:\ProgramData\tensorflow\tensorflow\contrib\cmake\build> set PreferredToolArchitecture=x64 

D:\ProgramData\tensorflow\tensorflow\contrib\cmake\build> MSBuild /p:Configuration=Release ALL_BUILD.vcxproj
""

**Describe the problem** : 
I build TensorFlow-CPU-r1.8 successfully (win10-x64 + Anaconda3-python3.6 + **VS2017**(not VS2015) +  tensorflow1.8 + cmake3.11.1 + SwigWin3.0.12).
 
But when I build tensorflow-**GPU** version(both tensorflow-r1.7 and r1.8), the only error has occurred: The library objects cannot exceed 65,536. (If I use VS2017, it shows error: "" the compiler is not supported for CUDA 9.0"" , so I switch Visual Studio version to VS2015, and CMAKE command is work successfully.)

After that, I switched to lower versions TensorFlow-r1.6 to TensorFlow-r1.5, but Link error occurred：

“libprotobufd.lib(text_format.obj) : error LNK2019: unresolved external symbol __std_reverse_trivially_swappable_8 referenced in function ""void _
_cdecl std::_Reverse_unchecked1<class google::protobuf::Message const * *>(class google::protobuf::Message const * * const,class google::protobuf:
:Message const * * const,struct std::integral_constant<unsigned __int64,8>)"" (??$_Reverse_unchecked1@PEAPEBVMessage@protobuf@google@@@std@@YAXQEAP
EBVMessage@protobuf@google@@0U?$integral_constant@_K$07@0@@Z) [D:\tf\tensorflowGPU\tensorflow\contrib\cmake\build\proto_text.vcxproj]
  libprotobufd.lib(wire_format.obj) : error LNK2001: unresolved external symbol __std_reverse_trivially_swappable_8 [D:\tf\tensorflowGPU\tensorflo
w\contrib\cmake\build\proto_text.vcxproj]
  D:\tf\tensorflowGPU\tensorflow\contrib\cmake\build\Debug\proto_text.exe : fatal error LNK1120: 1 unresolved externals [D:\tf\tensorflowGPU\tenso
rflow\contrib\cmake\build\proto_text.vcxproj]”

I don't know how to solve those errors..."
257,31199,1,"Adding AVX2 and FMA makes tensorflow slower on my CPUs. - Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: none 
- GPU model and memory: none

I have built several different tensorflow versions from source to compare the performance during inference. Two of my many versions are:
- prebuilt TF 1.14 C API libpackage (with AVX)
- self-built TF 1.14 CAPI libpackage  (with AVX,AVX2,FMA)

Strangely, I experience that the prebuilt version reaches better latency and throughput for every single comparison. I am a bit confused about that as adding additional instruction set architectures during the build should rather result in improved performance...? I am not sure where the difference might be. The self-built version is without XLA JIT.

My question: is there an obvious reason why this might occur? The hardware i use is:
Intel Xeon Gold 6130 GPU 2.10 GHz (on a cluster, starting with 24 cores up to 480 cores)"
825,12727,0,"Is tf.one_hot() w/ GPU not working under windows10?. I'm trying to run tensorflow in Windows10 environment.
When I use  function with GPU then it occur error.
Below is test code, and it's working find in Ubuntu.

### Test code


### Error 


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/5.1
- **GPU model and memory**: Titan XP (12GB)
- **Exact command to reproduce**:
"
270,35310,1,"Execution hangs after particular step in CUDA 10.1 TF 1.14.0. ### System information
- Have I written custom code: Yes
- OS Platform and Distribution: 18.04.3 LTS Ubuntu
- TensorFlow installed : pre installed container
- TensorFlow version : 1.14.0
- Python version: 3.5.2
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla V100-SXM3-32GB
- Exact command to reproduce: Running the command python train_blstm.py

### Describe the problem
I am working on an Bi-LSTM + CTC Loss + WordBeamSearch Architecture for online handwriting recognition. The problem is that the code hangs/stalls after a particular point everytime. I have also let it run for 2 days now but it's still the same. Any help in this regard would be appreciated.

### Source code / logs
Here is the traceback of the issue attached below:
keras.layers.Bidirectional(keras.layers.RNN(cell))keras.layers.RNN(cell)"
557,31412,0,"DeprecationWarning: the imp module is deprecated. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: tensorflow-gpu==2.0.0b1
- **Python version**: 3.6.8
- **CUDA/cuDNN version**: 10.0.130/7.6.2
- **Exact command to reproduce**:

Create  file with the following content:


Run pytest (5.0.1) on it:


Result:


### Describe the problem
TensorFlow is using ""imp"" which is deprecated since Python 3.4. Are there plans to update it?
Thanks!"
641,8557,0,"Error building TensorFlow with local LLVM repository. ( Using local_repository extension in bazel ). Essentially, I want to run TensorFlow with a custom LLVM repository and not the llvm-mirror that bazel pulls from. 

I made the following changes:

1. Changed the  rule in  to:

          native.local_repository (
              name = ""llvm"",
              path = ""/git/llvm/"",
          )

Where  is my local llvm repository.

2. In  I added the file  containing:

        workspace( name = ""llvm"" )


Error log:

    bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
    ERROR: /git/tensorflow/tensorflow/tools/pip_package/BUILD:81:1: no such package '@llvm//': BUILD file not found on package path and referenced by '//tensorflow/tools/pip_package:licenses'.
    ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
    INFO: Elapsed time: 0.219s


Installed TensorFlow from source. Here is the version info:

    $ git rev-parse HEAD
    4c3bb1aeb7bb46bea35036433742a720f39ce348

    $ bazel version
    Build label: 0.4.5
    Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
    Build time: Thu Mar 16 12:19:38 2017 (1489666778)
    Build timestamp: 1489666778
    Build timestamp as int: 1489666778


Now, obviously a  file is required, I just need to know how and where to place it. Also, how can I build tools like Polly from the bazel build files?

Thanks in advance!

"
1098,23807,0,"Convert pb model to tflite model failed in docker environment. I m using the docker image pulled from here:
https://hub.docker.com/r/tensorflow/tensorflow/tags
the tag is nightly-devel-py3.
And then I upgrade the tf-nightly to  tf-nightly 1.13.0.dev20181116 using pip.

**System information**
- i7-8650U+16G+ 1060
- Windows Pro Docker Environment(CPU mode)
- tf-nightly 1.13.0.dev20181116
- Python version:

It failed when I trying to convert pb model to tflite model.
The error info says that some operators doesn't supported in the TFLiteConverter.
Can anyone help me to fix it?
Thanks.

**My Command Line**
tflite_convert --output_file='/data/fast-neural-style-train&test/models/result.tflite' --graph_def_file='/data/fast-neural-style-train&test/models/test_model.pb' --input_arrays=input --output_arrays=output_new --input_shapes=256,256,3

**Error Info**
2018-11-16 03:45:33.142433: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 421, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 417, in run_main
    _convert_model(tflite_flags)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 170, in _convert_model
    output_data = converter.convert()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/lite.py"", line 456, in convert
    **converter_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/convert.py"", line 397, in toco_convert_impl
    input_data.SerializeToString())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2018-11-16 03:45:34.088574: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: Round
2018-11-16 03:45:34.095390: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: Round
2018-11-16 03:45:34.095586: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095621: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095652: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095684: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095801: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095848: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2018-11-16 03:45:34.095925: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096229: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096281: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.096329: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096374: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.096455: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096501: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.096631: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096783: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.096957: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.097208: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.097403: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.097626: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.097862: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098174: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098459: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098733: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098940: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.098991: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.099099: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.099149: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.099194: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: MirrorPad
2018-11-16 03:45:34.099242: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: SquaredDifference
2018-11-16 03:45:34.100307: F tensorflow/lite/toco/tooling_util.cc:1020] Check failed: array->has_shape()
Aborted
"
1005,12492,0,"Feature request: manual parallel. when I run my code in that way:

    for i in range(num):
        with tf.control_dependencies(None):
            output[i] = tf.identity(deepnn(x))

TF will run the deepnn(x) one by one.
Would it be possible to parallel it manually like this:

    #pragma omp for"
402,28654,0,"NotFoundError: Resource worker/embedding_layer. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- GPU model and memory: TPU


**Describe the current behavior**
When i try to call fit on a character level LSTM based model I get following error 

**Describe the expected behavior**
If I run the exact same model on a GPU, everything works as expected (eg. the model trains).

**Code to reproduce the issue**
I am following the official google guides as to how train a keras model on tpu using the tensorflow contrib keras to tpu model function

**Other info / logs**

"
196,18297,1,"Tensorflow-gpu performance drop. ### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Windows 10 64-bit 
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.6.0
- **Python version**: 3.6.3
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GeForce GTX 780, 3Gb

Using Keras in Python with tensorflow-gpu backend. Worked fine for weeks until a few days ago, when I have suffered a huge performance drop.

When Tensorflow is initialising, it all appears to work correctly and finds my GPU as normal. Output:

    2018-04-05 02:08:32.791893: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1212] Found device 0 with properties: 
    name: GeForce GTX 780 major: 3 minor: 5 memoryClockRate(GHz): 1.0195
    pciBusID: 0000:01:00.0
    totalMemory: 3.00GiB freeMemory: 2.46GiB
    2018-04-05 02:08:32.792360: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0
    2018-04-05 02:08:33.132555: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2187 MB memory) -> physical GPU (device: 0, name: GeForce GTX 780, pci bus id: 0000:01:00.0, compute capability: 3.5)

But whereas before with exactly the same model on the same data, each epoch took about 2-3 seconds, they now take ~17 seconds.

I had a look in Task Manager, and my IDE shows to be using GPU Engine - ""GPU 0 - Copy"". Also at the beginning of each epoch, the GPU will go under ~70% load for about a second, but then the load switches over to my CPU and Memory for the remaining 15 seconds or so and the GPU goes back down to its idle load around 2%."
956,31013,0,"[TF 2.0 Docs] Include @tf.function in site/en/r2/tutorials/generative/cvae.ipynb. Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
site/en/r2/tutorials/generative/cvae.ipynb

## Description of issue (what needs changing): implement @tf.function decorators in the computation to improve performance, and highlight one of the tf 2.0 features.

### Clear description
When implementing in colab the performance improves from 30s / epoch average to 3.6s /epoch which is a huge benefit, and well worth highlighting/recommending by adding only 4 lines of code.

### Submit a pull request?
I can do that, yes

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
533,32801,0,"UpSampling2D doesn't support bfloat16. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): nightly
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
TypeError: Value passed to parameter 'images' has DataType bfloat16 not in list of allowed values: int8, uint8, int16, uint16, int32, int64, float16, float32, float64
**Describe the expected behavior**
support bfloat16
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1429,343,0,"failed installing using virtualenv. (tensorflow)itay@ubuntu:~/tensorflow$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
  Downloading tensorflow-0.5.0-cp27-none-linux_x86_64.whl (10.9Mb): 10.9Mb downloaded
  Running setup.py egg_info for package from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
    Traceback (most recent call last):
      File ""<string>"", line 14, in <module>
    IOError: [Errno 2] No such file or directory: '/tmp/pip-2LuRsH-build/setup.py'
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""<string>"", line 14, in <module>

IOError: [Errno 2] No such file or directory: '/tmp/pip-2LuRsH-build/setup.py'

---

Command python setup.py egg_info failed with error code 1 in /tmp/pip-2LuRsH-build

any help?
"
128,32520,1,"Training stuck on Allocation exceeds 10% of system memory. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.9.0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: running on CPU
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
The training is stuck after throwing the following warning: 
2019-09-13 23:25:57.221146: W tensorflow/core/framework/allocator.cc:108] Allocation of 603979776 exceeds 10% of system memory.
start training iteration: 0 elapse: 1568442370.016824
2019-09-13 23:26:10.230583: W tensorflow/core/framework/allocator.cc:108] Allocation of 2147483648 exceeds 10% of system memory.
2019-09-13 23:26:11.860365: W tensorflow/core/framework/allocator.cc:108] Allocation of 2147483648 exceeds 10% of system memory.

**Describe the expected behavior**
Expect the training to slowly progress given the memory is not big enough to hold the tensors then the swap should be able to help. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

MN_REDUCED.py
    
    import tensorflow as tf
    from tensorflow.python.framework import ops
    from tensorflow.python.framework import dtypes
    import random
    import numpy as np
      
    BATCH_SIZE = 128
    OUTPUT=4096
    NUM_CLASSES = OUTPUT
    
    class MN_REDUCED(object):
     
          def __init__(self, trainable=True, dropout=0.5):
              self.trainable = trainable
              self.dropout = dropout
              self.parameters = []
    
          def build(self,rgb,train_mode=None):
              with tf.name_scope('conv_1') as scope:
                  kernel = tf.Variable(tf.truncated_normal([3, 3, 3, OUTPUT], dtype=tf.float32,
                                          stddev=1e-2), name='weights')
                  conv = tf.nn.conv2d(rgb, kernel, [1, 1, 1, 1], padding='SAME')
                  biases = tf.Variable(tf.constant(0.0, shape=[OUTPUT], dtype=tf.float32),
                                           trainable=True, name='biases')
                  conv1 = tf.nn.bias_add(conv, biases)
               #   shape = int(np.prod(out.get_shape()))
               #   flat = tf.reshape(out, [BATCH_SIZE, -1])
               #   self.out_0 = flat[:,0:OUTPUT]
              with tf.name_scope('conv_2') as scope:
                  kernel = tf.Variable(tf.truncated_normal([3, 3, OUTPUT, OUTPUT], type=tf.float32,
                                          stddev=1e-2), name='weights')
                  conv = tf.nn.conv2d(conv1, kernel, [1, 1, 1, 1], padding='SAME')
                  biases = tf.Variable(tf.constant(0.0, shape=[OUTPUT], dtype=tf.float32),
                                           trainable=True, name='biases')
                  out = tf.nn.bias_add(conv, biases)
                  shape = int(np.prod(out.get_shape()))
                  flat = tf.reshape(out, [BATCH_SIZE, -1])
                  self.out_0 = flat[:,0:OUTPUT]
     
          def loss(self, labels):
              labels = tf.cast(labels, tf.int32)
              oneHot = tf.one_hot (labels, NUM_CLASSES)
              loss = tf.reduce_mean(tf.square(self.out_0 - oneHot), name='loss')
              return loss
     
          def training(self, loss):
              optimizer = tf.train.GradientDescentOptimizer(0.001)
              train_op = optimizer.minimize(loss);
              return train_op


main.py

     import tensorflow as tf
     import random
     import os
     from DataInput import DataInput
     #from CNN_FULL_CPU import CNN_FULL_CPU
     from MN_REDUCED import MN_REDUCED
     import pdb
     from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file
     import time
     import numpy as np
     
     dataset_path = ""./""
     train_labels_file = ""dataset.txt""
     
     IMAGE_HEIGHT = 32
     IMAGE_WIDTH = 32
     NUM_CHANNELS = 3
     BATCH_SIZE = 128
     NUM_ITERATIONS = 1000
     #NUM_ITERATIONS = 10
     LEARNING_RATE = 0.001
     SUMMARY_LOG_DIR=""./summary-log""
     lasttime = 0
   
       def placeholder_inputs(batch_size):
               images_placeholder = tf.placeholder(tf.float32,
                                                                       shape=(batch_size, IMAGE_HEIGHT,
                                                                                  IMAGE_WIDTH, NUM_CHANNELS))
               labels_placeholder = tf.placeholder(tf.int32,
                                                                       shape=(batch_size))
       
               return images_placeholder, labels_placeholder
   
     def fill_feed_dict(images_pl, labels_pl, sess):
               #images_feed, labels_feed = sess.run([data_input.example_batch, data_input.label_batch])
       
       
               #feed_dict = {
               #       images_pl: images_feed,
               #       labels_pl: labels_feed,
               #}
               n = BATCH_SIZE * IMAGE_WIDTH * IMAGE_HEIGHT * NUM_CHANNELS
               k = IMAGE_WIDTH
               a = np.empty(n, dtype=np.float32)
               np.random.seed(0)
       
           for i in range(0, n, k):
                   a[i:i+k] = np.random.normal(loc=0, scale=1, size=k)
               rand = np.reshape(a, (BATCH_SIZE, IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS))
       
       
               n1 = BATCH_SIZE
               a1 = np.empty(n1, dtype=np.float32)
               for i in range(0, n1):
                   a1[i] = np.random.normal(loc=0, scale=1)
       
       
               feed_dict = {
                       images_pl: rand,
                       labels_pl: a1,
               }
       
               return feed_dict
   
       def do_eval(sess,
                           eval_correct,
                           logits,
                           images_placeholder,
                           labels_placeholder,
                           dataset):
   
           true_count = 0
           # // is flooring division
           steps_per_epoch = dataset.num_examples // BATCH_SIZE
           num_examples = steps_per_epoch * BATCH_SIZE
   
           for step in xrange(steps_per_epoch):
                   #feed_dict = fill_feed_dict(dataset, images_placeholder,        labels_placeholder)
                   feed_dict = fill_feed_dict(dataset, images_placeholder, labels_placeholder,sess)
                   count = sess.run(eval_correct, feed_dict=feed_dict)
                   true_count = true_count + count
   
           precision = float(true_count) / num_examples
           print ('  Num examples: %d, Num correct: %d, Precision @ 1: %0.04f' %
                           (num_examples, true_count, precision))

    def placeholder_inputs(batch_size):
        images_placeholder = tf.placeholder(tf.float32,
        shape=(batch_size, IMAGE_HEIGHT,
        IMAGE_WIDTH, NUM_CHANNELS))
        labels_placeholder = tf.placeholder(tf.int32,
        shape=(batch_size))  
       def main():
               with tf.Graph().as_default():
   
                   #data_input = DataInput(dataset_path, train_labels_file, BATCH_SIZE)
                   images_placeholder, labels_placeholder = placeholder_inputs(BATCH_SIZE)
                   cnn_full_cpu = MN_REDUCED()
                   cnn_full_cpu.build(images_placeholder)
   
                   summary = tf.summary.merge_all()
                   saver = tf.train.Saver()
                   sess = tf.Session()
                   #sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
                   summary_writer = tf.summary.FileWriter(SUMMARY_LOG_DIR, sess.graph)
                   coord = tf.train.Coordinator()
                   threads = tf.train.start_queue_runners(sess=sess, coord=coord)
   
                   loss = cnn_full_cpu.loss(labels_placeholder)
                   train_op = cnn_full_cpu.training(loss)
   
                   init = tf.global_variables_initializer()
                   sess.run(init)
                   eval_correct = evaluation(cnn_full_cpu.out_0, labels_placeholder)
   
                   try:
                           for i in range(NUM_ITERATIONS):
                                   feed_dict = fill_feed_dict(images_placeholder,
                                                                   labels_placeholder, sess)
                                   _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)
   
                                   print ('Step %d: loss = %.6f' % (i, loss_value))
   
                           coord.request_stop()
                           coord.join(threads)
                   except Exception as e:
                           print(e)
                   infer = sess.run([cnn_full_cpu.out_0], feed_dict=feed_dict)
           sess.close().   


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I checked the memory utilization of the virtual machine. 
Although the memory is almost fully utilized, but there is still swap area. The program should run at a cost of performance. 
              total        used        free      shared  buff/cache   available
Mem:           3945        3712         178           0          54          79
Swap:          4092        2169        1923

In order to simulate a memory constrained environment, I setup a VirtualBox with 4GB memory and 4 GB swap size. Only one cpu core. 
"
1120,7615,0,"tfdbg Dump root directory does not exist. I run 
python -m tensorflow.python.debug.examples.debug_mnist --debug
on my laptop.which has windows 10 and gtx1070.but after I type first run,it return an exception like this:
Traceback (most recent call last):
  File ""d:\Anaconda3\lib\runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\examples\debug_mnist.py"", line 138, in <module>
    tf.app.run()
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\examples\debug_mnist.py"", line 131, in main
    acc = sess.run(accuracy, feed_dict=feed_dict(False))
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\wrappers\framework.py"", line 419, in run
    run_end_resp = self.on_run_end(run_end_req)
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\wrappers\local_cli_wrapper.py"", line 262, in on_run_end
    self._dump_root, partition_graphs=partition_graphs)
  File ""d:\Anaconda3\lib\site-packages\tensorflow\python\debug\debug_data.py"", line 328, in __init__
    raise IOError(""Dump root directory %s does not exist"" % dump_root)
OSError: Dump root directory C:\Users\tony8\AppData\Local\Temp\tfdbg_cvtcdnhl does not exist


how can I fix it?Thank you"
1415,33266,0,"Installation of tensorflow 2.0 cpu on windows 10 not working. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
Installation of tensorflow 2.0 cpu on windows 10 not working. I'm unable to import tensorflow.
**Describe the expected behavior**
Run the command ""python -c ""import tensorflow as tf"" within conda CLI without error.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
1) 
#from conda CLI base create new environment
conda create --name tensorflow2_test
2)
#activate environment
conda activate tensorflow2_test
3)
#install pip
conda install pip
4)
#install environment tensorflow 2.0
pip install tensorflow
5)
#verify install: 
python -c ""import tensorflow as tf""

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

error trace:
------------

package list:
-------------
(tensorflow2_test) C:\~\install environment\production>conda list
# packages in environment at C:\Anaconda3\envs\tensorflow2_test:
#
# Name                    Version                   Build  Channel
absl-py                   0.8.1                    pypi_0    pypi
astor                     0.8.0                    pypi_0    pypi
ca-certificates           2019.8.28                     0
certifi                   2019.9.11                py37_0
gast                      0.2.2                    pypi_0    pypi
google-pasta              0.1.7                    pypi_0    pypi
grpcio                    1.24.1                   pypi_0    pypi
h5py                      2.10.0                   pypi_0    pypi
keras-applications        1.0.8                    pypi_0    pypi
keras-preprocessing       1.1.0                    pypi_0    pypi
markdown                  3.1.1                    pypi_0    pypi
numpy                     1.17.2                   pypi_0    pypi
openssl                   1.1.1d               he774522_2
opt-einsum                3.1.0                    pypi_0    pypi
pip                       19.2.3                   py37_0
protobuf                  3.10.0                   pypi_0    pypi
python                    3.7.4                h5263a28_0
setuptools                41.4.0                   py37_0
six                       1.12.0                   pypi_0    pypi
sqlite                    3.30.0               he774522_0
tensorboard               2.0.0                    pypi_0    pypi
tensorflow                2.0.0                    pypi_0    pypi
tensorflow-estimator      2.0.0                    pypi_0    pypi
termcolor                 1.1.0                    pypi_0    pypi
vc                        14.1                 h0510ff6_4
vs2015_runtime            14.16.27012          hf0eaf9b_0
werkzeug                  0.16.0                   pypi_0    pypi
wheel                     0.33.6                   py37_0
wincertstore              0.2                      py37_0
wrapt                     1.11.2                   pypi_0    pypi

(tensorflow2_test) C:\~\install environment\production>
"
1157,3085,0,"TensorFlow r0.9 warning warns about Deprecation and recommends itself.. TensorFlow r0.9:


"
1267,20959,0,"Build tensorflow-model-server for gpu - Cannot find cuda library libcudnn.so.6. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ip-172-30-1-83 4.4.0-1062-aws #71-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609

- **CUDA/cuDNN version**:

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7

But I also have these files:
/usr/lib/x86_64-linux-gnu/libcudnn.so -> /etc/alternatives/libcudnn_so
/usr/lib/x86_64-linux-gnu/libcudnn.so.6 -> libcudnn.so.6.0.21
/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21
/usr/lib/x86_64-linux-gnu/libcudnn.so.7 -> libcudnn.so.7.0.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5

- **GPU model and memory**: Tesla K80

### Describe the problem

I try to build tensorflow-model-server with gpu support, as I saw that the apt-get version is only for CPU.

I did:
1. git clone --recurse-submodules https://github.com/tensorflow/serving
2. bazel clean --expunge && export TF_NEED_CUDA=1
3. bazel query 'kind(rule, @local_config_cuda//...)'

And got:

Cuda Configuration Error: Cannot find cuda library libcudnn.so.6

When I do: bazel build -c opt --config=cuda tensorflow_serving/model_servers:tensorflow_model_server
I get the same error.
"
147,23846,1,"Distributed TensorFlow hangs at the end of one iteration when using DataSet api. Training epoch by epoch, the program will hangs on some random nodes forever at the end of one iteration, namely the end of one epoch. It appears certainly when using  api, while it is fine when using old  based data api. I have set . @mrry 

Env:
  Distributed running on Yarn.
  Node information: CentOS, linux kernel 3.10.0. Only CPU.
  TensorFlow: r1.12, built from source.

Demo(It's independent of the model. It's easier to reproduce by using more worker nodes. Here, I use 50 worker nodes and 20 ps nodes.)


Fully pstack: 
  https://github.com/formath/TensorFlow-Bugs/blob/master/39024.pstack

"
814,32002,0,"Tensorflow Lite compute library for hardware acceleration . Does Tensorflow lite has their own compute library for hardware(arm cpu, gpu, fpga, etc.) acceleration? If they have it, where is it located at? which directory? Thanks!"
1018,31439,0,"TensorFlow 1.14 eager execution not working with tf.function. **System information**
- OS Platform and Distribution: macOS Mojave
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: Python 3.7.3
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I use  to get the prediction in a function. But with the  declared before the function, there will raise an AttributeError: 'Tensor' object has no attribute 'numpy'.

**Code to reproduce the issue**
Similar to [this tutorial](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention), but with  inside the  to print the prediction.
"
490,32960,0,"Keras Nadam optmizer generates error when using MirroredStrategy. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18
- TensorFlow installed from (source or binary): Tensorflow 2
- TensorFlow version (use command below) : Tensorflow 2.0.0
- Python version: Python 3.7
- CUDA/cuDNN version: 10
- GPU model and memory: 2 x Nvidia 1080 TI

**Describe the current behavior**

The training crashes with an error( ValueError: You must specify an aggregation method to update a MirroredVariable in Replica Context.) If the model is compiled with the optimizer Nadam (tf.keras.optimizers.Nadam) along with a MirroredStrategy.

**Describe the expected behavior**
 Expect to be able to train with any optimizer from Keras' options.

"
1217,33591,0,"Person_detection.zip not present after update in reference to issue  #33552 and PR #33579 . This is issue is made in reference to Issue #33552 and the PR #33579:
when I click on the person_detection.zip link after update this pops out:
 
"
1378,5983,0,"[Windows] TensorBoard doesn't work. I installed the tensorflow for windows with

(I added the '--ignore-installed', for the official one didn't work on my pc)

Then I tried some code and ran tensorboard, however the browser shows nothing 
When running with '--debug' there appears some 404.
I also tried tensorboard on mac os, it worked well, so the problem is not caused by the .tfevent file
I've read through a bunch of related issues, and found that maybe the pip install didn't install tensorflow completely. And I searched the tensorflow file and did not found some certain files appears in mac os tensorflow, such as 'paper-toolkit'

"
142,15643,1,"fp16 inference is slower than fp32 on Nvidia Jetson TX2. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: Cuda 8.0 and cuDNN 7.0
- **GPU model and memory**: Jetson TX2

### The problem
I created a fully convolutional float 16 (half precision) neural network in tensorflow. When I run this network with some inputs, the inference time is slower than when I run the same network in float 32 (full precision) mode. 
I should also note that the following variables are set:



As Nvidia Jetson TX2 support FP16 operations, I expected an inference time not worse than when I use FP32, but surprisingly it is about 1.5 times worse! (36 miliseconds vs 22 miliseconds). I guess it is becuase of the overhead of internal type conversion in the tensorflow core between float16 and float32!

Is it a problem with Tensorflow or TX2?"
1258,496,0,"tensorflow/core/user_kernels needed. correct me if I am wrong, but I believe a subdir of tensorflow/core/user_kernels is needed to support user written GPU kernels (CPU is fine), this is due to the build rules in tensorflow/core/BUILD when building the user_ops_op_lib is a cc_library rather than say tf_cuda_library.
"
1196,15425,0,"TypeError: broadcast() takes 1 positional argument but 2 were given. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**: b'v1.3.0-rc1-6044-g0b80606' 1.4.0    (2 days ago)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Source code / logs
I ran , and got the following error.


The reason is that the invocation of  is different from its signature:
https://github.com/tensorflow/tensorflow/blob/17e725c0558581cba19bd6c409698b2c3f88efe5/tensorflow/contrib/all_reduce/python/all_reduce.py#L748
https://github.com/tensorflow/tensorflow/blob/17e725c0558581cba19bd6c409698b2c3f88efe5/tensorflow/contrib/nccl/python/ops/nccl_ops.py#L173-L182 
Problems still exists in current HEAD."
1052,11183,0,"ConnectionError: HTTPSConnectionPool(host='storage.googleapis.com', port=443): Max retries exceeded with url: /tensorflow/linux/gpu/tensorflow_gpu-1.2.1-cp27-none-linux_x86_64.whl (Caused by <class 'socket.error'>: [Errno 101] Network is unreachable). I have downloaded and removed Tensorflow a few times in order to try and fix a problems with my numpy package. I now get the error above when I try and download tensorflow. Is there a max amount of times I can download it, is that why I am getting this error? 

Here is the traceback: 
Exception:
Traceback (most recent call last):
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/commands/install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/req.py"", line 1197, in prepare_files
    do_download,
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/req.py"", line 1375, in unpack_url
    self.session,
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/download.py"", line 546, in unpack_http_url
    resp = session.get(target_url, stream=True)
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 395, in get
    return self.request('GET', url, **kwargs)
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/download.py"", line 237, in request
    return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 383, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 486, in send
    r = adapter.send(request, **kwargs)
  File ""/home/slkapur/tensorflow/local/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py"", line 378, in send
    raise ConnectionError(e)
ConnectionError: HTTPSConnectionPool(host='storage.googleapis.com', port=443): Max retries exceeded with url: /tensorflow/linux/cpu/tensorflow-1.2.1-cp27-none-linux_x86_64.whl (Caused by <class 'socket.error'>: [Errno 101] Network is unreachable)
 
Storing debug log for failure in /tmp/tmp2du06v
"
555,33802,0,"Pull Requests: Map needed for request workflow. At the moment external contributors don't have any way of knowing what the lifecycle of a pull request is. We have [documentation on contributing code](https://www.tensorflow.org/community/contribute/code), but it's silent on the details of what happens to a request once it's been submitted. Chandni presented a fantastic flowchart at the contributor's summit today that would be a great foundation for documentation explaining the stages that a PR goes through.

This would help external contributors understand what they need to do to successfully submit code to the project, and combined with https://github.com/tensorflow/tensorflow/issues/33801 will give them the visibility they need to be effective TensorFlow developers.

/cc @freddan80 @jenselofsson "
638,33827,0,"lowerbound not implemented (from searchsorted). **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): pip3
- TensorFlow version (or github SHA if from source): tensorflow           2.0.0


**Provide the text output from tflite_convert**


"
1253,22892,0,"How to parse the caffe-generated LMDB datasets with tf.LMDBReader. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
PC
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.9.0
- **Python version**:
2.7.15
- **Bazel version (if compiling from source)**:
Not compiled from source
- **GCC/Compiler version (if compiling from source)**:
Not compiled from source
- **CUDA/cuDNN version**:
CUDA 9.0
- **GPU model and memory**:
GTX 1060 6G

### Describe the problem
After importation through the tf.LMDBReader(), we are able to get the key and value of the .mdb. But we cannot further parse the value through the tf.parse_single_example API as no sub-keys like ""image_raw"" or ""label"" existed in the Caffe-generated .mdb dataset. As in Caffe, the properties of the image or the label data are available with this data block converted to Caffe-defined Datum structure and we can get the width through methods like Datum.width(). So what we should do in tensorflow?

### Source code / logs
filename_queue = tf.train.string_input_producer(['path_to_the_data.mdb'], num_epochs=None)
reader = tf.LMDBReader()
cur_key_val, serialized_example = reader.read(filename_queue)
features = tf.parse_single_example(
        serialized_example,
        features={
             '?????': tf.VarLenFeature(tf.string),
             '?????': tf.VarLenFeature(tf.string)
         })
label = features['?????']
image = features['?????']"
997,26464,0,"segment erro:tensorflow/tensorflow/examples/label_image/main.cc. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):r1.13
- Python version:3.6
- Bazel version (if compiling from source):0.23
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0/7.0
- GPU model and memory:1080ti/11gb


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
tensorflow/tensorflow/examples/label_image/main.cc
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
i think the reason is ""TF_RETURN_IF_ERROR(env->NewRandomAccessFile(filename, &file));""
https://github.com/tensorflow/tensorflow/blob/5fad2f50d610fdb765f4f99c7e0531b7a3ddbe94/tensorflow/examples/label_image/main.cc#L100"
840,24431,0,"No TensorFlow-GPU install from pip (seems to be only CPU version). Hello,
I am coming back to you for help. I am installing a DevBox under Ubuntu 18.04.1 LTS with 4 RTX 2080Ti Nvidia cards.
I have performed multiple Linux/Drivers installation without success. 
There is no GPU use by Keras and fore sure no multi_GPU_model calculation. It seems that despite of the pip install TensorFlow-gpu install, no gpu install is performed. 
At this time, the Nvidia drivers are 410.78 with CUDA 10.0.
The Nvidia-smi function shows me the good data : all the 4 GPUs, Drivers version, Cuda Revision
I am using Anaconda Python 3.6.5.
TensorFlow : 1.12.0 with GPU version
Keras : TensorFlow Backend 2.2.4

Additionally, is there anybody who has experienced starting issue with Ubuntu 18.04.1 LTS and Nvidia driver such as no starting ?
Thank you for your help
Best Regards
Edouard

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem."
0,8704,1,"TensorFlow drops the first batch?. ### Description

I'm trying to understand how TF generates batches from  file format, how to implement the basic idea of evaluate the whole validation dataset after a full epoch so I've done a small experiment.

Basically I've added an  key in the  file that looks like this:


The values of these ids are just numbers increasing from 0 to . Then I'm reading the data like this:



I've used ,  and  to ensure we are reading the instances in the validation set in order.

I've then defined a  that evaluates a mini-batch from the validation set after each step. I'm doing this to check and test we are not returning instances with duplicate ids in the same batch. The code looks like this:



Most of the output I'm seeing makes sense but the output from the first batch looks weird.



I'm assuming the first mini-batch returned should be  instead of . Plus the fact that we are starting from 17 instead of 16 also bothers me a bit. It seems for some unknown reason TF is dropping a batch.

I understand this probably won't affect the model training / validation process at all but I'm still pointing this out in case this is hiding a more serious root cause."
1431,7227,0,"ld: file not found: -lcudart.8.0 when building CUDA on MacOS and bazel 0.4.4. In today's head https://github.com/tensorflow/tensorflow/commit/b47dc70e548e6958919f87864b83866919473a92, and bazel 0.4.4, bazel building with XLA + CUDA on MacOS fails because it can't find cuda library when linking. I was able to build with the same setup from head from Jan 24 .



I see  in my  and similar issue [suggests](http://stackoverflow.com/questions/9633881/usr-bin-ld-cannot-find-lcudart) that linking needs to be done with -L$CUDA_HOME/lib flag, but I can't figure out how to massage that option into bazel config

I also tried building with flags  and copying  to 

Note that libcudart dependency dependency exists in CPU-only build as well because of https://github.com/tensorflow/tensorflow/issues/7216"
123,14075,1,"Momentum SGD is very slow with large embedding layer. Hi, I have an (256 col* 500000row) embedding layer in my model and I found that Momentum SGD is 10x slower than adam optimizer. When I printed the global variables using API :tf.global_variables(), I found a large Momentum  variable created by optimizer, does that means Momentum SGD have not  supported sparse update yet?"
1129,6021,0,"Linker error when building custom op. I was not getting this error a couple of weeks ago, but now when I repull it/update the repository I am getting an error when running configure.

git clone https://github.com/tensorflow/tensorflow.git
git pull
git status
    

Then I run ./configure:

Error:


Overall, I am trying to implement a new operator using tensorflow. I am able to easily import tensorflow, but when I try to build the new operator (following the tutorial on https://www.tensorflow.org/versions/r0.12/how_tos/adding_an_op/index.html), i get:



Thoughts? I have tried checking out the other branches as well and trying them, but no luck.

My pip freeze looks like:





"
572,27977,0,"increase throughput for `ResizeBilinearKernel`.  in [cuda file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc) there are 3 major changes for improvement:

1.  is  which makes reads and writes to global memory uncoalesced. By changing the input and output to , we can increase the overall speed by a few folds
2. instead of spawning each thread for every element, spawn only as many threads as , and have each thread iterate over  and  dimension. this has two benefits: it increases throughput roughly 30%, and it allows smaller kernel launches which allows more number of concurrent kernels to be present on the device.
3. use  modifier for both input and output. This alone increase throughput 2~4%

The above improvements is something that can be applied to general kernels that perform element-wise operation to a vector. I have a working code of  with increased throughput which I can push in a couple days as soon as I get a few uncertainties straightened out. I would really appreciate your help.

Thanks :)
"
727,33600,0,"AttributeError: module 'tensorflow.python.keras' has no attribute 'Model'. Running code from object Detection
not a custom code
I am using Window 10
Tensorflow 1.4 CPU
I am getting this error, while running below code.
i want to rectify the error, without updating Tensorflow,

import tensorflow as tf

BOX_ENCODINGS = 'box_encodings'
CLASS_PREDICTIONS_WITH_BACKGROUND = 'class_predictions_with_background'
MASK_PREDICTIONS = 'mask_predictions'


class BoxPredictor(object):
  """"""BoxPredictor.""""""

  def __init__(self, is_training, num_classes):
    """"""Constructor.

    Args:
      is_training: Indicates whether the BoxPredictor is in training mode.
      num_classes: number of classes.  Note that num_classes *does not*
        include the background category, so if groundtruth labels take values
        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
        assigned classification targets can range from {0,... K}).
    """"""
    self._is_training = is_training
    self._num_classes = num_classes

  @property
  def is_keras_model(self):
    return False

  @property
  def num_classes(self):
    return self._num_classes

  def predict(self, image_features, num_predictions_per_location,
              scope=None, **params):
    """"""Computes encoded object locations and corresponding confidences.

    Takes a list of high level image feature maps as input and produces a list
    of box encodings and a list of class scores where each element in the output
    lists correspond to the feature maps in the input list.

    Args:
      image_features: A list of float tensors of shape [batch_size, height_i,
      width_i, channels_i] containing features for a batch of images.
      num_predictions_per_location: A list of integers representing the number
        of box predictions to be made per spatial location for each feature map.
      scope: Variable and Op scope name.
      **params: Additional keyword arguments for specific implementations of
              BoxPredictor.

    Returns:
      A dictionary containing at least the following tensors.
        box_encodings: A list of float tensors. Each entry in the list
          corresponds to a feature map in the input  list. All
          tensors in the list have one of the two following shapes:
          a. [batch_size, num_anchors_i, q, code_size] representing the location
            of the objects, where q is 1 or the number of classes.
          b. [batch_size, num_anchors_i, code_size].
        class_predictions_with_background: A list of float tensors of shape
          [batch_size, num_anchors_i, num_classes + 1] representing the class
          predictions for the proposals. Each entry in the list corresponds to a
          feature map in the input  list.

    Raises:
      ValueError: If length of  is not equal to length of
        .
    """"""
    if len(image_features) != len(num_predictions_per_location):
      raise ValueError('image_feature and num_predictions_per_location must '
                       'be of same length, found: {} vs {}'.
                       format(len(image_features),
                              len(num_predictions_per_location)))
    if scope is not None:
      with tf.variable_scope(scope):
        return self._predict(image_features, num_predictions_per_location,
                             **params)
    return self._predict(image_features, num_predictions_per_location,
                         **params)

  # TODO(rathodv): num_predictions_per_location could be moved to constructor.
  # This is currently only used by ConvolutionalBoxPredictor.
  @abstractmethod
  def _predict(self, image_features, num_predictions_per_location, **params):
    """"""Implementations must override this method.

    Args:
      image_features: A list of float tensors of shape [batch_size, height_i,
        width_i, channels_i] containing features for a batch of images.
      num_predictions_per_location: A list of integers representing the number
        of box predictions to be made per spatial location for each feature map.
      **params: Additional keyword arguments for specific implementations of
              BoxPredictor.

    Returns:
      A dictionary containing at least the following tensors.
        box_encodings: A list of float tensors. Each entry in the list
          corresponds to a feature map in the input  list. All
          tensors in the list have one of the two following shapes:
          a. [batch_size, num_anchors_i, q, code_size] representing the location
            of the objects, where q is 1 or the number of classes.
          b. [batch_size, num_anchors_i, code_size].
        class_predictions_with_background: A list of float tensors of shape
          [batch_size, num_anchors_i, num_classes + 1] representing the class
          predictions for the proposals. Each entry in the list corresponds to a
          feature map in the input  list.
    """"""
    pass


class KerasBoxPredictor(tf.keras.Model):
  """"""Keras-based BoxPredictor.""""""

  def __init__(self, is_training, num_classes, freeze_batchnorm,
               inplace_batchnorm_update, name=None):
    """"""Constructor.

    Args:
      is_training: Indicates whether the BoxPredictor is in training mode.
      num_classes: number of classes.  Note that num_classes *does not*
        include the background category, so if groundtruth labels take values
        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
        assigned classification targets can range from {0,... K}).
      freeze_batchnorm: Whether to freeze batch norm parameters during
        training or not. When training with a small batch size (e.g. 1), it is
        desirable to freeze batch norm update and use pretrained batch norm
        params.
      inplace_batchnorm_update: Whether to update batch norm moving average
        values inplace. When this is false train op must add a control
        dependency on tf.graphkeys.UPDATE_OPS collection in order to update
        batch norm statistics.
      name: A string name scope to assign to the model. If , Keras
        will auto-generate one from the class name.
    """"""
    super(KerasBoxPredictor, self).__init__(name=name)

    self._is_training = is_training
    self._num_classes = num_classes
    self._freeze_batchnorm = freeze_batchnorm
    self._inplace_batchnorm_update = inplace_batchnorm_update

  @property
  def is_keras_model(self):
    return True

  @property
  def num_classes(self):
    return self._num_classes

  def call(self, image_features, **kwargs):
    """"""Computes encoded object locations and corresponding confidences.

    Takes a list of high level image feature maps as input and produces a list
    of box encodings and a list of class scores where each element in the output
    lists correspond to the feature maps in the input list.

    Args:
      image_features: A list of float tensors of shape [batch_size, height_i,
      width_i, channels_i] containing features for a batch of images.
      **kwargs: Additional keyword arguments for specific implementations of
            BoxPredictor.

    Returns:
      A dictionary containing at least the following tensors.
        box_encodings: A list of float tensors. Each entry in the list
          corresponds to a feature map in the input  list. All
          tensors in the list have one of the two following shapes:
          a. [batch_size, num_anchors_i, q, code_size] representing the location
            of the objects, where q is 1 or the number of classes.
          b. [batch_size, num_anchors_i, code_size].
        class_predictions_with_background: A list of float tensors of shape
          [batch_size, num_anchors_i, num_classes + 1] representing the class
          predictions for the proposals. Each entry in the list corresponds to a
          feature map in the input  list.
    """"""
    return self._predict(image_features, **kwargs)

  @abstractmethod
  def _predict(self, image_features, **kwargs):
    """"""Implementations must override this method.

    Args:
      image_features: A list of float tensors of shape [batch_size, height_i,
        width_i, channels_i] containing features for a batch of images.
      **kwargs: Additional keyword arguments for specific implementations of
              BoxPredictor.

    Returns:
      A dictionary containing at least the following tensors.
        box_encodings: A list of float tensors. Each entry in the list
          corresponds to a feature map in the input  list. All
          tensors in the list have one of the two following shapes:
          a. [batch_size, num_anchors_i, q, code_size] representing the location
            of the objects, where q is 1 or the number of classes.
          b. [batch_size, num_anchors_i, code_size].
        class_predictions_with_background: A list of float tensors of shape
          [batch_size, num_anchors_i, num_classes + 1] representing the class
          predictions for the proposals. Each entry in the list corresponds to a
          feature map in the input  list.
    """"""
    raise NotImplementedError

"
1442,13855,0,"compile source code fail on mac. ### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS Sierra 10.12.6
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**: 
1.3.0
- **Python version**: 
Python 2.7.10
- **Bazel version (if compiling from source)**:
macbookpro:tensorflow fredlee$ bazel version
Build label: 0.7.0-homebrew
Build target: bazel-out/darwin_x86_64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Oct 19 09:12:48 2017 (1508404368)
Build timestamp: 1508404368
Build timestamp as int: 1508404368
- **CUDA/cuDNN version**:
no
- **GPU model and memory**:
no 
- **Exact command to reproduce**:


step by step according to this guide
   step1:
       git clone https://github.com/tensorflow/tensorflow
  step2:
      cd tensorflow
      ./configure
  step3:
     create an example as (https://tensorflow.google.cn/api_guides/cc/guide)
  step4:
    bazel run -c opt //tensorflow/cc/example:example

  it return
      

> ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/cc/example:example failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3083.675s, Critical Path: 306.19s
ERROR: Build failed. Not running target.

compile log:
[compile.log.zip](https://github.com/tensorflow/tensorflow/files/1401503/compile.log.zip)


****can you tell me how to make it work?** thanks**

**best wishes.**
"
320,1279,0,"Arch doesn't support it. After  or as root, I got:


It seems it doesn't work with Python 3. What should I do?
"
609,21530,0,"[Bug] Broken Combination: Non-SGD Optimizer, tf.Variable(), and Estimator Framework. The following combination is broken when simultaneously used:
* Optimizer other than SGD
* Estimator framework
* using  rather than 
The bug might be more specific, since I am basing this directly off of the full setup found in the official resnet example in 

I have made a 3-line modification to the official resnet example illustrate problem:
https://github.com/tensorflow/models/compare/master...liuyipei:BUG_NonSGDOptimizer_Variable_Estimator

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I can illustrate the bug with minimal change to an official example.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
centos-release-7-4.1708.el7.centos.x86_64
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**:
3.4.5
- **CUDA/cuDNN version**:
CUDA 9, cuDNN 7
- **Bazel version**:
N/A
- **Mobile device**:
N/A
- **GPU model and memory**:
Nvidia GTX1080
- **Exact command to reproduce**:



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I changed 3 lines inside an officially supported model to illustrate my point.
https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py
I added a  variable. If I use  directly, an error happens at training time.


The error is:


I have the following additional anecdotal information:
* The bug does not manifest when  is used, presumably because it doesn't involve momentum updates.
* The bug does not manifest itself when I declare the variable using 
* I did some more digging (work not shown), but it seems to have something to do with the dtype of the object created directly by  having dtype  after coming through  inside . This is unlike the other variables, which have  in that context. Maybe there is some kind of confusion in type checking."
339,21939,0,"Tensorflow 1.10.1 incompatible with latest numpy version 1.15.1 . Installed TF with following command . The installation is successful but gave me this error.


Do I need to downgrade my numpy version?"
245,7624,1,"tensorflow codes totally different result with PyTorch's. Hi, I'm new to tensorflow and trying to refactor one of my project originally in (Py)Torch. However, though the two codes has the same network, the same loss function, the same optimizer parameters, the result is totally different. Is any one have any idea about such issue?

Thanks."
305,18087,0,"is the sytem wrong leading tensorboard No dashboards are active for the current data set.. 

### Describe the problem

 i fisrt input tensorboard --logdir='logs',then i enter ctrl+c, it shows:
![image](https://user-images.githubusercontent.com/3324257/38097184-45b93700-33a7-11e8-884d-72aaab629ce9.png)
is my system wrong?


"
1060,17456,0,"TensorForestEstimator with TensorFlow v1.4 ""No attribute model_fn"". Setting up a TensorForestEstimator on CloudML running TensorFlow version 1.4. 
The setup is identical to a working DNNClassifier estimator (of course, swapping the parameters within the Random Forest), however, an error occurs when the TensorForestEstimator has its model_fn called.
This seems to be a bug, since the model_fn attribute is defined during initialization [https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/tensor_forest/client/random_forest.py](url)

Here's the relevant parts of the code:



Below is the error traceback I get from the CloudML runner.

> Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/RF_Model/RF_task_tf14.py"", line 201, in <module> run_experiment(hparams) File ""/root/.local/lib/python2.7/site-packages/RF_Model/RF_task_tf14.py"", line 88, in run_experiment eval_spec) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 464, in train_and_evaluate getattr(executor, task_to_run)() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 562, in run_master self._start_distributed_training(saving_listeners=saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 687, in _start_distributed_training saving_listeners=saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train loss = self._train_model(input_fn, hooks, saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model features, labels, model_fn_lib.ModeKeys.TRAIN, self.config) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File ""/root/.local/lib/python2.7/site-packages/RF_Model/model1.py"", line 286, in _model_fn features=features, labels=labels, mode=mode, config=estimator.config) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/extenders.py"", line 222, in new_model_fn spec = estimator.model_fn(features, labels, mode, config) AttributeError: 'TensorForestEstimator' object has no attribute 'model_fn'

I'm attempting to use tf.contrib.estimator.forward_features to past a list of keys through to the prediction output (this is reflected in the traceback when new_model_fn tries to call the TensorForestEstimator's ""model_fn attribute"") [https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/estimator/python/estimator/extenders.py](url)

It's also probably worth mentioning that the model_fn call within contrib.estimator.forward_features passes 4 parameters, whereas the model function defined within TensorForestEstimator only accepts 3 (see line 392 in the first link). 

-OS Platform: (N/A - CloudML (likely Debian workers?))
-Tensorflow running v1.4
-Relevant code mentioned above.

"
1041,23364,0,"How to read particular text from scanned or image file using python with DL library. How to read particular text (like empno,name,salary,etx) from scanned or image file and store in db using python with any one deep learning library (like tensorflow, pytorch,etc)

pls. provide some sample code and which one we can use for this concepts"
1351,8720,0,"split not working correctly. 

NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Windows

Installed version of CUDA and cuDNN: 
(please attach the output of ):
See #2 below

If installed from binary pip package, provide:

1. A link to the pip package you installed:
How do I get this?
2. The output from .

(C:\Users\television2\Anaconda3) C:\cygwin64\home\television2\nn\hierarchy>python -c ""import tensorflow; print(tensorflow.__version__)""
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally
1.0.0

"
1074,11805,0,"Cannot build Tensorflow: 'GLIBCXX_3.4.21' not found. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 8.0/5.0
- **GPU model and memory**: GTX1080, 8G
- **Exact command to reproduce**: bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package


### Describe the problem

I cannot build Tensorflow from source. I have gcc libraries installed in ""/opt/ohpc/pub/compiler/gcc/5.4.0/lib64"". But the installer was not able to pick it up. It always tries to use the one in ""/usr/lib64"". I have set ""LDFLAGS"" ""LD_LIBRARY_PATH"", etc. Nothing works.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

$ bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

WARNING: ignoring http_proxy in environment.
WARNING: Output base '/home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3' is on NFS. This may lead to surprising failures and undetermined behavior.
WARNING: /home/kai/test/tensorflow-1.2.1/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.
WARNING: /home/kai/test/tensorflow-1.2.1/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.
INFO: Found 1 target...
ERROR: /home/kai/test/tensorflow-1.2.1/tensorflow/contrib/training/BUILD:339:1: null failed: protoc failed: error executing command
  (cd /home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3/execroot/org_tensorflow && \
  exec env - \
  bazel-out/host/bin/external/protobuf/protoc '--python_out=bazel-out/local_linux-py3-opt/genfiles/' -I. -Iexternal/protobuf/python -Ibazel-out/local_linux-py3-opt/genfiles/external/protobuf/python tensorflow/contrib/training/python/training/hparam.proto): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
/home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3/execroot/org_tensorflow/_bin/process-wrapper: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/kai/.cache/bazel/_bazel_kai/e5757f5d9b24da3fc563b551b579ddc3/execroot/org_tensorflow/_bin/process-wrapper)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3.922s, Critical Path: 0.03s"
1382,12378,0,"ssd_mobilenets model faild using in android demo. I am glad to see android demo is updated to be available for ssd_mobilenets models. I tried the demo on my phone, and it works well. But when I tried my own model, the app can not work normally. 
1.I used   in object_detection to convert .ckpt to .pb files
2.I replaced the original model by my .pb file
3. Also, I changed the label_list.txt. 
After doing this, I generate the app, but the app crushed. Is there anything wrong with my handle? "
1315,22367,0,"a problem about using the tflite_convert. ### System information
- **Have I written custom code**:  NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NO
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu==1.9
- **TensorFlow version (use command below)**: 1.9 GPU
- **Python version**: python 27
- **Bazel version (if compiling from source)**: NO
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: CUDA9.0 cuDNN 7.0
- **GPU model and memory**: GTX1070 8G

### Describe the problem
   I want to conver the inception_v3 from .ckpt to .tflite, but I meet a problem when I conver the .pb to .tflite,    I use the following method

 1)  download the inception_v3_2016_08_28.tar.gz from  https://github.com/tensorflow/models/tree/master/research/slim
decompression it and move to /tmp

2)  Export nception_v3_inf_graph.pb file that without parameters



3)  freezon the pb file



4)  quantized pb file



above steps all successed, but

5)  .pb-->. tflite


### Source code


### Source  logs

2018-09-19 10:54:02.077094: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-09-19 10:54:02.163219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-19 10:54:02.163667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.77GiB
2018-09-19 10:54:02.163682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-09-19 10:54:02.346515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-19 10:54:02.346548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-09-19 10:54:02.346555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-09-19 10:54:02.346742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7502 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/icare/.local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 320, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 316, in run_main
    _convert_model(tflite_flags)
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 121, in _convert_model
    output_data = converter.convert()
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 309, in convert
    allow_custom_ops=self.allow_custom_ops)
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 225, in toco_convert
    input_data.SerializeToString())
  File ""/home/icare/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 107, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
2018-09-19 10:54:03.713531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.713589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.713603: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.713612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.713620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.713657: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.713668: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.713687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.713711: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.713719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.713741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.713787: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.713797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.713829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.713841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.713862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.713884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.713929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.713939: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.713962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedMaxPool
2018-09-19 10:54:03.713980: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714003: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714012: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714108: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714173: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714193: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedMaxPool
2018-09-19 10:54:03.714212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714275: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714286: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714334: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714343: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714431: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714440: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714448: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714581: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714643: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714681: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714692: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714798: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.714827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714839: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714900: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.714909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.714941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.714953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.714964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.714973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.714981: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715015: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715061: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715073: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715084: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715093: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715192: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715235: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715254: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715293: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715304: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715321: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715353: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715399: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715408: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715417: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715451: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715500: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715508: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715517: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715562: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.715620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715631: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715640: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715705: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715750: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715769: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715879: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715888: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.715929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.715940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.715965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.715977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.715985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.715994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716027: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716074: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716085: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716192: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716242: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716253: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716299: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716379: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.716410: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716422: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716430: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716483: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.716901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.716910: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.716918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.716955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.716966: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.716974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717016: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717027: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717036: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717044: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717087: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717122: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717140: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717176: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717242: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717286: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717324: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedMaxPool
2018-09-19 10:54:03.717335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717381: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717405: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717414: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717422: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717512: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717524: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717535: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717587: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717648: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717690: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717735: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717748: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717814: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717823: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717883: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.717901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.717934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.717945: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.717975: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.717987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.717995: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718039: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718100: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718109: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718185: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718309: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718321: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718373: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718392: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718429: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.718465: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718477: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718540: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718549: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718674: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718747: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718758: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.718887: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.718923: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.718933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.718974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.718987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.718996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719040: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719050: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719059: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719125: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719134: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719143: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719291: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719369: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719405: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719416: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719465: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719483: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719529: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719592: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719637: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719648: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719677: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719688: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.719724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719754: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719809: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.719881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.719890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.719899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.719947: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.719955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.719996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720009: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720073: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720150: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720198: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720239: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720251: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720305: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720316: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720388: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720397: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720500: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720509: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720564: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720623: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720631: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720678: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720715: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720780: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720831: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.720852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.720861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720908: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.720917: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.720937: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.720948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.720983: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.720996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721005: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721014: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721050: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721061: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721142: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721151: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721205: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721214: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721289: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721334: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721344: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721467: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721515: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721528: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721537: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721602: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721646: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721659: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721670: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721679: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721723: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721800: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721855: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.721899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.721912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.721921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.721930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.721966: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.721977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722021: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722167: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722223: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.722298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722310: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722319: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722365: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722467: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722520: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722611: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722623: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722632: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722680: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722690: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722754: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722765: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.722887: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.722896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.722905: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.722943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.722953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.722998: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723065: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723076: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723128: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723140: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723149: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723287: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedMaxPool
2018-09-19 10:54:03.723308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723382: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723415: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723423: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723460: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723480: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723581: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723598: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723784: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.723872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.723882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.723890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.723928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.723938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.723948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.724042: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.724055: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.724066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.724075: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.724084: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.724121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.724132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.724701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.724719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.724728: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.724737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.724775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.724786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.724851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.724864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.724873: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.724881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.724918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.724928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.724937: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725013: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725030: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725087: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725124: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.725171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725192: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725248: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725382: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725399: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725446: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725455: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725562: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725576: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725587: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725596: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725604: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725643: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725730: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725748: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725796: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725805: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.725882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.725891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.725899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.725936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.725947: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.725955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.726322: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.726337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.726348: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.726356: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.726364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.726408: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.726420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.726984: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.727001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.727010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.727019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727069: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.727132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.727145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.727154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.727163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727200: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727211: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.727219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727282: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.727294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.727303: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.727312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727348: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727359: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.727367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727393: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.727464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.727476: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.727484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.727493: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727529: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedRelu
2018-09-19 10:54:03.727548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.727574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.727585: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedAvgPool
2018-09-19 10:54:03.728329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedConv2D
2018-09-19 10:54:03.728346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.728356: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.728374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedBiasAdd
2018-09-19 10:54:03.728384: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RequantizationRange
2018-09-19 10:54:03.728393: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Requantize
2018-09-19 10:54:03.728402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.728429: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.728439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedReshape
2018-09-19 10:54:03.728448: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.728475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizeV2
2018-09-19 10:54:03.728485: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: QuantizedReshape
2018-09-19 10:54:03.728494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize
2018-09-19 10:54:03.794554: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1479 operators, 3498 arrays (0 quantized)
2018-09-19 10:54:03.893920: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1479 operators, 3498 arrays (0 quantized)
2018-09-19 10:54:04.023420: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1292 operators, 3014 arrays (1 quantized)
2018-09-19 10:54:04.147459: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 1292 operators, 3014 arrays (1 quantized)
2018-09-19 10:54:04.224980: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 1292 operators, 3014 arrays (1 quantized)
2018-09-19 10:54:04.322794: F tensorflow/contrib/lite/toco/tooling_util.cc:1589] Array InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D_eightbit_min_input, which is an input to the (Unsupported TensorFlow op: QuantizeV2) operator producing the output array InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D_eightbit_quantize_input, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.
Aborted (core dumped)

None



I also try
### Source code





### Source log

usage: tflite_convert [-h] --output_file OUTPUT_FILE
                      (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR)
                      [--output_format {TFLITE,GRAPHVIZ_DOT}]
                      [--inference_type {FLOAT,QUANTIZED_UINT8}]
                      [--inference_input_type {FLOAT,QUANTIZED_UINT8}]
                      [--input_arrays INPUT_ARRAYS]
                      [--input_shapes INPUT_SHAPES]
                      [--output_arrays OUTPUT_ARRAYS]
                      [--saved_model_tag_set SAVED_MODEL_TAG_SET]
                      [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
                      [--std_dev_values STD_DEV_VALUES]
                      [--mean_values MEAN_VALUES]
                      [--default_ranges_min DEFAULT_RANGES_MIN]
                      [--default_ranges_max DEFAULT_RANGES_MAX]
                      [--drop_control_dependency DROP_CONTROL_DEPENDENCY]
                      [--reorder_across_fake_quant REORDER_ACROSS_FAKE_QUANT]
                      [--change_concat_input_ranges CHANGE_CONCAT_INPUT_RANGES]
                      [--allow_custom_ops ALLOW_CUSTOM_OPS]
tflite_convert: error: --default_ranges_min and --default_ranges_max must be used together


How should I solve the problem ?
"
973,32982,0,"TF2 Stubs for intellisense; current way tf2 imports modules does not support intellisense. **System information**
- TensorFlow version (you are using): 2.0.0
- Are you willing to contribute it (Yes/No): Happy to help out

**Describe the feature and the current behavior/state.**
Intellisense does not work because of the way tf2 has import statements.

![image](https://user-images.githubusercontent.com/9828683/66023599-f7f83180-e4f1-11e9-83d0-fe931c4486f6.png)


**Will this change the current api? How?**

Add stubs.

**Who will benefit with this feature?**

Vscode users

**Any Other info.**

Discussed here: 
https://github.com/microsoft/python-language-server/issues/818#issuecomment-537143000"
502,34039,0,"tf.data.Dataset fixed size batching with subsequent map() under tf.distribute.MirroredStrategy leads to a crash. **System information**
The same environment as in https://github.com/tensorflow/tensorflow/issues/33531

**Code to reproduce the issue**

It took me a few **weeks** of debugging to reproduce! **IMPORTANT: I DO NOT THINK IT WILL REPRODUCE IN COLAB, YOU NEED AT LEAST 2 GPUS.**



As you see, I am feeding a  pipeline to a Keras model under . In my case, there are 4 GPUs. Here is the log which indicates a crash:

<details>
<summary>Full log</summary>
<pre>
2019-11-06 11:09:37.077575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3
2019-11-06 11:09:37.077858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutorwith strength 1 edge matrix:
2019-11-06 11:09:37.077880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 2 3
2019-11-06 11:09:37.077894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y N N
2019-11-06 11:09:37.077904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N N N
2019-11-06 11:09:37.077914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 2:   N N N Y
2019-11-06 11:09:37.077923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 3:   N N Y N
2019-11-06 11:09:37.084775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2019-11-06 11:09:37.086075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:1 with 10470 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2019-11-06 11:09:37.087140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:2 with 10470 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)
2019-11-06 11:09:37.088126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:3 with 10470 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)
Model: ""densenet121""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(3, 372, 558, 3)]   0
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (3, 378, 564, 3)     0           input_1[0][0]
__________________________________________________________________________________________________
conv1/conv (Conv2D)             (3, 186, 279, 64)    9408        zero_padding2d[0][0]
__________________________________________________________________________________________________
conv1/bn (BatchNormalization)   (3, 186, 279, 64)    256         conv1/conv[0][0]
__________________________________________________________________________________________________
conv1/relu (Activation)         (3, 186, 279, 64)    0           conv1/bn[0][0]
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (3, 188, 281, 64)    0           conv1/relu[0][0]
__________________________________________________________________________________________________
pool1 (MaxPooling2D)            (3, 93, 140, 64)     0           zero_padding2d_1[0][0]
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (3, 93, 140, 64)     256         pool1[0][0]
__________________________________________________________________________________________________
conv2_block1_0_relu (Activation (3, 93, 140, 64)     0           conv2_block1_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (3, 93, 140, 128)    8192        conv2_block1_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (3, 93, 140, 128)    512         conv2_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (3, 93, 140, 128)    0           conv2_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (3, 93, 140, 32)     36864       conv2_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block1_concat (Concatenat (3, 93, 140, 96)     0           pool1[0][0]
                                                                 conv2_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_0_bn (BatchNormali (3, 93, 140, 96)     384         conv2_block1_concat[0][0]
__________________________________________________________________________________________________
conv2_block2_0_relu (Activation (3, 93, 140, 96)     0           conv2_block2_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (3, 93, 140, 128)    12288       conv2_block2_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (3, 93, 140, 128)    512         conv2_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (3, 93, 140, 128)    0           conv2_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (3, 93, 140, 32)     36864       conv2_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block2_concat (Concatenat (3, 93, 140, 128)    0           conv2_block1_concat[0][0]
                                                                 conv2_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_0_bn (BatchNormali (3, 93, 140, 128)    512         conv2_block2_concat[0][0]
__________________________________________________________________________________________________
conv2_block3_0_relu (Activation (3, 93, 140, 128)    0           conv2_block3_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (3, 93, 140, 128)    16384       conv2_block3_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (3, 93, 140, 128)    512         conv2_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (3, 93, 140, 128)    0           conv2_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (3, 93, 140, 32)     36864       conv2_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block3_concat (Concatenat (3, 93, 140, 160)    0           conv2_block2_concat[0][0]
                                                                 conv2_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block4_0_bn (BatchNormali (3, 93, 140, 160)    640         conv2_block3_concat[0][0]
__________________________________________________________________________________________________
conv2_block4_0_relu (Activation (3, 93, 140, 160)    0           conv2_block4_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block4_1_conv (Conv2D)    (3, 93, 140, 128)    20480       conv2_block4_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block4_1_bn (BatchNormali (3, 93, 140, 128)    512         conv2_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block4_1_relu (Activation (3, 93, 140, 128)    0           conv2_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block4_2_conv (Conv2D)    (3, 93, 140, 32)     36864       conv2_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block4_concat (Concatenat (3, 93, 140, 192)    0           conv2_block3_concat[0][0]
                                                                 conv2_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block5_0_bn (BatchNormali (3, 93, 140, 192)    768         conv2_block4_concat[0][0]
__________________________________________________________________________________________________
conv2_block5_0_relu (Activation (3, 93, 140, 192)    0           conv2_block5_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block5_1_conv (Conv2D)    (3, 93, 140, 128)    24576       conv2_block5_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block5_1_bn (BatchNormali (3, 93, 140, 128)    512         conv2_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block5_1_relu (Activation (3, 93, 140, 128)    0           conv2_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block5_2_conv (Conv2D)    (3, 93, 140, 32)     36864       conv2_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block5_concat (Concatenat (3, 93, 140, 224)    0           conv2_block4_concat[0][0]
                                                                 conv2_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block6_0_bn (BatchNormali (3, 93, 140, 224)    896         conv2_block5_concat[0][0]
__________________________________________________________________________________________________
conv2_block6_0_relu (Activation (3, 93, 140, 224)    0           conv2_block6_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block6_1_conv (Conv2D)    (3, 93, 140, 128)    28672       conv2_block6_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block6_1_bn (BatchNormali (3, 93, 140, 128)    512         conv2_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block6_1_relu (Activation (3, 93, 140, 128)    0           conv2_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block6_2_conv (Conv2D)    (3, 93, 140, 32)     36864       conv2_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block6_concat (Concatenat (3, 93, 140, 256)    0           conv2_block5_concat[0][0]
                                                                 conv2_block6_2_conv[0][0]
__________________________________________________________________________________________________
pool2_bn (BatchNormalization)   (3, 93, 140, 256)    1024        conv2_block6_concat[0][0]
__________________________________________________________________________________________________
pool2_relu (Activation)         (3, 93, 140, 256)    0           pool2_bn[0][0]
__________________________________________________________________________________________________
pool2_conv (Conv2D)             (3, 93, 140, 128)    32768       pool2_relu[0][0]
__________________________________________________________________________________________________
pool2_pool (AveragePooling2D)   (3, 46, 70, 128)     0           pool2_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (3, 46, 70, 128)     512         pool2_pool[0][0]
__________________________________________________________________________________________________
conv3_block1_0_relu (Activation (3, 46, 70, 128)     0           conv3_block1_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (3, 46, 70, 128)     16384       conv3_block1_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (3, 46, 70, 128)     512         conv3_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (3, 46, 70, 128)     0           conv3_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (3, 46, 70, 32)      36864       conv3_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block1_concat (Concatenat (3, 46, 70, 160)     0           pool2_pool[0][0]
                                                                 conv3_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_0_bn (BatchNormali (3, 46, 70, 160)     640         conv3_block1_concat[0][0]
__________________________________________________________________________________________________
conv3_block2_0_relu (Activation (3, 46, 70, 160)     0           conv3_block2_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (3, 46, 70, 128)     20480       conv3_block2_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (3, 46, 70, 128)     512         conv3_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (3, 46, 70, 128)     0           conv3_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (3, 46, 70, 32)      36864       conv3_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block2_concat (Concatenat (3, 46, 70, 192)     0           conv3_block1_concat[0][0]
                                                                 conv3_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_0_bn (BatchNormali (3, 46, 70, 192)     768         conv3_block2_concat[0][0]
__________________________________________________________________________________________________
conv3_block3_0_relu (Activation (3, 46, 70, 192)     0           conv3_block3_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (3, 46, 70, 128)     24576       conv3_block3_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (3, 46, 70, 128)     512         conv3_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (3, 46, 70, 128)     0           conv3_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (3, 46, 70, 32)      36864       conv3_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block3_concat (Concatenat (3, 46, 70, 224)     0           conv3_block2_concat[0][0]
                                                                 conv3_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_0_bn (BatchNormali (3, 46, 70, 224)     896         conv3_block3_concat[0][0]
__________________________________________________________________________________________________
conv3_block4_0_relu (Activation (3, 46, 70, 224)     0           conv3_block4_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (3, 46, 70, 128)     28672       conv3_block4_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (3, 46, 70, 128)     512         conv3_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (3, 46, 70, 128)     0           conv3_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (3, 46, 70, 32)      36864       conv3_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block4_concat (Concatenat (3, 46, 70, 256)     0           conv3_block3_concat[0][0]
                                                                 conv3_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block5_0_bn (BatchNormali (3, 46, 70, 256)     1024        conv3_block4_concat[0][0]
__________________________________________________________________________________________________
conv3_block5_0_relu (Activation (3, 46, 70, 256)     0           conv3_block5_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block5_1_conv (Conv2D)    (3, 46, 70, 128)     32768       conv3_block5_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block5_1_bn (BatchNormali (3, 46, 70, 128)     512         conv3_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block5_1_relu (Activation (3, 46, 70, 128)     0           conv3_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block5_2_conv (Conv2D)    (3, 46, 70, 32)      36864       conv3_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block5_concat (Concatenat (3, 46, 70, 288)     0           conv3_block4_concat[0][0]
                                                                 conv3_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block6_0_bn (BatchNormali (3, 46, 70, 288)     1152        conv3_block5_concat[0][0]
__________________________________________________________________________________________________
conv3_block6_0_relu (Activation (3, 46, 70, 288)     0           conv3_block6_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block6_1_conv (Conv2D)    (3, 46, 70, 128)     36864       conv3_block6_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block6_1_bn (BatchNormali (3, 46, 70, 128)     512         conv3_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block6_1_relu (Activation (3, 46, 70, 128)     0           conv3_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block6_2_conv (Conv2D)    (3, 46, 70, 32)      36864       conv3_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block6_concat (Concatenat (3, 46, 70, 320)     0           conv3_block5_concat[0][0]
                                                                 conv3_block6_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block7_0_bn (BatchNormali (3, 46, 70, 320)     1280        conv3_block6_concat[0][0]
__________________________________________________________________________________________________
conv3_block7_0_relu (Activation (3, 46, 70, 320)     0           conv3_block7_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block7_1_conv (Conv2D)    (3, 46, 70, 128)     40960       conv3_block7_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block7_1_bn (BatchNormali (3, 46, 70, 128)     512         conv3_block7_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block7_1_relu (Activation (3, 46, 70, 128)     0           conv3_block7_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block7_2_conv (Conv2D)    (3, 46, 70, 32)      36864       conv3_block7_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block7_concat (Concatenat (3, 46, 70, 352)     0           conv3_block6_concat[0][0]
                                                                 conv3_block7_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block8_0_bn (BatchNormali (3, 46, 70, 352)     1408        conv3_block7_concat[0][0]
__________________________________________________________________________________________________
conv3_block8_0_relu (Activation (3, 46, 70, 352)     0           conv3_block8_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block8_1_conv (Conv2D)    (3, 46, 70, 128)     45056       conv3_block8_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block8_1_bn (BatchNormali (3, 46, 70, 128)     512         conv3_block8_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block8_1_relu (Activation (3, 46, 70, 128)     0           conv3_block8_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block8_2_conv (Conv2D)    (3, 46, 70, 32)      36864       conv3_block8_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block8_concat (Concatenat (3, 46, 70, 384)     0           conv3_block7_concat[0][0]
                                                                 conv3_block8_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block9_0_bn (BatchNormali (3, 46, 70, 384)     1536        conv3_block8_concat[0][0]
__________________________________________________________________________________________________
conv3_block9_0_relu (Activation (3, 46, 70, 384)     0           conv3_block9_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block9_1_conv (Conv2D)    (3, 46, 70, 128)     49152       conv3_block9_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block9_1_bn (BatchNormali (3, 46, 70, 128)     512         conv3_block9_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block9_1_relu (Activation (3, 46, 70, 128)     0           conv3_block9_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block9_2_conv (Conv2D)    (3, 46, 70, 32)      36864       conv3_block9_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block9_concat (Concatenat (3, 46, 70, 416)     0           conv3_block8_concat[0][0]
                                                                 conv3_block9_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block10_0_bn (BatchNormal (3, 46, 70, 416)     1664        conv3_block9_concat[0][0]
__________________________________________________________________________________________________
conv3_block10_0_relu (Activatio (3, 46, 70, 416)     0           conv3_block10_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block10_1_conv (Conv2D)   (3, 46, 70, 128)     53248       conv3_block10_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block10_1_bn (BatchNormal (3, 46, 70, 128)     512         conv3_block10_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block10_1_relu (Activatio (3, 46, 70, 128)     0           conv3_block10_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block10_2_conv (Conv2D)   (3, 46, 70, 32)      36864       conv3_block10_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block10_concat (Concatena (3, 46, 70, 448)     0           conv3_block9_concat[0][0]
                                                                 conv3_block10_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block11_0_bn (BatchNormal (3, 46, 70, 448)     1792        conv3_block10_concat[0][0]
__________________________________________________________________________________________________
conv3_block11_0_relu (Activatio (3, 46, 70, 448)     0           conv3_block11_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block11_1_conv (Conv2D)   (3, 46, 70, 128)     57344       conv3_block11_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block11_1_bn (BatchNormal (3, 46, 70, 128)     512         conv3_block11_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block11_1_relu (Activatio (3, 46, 70, 128)     0           conv3_block11_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block11_2_conv (Conv2D)   (3, 46, 70, 32)      36864       conv3_block11_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block11_concat (Concatena (3, 46, 70, 480)     0           conv3_block10_concat[0][0]
                                                                 conv3_block11_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block12_0_bn (BatchNormal (3, 46, 70, 480)     1920        conv3_block11_concat[0][0]
__________________________________________________________________________________________________
conv3_block12_0_relu (Activatio (3, 46, 70, 480)     0           conv3_block12_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block12_1_conv (Conv2D)   (3, 46, 70, 128)     61440       conv3_block12_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block12_1_bn (BatchNormal (3, 46, 70, 128)     512         conv3_block12_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block12_1_relu (Activatio (3, 46, 70, 128)     0           conv3_block12_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block12_2_conv (Conv2D)   (3, 46, 70, 32)      36864       conv3_block12_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block12_concat (Concatena (3, 46, 70, 512)     0           conv3_block11_concat[0][0]
                                                                 conv3_block12_2_conv[0][0]
__________________________________________________________________________________________________
pool3_bn (BatchNormalization)   (3, 46, 70, 512)     2048        conv3_block12_concat[0][0]
__________________________________________________________________________________________________
pool3_relu (Activation)         (3, 46, 70, 512)     0           pool3_bn[0][0]
__________________________________________________________________________________________________
pool3_conv (Conv2D)             (3, 46, 70, 256)     131072      pool3_relu[0][0]
__________________________________________________________________________________________________
pool3_pool (AveragePooling2D)   (3, 23, 35, 256)     0           pool3_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (3, 23, 35, 256)     1024        pool3_pool[0][0]
__________________________________________________________________________________________________
conv4_block1_0_relu (Activation (3, 23, 35, 256)     0           conv4_block1_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (3, 23, 35, 128)     32768       conv4_block1_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (3, 23, 35, 128)     512         conv4_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (3, 23, 35, 128)     0           conv4_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (3, 23, 35, 32)      36864       conv4_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block1_concat (Concatenat (3, 23, 35, 288)     0           pool3_pool[0][0]
                                                                 conv4_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_0_bn (BatchNormali (3, 23, 35, 288)     1152        conv4_block1_concat[0][0]
__________________________________________________________________________________________________
conv4_block2_0_relu (Activation (3, 23, 35, 288)     0           conv4_block2_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (3, 23, 35, 128)     36864       conv4_block2_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (3, 23, 35, 128)     512         conv4_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (3, 23, 35, 128)     0           conv4_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (3, 23, 35, 32)      36864       conv4_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block2_concat (Concatenat (3, 23, 35, 320)     0           conv4_block1_concat[0][0]
                                                                 conv4_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_0_bn (BatchNormali (3, 23, 35, 320)     1280        conv4_block2_concat[0][0]
__________________________________________________________________________________________________
conv4_block3_0_relu (Activation (3, 23, 35, 320)     0           conv4_block3_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (3, 23, 35, 128)     40960       conv4_block3_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (3, 23, 35, 128)     512         conv4_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (3, 23, 35, 128)     0           conv4_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (3, 23, 35, 32)      36864       conv4_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block3_concat (Concatenat (3, 23, 35, 352)     0           conv4_block2_concat[0][0]
                                                                 conv4_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_0_bn (BatchNormali (3, 23, 35, 352)     1408        conv4_block3_concat[0][0]
__________________________________________________________________________________________________
conv4_block4_0_relu (Activation (3, 23, 35, 352)     0           conv4_block4_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (3, 23, 35, 128)     45056       conv4_block4_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (3, 23, 35, 128)     512         conv4_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (3, 23, 35, 128)     0           conv4_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (3, 23, 35, 32)      36864       conv4_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block4_concat (Concatenat (3, 23, 35, 384)     0           conv4_block3_concat[0][0]
                                                                 conv4_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_0_bn (BatchNormali (3, 23, 35, 384)     1536        conv4_block4_concat[0][0]
__________________________________________________________________________________________________
conv4_block5_0_relu (Activation (3, 23, 35, 384)     0           conv4_block5_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (3, 23, 35, 128)     49152       conv4_block5_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (3, 23, 35, 128)     512         conv4_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (3, 23, 35, 128)     0           conv4_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (3, 23, 35, 32)      36864       conv4_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block5_concat (Concatenat (3, 23, 35, 416)     0           conv4_block4_concat[0][0]
                                                                 conv4_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_0_bn (BatchNormali (3, 23, 35, 416)     1664        conv4_block5_concat[0][0]
__________________________________________________________________________________________________
conv4_block6_0_relu (Activation (3, 23, 35, 416)     0           conv4_block6_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (3, 23, 35, 128)     53248       conv4_block6_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (3, 23, 35, 128)     512         conv4_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (3, 23, 35, 128)     0           conv4_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (3, 23, 35, 32)      36864       conv4_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block6_concat (Concatenat (3, 23, 35, 448)     0           conv4_block5_concat[0][0]
                                                                 conv4_block6_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block7_0_bn (BatchNormali (3, 23, 35, 448)     1792        conv4_block6_concat[0][0]
__________________________________________________________________________________________________
conv4_block7_0_relu (Activation (3, 23, 35, 448)     0           conv4_block7_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block7_1_conv (Conv2D)    (3, 23, 35, 128)     57344       conv4_block7_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block7_1_bn (BatchNormali (3, 23, 35, 128)     512         conv4_block7_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block7_1_relu (Activation (3, 23, 35, 128)     0           conv4_block7_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block7_2_conv (Conv2D)    (3, 23, 35, 32)      36864       conv4_block7_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block7_concat (Concatenat (3, 23, 35, 480)     0           conv4_block6_concat[0][0]
                                                                 conv4_block7_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block8_0_bn (BatchNormali (3, 23, 35, 480)     1920        conv4_block7_concat[0][0]
__________________________________________________________________________________________________
conv4_block8_0_relu (Activation (3, 23, 35, 480)     0           conv4_block8_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block8_1_conv (Conv2D)    (3, 23, 35, 128)     61440       conv4_block8_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block8_1_bn (BatchNormali (3, 23, 35, 128)     512         conv4_block8_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block8_1_relu (Activation (3, 23, 35, 128)     0           conv4_block8_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block8_2_conv (Conv2D)    (3, 23, 35, 32)      36864       conv4_block8_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block8_concat (Concatenat (3, 23, 35, 512)     0           conv4_block7_concat[0][0]
                                                                 conv4_block8_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block9_0_bn (BatchNormali (3, 23, 35, 512)     2048        conv4_block8_concat[0][0]
__________________________________________________________________________________________________
conv4_block9_0_relu (Activation (3, 23, 35, 512)     0           conv4_block9_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block9_1_conv (Conv2D)    (3, 23, 35, 128)     65536       conv4_block9_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block9_1_bn (BatchNormali (3, 23, 35, 128)     512         conv4_block9_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block9_1_relu (Activation (3, 23, 35, 128)     0           conv4_block9_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block9_2_conv (Conv2D)    (3, 23, 35, 32)      36864       conv4_block9_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block9_concat (Concatenat (3, 23, 35, 544)     0           conv4_block8_concat[0][0]
                                                                 conv4_block9_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block10_0_bn (BatchNormal (3, 23, 35, 544)     2176        conv4_block9_concat[0][0]
__________________________________________________________________________________________________
conv4_block10_0_relu (Activatio (3, 23, 35, 544)     0           conv4_block10_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block10_1_conv (Conv2D)   (3, 23, 35, 128)     69632       conv4_block10_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block10_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block10_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block10_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block10_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block10_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block10_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block10_concat (Concatena (3, 23, 35, 576)     0           conv4_block9_concat[0][0]
                                                                 conv4_block10_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block11_0_bn (BatchNormal (3, 23, 35, 576)     2304        conv4_block10_concat[0][0]
__________________________________________________________________________________________________
conv4_block11_0_relu (Activatio (3, 23, 35, 576)     0           conv4_block11_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block11_1_conv (Conv2D)   (3, 23, 35, 128)     73728       conv4_block11_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block11_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block11_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block11_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block11_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block11_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block11_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block11_concat (Concatena (3, 23, 35, 608)     0           conv4_block10_concat[0][0]
                                                                 conv4_block11_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block12_0_bn (BatchNormal (3, 23, 35, 608)     2432        conv4_block11_concat[0][0]
__________________________________________________________________________________________________
conv4_block12_0_relu (Activatio (3, 23, 35, 608)     0           conv4_block12_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block12_1_conv (Conv2D)   (3, 23, 35, 128)     77824       conv4_block12_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block12_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block12_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block12_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block12_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block12_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block12_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block12_concat (Concatena (3, 23, 35, 640)     0           conv4_block11_concat[0][0]
                                                                 conv4_block12_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block13_0_bn (BatchNormal (3, 23, 35, 640)     2560        conv4_block12_concat[0][0]
__________________________________________________________________________________________________
conv4_block13_0_relu (Activatio (3, 23, 35, 640)     0           conv4_block13_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block13_1_conv (Conv2D)   (3, 23, 35, 128)     81920       conv4_block13_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block13_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block13_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block13_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block13_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block13_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block13_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block13_concat (Concatena (3, 23, 35, 672)     0           conv4_block12_concat[0][0]
                                                                 conv4_block13_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block14_0_bn (BatchNormal (3, 23, 35, 672)     2688        conv4_block13_concat[0][0]
__________________________________________________________________________________________________
conv4_block14_0_relu (Activatio (3, 23, 35, 672)     0           conv4_block14_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block14_1_conv (Conv2D)   (3, 23, 35, 128)     86016       conv4_block14_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block14_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block14_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block14_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block14_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block14_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block14_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block14_concat (Concatena (3, 23, 35, 704)     0           conv4_block13_concat[0][0]
                                                                 conv4_block14_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block15_0_bn (BatchNormal (3, 23, 35, 704)     2816        conv4_block14_concat[0][0]
__________________________________________________________________________________________________
conv4_block15_0_relu (Activatio (3, 23, 35, 704)     0           conv4_block15_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block15_1_conv (Conv2D)   (3, 23, 35, 128)     90112       conv4_block15_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block15_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block15_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block15_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block15_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block15_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block15_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block15_concat (Concatena (3, 23, 35, 736)     0           conv4_block14_concat[0][0]
                                                                 conv4_block15_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block16_0_bn (BatchNormal (3, 23, 35, 736)     2944        conv4_block15_concat[0][0]
__________________________________________________________________________________________________
conv4_block16_0_relu (Activatio (3, 23, 35, 736)     0           conv4_block16_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block16_1_conv (Conv2D)   (3, 23, 35, 128)     94208       conv4_block16_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block16_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block16_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block16_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block16_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block16_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block16_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block16_concat (Concatena (3, 23, 35, 768)     0           conv4_block15_concat[0][0]
                                                                 conv4_block16_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block17_0_bn (BatchNormal (3, 23, 35, 768)     3072        conv4_block16_concat[0][0]
__________________________________________________________________________________________________
conv4_block17_0_relu (Activatio (3, 23, 35, 768)     0           conv4_block17_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block17_1_conv (Conv2D)   (3, 23, 35, 128)     98304       conv4_block17_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block17_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block17_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block17_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block17_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block17_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block17_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block17_concat (Concatena (3, 23, 35, 800)     0           conv4_block16_concat[0][0]
                                                                 conv4_block17_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block18_0_bn (BatchNormal (3, 23, 35, 800)     3200        conv4_block17_concat[0][0]
__________________________________________________________________________________________________
conv4_block18_0_relu (Activatio (3, 23, 35, 800)     0           conv4_block18_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block18_1_conv (Conv2D)   (3, 23, 35, 128)     102400      conv4_block18_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block18_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block18_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block18_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block18_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block18_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block18_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block18_concat (Concatena (3, 23, 35, 832)     0           conv4_block17_concat[0][0]
                                                                 conv4_block18_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block19_0_bn (BatchNormal (3, 23, 35, 832)     3328        conv4_block18_concat[0][0]
__________________________________________________________________________________________________
conv4_block19_0_relu (Activatio (3, 23, 35, 832)     0           conv4_block19_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block19_1_conv (Conv2D)   (3, 23, 35, 128)     106496      conv4_block19_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block19_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block19_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block19_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block19_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block19_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block19_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block19_concat (Concatena (3, 23, 35, 864)     0           conv4_block18_concat[0][0]
                                                                 conv4_block19_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block20_0_bn (BatchNormal (3, 23, 35, 864)     3456        conv4_block19_concat[0][0]
__________________________________________________________________________________________________
conv4_block20_0_relu (Activatio (3, 23, 35, 864)     0           conv4_block20_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block20_1_conv (Conv2D)   (3, 23, 35, 128)     110592      conv4_block20_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block20_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block20_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block20_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block20_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block20_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block20_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block20_concat (Concatena (3, 23, 35, 896)     0           conv4_block19_concat[0][0]
                                                                 conv4_block20_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block21_0_bn (BatchNormal (3, 23, 35, 896)     3584        conv4_block20_concat[0][0]
__________________________________________________________________________________________________
conv4_block21_0_relu (Activatio (3, 23, 35, 896)     0           conv4_block21_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block21_1_conv (Conv2D)   (3, 23, 35, 128)     114688      conv4_block21_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block21_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block21_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block21_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block21_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block21_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block21_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block21_concat (Concatena (3, 23, 35, 928)     0           conv4_block20_concat[0][0]
                                                                 conv4_block21_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block22_0_bn (BatchNormal (3, 23, 35, 928)     3712        conv4_block21_concat[0][0]
__________________________________________________________________________________________________
conv4_block22_0_relu (Activatio (3, 23, 35, 928)     0           conv4_block22_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block22_1_conv (Conv2D)   (3, 23, 35, 128)     118784      conv4_block22_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block22_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block22_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block22_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block22_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block22_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block22_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block22_concat (Concatena (3, 23, 35, 960)     0           conv4_block21_concat[0][0]
                                                                 conv4_block22_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block23_0_bn (BatchNormal (3, 23, 35, 960)     3840        conv4_block22_concat[0][0]
__________________________________________________________________________________________________
conv4_block23_0_relu (Activatio (3, 23, 35, 960)     0           conv4_block23_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block23_1_conv (Conv2D)   (3, 23, 35, 128)     122880      conv4_block23_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block23_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block23_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block23_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block23_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block23_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block23_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block23_concat (Concatena (3, 23, 35, 992)     0           conv4_block22_concat[0][0]
                                                                 conv4_block23_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block24_0_bn (BatchNormal (3, 23, 35, 992)     3968        conv4_block23_concat[0][0]
__________________________________________________________________________________________________
conv4_block24_0_relu (Activatio (3, 23, 35, 992)     0           conv4_block24_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block24_1_conv (Conv2D)   (3, 23, 35, 128)     126976      conv4_block24_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block24_1_bn (BatchNormal (3, 23, 35, 128)     512         conv4_block24_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block24_1_relu (Activatio (3, 23, 35, 128)     0           conv4_block24_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block24_2_conv (Conv2D)   (3, 23, 35, 32)      36864       conv4_block24_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block24_concat (Concatena (3, 23, 35, 1024)    0           conv4_block23_concat[0][0]
                                                                 conv4_block24_2_conv[0][0]
__________________________________________________________________________________________________
pool4_bn (BatchNormalization)   (3, 23, 35, 1024)    4096        conv4_block24_concat[0][0]
__________________________________________________________________________________________________
pool4_relu (Activation)         (3, 23, 35, 1024)    0           pool4_bn[0][0]
__________________________________________________________________________________________________
pool4_conv (Conv2D)             (3, 23, 35, 512)     524288      pool4_relu[0][0]
__________________________________________________________________________________________________
pool4_pool (AveragePooling2D)   (3, 11, 17, 512)     0           pool4_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (3, 11, 17, 512)     2048        pool4_pool[0][0]
__________________________________________________________________________________________________
conv5_block1_0_relu (Activation (3, 11, 17, 512)     0           conv5_block1_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (3, 11, 17, 128)     65536       conv5_block1_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (3, 11, 17, 128)     512         conv5_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (3, 11, 17, 128)     0           conv5_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (3, 11, 17, 32)      36864       conv5_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block1_concat (Concatenat (3, 11, 17, 544)     0           pool4_pool[0][0]
                                                                 conv5_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_0_bn (BatchNormali (3, 11, 17, 544)     2176        conv5_block1_concat[0][0]
__________________________________________________________________________________________________
conv5_block2_0_relu (Activation (3, 11, 17, 544)     0           conv5_block2_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (3, 11, 17, 128)     69632       conv5_block2_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (3, 11, 17, 128)     512         conv5_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (3, 11, 17, 128)     0           conv5_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (3, 11, 17, 32)      36864       conv5_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block2_concat (Concatenat (3, 11, 17, 576)     0           conv5_block1_concat[0][0]
                                                                 conv5_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_0_bn (BatchNormali (3, 11, 17, 576)     2304        conv5_block2_concat[0][0]
__________________________________________________________________________________________________
conv5_block3_0_relu (Activation (3, 11, 17, 576)     0           conv5_block3_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (3, 11, 17, 128)     73728       conv5_block3_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (3, 11, 17, 128)     512         conv5_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (3, 11, 17, 128)     0           conv5_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (3, 11, 17, 32)      36864       conv5_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block3_concat (Concatenat (3, 11, 17, 608)     0           conv5_block2_concat[0][0]
                                                                 conv5_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block4_0_bn (BatchNormali (3, 11, 17, 608)     2432        conv5_block3_concat[0][0]
__________________________________________________________________________________________________
conv5_block4_0_relu (Activation (3, 11, 17, 608)     0           conv5_block4_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block4_1_conv (Conv2D)    (3, 11, 17, 128)     77824       conv5_block4_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block4_1_bn (BatchNormali (3, 11, 17, 128)     512         conv5_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block4_1_relu (Activation (3, 11, 17, 128)     0           conv5_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block4_2_conv (Conv2D)    (3, 11, 17, 32)      36864       conv5_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block4_concat (Concatenat (3, 11, 17, 640)     0           conv5_block3_concat[0][0]
                                                                 conv5_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block5_0_bn (BatchNormali (3, 11, 17, 640)     2560        conv5_block4_concat[0][0]
__________________________________________________________________________________________________
conv5_block5_0_relu (Activation (3, 11, 17, 640)     0           conv5_block5_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block5_1_conv (Conv2D)    (3, 11, 17, 128)     81920       conv5_block5_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block5_1_bn (BatchNormali (3, 11, 17, 128)     512         conv5_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block5_1_relu (Activation (3, 11, 17, 128)     0           conv5_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block5_2_conv (Conv2D)    (3, 11, 17, 32)      36864       conv5_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block5_concat (Concatenat (3, 11, 17, 672)     0           conv5_block4_concat[0][0]
                                                                 conv5_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block6_0_bn (BatchNormali (3, 11, 17, 672)     2688        conv5_block5_concat[0][0]
__________________________________________________________________________________________________
conv5_block6_0_relu (Activation (3, 11, 17, 672)     0           conv5_block6_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block6_1_conv (Conv2D)    (3, 11, 17, 128)     86016       conv5_block6_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block6_1_bn (BatchNormali (3, 11, 17, 128)     512         conv5_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block6_1_relu (Activation (3, 11, 17, 128)     0           conv5_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block6_2_conv (Conv2D)    (3, 11, 17, 32)      36864       conv5_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block6_concat (Concatenat (3, 11, 17, 704)     0           conv5_block5_concat[0][0]
                                                                 conv5_block6_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block7_0_bn (BatchNormali (3, 11, 17, 704)     2816        conv5_block6_concat[0][0]
__________________________________________________________________________________________________
conv5_block7_0_relu (Activation (3, 11, 17, 704)     0           conv5_block7_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block7_1_conv (Conv2D)    (3, 11, 17, 128)     90112       conv5_block7_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block7_1_bn (BatchNormali (3, 11, 17, 128)     512         conv5_block7_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block7_1_relu (Activation (3, 11, 17, 128)     0           conv5_block7_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block7_2_conv (Conv2D)    (3, 11, 17, 32)      36864       conv5_block7_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block7_concat (Concatenat (3, 11, 17, 736)     0           conv5_block6_concat[0][0]
                                                                 conv5_block7_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block8_0_bn (BatchNormali (3, 11, 17, 736)     2944        conv5_block7_concat[0][0]
__________________________________________________________________________________________________
conv5_block8_0_relu (Activation (3, 11, 17, 736)     0           conv5_block8_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block8_1_conv (Conv2D)    (3, 11, 17, 128)     94208       conv5_block8_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block8_1_bn (BatchNormali (3, 11, 17, 128)     512         conv5_block8_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block8_1_relu (Activation (3, 11, 17, 128)     0           conv5_block8_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block8_2_conv (Conv2D)    (3, 11, 17, 32)      36864       conv5_block8_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block8_concat (Concatenat (3, 11, 17, 768)     0           conv5_block7_concat[0][0]
                                                                 conv5_block8_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block9_0_bn (BatchNormali (3, 11, 17, 768)     3072        conv5_block8_concat[0][0]
__________________________________________________________________________________________________
conv5_block9_0_relu (Activation (3, 11, 17, 768)     0           conv5_block9_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block9_1_conv (Conv2D)    (3, 11, 17, 128)     98304       conv5_block9_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block9_1_bn (BatchNormali (3, 11, 17, 128)     512         conv5_block9_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block9_1_relu (Activation (3, 11, 17, 128)     0           conv5_block9_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block9_2_conv (Conv2D)    (3, 11, 17, 32)      36864       conv5_block9_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block9_concat (Concatenat (3, 11, 17, 800)     0           conv5_block8_concat[0][0]
                                                                 conv5_block9_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block10_0_bn (BatchNormal (3, 11, 17, 800)     3200        conv5_block9_concat[0][0]
__________________________________________________________________________________________________
conv5_block10_0_relu (Activatio (3, 11, 17, 800)     0           conv5_block10_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block10_1_conv (Conv2D)   (3, 11, 17, 128)     102400      conv5_block10_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block10_1_bn (BatchNormal (3, 11, 17, 128)     512         conv5_block10_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block10_1_relu (Activatio (3, 11, 17, 128)     0           conv5_block10_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block10_2_conv (Conv2D)   (3, 11, 17, 32)      36864       conv5_block10_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block10_concat (Concatena (3, 11, 17, 832)     0           conv5_block9_concat[0][0]
                                                                 conv5_block10_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block11_0_bn (BatchNormal (3, 11, 17, 832)     3328        conv5_block10_concat[0][0]
__________________________________________________________________________________________________
conv5_block11_0_relu (Activatio (3, 11, 17, 832)     0           conv5_block11_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block11_1_conv (Conv2D)   (3, 11, 17, 128)     106496      conv5_block11_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block11_1_bn (BatchNormal (3, 11, 17, 128)     512         conv5_block11_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block11_1_relu (Activatio (3, 11, 17, 128)     0           conv5_block11_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block11_2_conv (Conv2D)   (3, 11, 17, 32)      36864       conv5_block11_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block11_concat (Concatena (3, 11, 17, 864)     0           conv5_block10_concat[0][0]
                                                                 conv5_block11_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block12_0_bn (BatchNormal (3, 11, 17, 864)     3456        conv5_block11_concat[0][0]
__________________________________________________________________________________________________
conv5_block12_0_relu (Activatio (3, 11, 17, 864)     0           conv5_block12_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block12_1_conv (Conv2D)   (3, 11, 17, 128)     110592      conv5_block12_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block12_1_bn (BatchNormal (3, 11, 17, 128)     512         conv5_block12_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block12_1_relu (Activatio (3, 11, 17, 128)     0           conv5_block12_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block12_2_conv (Conv2D)   (3, 11, 17, 32)      36864       conv5_block12_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block12_concat (Concatena (3, 11, 17, 896)     0           conv5_block11_concat[0][0]
                                                                 conv5_block12_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block13_0_bn (BatchNormal (3, 11, 17, 896)     3584        conv5_block12_concat[0][0]
__________________________________________________________________________________________________
conv5_block13_0_relu (Activatio (3, 11, 17, 896)     0           conv5_block13_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block13_1_conv (Conv2D)   (3, 11, 17, 128)     114688      conv5_block13_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block13_1_bn (BatchNormal (3, 11, 17, 128)     512         conv5_block13_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block13_1_relu (Activatio (3, 11, 17, 128)     0           conv5_block13_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block13_2_conv (Conv2D)   (3, 11, 17, 32)      36864       conv5_block13_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block13_concat (Concatena (3, 11, 17, 928)     0           conv5_block12_concat[0][0]
                                                                 conv5_block13_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block14_0_bn (BatchNormal (3, 11, 17, 928)     3712        conv5_block13_concat[0][0]
__________________________________________________________________________________________________
conv5_block14_0_relu (Activatio (3, 11, 17, 928)     0           conv5_block14_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block14_1_conv (Conv2D)   (3, 11, 17, 128)     118784      conv5_block14_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block14_1_bn (BatchNormal (3, 11, 17, 128)     512         conv5_block14_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block14_1_relu (Activatio (3, 11, 17, 128)     0           conv5_block14_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block14_2_conv (Conv2D)   (3, 11, 17, 32)      36864       conv5_block14_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block14_concat (Concatena (3, 11, 17, 960)     0           conv5_block13_concat[0][0]
                                                                 conv5_block14_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block15_0_bn (BatchNormal (3, 11, 17, 960)     3840        conv5_block14_concat[0][0]
__________________________________________________________________________________________________
conv5_block15_0_relu (Activatio (3, 11, 17, 960)     0           conv5_block15_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block15_1_conv (Conv2D)   (3, 11, 17, 128)     122880      conv5_block15_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block15_1_bn (BatchNormal (3, 11, 17, 128)     512         conv5_block15_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block15_1_relu (Activatio (3, 11, 17, 128)     0           conv5_block15_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block15_2_conv (Conv2D)   (3, 11, 17, 32)      36864       conv5_block15_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block15_concat (Concatena (3, 11, 17, 992)     0           conv5_block14_concat[0][0]
                                                                 conv5_block15_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block16_0_bn (BatchNormal (3, 11, 17, 992)     3968        conv5_block15_concat[0][0]
__________________________________________________________________________________________________
conv5_block16_0_relu (Activatio (3, 11, 17, 992)     0           conv5_block16_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block16_1_conv (Conv2D)   (3, 11, 17, 128)     126976      conv5_block16_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block16_1_bn (BatchNormal (3, 11, 17, 128)     512         conv5_block16_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block16_1_relu (Activatio (3, 11, 17, 128)     0           conv5_block16_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block16_2_conv (Conv2D)   (3, 11, 17, 32)      36864       conv5_block16_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block16_concat (Concatena (3, 11, 17, 1024)    0           conv5_block15_concat[0][0]
                                                                 conv5_block16_2_conv[0][0]
__________________________________________________________________________________________________
bn (BatchNormalization)         (3, 11, 17, 1024)    4096        conv5_block16_concat[0][0]
__________________________________________________________________________________________________
relu (Activation)               (3, 11, 17, 1024)    0           bn[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (3, 1024)            0           relu[0][0]
__________________________________________________________________________________________________
fc1000 (Dense)                  (3, 10)              10250       avg_pool[0][0]
==================================================================================================
Total params: 7,047,754
Trainable params: 6,964,106
Non-trainable params: 83,648
__________________________________________________________________________________________________
Train for 100 steps, validate for 10 steps
2019-11-06 11:12:15.235702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcublas.so.10.0
shape TensorShape([12, 372, 558, 3]) [12 372 558 3]
2019-11-06 11:12:15.481528: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index 10 of dimension 0 out of bounds.
shape TensorShape([12, 372, 558, 3]) [12 372 558 3]
2019-11-06 11:12:15.485747: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index 10 of dimension 0 out of bounds.
shape TensorShape([12, 372, 558, 3]) [12 372 558 3]
2019-11-06 11:12:15.488817: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index 10 of dimension 0 out of bounds.
2019-11-06 11:12:15.489183: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: {{function_node __inference_Dataset_map_batch_print_35}} slice index 10 of dimension 0 out of bounds.
         [[{{node strided_slice}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext_2]]
         [[Identity_4/_188]]
2019-11-06 11:12:15.489398: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: {{function_node __inference_Dataset_map_batch_print_35}} slice index 10 of dimension 0 out of bounds.
         [[{{node strided_slice}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext_2]]
shape TensorShape([12, 372, 558, 3]) [12 372 558 3]
2019-11-06 11:12:15.494247: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index 10 of dimension 0 out of bounds.
2019-11-06 11:12:15.887854: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: {{function_node __inference_Dataset_map_batch_print_35}} slice index 10 of dimension 0 out of bounds.
         [[{{node strided_slice}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext_2]]
         [[replica_2/metrics/accuracy/AssignAddVariableOp_1/_39]]
  1/100 [..............................] - ETA: 3:57:11Traceback (most recent call last):
  File ""/user/vmarkovtsev/images/efficientoffice/efficientoffice/shape_bug.py"", line 45, in <module>
    sys.exit(main())
  File ""/user/vmarkovtsev/images/efficientoffice/efficientoffice/shape_bug.py"", line 41, in main
    model.fit(ds_train, validation_data=ds_val, epochs=1, steps_per_epoch=100)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 324, in fit
    total_epochs=epochs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py"", line 520, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 1141, in _filtered_call
    self.captured_inputs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 511, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: 4 root error(s) found.
  (0) Invalid argument:   slice index 10 of dimension 0 out of bounds.
         [[node strided_slice (defined at /local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext_2]]
         [[Identity_4/_188]]
  (1) Invalid argument:   slice index 10 of dimension 0 out of bounds.
         [[node strided_slice (defined at /local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext_2]]
  (2) Cancelled:
  (3) Cancelled:
0 successful operations.
1 derived errors ignored. [Op:__inference_distributed_function_166689]

Function call stack:
distributed_function -> distributed_function -> distributed_function -> distributed_function -> distributed_function ->distributed_function
</pre>
</details>

This is how the log ends - the crash:



This is why the bug is so spicy: both the static and dynamic shapes are 12, but if you try to access an element under index 3+ (3 = 12 / 4), you crash. I am really interested in why.

If you remove , the code works."
574,29686,0,"[Find header file after build manually tensor flow lite]. Hello, I built tensorflow lite for ARM 64 on Linux.
And After building successful I have a library called ""libtensorflow-lite.a""
As you know, if we want to use this lib, we have both lib and header file (.h file in my case).
But where can I get all header file for libtensorflow lib.
In result folder (gen folder in my case), I see bin, lib, object but include folder does't exist."
1322,3295,0,"tf.train.Server.create_local_server  has two problems :1)performance drop 2 times compare to without it,2) gpu memory fraction limitations fails. . tf.train.Server.create_local_server has two problems :1)performance drop 2 times compare to without it,2) gpu memory fraction limitations fails

my model is a dnn network with 12 layers, same as the alphago policy network!
## code1: whitout server = tf.train.Server.create_local_server(start = True)


## \- code2:  using server = tf.train.Server.create_local_server(start = True)


# Questions:

**1. code2 performance drop 3 times compare to code1( 3 steps/seconds vs 11 steps/seconds), why?**
**2. code2 will broke the  limitations of gpu memory. while code1 is ok**

ps: Is this relevant to this issue than the training data is read from queue?
"
1241,26416,0,"Installation issues with cuda10.1. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS  Linux Ubuntu 16.04:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version:2.0.0-alpha0
- Python version:2.7
- Installed using virtualenv? pip? conda?:pip 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:Titan



**Describe the problem**

After the installation of tensorflow2.0 alpha package and running the python file
with 
*import tensorflow as tf*
getting below error :

""ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory
""

**Any other info / logs**
File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/_api/v2/audio/__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory

"
331,1277,0,"Cifar breaking. I am running cifar multiGPU example with different batch sizes and number of GPUs. The code is breaking this way:

tensorflow.python.framework.errors.OutOfRangeError: RandomShuffleQueue '_1_tower_0/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 128, current size 0)
     [[Node: tower_0/shuffle_batch = QueueDequeueMany[component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](tower_0/shuffle_batch/random_shuffle_queue, tower_0/shuffle_batch/n/_775)]]
     [[Node: tower_1/shuffle_batch/n/_664 = _HostSend[T=DT_INT32, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:1"", send_device_incarnation=1, tensor_name=""edge_170_tower_1/shuffle_batch/n"", _device=""/job:localhost/replica:0/task:0/gpu:1""](tower_1/shuffle_batch/n)]]
Caused by op u'tower_0/shuffle_batch', defined at:
  File ""lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py"", line 224, in <module>
    tf.app.run()
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py"", line 222, in main
    train()
  File ""lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py"", line 150, in train
    loss = tower_loss(scope)
  File ""lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py"", line 65, in tower_loss
    images, labels = cifar10.distorted_inputs()
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10.py"", line 119, in distorted_inputs
    batch_size=FLAGS.batch_size)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_input.py"", line 153, in distorted_inputs
    min_queue_examples, batch_size)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_input.py"", line 104, in _generate_image_and_label_batch
    min_after_dequeue=min_queue_examples)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 496, in shuffle_batch
    return queue.dequeue_many(batch_size, name=name)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 287, in dequeue_many
    self._queue_ref, n, self._dtypes, name=name)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 319, in _queue_dequeue_many
    timeout_ms=timeout_ms, name=name)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py"", line 664, in apply_op
    op_def=op_def)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1043, in __init__
    self._traceback = _extract_stack()

The values that are passed to shuffle_batch():



are:
batch size 128, num_threads 16, capacity 20384, min_after_deque 20000
"
1176,29573,0,"Tensorflow 2 eager execution is off by default. This code:

Produces this output:


I build it from source.
"
803,24504,0,"Add parameter for exporting TensorFlow models to override existed files. 
**System information**
- TensorFlow version (you are using): TensorFlow 1.8.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Now we have  and  APIs to export the TensorFlow models. However, there is no parameter to override the model files if we want to do that. Using extra HDFS APIs to delete remote files requires more dependencies for user's Python scripts. And we can do that with the TensorFlow FileSystem APIs.

**Will this change the current api? How?**

Yes. Add the new parameter for  function. It could not override the files by default so it may be compatible with the older versions.

**Who will benefit with this feature?**

The TensorFlow end users.

**Any Other info.**

No."
520,582,0,"error parsing inception v3 file: 64MB python protobuf parsing limit. I just upgraded TF to 469b3ddb4bb914a02f496748dd4ee5dbcbd7fce0 and I'm now getting an error importing the inception v3 model provided at https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip

I didn't have this problem before the upgrade. Here is the error



And this is the code I'm using


"
214,33177,1,"Efficiency of model.fit_generator() are greatly reduced in 2.0.0. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):  2.0.0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0, 7.6
- GPU model and memory: Titan Xp, 12G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
My code takes 21s/epoch in tf 2.0.0alpha while it takes 76s/epoch in tf 2.0.0. I found that the problem is model.fit_generator(). When I replace it with model.fit(), the efficiency of the code is exactly the same in both versions. But I need to use ImageDataGenerator.

**Describe the expected behavior**
The efficiency of the code should be same.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
(train_images, train_labels, test_images, test_labels) = load_CIFAR('/home/user/Documents/dataset/Cifar-10')
datagen = ImageDataGenerator(horizontal_flip=True,
                                 width_shift_range=0.125,
                                 height_shift_range=0.125,
                                 fill_mode='constant', cval=0.)
datagen.fit(train_images)
datagenflow = datagen.flow(train_images, train_labels, batch_size=batch_size)
model.fit_generator(datagenflow,
              steps_per_epoch=iterations,
              epochs=epoch_num,
              callbacks=[change_lr],
              validation_data=(test_images, test_labels))

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1145,28752,0,"""ValueError: Unknown layer: name"" when creating deep copy of model. I created a custom layer, which looks as follows:



The layer itself seems to work. However, the error arises when working with the following model, which makes use of the layer :



Particularly, the error occurs when I try to make a deepcopy of the network, which looks as follows:



The error raised looks as follows:



Apparently, Tensorflow seems to be incapable of making a deepcopy of the custom layer, since, as soon as I replace the two instances of the custom layer by dense layers, the code works again. 

So, I was  wondering whether there is some way to tell Tensorflow how to interpret the custom layer when executing the deepcopy command.

I am using Tensorflow 1.12.0 in Python3.5.2 on Ubuntu 16.04. Thanks in advance!"
770,19834,0,"Attention Wrapper state error.. Hello! I am implementing a seq2seq model with attention for multi-step time series forecasting. I am encountering an error with implementing attention for MultiRNN cell.

System Information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:   Yes , I have attached my code below.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)** - Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**: - Source

- **TensorFlow version (use command below)**: -VERSION = 1.4.1

- **Python version**: 2.7.12

- **Bazel version (if compiling from source)**:  - NA

- **GCC/Compiler version (if compiling from source)**: COMPILER_VERSION = v1.4.0-19-ga52c8d9

- **CUDA/cuDNN version**: - NA

- **GPU model and memory**: - NA

- **Exact command to reproduce**:
I am implementing a seq2seq model with attention for multi-step time series forecasting.

I am facing an issue with attention wrapper state.

My code is given below.

Function for defining the decoder 


    def decoder_network(attention_mechanism,dropout=config.dropout ):
        cells = []
        for i in range(config.num_stacked_layers)
    
            lstm_cell = tf.contrib.rnn.LSTMCell(config.hidden_dim)
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(lstm_cell, attention_mechanism=attention_mechanism, attention_layer_size=config.hidden_dim)
            cells.append(decoder_cell)
    
    
        cell = tf.contrib.rnn.MultiRNNCell(cells)
        return cell,cells
		
		

Using the above function

    cell,cells = decoder_network(attention_mechanism)
    new_state = cells[0].zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state = initial_state)

	for i, inp in enumerate(decoder_inputs):
	      output, new_state = cell(inp, state=new_state)
	


However, I am facing the following error :

    Traceback (most recent call last):
      File ""seq2seq.py"", line 351, in <module>
        train()
      File ""seq2seq.py"", line 274, in train
        rnn_model = build_train_graph(feed_previous=True)
      File ""seq2seq.py"", line 247, in build_train_graph
        dec_outputs, dec_memory = _basic_rnn_seq2seq(enc_inp, dec_inp, cell, Why , by , feed_previous=feed_previous)
      File ""seq2seq.py"", line 165, in _basic_rnn_seq2seq
        return _rnn_decoder(decoder_inputs, enc_state, attention_mechanism, Why , by ,_loop_function)
      File ""seq2seq.py"", line 147, in _rnn_decoder
        output, new_state = cell(inp, state=new_state)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 183, in __call__
        return super(RNNCell, self).__call__(inputs, state)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 575, in __call__
        outputs = self.call(inputs, *args, **kwargs)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1066, in call
        cur_inp, new_state = cell(cur_inp, cur_state)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 183, in __call__
        return super(RNNCell, self).__call__(inputs, state)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 575, in __call__
        outputs = self.call(inputs, *args, **kwargs)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py"", line 1289, in call
        ""Received type %s instead.""  % type(state))
    TypeError: Expected state to be instance of AttentionWrapperState. Received type <class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'> instead.
    


I have tried printing the type of new state and i got the following type


Even though new state is an instance of AttentionWrapperState it still gives an error.

Thanks in advance for your replies ."
623,31579,0,"[TF 2.0] allow tf.function input_signature to be specified by annotations. **System information**

- TensorFlow version (you are using): 2.0.0-rc0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

 has an argument  which I have been using to try and make my code a bit safer and ensure I don't keep re-tracing functions. The  specifies the tensor type for each of the function arguments. It would be much nicer (I think) to specify these types using python (>=3.5) annotations, where a suitable version of python is available. A very rough example looks like:



Which I think is nicer than the current signature:



I think the main benefit of the annotation approach is that the argument name and type are beside each other, and this syntax is already widely used in python.

In order to enable using annotations as the  I think there should be an extra boolean argument to  called e.g.  which defaults to .

Also note I have set  here to avoid a warning:

> Cause: name 'foo_scope' is not defined

I am guessing a proper implementation inside of  would not have this problem.

**Will this change the current api? How?**

It would add an additional argument to  which at the default value would not change anything.

**Who will benefit with this feature?**

Anyone using python >= 3.5 who would like to specify the tensor types of their functions.

**Any Other info.**

None"
1393,10847,0,"Something error when I run  iris_monitors.py . ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.0.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:import tensorflow

### Describe the problem
When I run iris_monitors.py in Pycharm file(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/monitors/iris_monitors.py), It has an error :
Traceback (most recent call last):
  File ""F:/temp/Python/temp.py"", line 92, in <module>
    tf.app.run()
  File ""C:\software\Python\Python35\lib\site-packages\tensorflow\python\platform\app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""F:/temp/Python/temp.py"", line 35, in main
    filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float)
  File ""C:\software\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py"", line 47, in load_csv_with_header
    header = next(data_file)
StopIteration

### Source code / logs
C:\software\Python\Python35\python.exe F:/temp/Python/temp.py
Traceback (most recent call last):
  File ""F:/temp/Python/temp.py"", line 92, in <module>
    tf.app.run()
  File ""C:\software\Python\Python35\lib\site-packages\tensorflow\python\platform\app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""F:/temp/Python/temp.py"", line 35, in main
    filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float)
  File ""C:\software\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py"", line 47, in load_csv_with_header
    header = next(data_file)
StopIteration

Process finished with exit code 1
"
1435,11963,0,"Infiniband with tensorflow. -----------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
commit 3bee923c9
- **Bazel version (if compiling from source)**:
0.4.5
- **CUDA/cuDNN version**:
cuda 8.0/cudnn 5.1.5
- **GPU model and memory**:
Titan Xp
- **Exact command to reproduce**:

### Describe the problem
I tried to use Infiniband with tensorflow using .
But the session is not hanged.
I used S1, and S2, the cluster is configured with the Infiniband ip address. Is there something that I missed?


ibdev2netdev 
S1
mlx5_0 port 1 ==> ib0 (Down)
mlx5_1 port 1 ==> ib1 (Up)
S2
mlx5_0 port 1 ==> ib0 (Up)
mlx5_1 port 1 ==> ib1 (Up)

### Source code / logs"
175,27472,1,"10x performance loss when using eager execution with tf.keras.fit(tf.data.Dataset) . <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS (Bionic Beaver)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When training  with eager execution, where dataset is a , I am finding a ~10x performance loss as compared to turning off eager execution. 

**Describe the expected behavior**
In the code attached below, I have tested training with and without eager execution, and with and without tf.data.Dataset.

Here are the training times for one epoch of training on a 4 core CPU:
Eager               + tf.data.Dataset : 219s  - 22ms/step
Without Eager + tf.data.Dataset : 25s  - 3ms/step
Eager               + numpy dataset : 26s  - 259us/sample
Without Eager + numpy dataset : 26s  - 257us/sample

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
891,28899,0," Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count . This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
1354,5848,0,"/usr/local/computecpp not found when install tensorflow.. when run tensorflow configure, it shows /usr/local/computecpp not found when install tensorflow.
I followed the instrcution, But i can not find how to install computecpp. plz help."
700,23310,0,"Win10 + Anaconda3  cannot install tensorflow-gpu. Hi All,
I'm new to setup GPU environment. Has been stuck for a few hours, hope to get some solutions.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- TensorFlow installed from (source or binary): pip install --ignore-installed --upgrade tensorflow-gpu
- Python version: Python 3.5.3
- CUDA/cuDNN version: CUDA 9.0.176,  cudnn-9.0-windows10-x64-v7.3.1.20
- GPU model and memory: NVIDIA GeForce GTX 1050, 4GB


**Describe the current behavior**
Cannot install. Error message as below. 

(PY35) C:\>pip install --ignore-installed --upgrade tensorflow-gpu
Collecting tensorflow-gpu
  Using cached https://files.pythonhosted.org/packages/43/93/07f5cae2c8e02b37c4d64fa9c9eb1f8d3b39128247d0c1acd9110da45f67/tensorflow_gpu-1.11.0-cp35-cp35m-win_amd64.whl
Collecting six>=1.10.0 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl
Collecting tensorboard<1.12.0,>=1.11.0 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl
Collecting grpcio>=1.8.6 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/5e/8c/da9316699398607a22c91e39e16e4c0f3e8233e0faa88ed52df736f2b1d6/grpcio-1.16.0-cp35-cp35m-win_amd64.whl
Collecting keras-preprocessing>=1.0.3 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl
Collecting termcolor>=1.1.0 (from tensorflow-gpu)
  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\site-packages\setuptools\__init__.py"", line 191, in <module>
        monkey.patch_all()
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\site-packages\setuptools\monkey.py"", line 101, in patch_all
        patch_for_msvc_specialized_compiler()
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\site-packages\setuptools\monkey.py"", line 138, in patch_for_msvc_specialized_compiler
        msvc = import_module('setuptools.msvc')
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\importlib\__init__.py"", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\site-packages\setuptools\msvc.py"", line 58, in <module>
        from distutils.msvc9compiler import Reg
      File ""c:\users\ericx\appdata\local\conda\conda\envs\py35\lib\distutils\msvc9compiler.py"", line 254
        log.debug(""Unable to find vcvarsall.bat Eric edit C:\Users\ericx\AppData\Local\conda\conda\envs\r-tensorflow\Lib\site-packages\numpy\distutils"")
                 ^
    SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 41-42: truncated \UXXXXXXXX escape

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\ericx\AppData\Local\Temp\pip-install-mxk6dgpl\termcolor\

**Describe the expected behavior**

**Code to reproduce the issue**
Cannot install, every time same error. "
911,14342,0,"[Feature Request] Add payload only compression support for TFRecord files. This feature request is a follow up to PR #12369 and issue #12344.

Issue #12344 raised the feature request of supporting gzipped TFRecord files. However, the compression means the TFRecord file is gzipped as a whole and the header is compressed as well.

In certain situations, it might be desirable to expose the header and only compress the payload for better visibility and lookup performance.

This issue is a feature request to support payload only compression for TFRecord files."
41,19027,1,"Speed regression on tensorflow version > 1.4.1. We have observed some massive slowdowns in some Keras/tensorflow models with tensorflow versions newer than 1.4.1.
Not sure if the issue is with tensorflow or with the way keras creates the tensorflow models, so I am cross-posting these issue to both repos (apologies for that!).

Here is a script reproducing the issue:

## Setup


## Fitting



Here are some timings for the fit method:
* Tensorflow 1.4.1:  **2.91 s ± 452 ms** per loop  (obtained using ipython's  magic, 7 loops)
* Tensorflow 1.5.0:  CPU times: user **2min 19s**, sys: 5min 22s, total: 7min 41s Wall time: 1min 2s
* Tensorflow 1.6.0: CPU times: user **5min 5s**, sys: 12min 31s, total: 17min 36s Wall time: 2min 37s
* Tensorflow 1.7.0: CPU times: user **5min 5s**, sys: 12min 39s, total: 17min 45s Wall time: 2min 39s

So, it seems there was a massive slowdown in version 1,5, and then a further one in 1.6 (which similar speed in 1.7). All the tests are run on a conda environment with python 3.6.5 and keras 2.1.5, with the corresponding tensorflow versions all coming from the anaconda  channel.

The GPU accelerated version of keras/tensorflow ( conda package) does not present the issue.

Thanks in advance!"
957,20418,0,"Error when using tf.contrib.metrics.count in eval_metric_ops of tf.estimator.EstimatorSpec. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac Os 10.13.5
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**:  tested on v1.8.0-0-g93bc2e2072 1.8.0 and v1.9.0-rc0-35-g17d6639b55 1.9.0-rc1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: no gpu
- **Exact command to reproduce**: see description

### Describe the problem

Hello,

It seems that tf.contrib.metrics.count does not work into
eval_metric_ops of tf.estimator.EstimatorSpec during validation
(I just wanted to check the size of my validation set when debugging)

It raises


It seems that the first returned argument (count_) in 
https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3722

is type 

Shoud not it be converted into 



using math_ops.float32() (or something else ?) like in other metrics definitions ?

### Source code / logs

Somewhere in my generate_model_fn():



with 



raises the  during validation phase

but works with

"
1078,32959,0,"Tensorflow to CoreML with tf-coreml: `Retval[26]` error. **System information**
- Have I written custom code: a mix of both
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): 
- TensorFlow version: 1.14.0
- Python version: 3.6.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA-SMI 418.87.00 / Driver Version: 418.87.00

**Describe the current behavior**
I have a multi-input network that uses a   to manage how batch normalization is executed in training and validation / testing.
I’ve been trying to convert this trained model to  via  library with no success, with below error:

I've also encountered: 

I understand that this error states that there is a certain node that’s missing a value so the converter can execute the model. I also understand this error is connected to control flow operations (linked to the batch normalization method creating operations like  and ). The [source code](https://github.com/tensorflow/tensorflow/blob/1c7993f7c597745e3639b9918c3b3ae2165d4af2/tensorflow/python/kernel_tests/control_flow_ops_py_test.py#L244) shows this:


Note that my error is  (I’ve gotten [24], etc.), not . I’m assuming it tests the  “dead branch”, which should be the non-used branch for inference. The code also does the same with  “dead branch”.

Is there any detail I’m missing that may be causing this error (not the first error I’ve faced during conversion, of course)? The way the inference is done? The way batch normalization is implemented? The way the model is saved? 

What I’ve done so far:
- I know  creates operations  and , which are not CoreML compatible
- I’ve tried converting to  with similar issues
- I’ve follow  (this model uses the same  logic for training, validation, testing) conversion process with no success
- I’ve tried the [](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md) library
- I’ve tried scripts to remove / modify the control flow 
- I’ve created separate graphs to avoid the extra ops with no success

_Note: I’ve abstracted big part of the code and network to post this issue._

**Code to reproduce the issue**

This is how batch normalization is implemented (within a convolution block):



Below is the code to train and save the model.



Below is the code to load, update inputs, perform inference, and freeze the model.



Below is the code to convert to CoreML.



**Other info / logs**
Below is the error thrown by .

"
1273,21217,0,"cuda 9.0 on windows 10 goes extremely unstable, prebuilt should be migrated to cuda 9.2. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10 x64
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0rc0
- **Python version**: 3.6.6 x64
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: MSVC15
- **CUDA/cuDNN version**: 9.2/7.1.5
- **GPU model and memory**: GTX1050Ti GDDR5 4GB
- **Exact command to reproduce**:
CUDA 9.0 no longer supported


This is not a direct tensorflow issue. This is a suggestion for all windows 10 tensorflow-gpu user

These days NVIDIA geforce driver starts to reject CUDA 9.0 since this version is too old and deprecated



There is no reason tensorflow-gpu prebuilt binary is provided with cuda 9.0 and cudnn 7.0


Prebuild binary sholud be built with latest cuda and cudnn version"
949,178,0,"Installation error. While running the , I get the following error:



However, I installed numpy from the source listed and it worked perfectly. Any suggestions?
"
707,20032,0,"Failed to find any matching files for slim_pretrained\inception_v1.ckpt.  Unsuccessful TensorSliceReader constructor: Failed to
find any matching files for slim_pretrained\inception_v1.ckpt
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_
FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:
0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_
and_slices)]]"
512,45947,0,"https://youtu.be/IGpHWo0ySBU. <em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: 
2. TF 2.0: 


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
1111,12417,0,"Can't use reshape in a while_loop   InvalidArgumentError . tf.reshape( x ,[batch_size,-1])

Batch_size is a global variable. This instruction within the body of a while loop  produce the error:

tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'Reshape' has inputs from different frames. The input 'while/strided_slice' is in frame 'while/while/'. The input 'Reshape/shape' is in frame ''.

It's very layered, there are many other similar instructions in the code, reshape that involve global variables, but are compiled correctly.

I'm using tensorflow 1.1 on python3 installed by Anaconda."
930,8116,0,"Errno 110 occured when running all python files in /examples/tutorials/mnist/ directory.. davidtest@CaffeVM:~/tensorflow/tensorflow/tensorflow/examples/tutorials/mnist$ python fully_connected_feed.py 
Traceback (most recent call last):
  File ""fully_connected_feed.py"", line 277, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""fully_connected_feed.py"", line 222, in main
    run_training()
  File ""fully_connected_feed.py"", line 120, in run_training
    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 211, in read_data_sets
    SOURCE_URL + TRAIN_IMAGES)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 208, in maybe_download
    temp_file_name, _ = urlretrieve_with_retry(source_url)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 165, in wrapped_fn
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 190, in urlretrieve_with_retry
    return urllib.request.urlretrieve(url, filename)
  File ""/usr/lib/python2.7/urllib.py"", line 94, in urlretrieve
    return _urlopener.retrieve(url, filename, reporthook, data)
  File ""/usr/lib/python2.7/urllib.py"", line 240, in retrieve
    fp = self.open(url, data)
  File ""/usr/lib/python2.7/urllib.py"", line 208, in open
    return getattr(self, name)(url)
  File ""/usr/lib/python2.7/urllib.py"", line 345, in open_http
    h.endheaders(data)
  File ""/usr/lib/python2.7/httplib.py"", line 1013, in endheaders
    self._send_output(message_body)
  File ""/usr/lib/python2.7/httplib.py"", line 864, in _send_output
    self.send(msg)
  File ""/usr/lib/python2.7/httplib.py"", line 826, in send
    self.connect()
  File ""/usr/lib/python2.7/httplib.py"", line 807, in connect
    self.timeout, self.source_address)
  File ""/usr/lib/python2.7/socket.py"", line 571, in create_connection
    raise err
"
192,30068,1,"Device getting slower when running tensorflow c++. Dear all, I have encountered a problem that tensorflow inference time is getting slower. 

Situation:

Our device is used in the production line and tensorflow prediction is one of the module in the whole project. Since in production line the device is operating for a very long time (24 hours non-stop), after a few days (probably 3-5 days) we can feel that the device is getting slower and tensorflow inference time is about 2-3 times as usual. We suspect it was caused by GPU memory leak or some other issues. May I ask has anyone encountered such problem? How should I do with this situation?

Environment:

OS: Windows 10 (64 bits)
CPU: I7 - 6th Generation
GPU: Nvidia GTX 1050 Ti
TF Version: r1.4
CMake build with VS 2015
"
90,33988,1,"Keras .fit() yields incorrect results when using a custom loss function. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow version (use command below): 2.0.0

**Describe the current behavior**
When using a custom loss function, Keras'  produces incorrect results.

**Code to reproduce the issue**
See this [Colab Notebook](https://colab.research.google.com/drive/1Q5hVfxVKdeqtIPWrYuvsPQ-IE0dUOeWO)."
284,26998,0,"2.0 Reference Models: MobileNetV2 (1 GPU, 8 GPU with dist strat and Keras). **MobileNetV2** is a significant improvement over [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html) and pushes the state of the art for mobile visual recognition including classification, object detection and semantic segmentation. It builds upon the ideas from MobileNetV1, using depthwise separable convolution as efficient building blocks. However, V2 introduces two new features to the architecture: 

1. Linear bottlenecks between the layers, and 
2. Shortcut connections between the bottlenecks1.

The research team's academic paper describes MobileNetV2 in detail: https://arxiv.org/abs/1801.04381.

An example of MobileNetV2 implemented with Slim can be found [here](https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
1311,26967,0,"InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match:. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When I am trianging my model with tf.estimator and tf.data, this issue occurs:

the batch of data is 128, so the first feature' s is correct, the second wrong. 
Does anyone else have the same problem? Thanks in advance.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
420,9853,0,"Multiprocessing for input pipeline . I have asked this [question](http://stackoverflow.com/questions/43889941/enqueuing-a-tf-randomshufflequeue-from-multiple-processes-using-multiprocessing) on StackOverFlow but I also feel that it can also be seen as a feature request. 

------------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
   : YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    : Linux version 3.10.0-229.11.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) 
- **TensorFlow installed from (source or binary)**:
    :  Installed from sources
- **TensorFlow version (use command below)**:
    :  v1.0.0-63-g5b4bb03-dirty' 1.0.1
- **Bazel version (if compiling from source)**:
    :  0.4.4
- **CUDA/cuDNN version**:
    :  CUDA 8.0 
    :  CuDNN 5.1
- **GPU model and memory**:
    : NVidia Titan X (Maxwell)
- **Exact command to reproduce**:
    : It is a feature request or clarification


### Describe the problem

**Basic Objective**
I have to train the OverFeat architecture for patch classification from scratch.

**Problem scope**
Input to the graph

**Problem description**
During the training of the OverFeat model, for each image 5 random crops and their mirrored versions (a total of 10 augmented images) are created and they are fed to the model.

I previously used  to provide input to the model. However, it led to a very very slow  training and turned out to be impractical when handling 12 million input images (1.2 million of imagenet X 10).
Hence, I want to use native Tensorflow queues to provide input to the graph.

I have used  module of Python to create sharded TFRecord files of Imagenet dataset. 
Now, I would like to use  to enqueue augmented images to a  from which the graph takes it input. 
I could have used  but it is still slow. Seeing the speedup in creation of TFRecord files through  as opposed to , I am convinced that using  would increase the speed of preprocessing.

The problem is that, it is not clear to me that how do I use  in TensorFlow. One code snippet is [here](https://github.com/WeiTang114/tf-image-classification/blob/ff790fcb1ab6062979688647bb078c2765ad0e7a/input.py)  , but it ends up using a  at the end of the pipeline which leads to one extra copy operation. 

So, I want to launch like 20 processes using , each of which will process a range of shards and enqueue the augmented images and corresponding labels to a 
It would be very helpful if there is a feature for using multiprocessing with TensorFlow for input pipeline and data augmentation. A good guideline in that direction would be very helpful.


### Source code / logs
The source code which is used for creating the TFRecord files is [here](https://gitlab.inria.fr/uujjwal/overfeat-tensorflow/blob/master/buildtfrecords.py)

I also wrote a basic code to try out multiprocessing (to enqueue a queue from multiple processes) but it does not work.


The output is 



As can be seen in the subprocesses, the queue is being enqueued, but the size as shown in the main module is continuously zero.
"
971,9794,0,"how to get image shape after decode in C++ . ### System information
- **OS Platform and Distribution**: Debian
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.0.1
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: cuda-8.0, cudnn5.1.5
- **GPU model and memory**: 12GB

I follow the tutorial of [inception label_image](https://www.tensorflow.org/tutorials/image_recognition),  
[source codes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) , 
[README.md](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image) , I can compile and run the demo c++ code successfully.

I want to adapt this demo to my own project, the input images to my own Network is height fixed, while width varies accordingly, for example, the original image is size of 64x100, and I want to resize it to 32x50, as I said 32 is the new_height, and I want to know original image size after reading from the file, how can I get width=100 and height=64? then I can get new_width = new_height/height x width=32/64x100=50

the following is a small piece of the image_recognition tutorial C++ codes, resize is hard coded to a pre-define size, I try , , , etc, all failed(,  are all not , I don't know why Google design like this, really slow down the development, and I find no documentation about this), is there any easy way to get the image size? or cast the  type to ?

one possible way is first use opencv to load the image, and resize it, then copy the elements to tensor like this [example](https://gist.github.com/kyrs/9adf86366e9e4f04addb) **pixel by pixel**, but the performance is the main problem and it seems hard to compile tensorflow along with opencv.  Any one knows some methods using tensorflow's API?

Thanks in advance!

"
800,10853,0,"Mismatched delete in mkl_tfconv_op.cc. tensorflow\core\kernels\mkl_tfconv_op.cc line 120
delete in_sizes;

should be: delete [] in_sizes;"
188,34158,1,"Loss and metrics differ with masking or sample weights. Update on Jan 8, 2021: I updated the title from ""Metrics incorrect for RNN with mask"" as I discovered more information that widens the scope of this issue. See comment on that date.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**
- TensorFlow installed from (source or binary): **binary (conda)**
- TensorFlow version (use command below): **2.0.0**
- Python version: **3.6.8**
- Bazel version (if compiling from source): **n/a**
- GCC/Compiler version (if compiling from source): **n/a**
- CUDA/cuDNN version: **10.0.130 / 7.6.0**
- GPU model and memory: **1080 Ti**

**Describe the current behavior**
I am training an RNN (GRU) where my varying-length sequences are right-padded with 0s and a mask is applied. _Many_ sequences are more than half 0s (padding). I compile the model with a loss of  and a metric of , but the output is different when the mask is in effect.

Or equivalently:


Example output (note the different values for  vs.  for both training and validation):


When I disable the masking, I get the following output:

Without the mask, the values for  and  agree. For the validation set, the values are not really improving and the value of 6.7e-06 seems to be what you get when you evaluate on the 0s that would otherwise be ignored by the masking. Comparing the values between the runs suggests that the  calculations are not using the mask when it is in effect, but the  calculations do use the mask. (We'd expect lower values when we correctly ignore irrelevant time steps.)

**Describe the expected behavior**
The values for  and  should agree and both use the masking.

**Code to reproduce the issue**
I don't have full code and data to share since my model and data are proprietary.

**Other info / logs**
I can't think of any relevant logs.
"
1399,18191,0,"How to add unsupported operation in TF-Lite?(ResizeNearestNeighbor). Converting Hourglass model, I encountered an unsupported error.
This [document] is very simple. so I have to need more information(example code)(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md)

[Custom operation](https://github.com/tensorflow/tensorflow/pull/17074) Is there only this way? It is a difficulty for me.

**Error**


### System information
-**OS Platform and Distribution : Ubuntu 16.04 LTS
-**TensorFlow installed from : official
-**TensorFlow version : 1.5 version
-**Bazel version : 0.9.0
-**CUDA/cuDNN version : V 8.0.61 (8.0)
-**GPU model and memory : GeForce GTX 1060 / 3G
Exact command to reproduce :


Have a nice day.
"
1293,7876,0,"Import meta graph followed by save overwrites the previous checkpoints. NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of ): using CPU version of tensorflow 1.0.0

If installed from binary pip package, provide:

1. A link to the pip package you installed: pip install tensorflow
2. The output from . 1.0.0

If installed from source, provide 

1. The commit hash ()
2. The output of 

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Essentially I am checkpointing a graph and then importing it. On trying to save the next checkpoint, the saver overwrites the previous checkpoint in the checkpoint file (the actual meta, index and data files are not overwritten) and only the last saved checkpoint is present in the  file. Is this the intended behavior? Is there any way to preserve the checkpoints across multiple saves of the graph.

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
664,2695,0,"RNN's state_is_tuple doesn't work with initial_state. Assuming I want to batch series of inputs and propagate the cell state from one session run towards another for an epoch:



Since using state_is_tuple in the cells makes the state be a tuple on return:
- using .eval() doesn't work for an initial state
- subsequent states are returned as tuples and cannot be fed back into the session as tuples
"
720,29437,0,"Nesting Variables in tensorflow. Hi,
Hope this is simple to answer for experts. Why cant we nest variables in tensorflow. For example, in below snippet, why w1 will not be updated by optimizer?


"
466,33893,0,"Support INT8 quantisation for RESIZE_NEAREST_NEIGHBOR with TFLITE_BUILTINS_INT8 OpsSet. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.0.0

TFLiteConverter post-training quantisation flow does not support RESIZE_NEAREST_NEIGHBOR op

**Provide the text output from tflite_convert**



**Any other info / logs**

Source code (as in [guide](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations)):



Full traceback:

"
936,19370,0,"Error with version of protocol buf. Helo ,  the error is shwon as
 [libprotobuf FATAL google/protobuf/stubs/common.cc:61] This program requires version 3.5.0 of the Protocol Buffer runtime library, but the installed version is 2.6.1.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""bazel-out/arm-opt/genfiles/tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.cc"".)
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  This program requires version 3.5.0 of the Protocol Buffer runtime library, but the installed version is 2.6.1.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""bazel-out/arm-opt/genfiles/tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.cc"".)
Aborted (core dumped)

why 2.6.1?
nvidia@tegra-ubuntu:~$ protoc --version
libprotoc 3.5.1

nvidia@tegra-ubuntu:~$ pip list
protobuf (3.5.2.post1)

WhAT happend? Thank u!


"
290,24982,0,"Enable reference code of kernels(internal) in TF-Lite. Hi,
I want to benchmark the model based on reference code and Neon optimized code for kernels.So that I  want to enable the reference code implementation while executing the Tensorflow Lite model.

Is there is any way to run reference code ?
I tried to make changes in build file but there is some compilation issues.
Thanks and Regards,
Amar kumar
"
527,28337,0,"Distributed TensorFlow does not work when using more than 26 workers. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5
- TensorFlow version (use command below): TF Alpha 2.0 and TF. 1.13 CPU only
- Python version: 2.7
- Intel Xeon Gold 6150 2.7GHz 18 cores (16 cores enabled) 24.75MB L3 Cache (Max Turbo Freq. 3.7GHz, Min 3.4GHz), 180GB RAM (Six Channel), 4.8TB of Disk Space

I have a performance issue using distributed TF for CPU machine with MultiWorkerStrategy using 58 workers. However, it seems that it does not work as the following errors:

WARNING: Logging before flag parsing goes to stderr.
W0502 21:43:31.821069 47528963346944 cross_device_ops.py:1106] Not all devices in  are visible to TensorFlow.
WARNING: Logging before flag parsing goes to stderr.
W0502 21:43:31.821974 47863205896704 cross_device_ops.py:1106] Not all devices in  are visible to TensorFlow.
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable

Typically, how many workers can be used for this scheme?  and how to solve this problem? Is it because I do not have sufficient memory?

"
1180,1553,0,"SSL certification error. I install the SSL certification but still got this error

please notify what i have to do

/usr/bin/pip run on Sat Mar 19 15:15:43 2016
Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
Cleaning up...
  Removing temporary dir /tmp/pip_build_trutech...
**Exception:**
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/usr/lib/python2.7/dist-packages/pip/commands/install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/usr/lib/python2.7/dist-packages/pip/req.py"", line 1198, in prepare_files
    do_download,
  File ""/usr/lib/python2.7/dist-packages/pip/req.py"", line 1376, in unpack_url
    self.session,
  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 546, in unpack_http_url
    resp = session.get(target_url, stream=True)
  File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/sessions.py"", line 467, in get
    return self.request('GET', url, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 237, in request
    return super(PipSession, self).request(method, url, _args, *_kwargs)
  File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/sessions.py"", line 455, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/sessions.py"", line 558, in send
    r = adapter.send(request, *_kwargs)
  File ""/usr/share/python-wheels/requests-2.2.1-py2.py3-none-any.whl/requests/adapters.py"", line 385, in send
    raise SSLError(e)
**SSLError: [Errno 1] _ssl.c:510: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed**
"
1455,17627,0,"Unknown Issue in working Tensorflow. I use tensorflow as backend in keras. Implementation is done in R. Everything was working properly but today I got an error calling lstm model. Could you please help inrectifying the problem. Below is the message I get and then R stops working

2018-03-11 11:12:48.620511: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-11 11:12:48.977900: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:955] Found device 0 with properties: 
name: Quadro K2000
major: 3 minor: 0 memoryClockRate (GHz) 0.954
pciBusID 0000:03:00.0
Total memory: 2.00GiB
Free memory: 1.64GiB
2018-03-11 11:12:48.978254: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:976] DMA: 0 
2018-03-11 11:12:48.978386: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:986] 0:   Y 
2018-03-11 11:12:48.978612: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K2000, pci bus id: 0000:03:00.0)"
241,28608,1,"Tensorflow gives incorrect results for simple example. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.13.1-2-g09e3b09e69 1.13.1
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: GeForce GTX 1080

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
The actual output is [0.7853982 1.1071488]

**Describe the expected behavior**
The expected output is [0.7853982 0.7853982]

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



"
504,1658,0,"Tensorflow with GPU-support crashes when opening many CPU-only sessions in parallel. The following code works fine in a CPU-only Tensorflow, but crashes on a GPU-enabled Tensorflow installation (0.7.1, with a Titan X, NVidia driver 352.79) when run many times in parallel:



Bash command to run it in parallel:



If you then look into the  files, you will see that most of the processes crashed. The output will look like this:



For the sake of completeness, here is the use case: applying a Tensorflow model is just a small part in processing large amounts of data; processing is parallelised simply by using multiple processes.

I also tried wrapping the  call in , but that didn't change anything. I assume that it is trying to get exclusive access to the GPU, and if it can't do that, it crashes. This is a bit annoying, since by setting , I am actively trying to disable the GPU.

Ideas?
"
568,33023,0,"optimize gfile for sequential reading. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12 and 2.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The current gfile only contain one buffer and most of behave is sequential reading, I think it is worthwhile to let gfile to contain double buffers to overlap reading and processing completely. I will make sure there is no performance regression even the handle is used for random access（seek or other interface）. The feature will be very useful especially when use tensorflow in cloud environment and IO has a very long latency.

**Will this change the current api? How?**
I maybe will introduce additonal optional argument to let user to tell if the handle will be used as sequential reading or random reading.

**Who will benefit with this feature?**
anyone who will use the gfile, and most of behave is sequential reading, and the read latency is a little long.

**Any Other info.**
I will prepare the change and send CR"
862,2533,0,"The docker image of gcr.io/tensorflow/tensorflow-full does not exist. We follow the [official docs](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) and try to pull this image but it doesn't exist.



After searching in StackOverflow and we found  works.
### Environment info

Operating System: Ubuntu 16.10
### Steps to reproduce
1. docker pull gcr.io/tensorflow/tensorflow-full(not work)
### What have you tried?
1. docker pull b.gcr.io/tensorflow/tensorflow-full(work)
"
4,23049,1,"Ghost Batch Normalization performance. GBN seems to be at least twice as slow as regular BN. This is the case even when using the full batch for normalization in GBN mode ().

You can reproduce that with tensorflow/models cifar10 example by running the script below, also available at (https://gist.github.com/MustafaMustafa/a12485746e4c877620a818d227982e1c):



Output:

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow version (use command below)**: v1.12.0-rc0-0-g1a6dea3 1.12.0-rc0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**: 7.1.0
- **CUDA/cuDNN version**: 9.2/7.1.4
- **GPU model and memory**: Titan X Pascal 12GB
- **Have I written custom code**: No
- **TensorFlow installed from**: Compiled from source
- **Exact command to reproduce**: bash script for how to reproduce provided in the description.
- **Mobile device**: N/A"
892,13616,0,"Force tensor evaluation inside while-loop, scan and others.. Hello everyone,

I had big plans for  until I discovered that it is impossible to re-evaluate tensor inside it. Let's dive into the the issue and potential useful feature:



I created  variable and tensor  which equals to . In fact, we don't have control over them, they are our input as  and  and we know that  depends on . I would like to assign new value inside TensorFlow loop to  (equavalent to  at example) and evaluate fresh  ( inside example) at each iteration of the loop. Meanwhile we can do other evaluations inside , but most important is that I need to update  and get updated . Currently, assigning operation doesn't propagate updates down to the dependant nodes and it shouldn't, but when someone calls tensor depending on value which were updated via assign inside , I suppose the tensor node must detect this change and evaluate new tensor value again.

Thanks!"
38,34380,1,"Why is autoencoder with tensorflow 2.0 is performing very bad compared to the same code in keras?. I am training an autoencoder on the mnist data With keras the validation loss is great (starting from 0.2687 to .1) While with tensorflow(version 2.0).keras validation loss is stuck at (.6) Even though I am using the same code.

Below is the code with keras ([you can test it in colab](https://colab.research.google.com/drive/1AD5Z_pXsaM6Ubn6Yfg0uD230J9o8AfRS)) and followed by the code with tf.keras ([you can test it in colab](https://colab.research.google.com/drive/1Hj3-_Bjfkp3-dmd38-1CmIR4VLOV98hB))



60000/60000 [==============================] - 5s 88us/step - loss: 0.3494 - val_loss: 0.2688 Epoch 2/50 60000/60000 [==============================] - 4s 74us/step - loss: 0.2578 - val_loss: 0.2445 Epoch 3/50


Train on 60000 samples, validate on 10000 samples Epoch 1/50 60000/60000 [==============================] - 4s 59us/sample - loss: 0.6941 - val_loss: 0.6939 Epoch 2/50 60000/60000 [==============================] - 3s 47us/sample - loss: 0.6937 - val_loss: 0.6936
"
367,4267,0,"libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program. i am trying to install tensorflow on my laptop LenovoY700, so that Ubuntu16.04 is more suitable with laptop.
I installed cuda7.5+cudnn4 with GTX960M and nvidia-version is 367.

When I was running 'convolutional.py', something goes wrong.

I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/cuda-7.5/lib64:
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: Y700
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.44  Wed Aug 17 22:24:07 PDT 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 367.44.0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1077] LD_LIBRARY_PATH: /usr/local/cuda-7.5/lib64:
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally

Could somebody give me a hand?
Thanks for your help.
Simon
"
1441,6736,0,"batch_normalization layer argument name and documentation wrong in core and contrib layers. The batch normalization layers both in core and contrib take a  argument that controls whether a variable beta should be created that is then **added after the normalization**. The name of the argument () and even more so the documentation (_center: If True, subtract . If False,  is ignored._) indicate however that this argument controls whether the mean of the input is subtracted as part of the normalization."
875,3517,0,"Error when running distributed MNIST example. Hi I am new to tensorflow and distributed tensorflow. I was trying to run an example from #2726 

Right now I am using tensorflow 0.9 on a cluster of Raspberry Pi3, using slurm to manage the nodes in the cluster, the script is below:
# !/bin/bash
# SBATCH -N 4
# SBATCH --nodelist=piw[25-28]

node1=piw25
node2=piw26
node3=piw27
node4=piw28
# On node1:

srun -N 1 -n 1 python mnist_yetanother.py \
     --ps_hosts=$node1:2223 \
     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \
     --job_name=ps --task_index=0 &
# On node2:

srun -N 1 -n 1 python mnist_yetanother.py \
     --ps_hosts=$node1:2223 \
     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \
     --job_name=worker --task_index=0 &
# On node3:

srun -N 1 -n 1 python mnist_yetanother.py \
     --ps_hosts=$node1:2223 \
     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \
     --job_name=worker --task_index=1 &
# On node4:

srun -N 1 -n 1 python mnist_yetanother.py \
     --ps_hosts=$node1:2223 \
     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \
     --job_name=worker --task_index=2
wait

And this is the error message:

I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw25:2223}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw26:2223, piw27:2223, localhost:2223}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2223}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw26:2223, piw27:2223, piw28:2223}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw25:2223}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw25:2223}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2223, piw27:2223, piw28:2223}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw26:2223, localhost:2223, piw28:2223}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Initialized!
Traceback (most recent call last):
  File ""mnist_yetanother.py"", line 351, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""mnist_yetanother.py"", line 310, in main
    with sv.prepare_or_wait_for_session(server.target, config=None) as sess:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 684, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 176, in prepare_session
    sess.run(init_op, feed_dict=init_feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 372, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 636, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 708, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 728, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: /job:worker/replica:0/task:0/cpu:0 unknown device.
     [[Node: truncated_normal_2_S5 = _Recv[client_terminated=false, recv_device=""/job:ps/replica:0/task:0/cpu:0"", send_device=""/job:worker/replica:0/task:0/cpu:0"", send_device_incarnation=-8305312558883263444, tensor_name=""edge_62_truncated_normal_2"", tensor_type=DT_FLOAT, _device=""/job:ps/replica:0/task:0/cpu:0""]()]]
srun: error: piw28: task 0: Exited with exit code 1
E0726 15:08:58.966743325   10814 tcp_client_posix.c:173]     failed to connect to 'ipv4:192.168.50.38:2223': socket error: connection refused
E tensorflow/core/distributed_runtime/master.cc:202] Master init: Unavailable: 
E0726 15:09:04.384147901   10819 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:09:09.463843371   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:09:18.410973793   10820 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:09:29.903416857   10814 tcp_client_posix.c:173]     failed to connect to 'ipv4:192.168.50.38:2223': socket error: connection refused
E tensorflow/core/distributed_runtime/master.cc:202] Master init: Unavailable: 
E0726 15:10:00.977546704   10814 tcp_client_posix.c:173]     failed to connect to 'ipv4:192.168.50.38:2223': socket error: connection refused
E0726 15:10:47.956032631   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:11:48.090654174   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:13:21.260355773   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:15:21.261626939   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:17:21.263101653   10819 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:19:21.264292261   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:21:21.265185108   10820 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred
E0726 15:23:21.265939820   10801 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred

I killed the program after a while. I'm not sure why I would get ""[Node: truncated_normal_2_S5 = _Recv[client_terminated=false, recv_device=""/job:ps/replica:0/task:0/cpu:0"", send_device=""/job:worker/replica:0/task:0/cpu:0"", send_device_incarnation=-8305312558883263444, tensor_name=""edge_62_truncated_normal_2"", tensor_type=DT_FLOAT, _device=""/job:ps/replica:0/task:0/cpu:0""]()]"" such error message. Searching online does not help too much...

Any help or advice will be greatly appreciated!
"
1065,8343,0,"The inputs of dynamic_rnn must be 3-D tensor?. hi,
    I want to use _dynamic_rnn_ to train my convLSTM, the original data should be videos with dimension: [batch_size, max_time_step, high, width,channel]. But i failed to feed the data to dynamic_rnn.
I get such error:

what should i do to use dynamic rnn?
version: tf 1.0"
149,34088,1,"metrics = ['accuracy'] and metrics = [tf.metrics.Accuracy()] produces different results. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v2.0.0-rc2-26-g64c3d38 2.0.0
- **Python version**: Python 3.6.8
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Declaring metrics as a list of tensorflow.python.keras.metrics.BinaryAccuracy or tensorflow.python.keras.metrics.Accuracy produces an error. Declaring the metrics as a string list works as expected.

### Source code / logs

Below is the source that can reproduce the issue.



### Output using metrics=['accuracy']


### Output using metrics = [tf.metrics.Accuracy()]



"
181,22364,1,"Estimator with MirroredStrategy on multiple GPUs seems to start with non-initialized weights on one of the GPUs. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: slightly modified example code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: ('v1.10.1-0-g4dcfddc5d1', '1.10.1')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.0/7.1.4
- **GPU model and memory**: GeForce GTX 1080
- **Exact command to reproduce**:

Slightly modified version of https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-estimator-api to add some print nodes.



The output I get is the following:

    WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpygBKCI
    INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3bd6649b90>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpygBKCI', '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f3bd6649b10>, '_save_summary_steps': 100}
    2018-09-18 23:44:42.051140: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    2018-09-18 23:44:42.244608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
    name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
    pciBusID: 0000:08:00.0
    totalMemory: 10.92GiB freeMemory: 9.93GiB
    2018-09-18 23:44:42.392404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2018-09-18 23:44:42.394060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties:
    name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
    pciBusID: 0000:42:00.0
    totalMemory: 10.92GiB freeMemory: 10.76GiB
    2018-09-18 23:44:42.398325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1
    2018-09-18 23:44:42.808254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-09-18 23:44:42.808317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1
    2018-09-18 23:44:42.808325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y
    2018-09-18 23:44:42.808331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N
    2018-09-18 23:44:42.808726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with 9603 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
    2018-09-18 23:44:42.887416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)
    INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
    INFO:tensorflow:Configured nccl all-reduce.
    2018-09-18 23:44:42.999232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1
    2018-09-18 23:44:42.999351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-09-18 23:44:42.999364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1
    2018-09-18 23:44:42.999371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y
    2018-09-18 23:44:42.999379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N
    2018-09-18 23:44:42.999591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9603 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
    2018-09-18 23:44:42.999726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)
    INFO:tensorflow:Calling model_fn.
    INFO:tensorflow:Calling model_fn.
    INFO:tensorflow:batch_all_reduce invoked for batches size = 2 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Create CheckpointSaverHook.
    INFO:tensorflow:Graph was finalized.
    2018-09-18 23:44:43.215770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1
    2018-09-18 23:44:43.215877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-09-18 23:44:43.215889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1
    2018-09-18 23:44:43.215897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y
    2018-09-18 23:44:43.215904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N
    2018-09-18 23:44:43.216104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9603 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
    2018-09-18 23:44:43.216251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.
    INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpygBKCI/model.ckpt.
    features = [[1]]
    logits = [[0]]
    features = [[1]]
    logits = [[-0.225562453]]
    INFO:tensorflow:loss = 1.2510016, step = 0
    features = [[1]]
    logits = [[0]]
    features = [[1]]
    logits = [[0.6646626]]
    features = [[1]]
    features = [[1]]
    logits = [[-5.94375372]]
    logits = [[1.19879758]]
    features = [[1]]
    logits = [[-23.9573383]]
    features = [[1]]
    logits = [[3.89678]]
    features = [[1]]
    features = [[1]]
    logits = [[-82.8739166]]
    logits = [[12.7210035]]
    INFO:tensorflow:Saving checkpoints for 5 into /tmp/tmpygBKCI/model.ckpt.
    INFO:tensorflow:Loss for final step: 3586.108.
    INFO:tensorflow:Calling model_fn.
    INFO:tensorflow:Done calling model_fn.
    INFO:tensorflow:Starting evaluation at 2018-09-18-23:44:45
    INFO:tensorflow:Graph was finalized.
    2018-09-18 23:44:45.322872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1
    2018-09-18 23:44:45.322972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
    2018-09-18 23:44:45.322984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1
    2018-09-18 23:44:45.322992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y
    2018-09-18 23:44:45.323000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N
    2018-09-18 23:44:45.323190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9603 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1)
    2018-09-18 23:44:45.323319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)
    INFO:tensorflow:Restoring parameters from /tmp/tmpygBKCI/model.ckpt-5
    INFO:tensorflow:Running local_init_op.
    INFO:tensorflow:Done running local_init_op.

Notice that the logits comes out printed as zero the first and third time, which suggests the weights for at least one of the GPU towers is all zeroes at the start (or both are all zeros initially but the prints are showing up interwoven between the two towers).

If I run the code with just a single GPU using OneDeviceStrategy, this does not happen (first logit value printed out is non-zero due to random initial weights).

This affects training for more complicated scenarios since it could lead to incorrect loss computation.

Still happens if I upgrade to TensorFlow 1.11.0-rc1 ('v1.11.0-rc1-0-ge4c4b20805').

I posted on [StackOverflow](https://stackoverflow.com/questions/52396116/tensorflow-estimator-with-mirroredstrategy-on-multiple-gpus-seems-to-start-with) but also decided to open this issue since it seems somewhat similar to the variable initialization issue reported in #19069 which is supposedly fixed. If this is more appropriate on StackOverflow, I can wait for responses there. Thanks!
"
186,4968,1,"TensorFlow produces arbitrary results without error when using large amounts of GPU memory. ### Environment info

Operating System: Docker container based on  running on CentOS

If installed from binary pip package, provide:
1. A link to the pip package you installed: 
2. The output from .


### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)



Executed using 
#### Expected output:


#### Actual output:


### What other attempted solutions have you tried?

I've verified that this problem exists on at least one other GPU (I've seen it on Nvidia GPUs K40m and K80).

I've run similar code in Theano without any problems (same GPU, same Docker container):



Executed using 
#### Output


"
634,3067,0,"reshape on bad data: ""segment_ids[0] = -1 is out of range"". This piece of code fails in  inside  op. It works just fine for a few batches and then fails on one specific batch of data: it just so happened (bug in my code) that the length of one line of the input of rnn turned out to be zero (and  param in  was also zero), and I was passing the resulting tensor to a reshape, so it gave me this error:



here's the piece of code on which it fails:



just wondering if error message could be less obscure.
"
301,10523,0,"I update libprotobuf but tendoflow still use the old version. i update libprotobuf 

>    ldconfig -p
>         libprotobuf.so.13 (libc6,x86-64) => /usr/local/lib/libprotobuf.so.13
>         libprotobuf.so.10 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libprotobuf.so.10
>         libprotobuf.so (libc6,x86-64) => /usr/local/lib/libprotobuf.so
>         libprotobuf-lite.so.13 (libc6,x86-64) => /usr/local/lib/libprotobuf-lite.so.13
>         libprotobuf-lite.so.10 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libprotobuf-lite.so.10
>         libprotobuf-lite.so (libc6,x86-64) => /usr/local/lib/libprotobuf-lite.so


and it worked 
     

>   protoc --version
>           libprotoc 3.3.0

but tensorflow still use uses the 3.0.0. version

> 
>   [libprotobuf FATAL google/protobuf/stubs/common.cc:67] This program requires version 3.3.0 of the   Protocol Buffer runtime library, but the installed version is 3.0.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""google/protobuf/descriptor.pb.cc"".)
 terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  This program requires version 3.3.0 of the Protocol Buffer runtime library, but the installed version is 3.0.0.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""google/protobuf/descriptor.pb.cc"".)

can someone help me
 "
1353,5786,0,"Gradients and variables was not shared in Adam optimizers when using bucketing. All,

I used bucketing-like technology for seq2seq task:


And get shared model's parameters:




Model's optimizer is like below:


But I find that the optimizers don't share gradient:

With the growth of the number of buckets, the GPU memory will grow too. And meanwhile I get a larger model in tf.train.Saver.save().

So is it possible to share gradient in bucketing?





"
209,12689,1,"multi-GPU training too slow when L2 regularizer enabled . ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 home edition
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.3.0
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
Cuda 8.0 / cudnn 6.0
- **GPU model and memory**:
Nvidia 1080 GTX 8GB x 2
- **Exact command to reproduce**:

### Describe the problem
I try to train a face recognition classifier using inception-resnet-v1 model with 2 GPUs. On single GPU the training proceeds just fine, with a processing capacity around 220 images/sec, but when I train with 2 GPUs I only observe marginal benefit (capacity increases to around 280 images/sec). After some profiling I found out that the poor performance was somehow due to the introduced L2 regularizer. When the regularizer is enabled, the computation of the gradient becomes unexpectedly slow, resulting in the slowdown of the entire training cycle. As a comparison, if the regularizer is disabled, I could obtain a processing capacity around 350 images/sec, which although not perfect, is more or less satisfactory. The boost in the performance in the latter scenario cannot be attributed to the reduced computation complexity from the removal of the regularizer. This is evidenced by a reference experiment with exactly the same parameter except for on a single GPU, see below. 

I cannot figure out an explanation for this. It took me several days to narrow down the problem to, seemingly,  the introduction of L2 regularizer and the computation of gradient.

In my program I used standard tf.slim layers together with a downloaded inception-resnet-v1 model script.  The main part of the code is the following:


where  is defined as follows:



The whole script (contains codes to generate synthetic data) can be found [here](https://github.com/TianwenWei/TianwenWei/blob/master/issue.py). The inception-resnet-v1 model script can be found [here](https://github.com/TianwenWei/TianwenWei/blob/master/inception_resnet_modified.py).

To reproduce the problem, just run 

The output would be something like

In contrast, with the presence of L2 regularizer, running

one would obtain


As a reference, with single GPU, the difference of processing capacity between with and without regularizer is negligeable, at least in this demo. To check it, run

gives

and

gives


"
910,10253,0,"label_keys type error on DNNCLassifier Tensorflow. I got the following error when I tried to embed an array of label_keys of type string into a DNNClassifier. At the first attempt I folowed the official documentation for instantiating a DNNClassifier.



Here is the ""guilty"" piece of my code:





On the other hand, if I make the  column a 


I will get the following error:




"
46,23641,1,"Calculation precision issue . <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows with Tensorflow Docker 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12
- Python version:2.7
- Bazel version (if compiling from source):na
- GCC/Compiler version (if compiling from source):na
- CUDA/cuDNN version:9, 7.3.1
- GPU model and memory:2080,8G 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
import tensorflow as tf

def compute_area(sides):
  a = sides[:,0]  # 5.0, 2.3
  b = sides[:,1]  # 3.0, 4.1
  c = sides[:,2]  # 7.1, 4.8
  
  s = (a + b + c) * 0.5   # (a + b) is a short-cut to tf.add(a, b)
  areasq = s * (s - a) * (s - b) * (s - c) # (a * b) is a short-cut to tf.multiply(a, b), not tf.matmul(a, b)
  return tf.sqrt(areasq)

with tf.Session() as sess:
  area = compute_area(tf.constant([
      [3.000000001, 4.0, 5.0],
    ],dtype='float64'))
  result = sess.run(area)
  print(result)  

[6.]

**Describe the expected behavior**
import numpy as np

s = (3.000000001 + 4 + 5) * 0.5
areasq = s * (s - 3.000000001) * (s - 4) * (s - 5)

print(np.sqrt(areasq))

6.000000001999999

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Attached as above 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

N/A 
"
435,629,0,"How to print or see the value of Tensor? . Variable
Tensor
...
"
672,1733,0,"rnn.dynamic_rnn() causes gradients graph building error. I found rnn.dynamic_rnn() that seems to do what I want, but when I modified the following lines 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L114-L116 into 



It throws out error at the line containing 



I couldn't find any examples in the repo of using dynamic_rnn(), and hope someone could point out where it went wrong. The complete file is attached below (adapted based on ). LOOP_VERSION=0 and 1 run all right, 2 raises the issue.  


"
1269,485,0,"Backport for moved models.rnn.linear doesn't work. In v0.6 linear got moved into , but backport here  points to .

This break code with v0.5.
"
899,4432,0,"wrong tf.control_dependencies using tf.case  and tf.cond. add tf.control_dependencies in one branch of tf.case infulunce the other brach:
tf version: rc10

as we can see below, the tfvar is increasing no mather brach f1 or f2 is executed
change tf.case to tf.cond will get the same output.


isTraining=tf.placeholder(tf.bool,shape=[])
tfvar=tf.Variable(tf.constant(0),tf.int32);

increase_tfvar_op=tf.assign_add(tfvar.ref(), 1);

sess=tf.Session()
sess.run(tf.initialize_all_variables())

def f1():
    print ('f1')
    with tf.control_dependencies([increase_tfvar_op]):
        return -tfvar;  #if return tfvar directly, no control_dependencies is added

def f2():
    print('f2')
    with tf.control_dependencies([]):
        return tfvar*10;
caseResult = tf.case([(isTraining, f1)], default=f2)

b=True;
for t in range(4):
    b=not b;
    print('\n------')
    print('isTraining: ',end='')
    print(b)
    beforeCase=sess.run(tfvar)
    print('\ttfvar before run case: %d'%beforeCase)
    r=sess.run(caseResult,feed_dict={isTraining:b})
    print('case result:%d'%r)
    aftercase=sess.run(tfvar)
    print('\ttfvar after run case: %d'%aftercase)

output:
f2
f2
f1

---

isTraining: False
    tfvar before run case: 0
case result:10
    tfvar after run case: 1

---

isTraining: True
    tfvar before run case: 1
case result:-2
    tfvar after run case: 2

---

isTraining: False
    tfvar before run case: 2
case result:30
    tfvar after run case: 3

---

isTraining: True
    tfvar before run case: 3
case result:-4
    tfvar after run case: 4
"
913,28403,0,"dataset in tf2.0 lack of key properties and methods which already in tf1.13. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below):2.0 alpha
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
dataset in tf2.0 lack of key properties and methods which already in tf1.13, for example:
properties: output_shapes, output_types
methods: make_one_shot_iterator

**Describe the expected behavior**
these key properties and methods should in dataset of tf2.0

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
560,32918,0,"Tensorflow 2.0 custom_scalar plugin for tensorboard. To set layout for custom_layout plugin in tensorflow 1.x you should use method add_summary in FileWriter class. So the code should look like this:



In tensorflow 2 add_summary method was removed. How can I set custom_scalar layout now? Is it a bug or a desired API change?"
1307,27935,0,"ModuleNotFoundError: No module named 'tensorflow'  in spyder | windows + anaconda. **System information**
- OS Platform and Distribution : Windows 10 64-bits
- TensorFlow installed from (source or binary): Python 3.6 CPU-only | https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl
- TensorFlow version: 1.13.1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: conda


**Describe the problem**

Hi,
I have struggled for hours to install tensorflow to Anaconda and finally succeeded. Hope my experience can help those who have same problem with installation.

I have tried a lot of methods before, such as:

etc,.
These methods might act as it installed properly but when I tested it in spyder, there were some problems, such as:

> ImportError: cannot import name 'abs'
> attributeerror: module 'tensorflow' has no attribute '__version__'

I  just installed tensorflow successfully in Anaconda following the instructions from: https://www.tensorflow.org/install/pip
2. Create a virtual environment (recommended)


Then I test it in terminal:


It works!
But when I tried to import tensorflow in spyder:
> ModuleNotFoundError: No module named 'tensorflow'


**Solution**

This problem might cause by using virtual environment, and in Anaconda, your spyder or Jupyter Notebook works in default root, but tensorflow is installed in an isolated virtual environment 'venv'. So just install a new spyder or Jupyter Notebook under the virtual enviroment.
- 1. Open Anaconda Navigation
![image](https://user-images.githubusercontent.com/15613656/56325165-c273ad80-6136-11e9-9438-ec8f8eb759c9.png)
- 2. Change 'Applications on' to the virtual environment you just created ('venv')
- 3. Install a new spyder under 'venv'
- 4. Run the new spyder and test
![image](https://user-images.githubusercontent.com/15613656/56325929-c228e180-6139-11e9-9135-9d00715e03bf.png)


"
854,256,0,"Transpose convolution layer for tensorflow (was deconvolution). Has anyone already started implementing a deconvolutional layer for tensorflow as it's used e. g. here https://github.com/stokasto/caffe ? Is anyone else interested in such a functionality or is there any trivial way to implement this using the existing tensors?
"
528,33365,0,"No float64 support with batch normalization in Tensorflow 2.0?. Stock Ubuntu 19.04 with Cuda 10.0, Tensorflow 2.0.0 installed via pip3, Python 3.7.3, GTX1060.

I have a float64 valued dataset with a simple conv2d network that includes tf.keras.layers.BatchNormalization() which is where the error is being thrown I think.

The first set of issues:
tf.keras.backend.set_floatx('float64')
So is there perhaps another (hopefully drop in) method of batch normalization that supports float64? I don't want to go hacking at allowed_list and all that."
1479,6806,0,"convert_graphdef_memmapped_format produces corrupt graphs. I tried memory mapping an exported/frozen graph. Let's say the graph is named . After converting the graph like this

    ./path/to/convert_graphdef_memmapped_format --in_graph=graph.pb --out_graph=graph_mmap.pb

I get

-  for an unquantized graph
-  for the 8bit quantized version of the graph

If I try to load any of these output files using the C++ API  and then  the graph the following errors occur

- Unquantized: 
- Quantized: 

so I guess the output file is a corrupt protobuf file (or a different version).
"
715,12807,0,"Speech-Recognitin issue. Hi,

While running  ""python tensorflow/examples/speech_commands/train.py""

I am getting following issue.::
""ImportError: cannot import name audio_ops""

Please help. 
"
350,4930,0,"Example MNIST_RNN not working. Hello everyone,

I use the very last docker container, GPU Enabled (nvidia-docker). 
Everything works fine for what I could have tested so far.

Host : Ubuntu 14.04 Server with CUDA and NVIDIA drivers up to date

Except, this example is not working anymore :
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/mnist_rnn.py

**AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'**

![screen shot 2016-10-13 at 01 41 19](https://cloud.githubusercontent.com/assets/10923599/19331714/2fac0baa-90e6-11e6-8027-5f68dd10b160.png)
"
418,3812,0,"Tensorflow distributed training question: Do we have to manually copy ckpt files from worker 0  to ps servers? . Operating System:
Linux XNLPEXP2 4.4.0-24-generic #43-Ubuntu SMP Wed Jun 8 19:27:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
Azure VM 8 cores, 56GB memory
If installed from binary pip package, provide:
pip 8.1.2 from /home/xubixiong/.local/lib/python2.7/site-packages (python 2.7)
0.10.0rc0

When I am trying to restore training from latest checkpoint, I found that I have to copy checkpoint files to ps servers, from worker 0. 

Is it the right and recommended way? Or I did something wrong? ? 
"
40,29778,1,"GPU usage increases after dozens of steps. Could you support gpu trace tools for us.@tensorflow
I cant find cause of this bug. 

OOM error after nearly 100 steps. batch_size=1, gpu usage from 11G to 23G+. GPU usage increase after each twenty-odd steps.
train_dataset reader is tf.data. 

tensorflow 1.13 
GPU titan rtx 24G
cuda 10.0 
"
918,30804,0,"tf.keras.layers.Conv2D fails because it captures tensor from inner function. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.1-6461-gc6352706d6 1.14.0
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: Nvidia Geforce GTX 1080 Ti, 11 GB

**Describe the current behavior**
Since commit  a  in my training code fails with the following error:



This error only occurs with  set to 2. With  the training script runs.
It also works if I revert .

**Describe the expected behavior**
The training runs with .

**Code to reproduce the issue**
So far I could not come up with a small testcase to reproduce the issue. I will continue to try, but until then I only have the backtrace and the problematic commit."
771,24509,0,"Error message after importing Tensorflow. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 7, 64
- 
- TensorFlow installed from source
- TensorFlow version:1.12
- Python version:3.6
- Installed using pip
- 

**Describe the problem**

Ran pip install tensorflow on Anaconda prompt, then wrote the following code on Jupiter notebook:

import tensorflow as tf

a = tf.Variable(1, name=""a"")
b = tf.Variable(2, name=""b"")
f = a + b

init = tf.global_variables_initializer()
with tf.Session() as s:
    init.run()
    print( f.eval() )

 got the following error:

ImportError: DLL load failed with error code -1073741795


Please help!
"
172,23882,1,"The same op seed gives different results in eager execution. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux openSUSE Leap 42.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.4
- Bazel version (if compiling from source): none
- GCC/Compiler version (if compiling from source): none
- CUDA/cuDNN version: none
- GPU model and memory: none


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**



prints



When run in a graph, the output is the same.

**Describe the expected behavior**

I would have expected the output to be same in eager mode as well, just like in graph mode.
"
718,9600,0,"Why the basic function of different rnn cells are different?. I'd like to tackle two issues. The first one is about the initializer. I can define initializer in  but not in  cell. The second one is . There is  available for using layer normalization on LSTM cell. But there is not other kinds of cell used in that way. I wonder why not make this general functions available for all kinds of cells?
"
1215,9400,0,"Tensorflow is still taking up all GPU memory despite allocation of memory. **System Information:**
- OS Platform: Linux Ubuntu 16.04
- TensorFlow installed from binary
- TensorFlow version: 1.0.1
- CUDA version: 8.0
- cuDNN version: 5.1

output for print(tf.GIT_VERSION, tf.VERSION):
('v1.0.0-65-g4763edf-dirty', '1.0.1')

**Problem:**
I have three codes running GPU-enabled TensorFlow. Currently, when the GPU is allocated for two of the processes, the GPU does get allocated. However, no matter how much GPU I allocate to the last process, it takes up all the GPU, disallowing other two processes to run simultaneously. 

**First process (the one with problem)**
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1060
major: 6 minor: 1 memoryClockRate (GHz) 1.6705
pciBusID 0000:01:00.0
Total memory: 5.93GiB
Free memory: 5.68GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)

**Second process:**
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1060
major: 6 minor: 1 memoryClockRate (GHz) 1.6705
pciBusID 0000:01:00.0
Total memory: 5.93GiB
Free memory: 237.00MiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.19G (1273049856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.07G (1145744896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 983.40M (1031170560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 885.06M (928053504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 796.55M (835248128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 716.90M (751723264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 645.21M (676550912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 580.69M (608896000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 522.62M (548006400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 470.36M (493205760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 423.32M (443885312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 380.99M (399496960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 342.89M (359547392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 308.60M (323592704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 277.74M (291233536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 249.97M (262110208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 224.97M (235899392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
WARNING:apscheduler.scheduler:Execution of job ""session (trigger: interval[0:00:01], next run at: 2017-04-24 11:24:48.702309)"" skipped: maximum number of running instances reached (1)
E tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
E tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) 

**The code I am using to allocate GPU for the first process:**
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))

I have also looked up stackoverflow but it does not seem to have the same problems raised.
Some pages I referred to:
http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory 
https://www.tensorflow.org/tutorials/using_gpu 
https://github.com/tensorflow/tensorflow/issues/398 "
1249,9187,0,"README.md has a deprecated API call. ### System Information
Tensorflow ('v1.0.0-65-g4763edf-dirty', '1.0.1')


### Describe the problem clearly
In README.md line  should be  (see [source code for vgg](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py)). 
"
125,33024,1,"Performance: Training is much slower in TF v2.0.0 VS v1.14.0 when using `Tf.Keras` and `model.fit_generator` . <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>


**System information**
**NOTE**: I have provided Google Colab' notebooks to reproduce the slowness. 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): sortof, but is a basically and MNIST example.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab and Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): 2.0.0
- Python version: 3
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10 or Google Colab
- GPU model and memory: 1080 Ti, or Google Colab

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
It happens on the standard Colab GPU instance

**Describe the current behavior**
**Version 2.0.0** is SLOW compared to the identical code running **v1.14.0**. 
The code I have used to demonstrate it is very simple, and very similar to most existing Keras examples. 
A Larger NN on MNIST is going from  per epoch to   and that is a very major slowdown.

**Describe the expected behavior**
A new version should have similar or better performance than the previous version. 
If user error or a new limitation/feature is causing the problem, it should be warned about in Update Notes/Quick Start. This code was perfectly normal in TF 1.X

**Code to reproduce the issue**
See this (GPU) Colab Notebook example with MNIST Data:
https://colab.research.google.com/gist/Raukk/f0927a5e2a357f2d80c9aeef1202e6ee/example_slow_tf2.ipynb

See this (GPU) Colab Notebook example with numpy random for Data:
https://colab.research.google.com/gist/Raukk/518d3d21e08ad02089429529bd6c67d4/simplified_example_slow_tf2.ipynb

See this (GPU) Colab Notebook example using standard Conv2D (not DepthwiseConv2D):
https://colab.research.google.com/gist/Raukk/4f102e192f47a6dc144b890925b652f8/standardconv_example_slow_tf2.ipynb


**Please notify me if you cannot access any of these notebooks, or if they do not run, or don't sufficiently reproduce the issue.**


**Other info / logs**
Each example above starts with a **TLDR;** that gives a very basic summary of results. 


Thank you!"
522,24828,0,"Error : Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source and Binary (tried both)
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): gcc 5.4.0
- CUDA/cuDNN version: Cudnn - 7.4 ,  CUDA- 9.0
- GPU model and memory: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225 8GB




**Describe the problem**
I tried installting tensorflow 1.12 using both pip install and building from source.However when I am trying to run faster rcnn model  i get following error message:
Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.

I only get this with tf 1.12 and python 3.6 ,it works fine with python 3.6


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Traceback (most recent call last):
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_4__cf__7)]]
	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_21/Gather/GatherV2_2/_211}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7500_...GatherV2_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py"", line 103, in worker
    initializer(*initargs)
  File ""detection_app.py"", line 67, in worker
    output_q.put(y.get_stats_and_detection(frame))
  File ""/home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py"", line 142, in get_stats_and_detection
    boxes, scores, classes, num = self.processFrame(img)
  File ""/home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py"", line 76, in processFrame
    feed_dict={self.image_tensor: image_np_expanded})
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D (defined at /home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py:36)  = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_4__cf__7)]]
	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_21/Gather/GatherV2_2/_211}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7500_...GatherV2_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]

Caused by op 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D', defined at:
  File ""detection_app.py"", line 94, in <module>
    pool = Pool(args.num_workers, worker, (input_q, output_q))
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/context.py"", line 119, in Pool
    context=self.get_context())
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py"", line 174, in __init__
    self._repopulate_pool()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py"", line 239, in _repopulate_pool
    w.start()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/context.py"", line 277, in _Popen
    return Popen(process_obj)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/popen_fork.py"", line 19, in __init__
    self._launch(process_obj)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/popen_fork.py"", line 73, in _launch
    code = process_obj._bootstrap()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py"", line 103, in worker
    initializer(*initargs)
  File ""detection_app.py"", line 62, in worker
    y = DetectorAPI()
  File ""/home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py"", line 36, in __init__
    tf.import_graph_def(od_graph_def, name='')
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3440, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3440, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3299, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D (defined at /home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py:36)  = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_4__cf__7)]]
	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_21/Gather/GatherV2_2/_211}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7500_...GatherV2_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]

"
687,9583,0,"TensorArray: Tried to write to index 18 but array is not resizeable and size is: 18. ### Problem
I wrote an application using TensorArray. It prints out error about allocate memory.
For each sample. It must create a lot of TensorArray to store temporary data.
With few number of samples (around 20), it passed smoothly 100 epochs.
When I trained with the whole dataset (10.000 samples), it have never passed epoch 6. The stopped epochs are varied.

### Logs
2017-05-02 10:29:25,886 CFG INFO [Epoch 0] Shuffling data...                                                                                                                                     
(0.058350346982479095, 0.76190478)                                                                                                                                                               
2017-05-02 10:29:31.197277: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Tried to read from index 23 but array size is: 23                                                   
2017-05-02 10:29:31.197277: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: TensorArray bu/dec_c_ta_4628: Tried to write to index 23 but array is not resizeable and size is: 23
Traceback (most recent call last):                                                                                                                                                               
  File ""/work/vietld/py27/lib/python2.7/runpy.py"", line 174, in _run_module_as_main                                                                                                              
  File ""/work/vietld/py27/lib/python2.7/runpy.py"", line 72, in _run_code                                                                                                                         
    exec code in run_globals                                                                                                                                                                     
  File ""/home/s1610204/tree-lstm/py/run.py"", line 45, in <module>                                                                                                                                
    train(args)                                                                                                                                                                                  
  File ""/home/s1610204/tree-lstm/py/run.py"", line 29, in train                                                                                                                                   
    loss, acc = treelstm.train(session, train_data)                                                                                                                                              
  File ""py/lstmtree.py"", line 175, in train                                                                                                                                                      
    loss, acc, _ = session.run([self.full_loss, self.acc, self.train_op], feed_dict)                                                                                                             
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 778, in run                                                                                     
    run_metadata_ptr)                                                                                                                                                                            
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 982, in _run                                                                                    
    feed_dict_string, options, run_metadata)                                                                                                                                                     
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1032, in _do_run                                                                                
    target_list, options, run_metadata)                                                                                                                                                          
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1052, in _do_call                                                                               
    raise type(e)(node_def, op, message)                                                                                                                                                         
tensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray bu/dec_c_ta_4628: Tried to write to index 23 but array is not resizeable and size is: 23                               
         [[Node: bu/while_decode/dec/write_dec_c_ta/write_dec_c_ta = TensorArrayWriteV3[T=DT_FLOAT, _class=[""loc:@bu/dec_c_ta""], _device=""/job:localhost/replica:0/task:0/cpu:0""](bu/while_decode
/dec/cond_1/gather_dec_c_ta/Enter, bu/while_decode/Identity_3, bu/while_decode/dec/LSTM/add_6, bu/while_decode/Identity_1)]]                                                                     
                                                                                                                                                                                                 
Caused by op u'bu/while_decode/dec/write_dec_c_ta/write_dec_c_ta', defined at:                                                                                                                   
  File ""/work/vietld/py27/lib/python2.7/runpy.py"", line 174, in _run_module_as_mai ""__main__"", fname, loader, pkg_name)                                                                                                              
                                                                                                                                                         
  File ""/work/vietld/py27/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals                                                                                                                                                                                                                                                                                     
  File ""/home/s1610204/tree-lstm/py/run.py"", line 45, in <module>                                                                                                                                
    train(args)                                                                                                                                                                                  
  File ""/home/s1610204/tree-lstm/py/run.py"", line 20, in train                                                                                                                                   
    treelstm = RecursiveLSTM(config)                                                                                                                                                             
  File ""py/lstmtree.py"", line 137, in __init__                                                                                                                                                   
    name='while_decode')                                                                                                                                                                         
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2623, in while_loop                                                                       
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)                                                                                                                          
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2456, in BuildLoop                                                                        
    pred, body, original_loop_vars, loop_vars, shape_invariants)                                                                                                                                 
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2406, in _BuildLoop                                                                       
    body_result = body(*packed_vars_for_body)                                                                                                                                                    
  File ""py/lstmtree.py"", line 115, in decode_body                                                                                                                                                
    r_dec_c_ta = dec_c_ta.write(i, c, name='write_dec_c_ta')                                                                                                                                     
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 279, in write                                                                             
    name=name)                                                                                                                                                                                   
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 2823, in _tensor_array_write_v3                                                          
    name=name)                                                                                                                                                                                   
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op                                                                      
    op_def=op_def)                                                                                                                                                                               
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op                                                                               
    original_op=self._default_original_op, op_def=op_def)                                                                                                                                        
  File ""/work/vietld/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__                                                                                
    self._traceback = _extract_stack()                                                                                                                                                           
                                                                                                                                                                                                 
InvalidArgumentError (see above for traceback): TensorArray bu/dec_c_ta_4628: Tried to write to index 23 but array is not resizeable and size is: 23                                             
         [[Node: bu/while_decode/dec/write_dec_c_ta/write_dec_c_ta = TensorArrayWriteV3[T=DT_FLOAT, _class=[""loc:@bu/dec_c_ta""], _device=""/job:localhost/replica:0/task:0/cpu:0""](bu/while_decode
/dec/cond_1/gather_dec_c_ta/Enter, bu/while_decode/Identity_3, bu/while_decode/dec/LSTM/add_6, bu/while_decode/Identity_1)]]      

------------------------
### System information
- **Operating system**: Altix-UV 3000, SUSE Enterprise Server 12 SP2
- **TensorFlow installed from**: binary
- **TensorFlow version**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: No GPU, 256GB RAM
- **Exact command to reproduce**:

SYSTEM ENVIRONMENT: == cat /etc/issue ===============================================
Linux altix-uv 3.12.62-60.64.8-default #1 SMP Tue Oct 18 12:21:38 UTC 2016 (42e0a66) x86_64 x86_64 x86_64 GNU/Linux
VERSION = 12
VERSION=""12-SP1""
VERSION_ID=""12.1""

== are we in docker =============================================
No

== compiler =====================================================
c++ (SUSE Linux) 4.8.5
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux altix-uv 3.12.62-60.64.8-default #1 SMP Tue Oct 18 12:21:38 UTC 2016 (42e0a66) x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.1)
protobuf (3.2.0)
tensorflow (1.1.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
2017-05-02 10:42:09.139127: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-02 10:42:09.139314: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-02 10:42:09.139322: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-02 10:42:09.139328: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-02 10:42:09.139334: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sched_getaffinity: Invalid argument
can't determine number of CPU cores: assuming 4
sched_getaffinity: Invalid argument
can't determine number of CPU cores: assuming 4
tf.VERSION = 1.1.0
tf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5
tf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /opt/cuda/8.0/lib64:/opt/cuda/8.0/lib:/work/vietld/cuda/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 77: nvidia-smi: command not found

== cuda libs  ===================================================

### Source code / logs
I dont know why github cant attach this .zip file
Please download it from https://drive.google.com/open?id=0BxQsywFyW2C7UUVSeHBHZ25mWXM
"
1117,4505,0,"Pack error for SparseTensor. Hello, 
I'm using the git version of tensorflow (a6c5f8e4e013e54fed8dfcf49fb6de365f018022) without CUDA
I'm on Ubuntu 16.04

Does SparseTensor are considered as Tensor, because when i try to pack some of them, there is an error of conversion.



NB: the filter_op() method product SparseTensor 
"
7,28685,1,"The cycle detection algorithm in the variable creation has bad performance. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Don’t know
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- Bazel version (if compiling from source): Not used
- GCC/Compiler version (if compiling from source): Not used
- CUDA/cuDNN version: Not related
- GPU model and memory: Not related

**Describe the current behavior**

Maybe related: #17439.

I noticed some slow variable creations. It happens when the initial value is a tensor with complex dependencies.  After some digging, I found that it may be caused by the algorithm used in the cycle detection code:

https://github.com/tensorflow/tensorflow/blob/d1022145203c1edb81a39010da3f61207533091d/tensorflow/python/ops/variables.py#L2505-L2517

**Describe the expected behavior**

Creating a variable should be completed within acceptable time.

**Code to reproduce the issue**


You can reproduce the problem using the following code:



**Other info / logs**

Here is my output from running the code:



Notice the time used for creating a variable grows exponentially.

----

The cycle detection algorithm could be optimized to have a linear time complexity. Also, the algorithm should avoid stack overflow if the initial value has a long dependency chain."
735,22481,0,"Inconsistent behavior in tf.contrib.distributions.percentile for NaN values. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0
- **Python version**: 3.6.6 (Anaconda)
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
[](https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/percentile) behaves inconsistently when NaN values are given. While the documentation does not explicitly states how this case should be handled, one would normally expect to always receive NaN, or maybe ignore them in the computation (compute the percentile of non-NaN values), or even replace them with 0. But the current logic seems to somehow depend on the order of the values in the input: trying to compute the median of  results in , while for  it is .

The behavior seems to be different also between CPU and GPU. I obtained the results above while running on CPU. The same test on GPU (CUDA 9.0, CUDNN 7, Titan V) produced  in both cases; however, the computed median on GPU for  was again .

### Source code / logs

"
13,26132,1,"Tensorflow hang when specify 'nccl/xring' as all reduce alg . **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13.0-rc0
- Python version:2.7.12
- Bazel version (if compiling from source):0.20.0
- GCC/Compiler version (if compiling from source):5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)
- CUDA/cuDNN version:CUDA version:9.0.176 and cuDNN version:7.2.1.38
- GPU model and memory:Tesla V100 , 32480MiB

**Describe the current behavior**
I train NeuMF model in distribute environment, and I find that TensorFlow hang all the time when training finish.   
Below is some detail about distributed training:
Model : 
Dataset : 
Num_worker : 
Num_gpu_each_worker: 
Strategy :  
Reduce_alg : replace  with 
Nccl version : 

The distributed training run 120 steps then tensorflow quit when use default all reduce alg  of . And when replace default all reduce alg with  , distributed training run 120 steps, but TensorFlow hang all the time. Below is screen output of two conditions:  
log when normal exit:

log when hang:
 
Compare two log,  TensorFlow do not continue to save checkpoints and output final loss when use  all reduce alg. How to solve this problem, thanks.

**Code to reproduce the issue**
I get NeuMF model source code from official models repo, and original code only support local training with estimator, and I add below code to support distributed training:  


The  implement of   is   which is defined in , I change value of   parameter of   of class  from  to .
  
**Other info / logs**
When hang, stack of two worker is:


"
1397,26049,0,"official/utils/misc/distribution_utils.py uses the wrong API for OneDeviceStrategy resulting in runtime python error. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): b'unknown' 1.13.0-rc2
- Python version: Python 3.7.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Receive an attribute not found error on tf.distribute.OneDeviceStrategy(""device:CPU:0"")
**Describe the expected behavior**
Not see this error.
**Code to reproduce the issue**
Run cifar10_main.py on a macbook without an NVIDIA GPU, I imagine this would also happen on linux if no GPU was detected.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1356,16631,0,"Allow variable_overwrites on scope level. This is a request for allowing to pass a dict of  to variable scopes which to be returned when  is called instead of the usual procedure, if they are provided, otherwise do the usual procedure. A simple example of this beahviour is:

This is particularly usefull for being able easily to bootstrap neural network parameters coming from inside the layers trough a standard function interface. My specific usage is for HMC for NN parameters. This is a question on whether you guys are interested in this so that I spend more time on doing this properly."
1350,5987,0,"Request for documentation on recommended flow in slim for train, validation, and test sets. The examples in the [slim README.md](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) give basic documentation for training and evaluating models when used separately; however, there is guidance missing on how to do the classic cycle of mini-batch gradient descent using shuffled subsets of the training set, periodically evaluating validation set, and then evaluating on the test set post-training. 

Using the [MNIST tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/mnist/tf/index.html) and [this tutorial](https://github.com/mnuke/tf-slim-mnist) for reference, the best I came up with was something like this where I'm effectively monkey-patching the train_step_fn to periodically output accuracies: 



**Note**: one problem with this implementation is that the final test set is not guaranteed to be run in the case of early exit.
 
I've posted in the slack channel and Googled around, but haven't been able to find any examples for this basic use case. Accordingly, I would like to propose that an example providing the best practice to periodically evaluate batch trained models using the validate set and the trained model against the test set to be added to the slim README.md.

I think it would really help the community to have a clearer idea on the intentions of the slim team on how the batch training and evaluation paths were designed to be used together during and after training.
"
806,34900,0,"[TF2.0] can't model.save(save_format='h5') NotImplementedError. I cant use  like this link
https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model
python=3.7.5
tensorflow-gpu=2.0.0
transformers=2.2.1







But I did have InputLayer





I have no choice to use 




But when I loaded it,  I have to add an inputlayer and I losed my model layers structures.



Is it a bug?

Is it possible to use  or 
and still keep all layers of my model like this link?
https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model"
536,8858,0,"Machine restarts when running TensorFlow with GPU. A simple Python program which runs a few TensorFlow computations consequently crashes when running on GPU.

Code:



It should run the same computation 10 times, recreating a graph and a session every time. 
Works fine when I run it on CPU. When running on GPU, it fails on running computation for 2nd, 3rd or 4th session.

Console output:



Then the machine just restarts.
There are no relevant messages in syslog before the restart.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/39122984/system-auto-reboot-when-tensorflow-model-is-too-large
http://stackoverflow.com/questions/41237115/computer-restarts-with-large-mini-batches-in-tensorflow

When running other TensorFlow programs, I noticed that sometimes such crashes happen when I use large tensors. Issues above seem to be related, at least symptoms are similar.


### Environment info
GPU: GeForce GTX 980 Ti
Operating System: Ubuntu 16.04.2 LTS

Installed version of CUDA and cuDNN: CUDA 8.0.61, cuDNN 7.5
Output of :



TensorFlow:
1. ""pip install tensorflow-gpu"". Version 1.0.1
2. The output from :





### What other attempted solutions have you tried?

Tried to reinstall Ubuntu/CUDA/cuDNN/TensorFlow, didn't help.

"
962,30250,0,"How to maintain the FIFOQueue when training. Hi, I want to use FIFOQueue to keep K elements in training process.

the loss value can be see as the sum of all elements in Queue and the feed_dict is the element in the Queue..
I found when I use the code sess.run(model.enqueue_op, feed_dict=feed_dict), I think the loss value should be double. But the result is not. The result of with or without this line are same.

So can you help me explain it?

Thanks you!"
450,21342,0,"install TF using Anaconda. Hi

Using ""Use pip in Anaconda"" installation method to install TF on ubuntu 16.04 system with Intel Xeon E5645 CPU, I test installation which give a error message ""Illegal instruction (core dumped)"". **Why?**

But when I use conda in Anaconda to install TF directly (""conda install tensorflow""), I can test installation successfully. **Can this installation method install TF-CPU and TF-GPU versions at the same time？**

Alan"
117,35060,1,"Increasing predict time every iteration of loop. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 12.04
- TensorFlow version (use command below): conda install tensorflow-gpu=1.14.0 
- Python version: 3.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla k80

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
model predict time increases for every tensor. 

Using a resnet 32 layer model. No modifications except for input shape.

Call initializer instance with the dtype argument instead of passing it to the constructor
--- 2.172 seconds ---
1
--- 0.3469 seconds ---
1
--- 0.3441 seconds ---
1
--- 0.3465 seconds ---
1
--- 0.3543 seconds ---
1
--- 0.3583 seconds ---
1
--- 0.3638 seconds ---
1
--- 0.3676 seconds ---
1
--- 0.3727 seconds ---
1
--- 0.3779 seconds ---
1
--- 0.3838 seconds ---
1
--- 0.3918 seconds ---
0
--- 0.3958 seconds ---
0
--- 0.4069 seconds ---
0
--- 0.4084 seconds ---
0
--- 0.4144 seconds ---
0
--- 0.4143 seconds ---
0
--- 0.4173 seconds ---
0
--- 0.4180 seconds ---
0
--- 0.4244 seconds ---
0
--- 0.4355 seconds ---
0
--- 0.4313 seconds ---
0
--- 0.4429 seconds ---
0
--- 0.4446 seconds ---
0
--- 0.4435 seconds ---
0
--- 0.4547 seconds ---
0
--- 0.4513 seconds ---
0
--- 0.4641 seconds ---
0
--- 0.4697 seconds ---
0
--- 0.4717 seconds ---
0
--- 0.4737 seconds ---
0
--- 0.4794 seconds ---
0
--- 0.4816 seconds ---
0
--- 0.4845 seconds ---
0
--- 0.4906 seconds ---
0
--- 0.4923 seconds ---
0
--- 0.5027 seconds ---
0
--- 0.5028 seconds ---
0
--- 0.5088 seconds ---
0
--- 0.5185 seconds ---
0
--- 0.5173 seconds ---
0
--- 0.5223 seconds ---
0
--- 0.5292 seconds ---
0
--- 0.5280 seconds ---
0
--- 0.5391 seconds ---
0
--- 0.5429 seconds ---
0
--- 0.5413 seconds ---
0
--- 0.5503 seconds ---
0
--- 0.5559 seconds ---
1
--- 0.5666 seconds ---
1
--- 0.5622 seconds ---
1
--- 0.5678 seconds ---
0
--- 0.5780 seconds ---
0
--- 0.5866 seconds ---
1
--- 0.5863 seconds ---


**Describe the expected behavior**
constant predict times

** code **

    
     


"
1143,30521,0,"Loading Keras model: TypeError: __init__() missing 1 required positional argument: 'fn'. - Installed with , and also tried with  which yielded the same issue.
- tensorflow 1.14
- Keras-Applications 1.0.8 
- Keras-Preprocessing 1.1.0

After saving model with , we try to load with  and it yields the error that I reported in the title.
What possible conflict of packages may I have? Or is this a known bug?
"
1272,4964,0,"Where is the documentation for contrib/layers? (Docs in general ...). In another thread I asked about consolidation of the builder API's for TensorFlow. (https://github.com/tensorflow/tensorflow/issues/3771)

The response was that contrib/layers was going to be one of the official builder API's to be included in TensorFlow core along with contrib/learn. So I wanted to see how it works and learn how to use it. But the only documentation I could find for contrib/layers was this:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/README.md

https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers.html

Is this really all the documentation there is for contrib/layers? Surely there must be more? You couldn't possibly expect people to be able to learn how to use it from these two documents alone? The README doesn't even make any sense. Am I missing something?

Please allow me a short rant.

I'm sure you're familiar with scikit-learn's beautifully designed API and extensive, polished documentation. It might serve as a good inspiration for TensorFlow. You may think that you don't have time to streamline the TensorFlow API and improve the documentation because there's more important things that must be done. But I believe this is actually the single most important thing you could do to move the project forward. Here's why:

If each new person wastes 10 hours trying to learn TensorFlow and there's 10,000 people who are learning to use it, then it's 100,000 wasted developer-hours! I actually think those numbers are very conservative. Personally I've probably wasted more than 100 hours trying to figure out how the complicated and poorly documented TensorFlow API works, and it seems there's many more than 10,000 people using TensorFlow. So maybe it's more than 1 million wasted developer-hours in total! That is 500 developer-years (assuming a month is about 160 work-hours)! Imagine if this developer-time was put into more constructive use. All it takes is for the TensorFlow API and docs to be more polished so it would be easier to learn. It would be a tiny investment in time and effort from the dev-team, compared to the return you'll get in productivity from the community. I really wish this is something you would prioritize highly.

Thanks!
"
1159,6717,0,"Incorrect gradient for categorical distribution entropy. The **Categorical** distribution class provides an awesome **entropy** operator but apparently the **gradient** calculation w.r.t. the input operators **doesn't work**.



In the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn't work when I try to maximize this entropy using any optimizer.
"
29,3638,1,"tf.gather produces zeros for invalid indices on GPU. ### Environment info

Operating System: Ubuntu 16.04 LTS (64 bit)






### Steps to reproduce


"
779,29436,0,"Error using deepbinner:  ImportError: libcuda.so.1: cannot open shared object file: No such   file or directory   Failed to load the native TensorFlow runtime.. Hello.
I'm trying to use deepbinner software for demultiplexing native barcodes that separate by patients after MinION sequencing. I've obtained this error. Has anyone got any idea about what's happening? Thank you!


"
928,22273,0,"failed to build tensorflow android. Using : 
---------
OS: Ubuntu 16.04
Bazel : 0.8.1
tensorflow : 1.5.0
NDK : 12b -> sdk level 14
sdk : 23
build tools: 25.2.0

The full build log of error:
**https://justpaste.it/621im**
tried the follwing solutions but still getting the error:
**[](**
https://github.com/tensorflow/tensorflow/issues/8641)
**[](**
https://github.com/tensorflow/tensorflow/issues/6356)

Please help me out.

Below is the shot form of Error:

<command-line>:0:0: note: this is the location of the previous definition
ERROR: /home/kv/Desktop/Work/AR/TensorFlow/Source/Tensorflow_1.3/tensorflow-master/tensorflow/contrib/android/BUILD:29:1: C++ compilation of rule '//tensorflow/contrib/android:android_tensorflow_inference_jni' failed (Exit 1): arm-linux-androideabi-gcc failed: error executing command 
  (cd /home/kv/.cache/bazel/_bazel_kv/9644bdd546e0ad6ab4fab8f0ed146de9/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/kv/anaconda3/bin:/home/kv/bin:/home/kv/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/kv/anaconda3/bin/python \
    PYTHON_LIB_PATH=/home/kv/anaconda3/lib/python3.6/site-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -MD -MF bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/contrib/android/_objs/android_tensorflow_inference_jni/tensorflow/contrib/android/asset_manager_filesystem.d '-frandom-seed=bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/contrib/android/_objs/android_tensorflow_inference_jni/tensorflow/contrib/android/asset_manager_filesystem.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/armeabi-v7a-py3-opt/genfiles -iquote external/protobuf_archive -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/nsync -iquote external/fft2d -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/fft2d -iquote external/gemmlowp -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/gemmlowp -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-v7a-py3-opt/genfiles/external/protobuf_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/armeabi-v7a-py3-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/armeabi-v7a-py3-opt/genfiles/external/nsync/public -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-mfpu=neon' '-std=c++11' -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-12/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/contrib/android/asset_manager_filesystem.cc -o bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/contrib/android/_objs/android_tensorflow_inference_jni/tensorflow/contrib/android/asset_manager_filesystem.o)
tensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::{anonymous}::RandomAccessFileFromAsset::Read(tensorflow::uint64, size_t, tensorflow::StringPiece*, char*) const':
tensorflow/contrib/android/asset_manager_filesystem.cc:97:69: error: 'AAsset_seek64' was not declared in this scope
     off64_t new_offset = AAsset_seek64(asset.get(), offset, SEEK_SET);
                                                                     ^
tensorflow/contrib/android/asset_manager_filesystem.cc:98:52: error: 'AAsset_getLength64' was not declared in this scope
     off64_t length = AAsset_getLength64(asset.get());
                                                    ^
tensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::NewReadOnlyMemoryRegionFromFile(const string&, std::unique_ptr<tensorflow::ReadOnlyMemoryRegion>*)':
tensorflow/contrib/android/asset_manager_filesystem.cc:158:68: error: 'AAsset_openFileDescriptor64' was not declared in this scope
   int fd = AAsset_openFileDescriptor64(asset.get(), &start, &length);
                                                                    ^
tensorflow/contrib/android/asset_manager_filesystem.cc:173:44: error: 'AAsset_getLength64' was not declared in this scope
     length = AAsset_getLength64(asset.get());
                                            ^
tensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::GetFileSize(const string&, tensorflow::uint64*)':
tensorflow/contrib/android/asset_manager_filesystem.cc:214:38: error: 'AAsset_getLength64' was not declared in this scope
   *s = AAsset_getLength64(asset.get());
                                      ^
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
INFO: Elapsed time: 358.381s, Critical Path: 76.20s
FAILED: Build did NOT complete successfully















Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
129,27401,1,"longer latency after post-training quantization on NVIDIA Jetson TX2. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I modified the tutorial below to convert my customized models to print inference latency.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NVIDIA Jetson TX2
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
1.12.0
- Python version:
3.5.2
- CUDA/cuDNN version:
V9.0.252
- GPU model and memory:
GPU model: NVIDIA Pascal™, 256 CUDA cores
memory: 8 GB 128 bit LPDDR4

**Describe the problem**
I modified the tutorial below to convert three customized models to print inference latency.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb

In the 3-layer and 9-layer model,, the inference time of tflite model is shorter than the original one.
However, in resnet50 model, the inference time of tflite model is NOT shorter than the original one.
I summarized the time table and graph below.
![圖片](https://user-images.githubusercontent.com/40556694/55369939-36089e80-552b-11e9-925f-67ec4908b4f4.png)

I read the closed issue #23759:longer latency after post-training quantization. In that issue, the explanation of the longer latency is because the speed-up of integer arithmetic require special/optimized instructions/kernels, while such optimizations are not done on desktop CPU.
Is this explanation applicable to NVIDIA Jetson TX2?

**Any other info / logs**
Include any logs or source code
[tflite.zip](https://github.com/tensorflow/tensorflow/files/3031972/tflite.zip)
 that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
786,4623,0,"examples/image_retraining examples/android. hi guys:
i want to use a retraining model on the android device, and i moved the model.pb(retraining Inception v3 model ) and label.txt(containing two labels) to the examples/android/assets, but i got some problem as the figure.

(09-28 18:37:11.572 21326-21351/org.tensorflow.demo E/native: tensorflow_inference_jni.cc:160 Output [output:0] not found, aborting!)

Is there any document which explains detail about how to predict with a model in .cc file and how should i rewrite tensorflow_inference_jni.cc file to load the model.
<img width=""1143"" alt=""2016-09-28 6 38 03"" src=""https://cloud.githubusercontent.com/assets/12976847/18910659/975ef168-85ab-11e6-8fe0-4f6eaec3ba9a.png"">

thanks for your time
"
1163,28019,0,"can't do serving with model generated by TF2.0. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I trained model with tf.keras's functional api with tensorflow 2.0 and save model with tf.keras.Model.save() and convert the h5 model with tf.compat.v1.saved_model.simple_save(). When I try to serve the model with the command



 I get error message

>RunTimeError: The Session graph is empty. Add operations to the graph before calling run().

**Describe the expected behavior**

I expect that an output tensor representing the class of the input handwriting digit is returned from the serving.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Please refer to my project at https://github.com/breadbread1984/EagerExecutionDemo. after train with train_mnist.py, executing start_serving.sh will reproduce the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1124,35423,0,"TF 2.0 XLA JIT reporting error: ""./bin/ptxas not found"". **System information**
- OS Platform and Distribution: Ubuntu 16.04.6 LTS
- TensorFlow installed from (source or binary): pip3 install tensorflow-gpu
- TensorFlow version (use command below): 2.0.0
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0
- GPU model and memory: TITAN Xp

**Describe the current behavior**

The test code is running with error as bellow:



**Describe the expected behavior**

The test code is running successfully.

**Code to reproduce the issue**

"
1224,18371,0,"compile tf-lite error. I want to **cross compile** some components of **tf-lite**.So I do these:

in my **arm_build** folder the **BUILD** file is:

the **test.cc** is:

Actually I just want to compile the ""context,framework and schema_fbs_version"" components.
I got like this ""**error: '_Float128' does not name a type**""
![a](https://user-images.githubusercontent.com/14851411/38535744-98ca9ada-3c74-11e8-8ad3-e4e3ef9d7c40.png)
If compiled with native gcc ,it is okay.
**gcc version 7.3.1 20180312 (GCC)**
cross-toolchain **4.9.3 raspberry pi** which was downloaded by the **Bazel(version 0.11.1**)automatically.
OS:  4.15.15-1-ARCH
Can anyone help?"
965,16776,0,"Unnamed Op not showing in list_tensors command during debug session. I have these two unnamed op tensors  and  under a variable scope, but the  command isn't listing these two tensors under the op 'MatMul' and 'Softmax' during the  session after a test run on a checkpoint. Here is a snapshot of the code:

What can I do to retrieve these variables for testing purposes on ?  
For alternative purposes, can I retrieve them using the normal tensorflow session by using  as mentioned in [this answer][1]?


  [1]: https://stackoverflow.com/questions/44639260/retrieving-an-unnamed-variable-in-tensorflow"
470,13895,0,"In the estimator of Tensorflow, how does it work when model_fn is called multiple times?.     def model_fn(features, labels, mode, params):
      """"""Model function for Estimator.""""""
    
      # Connect the first hidden layer to input layer
      # (features[""x""]) with relu activation
      first_hidden_layer = tf.layers.dense(features[""x""], 10, activation=tf.nn.relu)
    
      # Connect the second hidden layer to first hidden layer with relu
      second_hidden_layer = tf.layers.dense(
          first_hidden_layer, 10, activation=tf.nn.relu)
    
      # Connect the output layer to second hidden layer (no activation fn)
      output_layer = tf.layers.dense(second_hidden_layer, 1)
    
      # Reshape output layer to 1-dim Tensor to return predictions
      predictions = tf.reshape(output_layer, [-1])
    
      # Provide an estimator spec for .
      if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(
            mode=mode,
            predictions={""ages"": predictions})
    
      # Calculate loss using mean squared error
      loss = tf.losses.mean_squared_error(labels, predictions)
    
      # Calculate root mean squared error as additional eval metric
      eval_metric_ops = {
          ""rmse"": tf.metrics.root_mean_squared_error(
              tf.cast(labels, tf.float64), predictions)
      }
    
      optimizer = tf.train.GradientDescentOptimizer(
          learning_rate=params[""learning_rate""])
      train_op = optimizer.minimize(
          loss=loss, global_step=tf.train.get_global_step())
    
      # Provide an estimator spec for  and  modes.
      return tf.estimator.EstimatorSpec(
          mode=mode,
          loss=loss,
          train_op=train_op,
          eval_metric_ops=eval_metric_ops)

Above is an example of the model_fn used by Tensorflow's [Estimator][1].

As mentioned in the tutorial, this model_fn could be called in different context (train, predict, evaluate). However, I'm a bit confused, because each time the model_fn is called, **instead of reusing existing graph, it seems to create a new graph.(or create new node in the graph)**

For example, firstly I called model_fn under TRAIN mode, then I called model_fn with PREDICT mode. How can I make sure the PREDICT one is reusing the weight of the trained values?

  [1]: https://www.tensorflow.org/extend/estimators"
602,20326,0,"codelabs tutorial poets doesn't work. Your tutorial does not work on a variety of fronts. You cannot complete the tutorial from start to end just trying to use your instructions. I dare you to do so. 

I tried this using Anaconda AND Pycharm

https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0"
307,29112,0,"tf.function runtime error when modifying file. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-2828-ga9a1a64d25 2.0.0-dev20190528
- Python version: 3.6.8
- CUDA/cuDNN version: CUDA version 10.0.130
- GPU model and memory: GeForce GTX 1080 Ti

**Describe the current behavior**
I'm running some experiments using tensorflow 2.0 nightly. I decorated the train step with . In the meantime I'm modifying the file involved in the training.
After some time I get a random error in a function invocation (inside the function decorated with . If I do not modify any file the error does not happen.
I guess it's something related to the implementation of , probably  creates some temp file and updates them. The file update however is not managed well since modifying files the training should not be affected.

**Code to reproduce the issue**
This is an issue difficult to reproduce.
"
1158,6171,0,"tfprof: Python3 incompatibility. [This line in tfprof_logger.py](https://github.com/tensorflow/tensorflow/blob/20c3d37ecc9bef0e106002b9d01914efd548e66b/tensorflow/contrib/tfprof/python/tools/tfprof/tfprof_logger.py#L127) uses , which breaks my Python 3 code.
"
497,10488,0,"Changing the cache_size in Gemmlowp/meta/single_thread_gemm.h cause random error in Requantize nodes. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux ubuntu 16.04

- **TensorFlow installed from (source or binary)**:

source, NDK built Android ARM64 binary

- **TensorFlow version (use command below)**:

commit id: f48673b5054b474fa1e51823edd075088cd16d5f
Author: Luke Iwanski <luke@codeplay.com>

- **Bazel version (if compiling from source)**:
0.4.5

- **CUDA/cuDNN version**:
no

- **GPU model and memory**:
no

- **Exact command to reproduce**:
1. bazel --output_base=../out/armv8_benchmark_model/ build -s -c opt --jobs=1 --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  tensorflow/tools/benchmark:benchmark_model 2>&1 |tee log.txt
2.adb push benchmark_model /data/local/tmp/
 
3. run_vgg1 on Nexus5X6P phone:
script:
time ./$1 --graph=$2   --input_layer=""images:0""   --input_layer_shape=""1,224,224,3""   --input_layer_type=""float""   --output_layer=""prob:0"" --num_runs=1

The final command line that I use:
./run_vgg1.sh benchmark_model vgg16.8bit.weightsnodes.model

### My problem
When I change the cache_size from 256*1024 to 128*1024 in the Gemmlowp/meta/single_thread_gemm.h, the benchmark_model randomly failed on the 8 bit quantized both node and weights vgg16 model.

### Source code / logs
My Tensorflow commit-id:
commit f48673b5054b474fa1e51823edd075088cd16d5f

My modify for the Gemmlowp:
in file  gemmlowp/meta/single_thread_gemm.h:
change all the ""int cache_size = 256 * 1024"" to ""int cache_size = 128 * 1024"".

The error log is :
native : benchmark_model.cc:381 Graph: [vgg16.8bit.weightsnodes.model]
native : benchmark_model.cc:382 Input layers: [images:0]
native : benchmark_model.cc:383 Input shapes: [1,224,224,3]
native : benchmark_model.cc:384 Input types: [float]
native : benchmark_model.cc:385 Output layers: [prob:0]
native : benchmark_model.cc:386 Num runs: [1]
native : benchmark_model.cc:387 Inter-run delay (seconds): [-1.0]
native : benchmark_model.cc:388 Num threads: [-1]
native : benchmark_model.cc:389 Benchmark name: []
native : benchmark_model.cc:390 Output prefix: []
native : benchmark_model.cc:391 Show sizes: [0]
native : benchmark_model.cc:392 Warmup runs: [2]
native : benchmark_model.cc:52 Loading TensorFlow.
native : benchmark_model.cc:59 Got config, 0 devices
can't determine number of CPU cores: assuming 4
can't determine number of CPU cores: assuming 4
native : benchmark_model.cc:257 Running benchmark for 2 iterations without detailed stat logging:
native : benchmark_model.cc:233 Error during inference: Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0
	 [[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=""/job:localhost/replica:0/task:0/cpu:0""](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]
native : benchmark_model.cc:268 Failed on run 0
native : benchmark_model.cc:451 Timing failed with Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0
	 [[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=""/job:localhost/replica:0/task:0/cpu:0""](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]
    0m11.12s real     0m27.55s user     0m01.87s system



"
637,19080,0,"Make canned estimators compute PR-AUC with right method.. As we are sure about the error produced by computing *PR-AUC* with **trapezoidal** rule according to [this commit](https://github.com/tensorflow/tensorflow/commit/1f5324ca69bc1017972eef8e418691cff9a86dd7).
And also i found there is no proper workaround to make a canned estimator(Let's say [DNNClassifier](https://github.com/tensorflow/tensorflow/blob/2dc7575123ffa0e6413fc3d2700968ef25f049de/tensorflow/python/estimator/canned/dnn.py#L194)) to compute PR-AUC with the right method **careful_interpolation**.
Then, i did a simple change to this embarrassed situation: https://github.com/tensorflow/tensorflow/pull/19079

Does anyone can confirm this for me?


#### Update template content:

- Have I written custom code: No, i just use a canned estimator DNNClassifier.
- OS Platform and Distribution: macOS High Sierra
- TensorFlow installed from: pip
- TensorFlow version: 1.8.0
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: N/A"
1024,15294,0,"Error tensorflow load library with Cuda 8 and 9. Hello,
I use TF 1.0.1, and in my PC (Ubuntu 17.03), I already installed both Cuda 8.0 and Cuda 9.0.

When I run a demo code (Fast R_CNN), I got this error:

I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcudnn.so.5. LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64
I tensorflow/stream_executor/cuda/cuda_dnn.cc:3517] Unable to load cuDNN DSO
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Traceback (most recent call last):
  File ""./tools/demo.py"", line 11, in <module>
    from networks.factory import get_network
  File ""/home/tung/Documents/Plate/Thinh/Faster-RCNN_TF/tools/../lib/networks/__init__.py"", line 8, in <module>
    from .VGGnet_train import VGGnet_train
  File ""/home/tung/Documents/Plate/Thinh/Faster-RCNN_TF/tools/../lib/networks/VGGnet_train.py"", line 2, in <module>
    from networks.network import Network
  File ""/home/tung/Documents/Plate/Thinh/Faster-RCNN_TF/tools/../lib/networks/network.py"", line 3, in <module>
    import roi_pooling_layer.roi_pooling_op as roi_pool_op
  File ""/home/tung/Documents/Plate/Thinh/Faster-RCNN_TF/tools/../lib/roi_pooling_layer/roi_pooling_op.py"", line 5, in <module>
    _roi_pooling_module = tf.load_op_library(filename)
  File ""/home/tung/Envs/Tensor1/lib/python3.4/site-packages/tensorflow/python/framework/load_library.py"", line 64, in load_op_library
    None, None, error_msg, error_code)
tensorflow.python.framework.errors_impl.NotFoundError: libcudart.so.9.0: cannot open shared object file: No such file or directory

It seem there is a conflict between cuda 8.0 and 9.0
Is there any suggest?
Thanks"
894,30820,0,"Can xla compile tf.estimator.DNNLinearCombinedEstimator?. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source 1.13
- TensorFlow version: 1.13
- Python version: 2.7
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 0.19
- GCC/Compiler version (if compiling from source):4.8
- CUDA/cuDNN version: n/a
- GPU model and memory:n/a



**Describe the problem**
trying to AOT compile a model trained using XLA 
error :  {{node dnn/input_from_feature_columns/input_layer/xyz/axyz_embedding_weights/SparseReshape}}
	.  Registered:  <no registered kernels>
){{node dnn/input_from_feature_columns/input_layer/ad__ad_id_embedding/ad__ad_id_embedding_weights/SparseReshape}}

Is there any ETA on this being supported ? 

How can AOT xla compile : tf.estimator.DNNLinearCombinedEstimator , is this even possible with current support ?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel-bin/tensorflow/compiler/aot/tfcompile --graph=mygrapht.pb --config=graphv.config.pbtxt --cpp_class=""mynamespace::MyComputation""


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1166,14542,0,"'Model' object has no attribute 'container_nodes'. ## Problem

Output:


## Environment
-System: Ubuntu 16.04
-Tensorflow-gpu bin v1.4.0-rc1-11-g130a514 1.4.0

"
631,2951,0,"Installation of Tensorflow on Mac OS X 10.11.5 wheel not supported?. Tried installing Tensorflow but wheel not supported on this platform. 
### Environment info

Operating System: Mac OS X 10.11.5

If installed from binary pip package, provide:
1. sudo -H pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl
2. Traceback (most recent call last):
   File ""<string>"", line 1, in <module>
   ImportError: No module named tensorflow

If installed from sources, provide the commit hash:
### Steps to reproduce
1. sudo -H pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl
### Logs or other output that would be helpful

tensorflow-0.8.0-py2-none-any.whl is not a supported wheel on this platform.
"
569,23147,0,"Trying to connect from process to Tensorflow Server, cannot use frozen graph. Hi everyone, I have an issue which can possibly be a bug, for which I am trying to find solution for quite some time:

Stack Overflow:
https://stackoverflow.com/questions/52700621/tensorflow-server-i-dont-want-to-initialize-global-variables-for-every-session

Discuss:
https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/rARPP1_ilNA

My story in short is that I create processes dynamically in Python and do model predictions using a session for Tensorflow Server . Graph is saved in global scope, as I hope that default python 'fork' mode will not create any overhead as long as data in RAM is not changed (""copy-on-change"" policy). Anyway, it's too long and expensive for me to reload model for each new process. Perhaps there might be a more efficient way to define model on server side, but I don't know it.

The problem is that somewhy I need to run initialization of global variables () each time I create new process and open new session. I tried to do dummy sun of the model before locking graph so that all the variables must be initialized, but no success - I still get the uninitialized variable error. Can anyone help me with that?

I have a small reproducible example of the problem compatible with Tensorflow 1.11:

https://github.com/hcl14/Tensorflow-server-launched-from-child-process

The places in the code are marked with  comment.

Also, maybe I am doing everything wrong and there is a better way to quickly do model predictions from dynamically created processes?
"
738,10620,0,How to code this imaging android app from scratch? . How can I code this imaging android app from scratch? 
518,17548,0,"Compilation flags are not always passed.. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: b'v1.6.0-0-gd2e24b6039' 1.6.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.10.1- (@non-git)
- **GCC/Compiler version (if compiling from source)**: 6.4
- **CUDA/cuDNN version**: 9.1/7.0 (also with 9.0)
- **GPU model and memory**: 1070 Ti
- **Exact command to reproduce**: See building script

Building with enabled MPI against Openmpi fails, as mpicxx.h is not included. This is despite explicitly passing the flag . I am also passing the  flag, and once installed, Tensorflow doesn't complain about my CPU having more capabilities, so this seems to be passed at least some of the time.

Adding  in  circumvents the problem, and Tensorflow compiles nicely.

Full error message:


Building script:
which python

Relevant extracto of  showing the flags being saved



"
798,3900,0,"translate.py not found in tensorflow libs on osx. OSX 10.11.6 (15G31)
Tensorflow: 0.10.0rc0
Python 3.5

I'm following https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html but translate.py is not in the tensorflow/models/rnn/translate directory.



When I ls in ""/usr/local/lib/python3.5/site-packages/tensorflow/models/rnn/translate"", I see the following files:

Is my installation broken somehow? I installed tensorflow with this code:


"
222,21929,1,"Poor performance of the model when enabling layer normalization with tf.contrib.rnn.LayerNormBasicLSTMCell. -------------------------------------------------------------------------------------------------------


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 

VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
No
- **TensorFlow installed from (source or binary)**:
 source
- **TensorFlow version (use command below)**:  
tf.VERSION = 1.9.0

- **Python version**: python 2.7

- **Bazel version (if compiling from source)**:  
0.11.1

- **GCC/Compiler version (if compiling from source)**:  
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609

- **CUDA/cuDNN version**: 
 9.1
- **GPU model and memory**:  
Tesla K80
- **Exact command to reproduce**:
( No command)

-------------------------------------------------------------------------------------------------------
Before writing my issue here,  I found a similar question on  [stackoverflow]( https://stackoverflow.com/questions/45150101/why-is-layernormbasiclstmcell-much-slower-and-less-accurate-than-lstmcell) where the author described the same problem I have but there were no valid answer and most of the comments where wrongly referring to technique related to batch_normalization which are non-applicable since we are talking about layer_normalization. 
( [Difference between layer_normalzation and batch_normalization](https://theneuralperspective.com/2016/10/27/gradient-topics/) )
I was trying to do some layer normalization with my model, and I have chosen the 
 where  is set to True.
I started training the model and as unexpected the training/validation losses are decreasing slowly.
The paper of layer normalization ( https://arxiv.org/pdf/1607.06450.pdf ) showed that this technique makes the training time much faster but the Tensorflow implementation showed the inverse case.
Are there please any hints on how to solve this problem? Any further explications on why the implementation makes the training slower? "
841,32113,0,"model.summary() isn't works when dropout layer is in the model.. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6
- TensorFlow installed from (source or binary): binary ()
- TensorFlow version (use command below): 2.0.0rc0
- Python version: 3.7

**Describe the current behavior**

When I call the model.summary() after define and the build model that is defined by , the error occurs.
After some trial, I found the reason would be the dropout layer.

The output is:
' + self.name +
-> 1587                          '.build(batch_input_shape)count_paramsdropout.build(batch_input_shape)
"
529,34833,0,"Error building from source. <em>I'm getting this error after running ""bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package ""</em>

**System information**
- Windows 10
- TensorFlow version: 1.13
- Python version: 3.6
- Installed using  conda:
- Bazel version : 0.21.0
- GCC/Compiler version : visual 2015


WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
c:\tensorflow-build\tensorflow/.bazelrc
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: c11f1d92-ce5b-430a-9113-7db1a5079995
INFO: Build options --copt and --define have changed, discarding analysis cache.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:499:1: In rule 'decorator_utils_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:499:1: In rule 'decorator_utils_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:34:1: In rule 'base_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:34:1: In rule 'base_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:54:1: In rule 'tracking_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:54:1: In rule 'tracking_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:81:1: In rule 'data_structures_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:81:1: In rule 'data_structures_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:132:1: In rule 'util_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:132:1: In rule 'util_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:168:1: In rule 'util_with_v1_optimizers_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/training/checkpointable/BUILD:168:1: In rule 'util_with_v1_optimizers_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:510:1: In rule 'tf_export_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:510:1: In rule 'tf_export_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:521:1: In rule 'deprecation_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:521:1: In rule 'deprecation_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:532:1: In rule 'dispatch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:532:1: In rule 'dispatch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:543:1: In rule 'keyword_args_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:543:1: In rule 'keyword_args_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:1240:1: In rule 'function_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:1240:1: In rule 'function_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:2868:1: In rule 'sparse_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:2868:1: In rule 'sparse_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:2895:1: In rule 'sort_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:2895:1: In rule 'sort_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3266:1: In rule 'gradient_checker_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3266:1: In rule 'gradient_checker_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3266:1: In rule 'gradient_checker_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3266:1: In rule 'gradient_checker_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3282:1: In rule 'gradient_checker_v2_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3282:1: In rule 'gradient_checker_v2_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3282:1: In rule 'gradient_checker_v2_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3282:1: In rule 'gradient_checker_v2_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3298:1: In rule 'gradients_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3298:1: In rule 'gradients_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3298:1: In rule 'gradients_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3298:1: In rule 'gradients_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3345:1: In rule 'image_grad_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3345:1: In rule 'image_grad_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3345:1: In rule 'image_grad_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3345:1: In rule 'image_grad_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3358:1: In rule 'image_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3358:1: In rule 'image_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3358:1: In rule 'image_ops_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3358:1: In rule 'image_ops_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3432:1: In rule 'nn_batchnorm_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3432:1: In rule 'nn_batchnorm_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3432:1: In rule 'nn_batchnorm_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3432:1: In rule 'nn_batchnorm_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3467:1: In rule 'nn_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3467:1: In rule 'nn_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3467:1: In rule 'nn_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3467:1: In rule 'nn_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3487:1: In rule 'nn_xent_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3487:1: In rule 'nn_xent_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3487:1: In rule 'nn_xent_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3487:1: In rule 'nn_xent_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3804:1: In rule 'function_utils_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3804:1: In rule 'function_utils_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4375:1: In rule 'localhost_cluster_performance_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4375:1: In rule 'localhost_cluster_performance_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4375:1: In rule 'localhost_cluster_performance_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4375:1: In rule 'localhost_cluster_performance_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4399:1: In rule 'sync_replicas_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4399:1: In rule 'sync_replicas_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4442:1: In rule 'session_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4442:1: In rule 'session_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adadelta_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adadelta_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adagrad_da_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adagrad_da_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adagrad_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'adagrad_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'basic_loops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'basic_loops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'coordinator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'coordinator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'device_setter_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'device_setter_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'ftrl_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'ftrl_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'gradient_descent_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'gradient_descent_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'learning_rate_decay_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'learning_rate_decay_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'learning_rate_decay_v2_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'learning_rate_decay_v2_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'momentum_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'momentum_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'proximal_adagrad_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'proximal_adagrad_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'proximal_gradient_descent_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'proximal_gradient_descent_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'quantize_training_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'quantize_training_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'queue_runner_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'queue_runner_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'rmsprop_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'rmsprop_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'slot_creator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'slot_creator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'tensorboard_logging_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'tensorboard_logging_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'training_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4679:1: In rule 'training_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4752:1: In rule 'saver_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4752:1: In rule 'saver_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4752:1: In rule 'saver_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4752:1: In rule 'saver_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4825:1: In rule 'saver_large_variable_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4825:1: In rule 'saver_large_variable_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4846:1: In rule 'saver_large_partitioned_variable_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4846:1: In rule 'saver_large_partitioned_variable_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4865:1: In rule 'session_manager_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4865:1: In rule 'session_manager_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4865:1: In rule 'session_manager_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4865:1: In rule 'session_manager_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4908:1: In rule 'basic_session_run_hooks_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4908:1: In rule 'basic_session_run_hooks_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4985:1: In rule 'warm_starting_util_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:4985:1: In rule 'warm_starting_util_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5004:1: In rule 'monitored_session_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5004:1: In rule 'monitored_session_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5068:1: In rule 'input_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5068:1: In rule 'input_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5296:1: In rule 'layers_normalization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5296:1: In rule 'layers_normalization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5296:1: In rule 'layers_normalization_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5296:1: In rule 'layers_normalization_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5379:1: In rule 'batch_norm_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5379:1: In rule 'batch_norm_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5379:1: In rule 'batch_norm_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5379:1: In rule 'batch_norm_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5399:1: In rule 'concat_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5399:1: In rule 'concat_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5399:1: In rule 'concat_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5399:1: In rule 'concat_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5416:1: In rule 'control_flow_ops_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5416:1: In rule 'control_flow_ops_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5416:1: In rule 'control_flow_ops_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5416:1: In rule 'control_flow_ops_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5449:1: In rule 'split_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5449:1: In rule 'split_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5449:1: In rule 'split_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5449:1: In rule 'split_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5467:1: In rule 'transpose_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5467:1: In rule 'transpose_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5467:1: In rule 'transpose_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5467:1: In rule 'transpose_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5486:1: In rule 'matmul_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5486:1: In rule 'matmul_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5486:1: In rule 'matmul_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5486:1: In rule 'matmul_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5507:1: In rule 'matmul_benchmark_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5507:1: In rule 'matmul_benchmark_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5507:1: In rule 'matmul_benchmark_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5507:1: In rule 'matmul_benchmark_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5529:1: In rule 'session_benchmark', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5529:1: In rule 'session_benchmark', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5529:1: In rule 'session_benchmark_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5529:1: In rule 'session_benchmark_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5709:1: In rule 'memory_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5709:1: In rule 'memory_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5735:1: In rule 'constant_folding_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5735:1: In rule 'constant_folding_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5735:1: In rule 'constant_folding_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5735:1: In rule 'constant_folding_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5758:1: In rule 'layout_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5758:1: In rule 'layout_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5758:1: In rule 'layout_optimizer_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:5758:1: In rule 'layout_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:1360:1: Target '//tensorflow/python:framework_for_generated_wrappers_v2' contains an error and its package is in error and referenced by '//tensorflow/python:test_ops'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:1360:1: Target '//tensorflow/python:framework/test_ops.py' contains an error and its package is in error and referenced by '//tensorflow/python:test_ops'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:32:1: In rule 'batch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:32:1: In rule 'batch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:132:1: In rule 'dataset_constructor_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:132:1: In rule 'dataset_constructor_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:151:1: In rule 'filter_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:151:1: In rule 'filter_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:171:1: In rule 'fixed_length_record_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:171:1: In rule 'fixed_length_record_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:190:1: In rule 'flat_map_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:190:1: In rule 'flat_map_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:215:1: In rule 'group_by_reducer_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:215:1: In rule 'group_by_reducer_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:234:1: In rule 'group_by_window_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:234:1: In rule 'group_by_window_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:273:1: In rule 'interleave_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:273:1: In rule 'interleave_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:294:1: In rule 'map_and_batch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:294:1: In rule 'map_and_batch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:313:1: In rule 'numa_map_and_batch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:313:1: In rule 'numa_map_and_batch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:332:1: In rule 'map_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:332:1: In rule 'map_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:393:1: In rule 'padded_batch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:393:1: In rule 'padded_batch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:413:1: In rule 'parallel_interleave_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:413:1: In rule 'parallel_interleave_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:434:1: In rule 'parallel_map_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:434:1: In rule 'parallel_map_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:459:1: In rule 'parse_example_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:459:1: In rule 'parse_example_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:517:1: In rule 'sample_from_datasets_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:517:1: In rule 'sample_from_datasets_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:553:1: In rule 'sequence_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:553:1: In rule 'sequence_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:590:1: In rule 'shuffle_and_repeat_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:590:1: In rule 'shuffle_and_repeat_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:608:1: In rule 'shuffle_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:608:1: In rule 'shuffle_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:648:1: In rule 'stats_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:648:1: In rule 'stats_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:669:1: In rule 'textline_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:669:1: In rule 'textline_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:688:1: In rule 'tf_record_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:688:1: In rule 'tf_record_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:707:1: In rule 'unbatch_dataset_serialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD:707:1: In rule 'unbatch_dataset_serialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:618:1: In rule 'framework_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:618:1: In rule 'framework_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:976:1: In rule 'session_debug_grpc_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:976:1: In rule 'session_debug_grpc_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:976:1: In rule 'session_debug_grpc_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:976:1: In rule 'session_debug_grpc_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1003:1: In rule 'grpc_large_data_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1003:1: In rule 'grpc_large_data_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1003:1: In rule 'grpc_large_data_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1003:1: In rule 'grpc_large_data_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1029:1: In rule 'dist_session_debug_grpc_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1029:1: In rule 'dist_session_debug_grpc_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1029:1: In rule 'dist_session_debug_grpc_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1029:1: In rule 'dist_session_debug_grpc_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1123:1: In rule 'examples_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/debug/BUILD:1123:1: In rule 'examples_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:23:1: In rule 'evaluation_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:23:1: In rule 'evaluation_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:70:1: In rule 'learning_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:70:1: In rule 'learning_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:168:1: In rule 'summaries_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/BUILD:168:1: In rule 'summaries_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:69:1: In rule 'datasets_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:69:1: In rule 'datasets_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:69:1: In rule 'datasets_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:69:1: In rule 'datasets_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:114:1: In rule 'saver_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:114:1: In rule 'saver_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:114:1: In rule 'saver_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:114:1: In rule 'saver_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:151:1: In rule 'metrics_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:151:1: In rule 'metrics_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:188:1: In rule 'evaluator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:188:1: In rule 'evaluator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:220:1: In rule 'network_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:220:1: In rule 'network_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:253:1: In rule 'remote_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:253:1: In rule 'remote_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:253:1: In rule 'remote_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/eager/python/BUILD:253:1: In rule 'remote_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:66:1: In rule 'core_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:66:1: In rule 'core_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:168:1: In rule 'ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:168:1: In rule 'ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:325:1: In rule 'ragged_tensor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:325:1: In rule 'ragged_tensor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:353:1: In rule 'ragged_eager_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:353:1: In rule 'ragged_eager_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:367:1: In rule 'ragged_range_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:367:1: In rule 'ragged_range_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:380:1: In rule 'ragged_tensor_bounding_shape_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:380:1: In rule 'ragged_tensor_bounding_shape_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:393:1: In rule 'ragged_row_lengths_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:393:1: In rule 'ragged_row_lengths_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:408:1: In rule 'ragged_gather_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:408:1: In rule 'ragged_gather_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:427:1: In rule 'ragged_batch_gather_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:427:1: In rule 'ragged_batch_gather_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:446:1: In rule 'ragged_gather_nd_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:446:1: In rule 'ragged_gather_nd_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:465:1: In rule 'ragged_row_splits_to_segment_ids_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:465:1: In rule 'ragged_row_splits_to_segment_ids_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:478:1: In rule 'ragged_segment_ids_to_row_splits_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:478:1: In rule 'ragged_segment_ids_to_row_splits_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:491:1: In rule 'ragged_from_tensor_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:491:1: In rule 'ragged_from_tensor_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:507:1: In rule 'ragged_to_sparse_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:507:1: In rule 'ragged_to_sparse_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:531:1: In rule 'ragged_from_sparse_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:531:1: In rule 'ragged_from_sparse_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:548:1: In rule 'ragged_to_tensor_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:548:1: In rule 'ragged_to_tensor_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:563:1: In rule 'ragged_segment_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:563:1: In rule 'ragged_segment_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:581:1: In rule 'ragged_reduce_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:581:1: In rule 'ragged_reduce_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:600:1: In rule 'ragged_map_flat_values_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:600:1: In rule 'ragged_map_flat_values_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:619:1: In rule 'ragged_const_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:619:1: In rule 'ragged_const_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:635:1: In rule 'ragged_constant_value_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:635:1: In rule 'ragged_constant_value_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:654:1: In rule 'convert_to_tensor_or_ragged_tensor_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:654:1: In rule 'convert_to_tensor_or_ragged_tensor_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:671:1: In rule 'ragged_boolean_mask_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:671:1: In rule 'ragged_boolean_mask_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:689:1: In rule 'ragged_concat_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:689:1: In rule 'ragged_concat_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:708:1: In rule 'ragged_stack_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:708:1: In rule 'ragged_stack_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:723:1: In rule 'ragged_tile_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:723:1: In rule 'ragged_tile_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:740:1: In rule 'ragged_util_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:740:1: In rule 'ragged_util_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:756:1: In rule 'ragged_expand_dims_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:756:1: In rule 'ragged_expand_dims_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:770:1: In rule 'ragged_where_op_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:770:1: In rule 'ragged_where_op_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:784:1: In rule 'ragged_dispatch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:784:1: In rule 'ragged_dispatch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:810:1: In rule 'ragged_operators_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:810:1: In rule 'ragged_operators_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:848:1: In rule 'ragged_tensor_shape_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/ops/ragged/BUILD:848:1: In rule 'ragged_tensor_shape_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tpu/BUILD:316:1: In rule 'datasets_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tpu/BUILD:316:1: In rule 'datasets_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tpu/BUILD:396:1: In rule 'topology_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tpu/BUILD:396:1: In rule 'topology_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:27:1: In rule 'interpreter_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:27:1: In rule 'interpreter_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:70:1: In rule 'lite_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:70:1: In rule 'lite_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:127:1: In rule 'convert_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:127:1: In rule 'convert_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:172:1: In rule 'convert_saved_model_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/python/BUILD:172:1: In rule 'convert_saved_model_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/pyct/common_transformers/BUILD:33:1: In rule 'anf_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/pyct/common_transformers/BUILD:33:1: In rule 'anf_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:145:1: In rule 'flat_map_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:145:1: In rule 'flat_map_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:166:1: In rule 'from_generator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:166:1: In rule 'from_generator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:248:1: In rule 'interleave_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:248:1: In rule 'interleave_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:267:1: In rule 'iterator_checkpoint_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:267:1: In rule 'iterator_checkpoint_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:314:1: In rule 'iterator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:314:1: In rule 'iterator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:314:1: In rule 'iterator_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:314:1: In rule 'iterator_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:370:1: In rule 'map_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:370:1: In rule 'map_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:402:1: In rule 'multi_device_iterator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:402:1: In rule 'multi_device_iterator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:402:1: In rule 'multi_device_iterator_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:402:1: In rule 'multi_device_iterator_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:654:1: In rule 'window_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/kernel_tests/BUILD:654:1: In rule 'window_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:54:1: In rule 'converter_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:54:1: In rule 'converter_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:65:1: In rule 'errors_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:65:1: In rule 'errors_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:80:1: In rule 'function_wrapping_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:80:1: In rule 'function_wrapping_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:90:1: In rule 'naming_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/autograph/core/BUILD:90:1: In rule 'naming_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:109:1: In rule 'saved_model_predictor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:109:1: In rule 'saved_model_predictor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:125:1: In rule 'predictor_factories_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:125:1: In rule 'predictor_factories_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:137:1: In rule 'core_estimator_predictor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:137:1: In rule 'core_estimator_predictor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:150:1: In rule 'contrib_estimator_predictor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/predictor/BUILD:150:1: In rule 'contrib_estimator_predictor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:44:1: In rule 'alexnet_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:44:1: In rule 'alexnet_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:190:1: In rule 'overfeat_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:190:1: In rule 'overfeat_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:304:1: In rule 'vgg_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/nets/BUILD:304:1: In rule 'vgg_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:38:1: In rule 'create_python_api_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:38:1: In rule 'create_python_api_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:52:1: In rule 'tensorflow_doc_srcs_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:52:1: In rule 'tensorflow_doc_srcs_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:68:1: In rule 'output_init_files_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/api/generator/BUILD:68:1: In rule 'output_init_files_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/session_bundle/BUILD:150:1: In rule 'gc_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/session_bundle/BUILD:150:1: In rule 'gc_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/session_bundle/BUILD:221:1: In rule 'session_bundle_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/session_bundle/BUILD:221:1: In rule 'session_bundle_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/specs/BUILD:38:1: In rule 'specs_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/specs/BUILD:38:1: In rule 'specs_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/specs/BUILD:52:1: In rule 'summaries_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/specs/BUILD:52:1: In rule 'summaries_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:237:1: In rule 'print_selective_registration_header_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:237:1: In rule 'print_selective_registration_header_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:260:1: In rule 'saved_model_cli_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:260:1: In rule 'saved_model_cli_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tensor_forest/hybrid/BUILD:206:1: In rule 'decisions_to_data_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/tensor_forest/hybrid/BUILD:206:1: In rule 'decisions_to_data_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/examples/tutorials/mnist/BUILD:92:1: In rule 'fully_connected_feed_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/examples/tutorials/mnist/BUILD:92:1: In rule 'fully_connected_feed_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:99:1: In rule 'tensor_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:99:1: In rule 'tensor_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:99:1: In rule 'tensor_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:99:1: In rule 'tensor_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:111:1: In rule 'backprop_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:111:1: In rule 'backprop_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:111:1: In rule 'backprop_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:111:1: In rule 'backprop_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:132:1: In rule 'core_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:132:1: In rule 'core_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:132:1: In rule 'core_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:132:1: In rule 'core_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:149:1: In rule 'function_argument_naming_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:149:1: In rule 'function_argument_naming_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:149:1: In rule 'function_argument_naming_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:149:1: In rule 'function_argument_naming_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:163:1: In rule 'function_defun_collection_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:163:1: In rule 'function_defun_collection_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:163:1: In rule 'function_defun_collection_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:163:1: In rule 'function_defun_collection_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:178:1: In rule 'function_gradients_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:178:1: In rule 'function_gradients_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:178:1: In rule 'function_gradients_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:178:1: In rule 'function_gradients_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:195:1: In rule 'function_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:195:1: In rule 'function_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:195:1: In rule 'function_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:195:1: In rule 'function_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:260:1: In rule 'execution_callbacks_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:260:1: In rule 'execution_callbacks_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:284:1: In rule 'graph_only_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:284:1: In rule 'graph_only_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:284:1: In rule 'graph_only_ops_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:284:1: In rule 'graph_only_ops_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:357:1: In rule 'benchmarks_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:357:1: In rule 'benchmarks_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:357:1: In rule 'benchmarks_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:357:1: In rule 'benchmarks_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:378:1: In rule 'tape_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:378:1: In rule 'tape_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:396:1: In rule 'ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:396:1: In rule 'ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:396:1: In rule 'ops_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:396:1: In rule 'ops_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:419:1: In rule 'pywrap_tfe_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:419:1: In rule 'pywrap_tfe_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:447:1: In rule 'memory_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:447:1: In rule 'memory_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:447:1: In rule 'memory_test_gpu', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:447:1: In rule 'memory_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:496:1: In rule 'def_function_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:496:1: In rule 'def_function_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:524:1: In rule 'wrap_function_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/eager/BUILD:524:1: In rule 'wrap_function_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/boosted_trees/BUILD:120:1: In rule 'gbdt_batch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/boosted_trees/BUILD:120:1: In rule 'gbdt_batch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:10:1: In rule 'bucket_by_sequence_length_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:10:1: In rule 'bucket_by_sequence_length_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:62:1: In rule 'csv_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:62:1: In rule 'csv_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:83:1: In rule 'dense_to_sparse_batch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:83:1: In rule 'dense_to_sparse_batch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:99:1: In rule 'directed_interleave_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:99:1: In rule 'directed_interleave_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:151:1: In rule 'group_by_reducer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:151:1: In rule 'group_by_reducer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:172:1: In rule 'group_by_window_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:172:1: In rule 'group_by_window_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:194:1: In rule 'ignore_errors_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:194:1: In rule 'ignore_errors_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:211:1: In rule 'indexed_dataset_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:211:1: In rule 'indexed_dataset_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:226:1: In rule 'make_batched_features_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:226:1: In rule 'make_batched_features_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:247:1: In rule 'make_csv_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:247:1: In rule 'make_csv_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:266:1: In rule 'make_tf_record_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:266:1: In rule 'make_tf_record_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:283:1: In rule 'map_and_batch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:283:1: In rule 'map_and_batch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:348:1: In rule 'cardinality_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:348:1: In rule 'cardinality_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:380:1: In rule 'parallel_interleave_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:380:1: In rule 'parallel_interleave_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:472:1: In rule 'rejection_resample_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:472:1: In rule 'rejection_resample_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:499:1: In rule 'restructured_dataset_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:499:1: In rule 'restructured_dataset_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:538:1: In rule 'shuffle_and_repeat_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:538:1: In rule 'shuffle_and_repeat_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:558:1: In rule 'sleep_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:558:1: In rule 'sleep_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:659:1: In rule 'unbatch_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/data/experimental/kernel_tests/BUILD:659:1: In rule 'unbatch_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:41:1: In rule 'candidates_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:41:1: In rule 'candidates_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:70:1: In rule 'external_regret_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:70:1: In rule 'external_regret_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:84:1: In rule 'swap_regret_optimizer_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/constrained_optimization/BUILD:84:1: In rule 'swap_regret_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:56:1: In rule 'train_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:56:1: In rule 'train_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:157:1: In rule 'losses_impl_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:157:1: In rule 'losses_impl_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:194:1: In rule 'tuple_losses_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:194:1: In rule 'tuple_losses_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:232:1: In rule 'conditioning_utils_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:232:1: In rule 'conditioning_utils_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:262:1: In rule 'random_tensor_pool_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:262:1: In rule 'random_tensor_pool_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:299:1: In rule 'virtual_batchnorm_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:299:1: In rule 'virtual_batchnorm_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:334:1: In rule 'clip_weights_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:334:1: In rule 'clip_weights_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:372:1: In rule 'classifier_metrics_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:372:1: In rule 'classifier_metrics_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:404:1: In rule 'eval_utils_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:404:1: In rule 'eval_utils_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:436:1: In rule 'summaries_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:436:1: In rule 'summaries_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:468:1: In rule 'head_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:468:1: In rule 'head_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:505:1: In rule 'gan_estimator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:505:1: In rule 'gan_estimator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:556:1: In rule 'stargan_estimator_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:556:1: In rule 'stargan_estimator_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:608:1: In rule 'sliced_wasserstein_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/gan/BUILD:608:1: In rule 'sliced_wasserstein_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/summary/BUILD:13:1: In rule 'summary_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/summary/BUILD:13:1: In rule 'summary_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/summary/BUILD:33:1: In rule 'summary_ops_graph_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/summary/BUILD:33:1: In rule 'summary_ops_graph_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/saved_model/BUILD:316:1: In rule 'save_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/saved_model/BUILD:316:1: In rule 'save_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/saved_model/BUILD:350:1: In rule 'load_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/saved_model/BUILD:350:1: In rule 'load_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/toco/python/BUILD:72:1: In rule 'toco_from_protos_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/lite/toco/python/BUILD:72:1: In rule 'toco_from_protos_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/data/BUILD:60:1: In rule 'dataset_data_provider_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/data/BUILD:60:1: In rule 'dataset_data_provider_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/data/BUILD:179:1: In rule 'tfexample_decoder_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/slim/python/slim/data/BUILD:179:1: In rule 'tfexample_decoder_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:53:1: In rule 'util_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:53:1: In rule 'util_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:65:1: In rule 'select_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:65:1: In rule 'select_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:77:1: In rule 'match_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:77:1: In rule 'match_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:89:1: In rule 'subgraph_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:89:1: In rule 'subgraph_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:101:1: In rule 'reroute_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:101:1: In rule 'reroute_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:114:1: In rule 'edit_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:114:1: In rule 'edit_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:127:1: In rule 'transform_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/graph_editor/BUILD:127:1: In rule 'transform_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:62:1: In rule 'graph_compute_order_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:62:1: In rule 'graph_compute_order_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:78:1: In rule 'parse_layer_parameters_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:78:1: In rule 'parse_layer_parameters_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:94:1: In rule 'receptive_field_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/receptive_field/BUILD:94:1: In rule 'receptive_field_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/cluster_resolver/BUILD:33:1: In rule 'cluster_resolver_initialization_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/cluster_resolver/BUILD:33:1: In rule 'cluster_resolver_initialization_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:21:1: In rule 'dct_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:21:1: In rule 'dct_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:34:1: In rule 'fft_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:34:1: In rule 'fft_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:50:1: In rule 'mel_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:50:1: In rule 'mel_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:61:1: In rule 'mfcc_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:61:1: In rule 'mfcc_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:75:1: In rule 'reconstruction_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:75:1: In rule 'reconstruction_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:92:1: In rule 'shape_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:92:1: In rule 'shape_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:130:1: In rule 'window_ops_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/kernel_tests/signal/BUILD:130:1: In rule 'window_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/compiler/BUILD:73:1: In rule 'xla_test', size 'medium' is not a valid size.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/contrib/compiler/BUILD:73:1: In rule 'xla_test', timeout 'illegal' is not a valid timeout.
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/examples/tutorials/mnist:package' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/lite/python:interpreter_test_data' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/lite/python:tflite_convert' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/lite/toco/python:toco_from_protos' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/autograph/core:test_lib' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/autograph/pyct/common_transformers:common_transformers' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:cond_v2' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:distributed_framework_test_lib' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:meta_graph_testdata' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:spectral_ops_test_util' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:util_example_parser_configuration' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/data/experimental/kernel_tests/serialization:dataset_serialization_test_base' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/data/experimental/kernel_tests:stats_dataset_test_base' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/data/kernel_tests:test_base' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/debug:debug_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/eager:eager_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/kernel_tests/signal:test_util' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/ops/ragged:ragged_test_util' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/saved_model:saved_model' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/tools:tools_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python/tools/api/generator:create_python_api' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:test_ops' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:while_v2' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/boosted_trees:boosted_trees_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/cluster_resolver:cluster_resolver_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/compiler:xla' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/constrained_optimization:constrained_optimization_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/eager/python:evaluator' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/gan:gan' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/graph_editor:graph_editor_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/labeled_tensor:labeled_tensor_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/predictor:predictor_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/receptive_field:receptive_field_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/session_bundle:session_bundle_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/slim:slim' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/slim/python/slim/data:data_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/slim/python/slim/nets:nets_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/specs:specs' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/summary:summary_test_util' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/tensor_forest/hybrid:hybrid_pip' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/contrib/tpu:tpu' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: C:/tensorflow-build/tensorflow/tensorflow/tools/pip_package/BUILD:138:1: Target '//tensorflow/python:pywrap_tensorflow_import_lib_file' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Cannot compute config conditions
INFO: Elapsed time: 1,698s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (6 packages loaded, 56 targets configured)
    currently loading: tensorflow/core
    Fetching @six_archive; fetching
"
324,25026,0,"Can not build debug wheel due to zip overflow(linux). **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.4
- TensorFlow installed from: Source
- TensorFlow version: 3ed46c325f70e2f1521b850b43d2b174101d9472
- Python version: 3.4
- Bazel version (if compiling from source):  0.19.2 (not from source)
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the problem**

Cant create wheel file for a build with debug symbols.

Command:

Output:


**Provide the exact sequence of commands / steps that you executed before running into the problem**
First build TF with debug symbols, no GPU support (runs succesfully):



Build pip wheel file (FAILS):



**Any other info / logs**

Thanks in advance"
91,34500,1,"[TF2.1] Performance: Control flow and scalar ops 225x slower than raw Python and 24000x slower than C++. **Summary**
TF Op performance for a simple subgraph (built with AutoGraph) is at-least 2 orders of magnitude slower than expected: looping over 100K numbers takes 4+ seconds instead of 18ms (or much faster)

**Benchmark code and repro instructions:**
https://github.com/divyekapoor/ml-op-benchmarks

1. Clone the repo
2. 


Raw python: Running the same code without tf.Module and @tf.function.
Raw C++: Equivalent implementation in straight C++.

Performance table:

FizzBuzz Iteration Counts | 100000 |   |   |  
-- | -- | -- | -- | --
  | Raw Latency (ms) | Per Run Latency (usec) | Python Multiplier | C++ Multiplier
Tensorflow Python | 4087 | 40.87 | **227.06** | 24327
Tensorflow Saved Model Python | 4046 | 40.46 | **224.78** | 24083
Raw Python | 18 | 0.18 | 1.00 | 107
Raw C++ | 0.168 | 0.00168 | 0.01 | 1

The multiplers use the corresponding Python and C++ code as unit = 1.
Benchmark script is attached at the bottom of this issue and only has a dependency on Tensorflow.

https://github.com/divyekapoor/ml-op-benchmarks has something to directly clone and execute.

(If it would help, TF ops are ~40% slower than Torch ops for the same FizzBuzz benchmark)
https://github.com/pytorch/pytorch/issues/30365 for the related PyTorch issue.

FizzBuzz Iteration Counts | 100000 |   |   |  
-- | -- | -- | -- | --
  | Raw Latency (ms) | Per Run Latency (usec) | Python Multiplier | C++ Multiplier
PyTorch Python | 4007 | 40.07 | 222.61 | 23851
PyTorch TorchScript Python (from Loaded TorchScript) | 2830 | 28.3 | **157.22** | 16845
PyTorch TorchScript C++ (Native) | 255 | 2.55 | **14.17** | 1518
PyTorch TorchScript C++ (Native + ATen Tensors) | 252 | 2.52 | **14.00** | 1500
Raw Python | 18 | 0.18 | 1.00 | 107
Raw C++ | 0.168 | 0.00168 | 0.01 | 1




Performance similar to raw Python is the expected behavior.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.6 (18G1012)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested.
- TensorFlow installed from (source or binary): binary.
- TensorFlow version (use command below): 
== tensorflow import ============================================
tf.version.VERSION = 2.1.0-dev20191107
tf.version.GIT_VERSION = v1.12.1-17543-gb4b5ce680c
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)
- Python version: 
== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 7, 4, 'final', 0)
- Bazel version (if compiling from source): NA.
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Setup**
Performance benchmark for conditional ops set up with a FizzBuzz test case:
Input: n -> a range limit (100K)
Output: a 3 element tensor with counts for (fizz, buzz, fizzbuzz)
Goal: To estimate the performance overhead of TF ops versus raw python / raw C++.
Benchmark file is attached.

**Describe the current behavior**
FizzBuzz with TF ops is 225x slower than the same code in Raw Python and 24K+x slower than the corresponding C++ implementation. 

**Describe the expected behavior**
FizzBuzz with TF ops should be within 10-50% of raw Python or faster.  

**Code to reproduce the issue**
Attached to this report.
To reproduce:

Full version: https://github.com/divyekapoor/ml-op-benchmarks

**Other info / logs**
Performance table:

FizzBuzz Iteration Counts | 100000 |   |   |  
-- | -- | -- | -- | --
  | Raw Latency (ms) | Per Run Latency (usec) | Python Multiplier | C++ Multiplier
Tensorflow Python | 4087 | 40.87 | 227.06 | 24327
Tensorflow Saved Model Python | 4046 | 40.46 | 224.78 | 24083
Raw Python | 18 | 0.18 | 1.00 | 107
Raw C++ | 0.168 | 0.00168 | 0.01 | 1

Raw latency == run with range input N = 100K
Per Run latency == Raw latency / 100K (one run through the op graph)
[fizz.tar.gz](https://github.com/tensorflow/tensorflow/files/3877175/fizz.tar.gz)
"
1213,31523,0,"error while building wheel. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version:  2.0.0-rc.0
- Python version: 3.7.4 x64
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source):  7.4.0
- CUDA/cuDNN version: 10.1/7.6.2
- GPU model and memory: GTX1080Ti GDDR5x 11GB X 6



**Describe the problem**
error while building wheels
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
306,7467,0,"Feature request: Add early stopping mechanism to slim.evaluation_loop. Would it be possible to add early stopping mechanism to slim.evaluation_loop?

"
94,21196,1,"MobileNet v2 slower than v1 when loading from Frozen GraphDef. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04.3 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Doesn't apply
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.9.0-0-g25c197e023 1.9.0
- **Python version**:
Python3
- **Bazel version (if compiling from source)**:
Doesn't apply
- **GCC/Compiler version (if compiling from source)**:
Doesn't apply
- **CUDA/cuDNN version**:
CUDA: 9.0 ; cuDNN: 7.1.4
- **GPU model and memory**:
GeForce GTX 1080 Ti
- **Exact command to reproduce**:

### Describe the problem

### Summary
MobileNet v2 is faster than v1 only when loading from the checkpoint format i.e, Variable Ops (meta, index, data) whereas it is slower than v1 when running in the frozen graph format (Const Ops) (.pb)

### Description
I have two TensorFlow trained models that are in the checkpoint format (meta, index, data) namely mobilenetv1_0.75.ckpt and mobilenetv2_0.75_6.ckpt. 

The model definitions are as described in the [MobileNetv1](https://arxiv.org/abs/1704.04861) and [MobileNetv2](https://arxiv.org/pdf/1801.04381.pdf) papers. Both models are trained with a width_multiplier of 0.75. MobileNetv2 has an expansion factor of 6. This means that MAC wise, v2 is better than v1 (v1: 26.5 Mil, v2: 20.6Mil) and is expected to be slightly faster than v1.

To compare how the models actually perform, I evaluated them in two ways
Method 1: After Training - Inference by loading the models from checkpoints
Method 2: During Deployment - Inference by loading the models from frozen GraphDefs ([freeze_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) for converting Variables to Consts)

**Tools Used for Testing:**
- TF's Timeline Trace Tool
- benchmark_model Tool
- Naive Python time module

### Timeline Tool
I used the TensorFlow's [timeline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py) tool to view the execution times in the Chrome Trace format. 

**Method 1:** 
MobileNetv1 Trace

<img width=""1440"" alt=""image"" src=""https://user-images.githubusercontent.com/7610546/43350531-b0ad3f8e-9225-11e8-95da-d21ac7cdc1e6.png"">

MobileNetv2 Trace

<img width=""1440"" alt=""image"" src=""https://user-images.githubusercontent.com/7610546/43350534-bc7a644a-9225-11e8-8032-ec10bddf52a9.png"">

Eyeballing it, V2 is only slightly faster than V1; measuring from Conv1 to SoftMax,

v1: 9.73 ms
v2: 8.153 ms

**Method 2:**

MobileNetv1 Trace

<img width=""1440"" alt=""image"" src=""https://user-images.githubusercontent.com/7610546/43350556-ea35af48-9225-11e8-8f2d-56ccd7e600d3.png"">

MobileNet v2 Trace

<img width=""1437"" alt=""image"" src=""https://user-images.githubusercontent.com/7610546/43350562-f60ebda0-9225-11e8-992d-5c7ae66a5eb4.png"">

As you can clearly see, when the models are loaded from a frozen format, v2 is slower than v1. This affects the performance timings for v2 especially while deployment since models are usually exported in the frozen format.


### Time Measurements with Python's time module

Apart from the time traces above, I calculated the FPS by measuring the time between session.run. Even though the frozen models are faster than the checkpoint format, v2 is slower than v1 in the frozen format. Why is that so?

Model | Checkpoint - FPS | Frozen (FPS)
-- | -- | --
MobileNetv1 | 97.6 | 254.86
MobileNetv2 | 159.22 | 185.04


### Benchmark Model Tool Results


**MobileNet_v1_75**

Command Used:


[Node type] | [count] | [avg ms] | [avg %] | [cdf %] | [mem KB] | [times called]
-- | -- | -- | -- | -- | -- | --
Conv2D | 15 | 1.044 | 21.20% | 21.20% | 8658.432 | 15
gpu:Conv2D | 15 | 0.571 | 11.60% | 32.80% | 0 | 35
BiasAdd | 28 | 0.557 | 11.31% | 44.11% | 0 | 28
Add | 27 | 0.493 | 10.01% | 54.12% | 0 | 27
Mul | 27 | 0.48 | 9.75% | 63.87% | 0 | 27
Relu6 | 27 | 0.414 | 8.41% | 72.28% | 0 | 27
DepthwiseConv2dNative | 13 | 0.392 | 7.96% | 80.24% | 540.672 | 13
Const | 113 | 0.37 | 7.51% | 87.75% | 0 | 113
gpu:Mul | 27 | 0.082 | 1.67% | 89.42% | 0 | 27
gpu:Add | 27 | 0.081 | 1.65% | 91.06% | 0 | 27
gpu:DepthwiseConv2dNative | 13 | 0.061 | 1.24% | 92.30% | 0 | 13
NoOp | 1 | 0.061 | 1.24% | 93.54% | 0 | 2
gpu:BiasAdd | 28 | 0.059 | 1.20% | 94.74% | 0 | 28
Transpose | 2 | 0.055 | 1.12% | 95.86% | 49.152 | 2
gpu:Relu6 | 27 | 0.054 | 1.10% | 96.95% | 0 | 27
gpu:MEMCPYHtoD | 1 | 0.043 | 0.87% | 97.83% | 0 | 1
Softmax | 1 | 0.04 | 0.81% | 98.64% | 0.512 | 1
AvgPool | 1 | 0.033 | 0.67% | 99.31% | 3.072 | 1
gpu:Softmax | 1 | 0.009 | 0.18% | 99.49% | 0 | 3
gpu:AvgPool | 1 | 0.006 | 0.12% | 99.61% | 0 | 1
_Arg | 1 | 0.006 | 0.12% | 99.74% | 0 | 1
gpu:Transpose | 1 | 0.005 | 0.10% | 99.84% | 0 | 1
_Retval | 1 | 0.004 | 0.08% | 99.92% | 0 | 1
Reshape | 1 | 0.003 | 0.06% | 99.98% | 0 | 1
gpu:MEMCPYDtoH | 1 | 0.001 | 0.02% | 100.00% | 0 | 1
Total |   | 4.924 |   |   |  

**MobileNet_v2_75**

Command Used:


[Node type] | [count] | [avg ms] | [avg %] | [cdf %] | [mem KB] | [times called]
-- | -- | -- | -- | -- | -- | --
Conv2D | 39 | 2.967 | 37.97% | 37.97% | 7955.2 | 39
Add | 53 | 1.217 | 15.58% | 53.55% | 0 | 53
gpu:Conv2D | 39 | 1.036 | 13.26% | 66.80% | 0 | 93
Relu6 | 36 | 0.554 | 7.09% | 73.89% | 0 | 36
DepthwiseConv2dNative | 17 | 0.408 | 5.22% | 79.11% | 959.232 | 17
Const | 129 | 0.403 | 5.16% | 84.27% | 0 | 129
Mul | 17 | 0.304 | 3.89% | 88.16% | 0 | 17
AddN | 12 | 0.264 | 3.38% | 91.54% | 0 | 12
gpu:Add | 53 | 0.161 | 2.06% | 93.60% | 0 | 53
NoOp | 1 | 0.076 | 0.97% | 94.57% | 0 | 2
gpu:Relu6 | 36 | 0.073 | 0.93% | 95.51% | 0 | 36
gpu:DepthwiseConv2dNative | 17 | 0.07 | 0.90% | 96.40% | 0 | 17
Transpose | 2 | 0.053 | 0.68% | 97.08% | 49.152 | 2
gpu:Mul | 17 | 0.052 | 0.67% | 97.75% | 0 | 17
gpu:MEMCPYHtoD | 1 | 0.044 | 0.56% | 98.31% | 0 | 1
Softmax | 1 | 0.041 | 0.53% | 98.84% | 0.512 | 1
AvgPool | 1 | 0.031 | 0.40% | 99.23% | 3.84 | 1
gpu:AddN | 12 | 0.025 | 0.32% | 99.55% | 0 | 12
gpu:Softmax | 1 | 0.009 | 0.12% | 99.67% | 0 | 3
gpu:AvgPool | 1 | 0.007 | 0.09% | 99.76% | 0 | 1
_Arg | 1 | 0.006 | 0.08% | 99.83% | 0 | 1
gpu:Transpose | 1 | 0.005 | 0.06% | 99.90% | 0 | 1
_Retval | 1 | 0.004 | 0.05% | 99.95% | 0 | 1
Reshape | 1 | 0.003 | 0.04% | 99.99% | 0 | 1
gpu:MEMCPYDtoH | 1 | 0.001 | 0.01% | 100.00% | 0 | 1
Total |   | 7.814 |   |   |  


**Avg Time
v1: 4.924 ms
v2: 7.814 ms**

### Source code / logs

How I load the model with method 1:



How I load the model with method 2:



I'm interested to know the details on why v2's performance is only half as good as v1 when in the paper it is discussed that v2 is supposed to be 35% faster.

Note:
I've cross checked this with other hardware such as P40, an i5 7th Gen CPU and an alternative TF version (1.5.0) and this pattern is the same."
170,11591,1,"slice_input_producer bug (slows down exponentially for larger datasets). My input pipeline involves sampling a filename and label with slice_input_producer, then data augmentation, then batching with tf.train.shuffle_batch. For smaller datasets (<1M), this works fine. For larger datasets, slice_input_producer slows down, even though all it's doing is sampling from two lists. It's bad enough that it's several orders of magnitude slower than all the rest of my input pipeline and training combined. I wrote a quick test to measure the time per call to slice_input_producer for different lengths of input lists:


Which gives:

> Datalength: 1000  ---  time = 0.172 ms/sample
> Datalength: 1000000  ---  time = 0.207 ms/sample
> Datalength: 10000000  ---  time = 0.537 ms/sample
> Datalength: 100000000  ---  time = 13.991 ms/sample


python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.2.0-0-g12f033d', '1.2.0')"
184,35167,1,"Batch norm internal variables keep changing during inference. **System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device if the issue happens on mobile device: Dell Precision Tower 7910
- TensorFlow installed from (source or binary): tensorflow-gpu 1.14.0
- TensorFlow version (use command below): tensorflow-gpu 1.14.0
- Installed from: conda
- Python version: python 3.6
- GCC/Compiler version (if compiling from source): 6.5.0
- CUDA/cuDNN version: 10.1 but nvcc 7.5
- GPU model and memory:Quadro M4000


**Describe the current behavior**
I've encountered weird behavior of batch normalization in TF1.14 (I will update ulteriorly for TF2). The accuracy decrease a lot after the graph freezing. I identified that the origin of this discrepency is due to the batch normalization. A smaller model code to reproduce this behavior is attached at the end of this post. 
After training, I freeze the graph with  then optimize with . From the results below, you can see that the from training to testing it changed slightly but **just after** freezing, the **moving_avg, beta and gamma changed tremendously** but not the moving_std:

> - [x] Training:   

>0%|          | 0/100 [00:00<=X, X/its] mov_avg: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],  mov_std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.],  beta: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.],  gamma: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
> 
> 100%|██████████| 100/100 [00:01<00:00, 79.00it/s] mov_avg:
> [**-0.18366139  0.24028867  0.91940075 -0.07153109**  0.08811506
> -0.5552308
>   0.12759416  0.11020644  0.47183645 -0.91372997 -0.35405988  0.5768641 ],  mov_std: [**0.36972958 0.36972958 0.36972958 0.36972958** 0.36972958 0.36972958
>  0.36972958 0.36972958 0.36972958 0.36972958 0.36972958 0.36972958],  beta: [**0.         0.         0.         0.00995897** 0.         0.
>  0.00995881 0.00995875 0.         0.00989968 0.         0.        ],  gamma: [**1.         1.         1.         1.0009596**  1.         1.
>  **0.99905616 0.99986655 1.         1.0005279**  1.         1.        ]
> 
> - [x] Testing: 

>0%|          | 0/5 [00:00<=X, Xit/s] mov_avg: [**-0.18473879  0.24169827  0.92479414 -0.07199407**  0.08863196
> -0.5584879
>   0.12828277  0.11082216  0.47460434 -0.9191201  -0.35613686  0.5802481 ],  mov_std: [**0.36603227 0.36603227 0.36603227 0.36603227** 0.36603227 0.36603227
>  0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227],  beta: [**0.         0.         0.         0.01005934** 0.         0.
>  0.01005919 0.01005913 0.         0.00999967 0.         0.        ],  gamma: [**1.         1.         1.         1.0009888**  1.         1.
>  0.99904144 0.999887   1.         1.0005565  1.         1.        ]
> 
> 100%|██████████| 100/100 [00:00<00:00, 182.68it/s] mov_avg:
> [**-0.18473879  0.24169827  0.92479414 -0.07199407  0.08863196**
> -0.5584879
>   0.12828277  0.11082216  0.47460434 -0.9191201  -0.35613686  0.5802481 ],  mov_std: [**0.36603227 0.36603227 0.36603227 0.36603227 0.36603227** 0.36603227
>  0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227],  beta: [**0.         0.         0.         0.01005934 0.**         0.
>  0.01005919 0.01005913 0.         0.00999967 0.         0.        ],  gamma: [**1.         1.         1.         1.0009888  1.**         1.
>  0.99904144 0.999887   1.         1.0005565  1.         1.        ]

> - [x] Optimized (is_training = False) 

>0%|          | 0/100 [00:00<=X, Xit/s] mov_avg: [ **0.31595632 -0.19200048 -0.80097497  0.22726376**
> **-0.2556441**  -0.1720414
>   0.05251308  0.5521817  -0.93566465 -0.14799471 -0.11172786 -0.03040024],  mov_std: [**0.36603227 0.36603227 0.36603227 0.36603227 0.36603227** 0.36603227
>  0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227],  beta: [**0.         0.         0.         0.         0.00999963** 0.
>  0.01005759 0.         0.         0.         0.         0.        ],  gamma: [**1.         1.         1.         1.         1.0017155**  1.
>  0.99796325 1.         1.         1.         1.         1.        ]
> 
> 100%|██████████| 100/100 [00:00<00:00, 404.37it/s] mov_avg: [
> 0.31595632 -0.19200048 -0.80097497  0.22726376 -0.2556441  -0.1720414
>   0.05251308  0.5521817  -0.93566465 -0.14799471 -0.11172786 -0.03040024],  mov_std: [0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227
>  0.36603227 0.36603227 0.36603227 0.36603227 0.36603227 0.36603227],  beta: [0.         0.         0.         0.         0.00999963 0.
>  0.01005759 0.         0.         0.         0.         0.        ],  gamma: [1.         1.         1.         1.         1.0017155  1.
>  0.99796325 1.         1.         1.         1.         1.        ]


**Describe the expected behavior**
I was expecting these local variables **not change when we switch the training parameter between True/False or while we freeze it**.

**Code to reproduce the issue**





Is this **an expected behavior?**. I don't think so, because the NN is very sensitive if we change even a very little bit of the beta and gamma. Is this be remedied in the 2.0 version? 
"
563,23553,0,"Relationships between Grappler, GraphOptimizer and GraphOptimizationPass. There are so many graph optimization tools in TensorFlow Runtime such as ,  and . This makes me confused.  Is there any plan to unify these interfaces？Currently, it seems that these optimizers could not replace with each other.

 is very light weight because it does graph optimization on  .The limitation is that  these  can only run after .  However,  some optimizers in  will rewrite placement information. Therefore,  this will possibly cause some placement errors without the protection of (Is it reasonable to  run  after running grappler?). Another, the  function is not being used in framework.

 does optimization on  object. The advantage is that these optimizers can be specified with execution stage(PRE_PLACEMENT, POST_PLACEMENT......) .This is necessary. I think the execution stage of graph optimizers should be treated differently. Some optimizers should be done before placement and the other may be suitable for post partitioning and etc. 

As for , I heard that it will not be updated in the future. 

It will be nice to unify them and bring their strength together.  @caisq "
300,19412,0,"[Batch Normal] Would you have plan to sync batch normalization?. For semantic segmentation, it is very import to syncbn. would you tell us how to do it?"
791,35190,0,"Conv2D accepts strings as 'filters' parameter, but fails to handle them correctly. **System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux, Linux-5.4.2-1-MANJARO-x86_64-with-arch-Manjaro-Linux
- TensorFlow installed from (source or binary):  binary using pip 
- TensorFlow version: 2.0.0
- Python version: 3.7.5
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

I create a Conv2D layer with string  as parameter for .
Creation of the object does not fail. However trying to create a tensor out of
this object results in the following exception:


You can also create a Conv2D object with other strings like  and it will not fail either.
Using """" yields:


**Describe the expected behavior**

I expect that creation of the Conv2D object fails with a ValueError when used with a string or that it tries to convert the string into int.
If the string cannot be converted to int, an exception should be raised at the creation time of the layer, not when building the tensor. Right now, this is different behavior to parameters like  which fails in the stage of building the layer object when presented with an not int-convertible string.

**Code to reproduce the issue**

and
"
87,28353,1,"mAP increase but classification loss increase. mAP increase, but classification loss increase in object detection training. (by tensorflow object detection API)
Train data : 1500, Val data : 150
Class : 1 (drone)
I don't know what happen


![image](https://user-images.githubusercontent.com/3411873/57116734-11a21b00-6d92-11e9-867b-d10a7e16e52d.png)
"
111,30953,1,"TF-Lite micro low performance. This issue regards the tflite micro.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: STM32F746NG
- TensorFlow installed from (source or binary): github
- TensorFlow version (use command below): 1.14.0-718503b075d
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
I've ported a model from Keras to TF and trained it. Then I've converted the model to tflite.

One issue is that the tf.lite.Optimize.OPTIMIZE_FOR_SIZE doesn't work as the tflite-micro complains that one of the layers is not compatible with int8 types. Anyway, I've converted the model without quantization and also used CMSIS-NN. The performance is extremely slow. It's 40x times slower on the same CPU compared to X-CUBE-AI API. 

**Describe the expected behavior**
I expected the inference to be much faster especially for the depthwise_conv.cc kernel which supports the cmsis-nn. It seems that using cmsis-nn, makes an insignificant difference.

**Code to reproduce the issue**
I have a repo here that you can use for validation:
https://bitbucket.org/dimtass/stm32f746-tflite-micro-mnist

**Other info / logs**
Also, I've written a blog post with the issue here:
https://www.stupid-projects.com/machine-learning-on-embedded-part-3/

**Edit**: I've found out that I haven't enabled the FPU during the g++ compilation. With the correct architecture flags (-mthumb -mcpu=cortex-m7 -mfpu=fpv5-sp-d16 -mfloat-abi=hard), the performance is now 3x times faster, but it's still ~12x times slower than the x-cube-ai.

This is the execution time for each layer with comparison to x-cube-ai. Have in mind that the x-cube-ai merges some layers, so I'm using '-' and the sum in the next layer:

Layer | tflite-micro /soft-float (msec) | tflite-micro /hard-float (msec) | x-cube-ai (msec)
-|-|-|-
DEPTHWISE_CONV_2D | 235 | 69 | -
MAX_POOL_2D | 23 | 7 | 11.2
CONV_2D | 2346 | 733 | -
MAX_POOL_2D | 7 | 2 | 57.19
CONV_2D | 348 | 108 | 8.69
FULLY_CONNECTED | 5 | 3 | -
FULLY_CONNECTED | 0 | 0 | -
SOFTMAX | 0 | 0 | 2
TOTAL TIME | 2964 | 922 | 78.2

That's almost 40 times slower on the same MCU. I'm not sure if I'm doing something so terribly wrong or this is the real performance of the API.

I really hope for some input. Thanks!"
1134,28200,0,"Deprecation warning in ctc_batch_cost function (python/keras/backend.py). This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

OS: Arch linux
Tensorflow: 2.0.0alpha0

WARNING: Logging before flag parsing goes to stderr. 
W0426 20:17:50.524740 140527476778624 deprecation.py:323] From /python3.7/site-packages/tensorflow/python/keras/backend.py:5151: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. 
Instructions for updating:
Use  instead.

W0426 20:17:50.689021 140527476778624 deprecation.py:323] From /python3.7/site-packages/tensorflow/python/keras/backend.py:5130: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use  instead.

"
95,23759,1,"longer latency after post-training quantization. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I modified the example script a bit to print inference latency
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Google Colaboratory with CPU
- **TensorFlow installed from (source or binary)**:
! pip install -U tf-nightly==1.13.0.dev20181027
- **TensorFlow version (use command below)**:
tf-nightly-1.13.0.dev20181027
- **Python version**:
2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
The post-training quantization documentation claims that it provides 3x lower latency: https://www.tensorflow.org/lite/performance/post_training_quantization

But when I try with the example script: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb
I found that quantized model is apparently slower than original one.
(You can see the logs below. Quantized model provides longer latency).

Why is the experiment result different from the documentation claim?
Should I try some other example/environment?


### Source code / logs
The original one with floating-point:
Accuracy after 500 images: 0.970000 average time: 0.002977
Accuracy after 1000 images: 0.960000 average time: 0.003007
Accuracy after 1500 images: 0.952667 average time: 0.003022
Accuracy after 2000 images: 0.951500 average time: 0.003026
Accuracy after 2500 images: 0.949600 average time: 0.003026
Accuracy after 3000 images: 0.954000 average time: 0.003024
Accuracy after 3500 images: 0.956857 average time: 0.003029
Accuracy after 4000 images: 0.954000 average time: 0.003031
Accuracy after 4500 images: 0.954222 average time: 0.003032
Accuracy after 5000 images: 0.954200 average time: 0.003038
Accuracy after 5500 images: 0.957091 average time: 0.003043
Accuracy after 6000 images: 0.958500 average time: 0.003047
Accuracy after 6500 images: 0.959692 average time: 0.003050
Accuracy after 7000 images: 0.960714 average time: 0.003050
Accuracy after 7500 images: 0.962400 average time: 0.003051
Accuracy after 8000 images: 0.964375 average time: 0.003052
Accuracy after 8500 images: 0.965059 average time: 0.003051
Accuracy after 9000 images: 0.966889 average time: 0.003052
Accuracy after 9500 images: 0.968000 average time: 0.003052
Accuracy after 10000 images: 0.966700 average time: 0.003054
total time: 30.540090
0.9667


With post-training quantization:
Accuracy after 500 images: 0.970000 average time: 0.004113
Accuracy after 1000 images: 0.959000 average time: 0.004096
Accuracy after 1500 images: 0.950667 average time: 0.004093
Accuracy after 2000 images: 0.950000 average time: 0.004084
Accuracy after 2500 images: 0.948000 average time: 0.004077
Accuracy after 3000 images: 0.952667 average time: 0.004081
Accuracy after 3500 images: 0.955714 average time: 0.004103
Accuracy after 4000 images: 0.953000 average time: 0.004125
Accuracy after 4500 images: 0.953111 average time: 0.004133
Accuracy after 5000 images: 0.953200 average time: 0.004130
Accuracy after 5500 images: 0.956182 average time: 0.004126
Accuracy after 6000 images: 0.957667 average time: 0.004132
Accuracy after 6500 images: 0.958923 average time: 0.004137
Accuracy after 7000 images: 0.960000 average time: 0.004142
Accuracy after 7500 images: 0.961600 average time: 0.004146
Accuracy after 8000 images: 0.963625 average time: 0.004149
Accuracy after 8500 images: 0.964353 average time: 0.004148
Accuracy after 9000 images: 0.966222 average time: 0.004145
Accuracy after 9500 images: 0.967368 average time: 0.004140
Accuracy after 10000 images: 0.966100 average time: 0.004136
total time: 41.360908
0.9661

We can see after post-training quantization, the latency is apparently longer......
But in the documentation, it claims to reduce latency....
"
1050,25543,0," tf.train.ExponentialMovingAverage scope issue. I copied the tower_loss method from https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py and I have a problem with the scope of the  variable. What is wrong and how can I fix it??




Trace:
"
1015,13932,0,"Non-determinism from `tf.data.Dataset.map` with random ops. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes -- please see the minimal reproducible example script below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12, Linux CentOS 7 (4.6.6-300.el7.centos.x86_64)
- **TensorFlow installed from (source or binary)**:  (also happens when built from source)
- **TensorFlow version (use command below)**: v1.3.0-rc1-3690-g9b9cbbe 1.5.0-dev20171023
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: N/A since nightly build reproduces the issue (but when built from source, I use 0.6.1-homebrew)
- **CUDA/cuDNN version**: a GPU is not needed to reproduce the issue (however, it has also been tested with CUDA 8.0.61 / cuDNN 7.0.1)
- **GPU model and memory**: N/A -- a GPU is not needed to reproduce the issue (however, it has also been tested with Tesla K80s)
- **Exact command to reproduce**: See minimal reproducible example below

### Describe the problem
The new  API contains a  function with a  parameter, which allows elements to be processed in parallel by multiple threads.  Although not explicitly mentioned in the API docs, prior discussions (such as a comment from [today](https://github.com/tensorflow/tensorflow/issues/13847#issuecomment-338772693)) have indicated that the  function should be deterministic (w.r.t. the graph seed) even if .  I have observed that if the function being mapped contains only non-random ops, then this determinism is observed (see step 2 below).  However, if the the function being mapped contains a random op, the results become non-deterministic for all values of .  This is unexpected, and prevents training experiments from being reproducible, unless .  Also, please note that the example below serves as a minimal example to reproduce the issue.  The real scenario involves running data augmentation during training.

### Source code / logs
1. 
2.  Run the following code to observe that  functions with only *non-random* ops are *deterministic* for *all* values of , which is the *expected* behavior:



3. Run the following code to observe that  functions with *random* ops are deterministic if , but are *non-deterministic* for values of , which seems to me to be an *unexpected* behavior:


4. Observe that swapping out the  line above with an entirely different random op such as  is also *non-deterministic* for values of ."
765,24217,0," After compiling tensorflow for golang, it was found that python tensorflow could not be imported normally.. when i import tensorflow ,it happend errors.
python 3.6.5, macOS
----> 1 import tensorflow as tf

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py in <module>
     22
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25
     26 try:

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>
     47 import numpy as np
     48
---> 49 from tensorflow.python import pywrap_tensorflow
     50
     51 from tensorflow.python.tools import component_api_helper

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKNSt3__112basic_stringIcNS4_11char_traitsIcEENS4_9allocatorIcEEEEPNS4_6vectorIPNS_6DeviceENS8_ISF_EEEE
  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Expected in: /Users/wqq/go/src/github.com/tensorflow/tensorflow/bazel-bin/tensorflow/libtensorflow_framework.so
 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"
995,33125,0,"[TF2.0] Keras layers with custom tensors as variables. (I've asked this question on [StackOverflow](https://stackoverflow.com/questions/58275253/tensorflow-2-0-keras-layers-with-custom-tensors-as-variables), but after looking closer into the source code, I'm not sure if a clean solution exits yet, so re-posting it here.)

In TF 1.x, it was possible to build layers with custom variables. Here's an example:


The printed variables of the constructed dense layer will be custom tensors we specified in the  dict:

This allows us to create layers/models that use provided tensors in  directly as their weights, so that we could further differentiate the output of the layers/models with respect to any tensors that  may depend on (particularly useful for implementing functionality in [modulating sub-nets](https://distill.pub/2018/feature-wise-transformations/), [parameter generation](https://arxiv.org/abs/1705.10301), [meta-learning](https://github.com/deepmind/learning-to-learn), etc.).

Variable scopes used to make it easy to nest all off graph-building inside scopes with custom getters and build models that used the provided tensors as their parameters. Since sessions and variable scopes are no longer advisable in TF 2.0 (and all of that low-level stuff is moved to ), what would be **the best practice** to implement the above using Keras and TF 2.0?

**Note:** It looks like  supports custom getters, but I'm running into issues passing arbitrary tensors instead of variables (e.g., the getter [gets wrapped internally](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L412-L416) which I'm not sure how affects non-variable tensors). So, not sure the use-case I outlined above is properly supported. What's the best way to go about it?"
994,2293,0,"Documentation update: virtualenv should not be run from ~. Hi,

The documentation suggests that you make a virtualenv essentially by “cd ~; mkdir tensorflow; virtualenv ~/tensorflow”. Unfortunately, for some reason I cannot comprehend, this makes virtualenv try to scan every HTML file (possibly also others) in ~; in my case, this hit a 22 MB file I had lying around, causing virtualenv to use 5+ minutes (I eventually aborted it) and 4.5 GB of RAM without really saying why.

A better workaround is “cd ~; mkdir tensorflow; cd tensorflow; virtualenv .”, since seemingly the scanning is of the current directory. The documentation should probably be updated.
"
413,11062,0,"C lib for windows with GPU support. I know that there is a windows c library built for cpu https://github.com/tensorflow/tensorflow/issues/10817.

However, I could not find one with gpu support. I've tried https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-1.2.0.zip, but obvious it does not exist. 

Is it possible to have one?

Thank you!"
45,34392,1,"Training stucking and strange GPU usage. Hi there,
Im recently trying to train my model using configs from zoo.
But sometimes my training is stucking.
full Issue is described here 
https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/issues/385

And the GPU usage looks like here - im uploading you a graph from MSI Afterburner :

https://drive.google.com/file/d/1Z6q6uQnOfMn2VGDlpcWAHAwYW10qI1jA/view?usp=sharing

Thanks for help.
"
1376,31195,0,"Can not build //tensorflow/core/kernels:cwise_ops_test. **System information**
- OS Platform and Distribution ( Linux Ubuntu 18.04):
- TensorFlow installed from (source ):
- TensorFlow version: r2.0
- Python version: Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
- Installed using virtualenv? pip? conda?: build test binary with bazel
- Bazel version (if compiling from source):Build label: 0.24.1
- GCC/Compiler version (if compiling from source): gcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) 
- CUDA/cuDNN version: cuda 10.0/ cudnn 7.6
- GPU model and memory: GeForce GTX 1050 / 2000M



**Describe the problem**
I am trying to build cwise_ops_test with bazel test -c opt --strip=never --config=cuda_clang -s //tensorflow/core/kernels:cwise_ops_test command , but failed with below error message. 
How to add --expt-relaxed-constexpr for clang? I saw this compiler option added with nvcc, so I guess clang also need a similar option, where I can add this option?

"
103,32987,1,"[tf2.0.0] tf.keras.layers.GRU incorrect output of model.fit_generator trying to run Francois Chollet's notebook. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have slightly modified Francoiis Chollet's Jupyter notebook 6.3 from his book ""Deep Learning with Python"" so that it runs on tensorflow.keras rather than keras (i.e. ""from tensorflow.keras import layers"" instead of ""from keras import layers"" etc)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): built from source
- TensorFlow version (use command below): 2.0.0 (i.e. the release)
- Python version: 3.7 conda
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10 / 7.6.4
- GPU model and memory: RTX 2080 Ti and Tesla V100 (tried on both. error occurs on both)

**Describe the current behavior**
Please see attached Jupyter Notebook 
[6.3-advanced-usage-of-recurrent-neural-networks-Copy1.zip](https://github.com/tensorflow/tensorflow/files/3680903/6.3-advanced-usage-of-recurrent-neural-networks-Copy1.zip).  I am going through Francois Chollet's book ""Deep Learning with Python"" and running the code in his Jupyter Notebooks in Tensorflow 2.0.0 adapting the code to ""import tensorflow.keras"" instead of ""import keras"".  Notebook 6.3, (under the heading ""1.6 Using recurrent dropout to fight overfitting"") has a model with a tensorflow.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, float_data.shape[-1])).  The data is read earlier in the notebook from jena_climate_2009_2016.csv. With tensorflow 2.0.0 and tensorflow.keras I get a loss of 20417499919998144512.0000 and val_loss: 1.1059 after the first epoch and similar figures after subsequent epochs.  These figures are simply wrong (see below).  It also runs about 10x slower than its supposed to.  I interrupted the kernel after 4 epochs.  The original notebook (from Francois Chollet) is here: [link to github](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb) and includes the correct output.

**Describe the expected behavior**
I ran the same code with keras (not tf.keras) using a tensorflow 1 backend a while ago and it gave correct results.  The loss after 1 or 2 epochs is supposed to be around 0.3  The validation loss is supposed to be a little less than 0.3.  The graph below this code was produced using keras and a tensorflow 1 backend.  It shows the correct output.  Francis Chollet's original notebook (as linked to github above) also shows the correct output.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Download the data as follows:
cd ~
mkdir Datasets
cd ~/Datasets
mkdir jena_climate
cd jena_climate
wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
unzip jena_climate_2009_2016.csv.zip

Run jupyter notebook and load the notebook that is attached to this issue in a Python 3.7 environment with tensorflow 2.0.0.
In the notebook replace /home/daniel with /home/(your username).
Run each cell from the beginning of the notebook so you load the data and create the generators before you get to the example under heading 1.6.  Then try to run the example.  You will find that the loss and val_loss are terribly wrong.

EDIT: Since writing this, I have tried to run more code in the notebook.  The code under heading ""1.7 Stacking recurrent layers"" also runs incorrectly in tensorflow 2.0.0 using tensorflow.keras.  The loss produced is ""nan"" (it should be around 0.3).  I think it is the same problem with layers.GRU

The problem does **not** occur with tensorflow.keras in tensorflow 1.1.4.

EDIT: The rest of the remaining code in the notebook runs correctly, e.g. bidirectional GRU runs OK"
377,1336,0,"pip install/upgrade version 0.7.1 issue -- ImportError: No module named protobuf. Had a previously working version of tensorflow 0.6

Executed single command which appeared to proceed without incident:



However, now protobuf not found during python runtime though pip seems to have installed protobuf==3.0.0b2 
### Environment info

Operating System: Centos 7

If installed from binary pip package, provide:
1. Which pip package you installed.


1. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".


"
686,3155,0,"More clear error message required. Tensorflow 0.9.0

Trying to load and execute that graph+checkpoint from C++ API: https://cloud.mail.ru/public/5yk3/No887jqS8

Model was prepared using 'freeze_graph.py'

Code based on Tensorflow examples(graph contains no input layer):


Error:

> Running model failed: Invalid argument: No OpKernel was registered to support Op 'PaddingFIFOQueue' with these attrs
>      [[Node: batch/padding_fifo_queue = PaddingFIFOQueue[capacity=32, component_types=[DT_FLOAT], container="""", shapes=[[-1,-1,3]], shared_name=""""]()]]

And that's all. Nothing that can help me to understand what to do with the error. The only information I was able to find is that the error might be due to GPU only operation, but same model runs fine from python in CPU only mode.

Clearly don't know what to do with this error message. Created stackoverflow question http://stackoverflow.com/questions/38155086/no-opkernel-was-registered-to-support-op-paddingfifoqueue-with-these-attrs And hope for some insights.

I think that it would be very useful to elaborate on this kind of errors in run_status.
"
552,20263,0,"Computing gradients in extracted subgraph which contains a 'while_loop'. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.8.0-0-g93bc2e2072 
- **Python version**: 
3.6.5
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
9.0/7.0.5
- **GPU model and memory**:
Titan XP
- **Exact command to reproduce**:

output:

### Describe the problem
TF  [issue #7404](https://github.com/tensorflow/tensorflow/issues/7404) describes that when trying to form a gradient op in an imported (sub)graph, a 'No attribute 'outer_context'' error occurs. This issue was closed with the recommendation to use  instead, so the outer context related to the while op is included.

However, this does not fully solve the problem. In some deep-learning related settings, one might want to train a model, extract a subgraph (i.e., remove training related ops), connect the extracted subgraph into a larger graph to serve as part of an ensemble, GAN and so on, and then retrain the larger graph. When there is no dependence on outer context, one can easily use graph editing tools such as  or  to achieve that, exporting and importing subgraphs and then forming new tf.gradients operations. 

The minimal example above, adapted from issue 7404, shows how this approach fails when a tf.while is used and the outer context is missing. Importing and exporting the metagraph instead would leave the tf.square op in the graph. While for this minimal example being forced to save also the loss tensor looks like a very minor limitation it is easy to conceive actual applications in which there are large parts of the graph which we might really want to exclude (e.g., a decoder network in Capsule-network training). 

Right now, the dependence on outer context for computing gradients for tf.while is incompatible with tf.graph_util.extract_sub_graph and similar operations that operate on graphdefs. I believe that this is not a negligible functionality limitation.  A [related StackOverflow question](https://stackoverflow.com/questions/50663594/computing-gradients-in-extracted-tensorflow-subgraph-which-contains-a-while-loo) was upvoted but left unanswered.

In general, from the perspective of an API user who is ignorant of the internal implementation of TF, any dependence of operations on information stored out of the graphdef is not expected, hinders graph editing (as I try to convey above) and seems patchy."
253,32218,1,"tf.signal.fft2d speed is slow and unstable in RTX2080Ti. **System information**
- OS Platform: Linux Ubuntu 18.04 (server)
- TensorFlow installed from: docker tensorflow/tensorflow  1.14.0-gpu-py3-jupyter 
- TensorFlow version: 1.14.0
- Python version: 3.6.8
- CUDA:  v10.0
- GPU model: RTX 2080Ti 

**Describe the current behavior**
The speed of fft2d operation  is very unstable at different iterations. Here is an example output of time every 100 iterations (code is shown below):

During the training, there is no other programs running.

Bur when running the same code in my local machine (GTX 1080Ti) with the same TensorFlow docker image. The speed is fast and stable.


**Describe the expected behavior**
The speed should be always very fast (about 7s/100iterations).

**Code to reproduce the issue**

"
1220,26225,0,"record_summaries_every_n_global_steps() should not execute code unless global_step % n == 0. * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
  No
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  Ubuntu 18.04
* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  No
* TensorFlow installed from (source or binary):
  Source
* TensorFlow version (use command below):
  b'v1.12.0-5845-g764109a352' 1.12.0
* Python version:
  3.6.7
* Bazel version (if compiling from source):
  Invocation ID: 42251854-036f-415c-8a52-76aac8520ea0
  Build label: 0.21.0
  Build time: Wed Dec 19 12:58:44 2018 (1545224324)
* GCC/Compiler version (if compiling from source):
  gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
* CUDA/cuDNN version:
  Cuda compilation tools, release 10.0, V10.0.130
* GPU model and memory:
  GTX 1060 Max Q, 6gb VRAM

**Describe the current behavior**
 may not be recording summaries unless , HOWEVER, it is running the code within its block anyways. In my opinion this is a huge waste of resources, especially when image logging is being done and transformations are required. Furthermore, there is no information of this behaviour on the documentation. Finally, it defeats the purpose of the function.
https://www.tensorflow.org/api_docs/python/tf/contrib/summary/record_summaries_every_n_global_steps
**Describe the expected behavior**
It should not run the code within its ""with"" block when , or documentation should be added that describes the current behaviour. It defeats the purpose of it when I can combine an if statement with  to produce better results.
**Code to reproduce the issue**

**Other info / logs**

The images were still logged every global_step % n == 0 iterations, yet as can be seen here, the code within the block still executed every global_step. I shouldn't need to add an  outside of the  function. It literally defeats the purpose of it, as I might as well just use  inside of an ."
1316,8950,0,"Execution Stuck after few steps in sess.run(). 
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Execution is stuck in between steps. For few steps, it seems to run fine but after that the execution just halts without throwing any exception or the error.

### Environment info
Operating System: Ubuntu 14.04
GPU: NVIDIA TITAN X (Pascal)
GPU Memory: 12GB

Installed version of CUDA and cuDNN: 
-rw-r--r-- 1 root root   556000 Mar 29 05:10 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Mar 29 05:10 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Mar 29 05:10 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rwxr-xr-x 1 root root   415432 Mar 29 05:10 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root   775162 Mar 29 05:10 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Apr  4 13:18 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       18 Apr  4 13:18 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 root root 84163560 Apr  4 13:18 /usr/local/cuda/lib64/libcudnn.so.5.1.10
lrwxrwxrwx 1 root root       18 Apr  4 12:55 /usr/local/cuda/lib64/libcudnn.so.6 -> libcudnn.so.6.0.20
-rwxrwxrwx 1 root root 84163560 Apr  4 13:17 /usr/local/cuda/lib64/libcudnn.so.6.0.20
-rwxrwxrwx 1 root root 70364814 Apr  4 13:18 /usr/local/cuda/lib64/libcudnn_static.a


If installed from binary pip package, provide:

1. A link to the pip package you installed:

https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl

2. The output from .
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I ran the mnist_deep.py script provided in tutorials.

### What other attempted solutions have you tried?
Previously, the display used to get hang showing error CUDA_LAUNCH_ERROR_TIMEOUT. On nvidia forum, someone suggested to switch off the X-server. I switched it off, but the problem still persisted.
I noticed that the script was hogging full memory of GPU, I tried to limit the allocation by using ""allow_growth"" flag. But the problem still persists. 

### Logs or other output that would be helpful
step 47, training accuracy 0.64
step 48, training accuracy 0.72
step 49, training accuracy 0.7
step 50, training accuracy 0.68
step 51, training accuracy 0.76
step 52, training accuracy 0.66
step 53, training accuracy 0.82
step 54, training accuracy 0.82
step 55, training accuracy 0.64
step 56, training accuracy 0.64
step 57, training accuracy 0.74
step 58, training accuracy 0.76
step 59, training accuracy 0.8
step 60, training accuracy 0.68
step 61, training accuracy 0.88
step 62, training accuracy 0.62
step 63, training accuracy 0.84
step 64, training accuracy 0.72
step 65, training accuracy 0.76
step 66, training accuracy 0.74
step 67, training accuracy 0.86
step 68, training accuracy 0.76
step 69, training accuracy 0.9
step 70, training accuracy 0.84
step 71, training accuracy 0.82
step 72, training accuracy 0.7

Attached is the screenshot of ""nvidia-smi""
![nvidia-smi](https://cloud.githubusercontent.com/assets/1628210/24651492/a5b54ada-194b-11e7-8ba2-9c3ff6dc2901.PNG)

"
1199,5470,0,"Cannot import graph_def for 8-bit Quantized cnn model. Following steps I did:
1. Train a CNN model on MNIST using tensorflow tutorials(Deep MNIST for Experts)(CONV+CONV+FC). Saved this model as binary protobuf(.pb).
2. Using 8-bit Quantization api, converted previously saved model to 8-bit model as binary proto(.pb). 
3. Read 8-bit binary-proto to tensorflow, by first creating graph_def and importing graph def to tf.Session()
 

However, this gives error on tf.import_graph_def as


### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?


### Environment info
Operating System: Ubuntu 16.04 + tensorflow 0.11.0rc2

Installed version of CUDA and cuDNN: No


If installed from binary pip package, provide:

1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl





### What other attempted solutions have you tried?
1. I am able to correctly import quantized graph def for pre-trained inception model(.pb).
2. When I define Multi-layer perceptron model for MNIST, it can import quantized graph_def. 
3. All the quantization headers are imported correctly as
 

"
130,5898,1,"pool efficiency on cpu. ### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I have searched about cpu usage and try everything I can. But it's still pool efficiency usage, **only use 30% of my cpu**
I thought it maybe the train-dataset read speed limitation?

### Environment info
Operating System:
centos6.5 with latest version of tensorflow 

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
full code here
https://github.com/tobegit3hub/deep_recommend_system/blob/master/a8a_classifier.py


### What other attempted solutions have you tried?
try the specify the thread number when init session , the shuffle_batch num_threads etc.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
here is a tensorboard screenshot

<img width=""750"" alt=""screen shot 2016-11-28 at 4 15 15 pm"" src=""https://cloud.githubusercontent.com/assets/918889/20661115/0014e198-b588-11e6-8c33-b692c32568cd.png"">

"
1030,20659,0,"tensorflow. 
"
173,4939,1,"System hangs while training. Hi everyone,

I am trying to train model for handwritten digits. My data consists of images, each of size 80X80. So I get 6400 features as inputs. 
While trying to train model as per code used in https://www.kaggle.com/kakauandme/digit-recognizer/tensorflow-deep-nn, my systems hangs after 200 iterations with training accuracy 0.06.

Why is this happening? I don't get any errors. My system just freezes.
Also, how do I set pooling and convolution layers parameters? 

PS: I'm not using GPU. 
"
1017,2375,0,"Matplotlib incompatibility when using virtualenv. RuntimeError when importing matplotlib.pyplot in a jupyter notebook from a virtual environment:



I have solved the problem using one of the solutions mentioned at: [Matplotlib VirtualEnv FAQ](http://matplotlib.org/faq/virtualenv_faq.html) and that is ok when using python from the command line or a script file, but not with the kernel used in jupyter.

So, the question is: how to create a jupyter kernel that uses a specific bash which properly setups python to use matplotlib within a virtual environment.

Another solution is:



But maybe that is not the best thing to do. So, any suggestions?.
Operating System: OS X El capitan
Tensorflow version: 0.8.0
"
606,12203,0,"Why not develop  32-bit？Thank you very much!. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
1035,8419,0,"Error in compiling TensorFlow from source on Ubuntu: bazel build fails.. I am compiling TensorFlow from source on an Ubuntu machine. This is a verbose output of the error log generated in the  execution. 

System details:
: Linux - 3.13.0-107-generic



    ERROR: /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/external/llvm/BUILD:418:5: Generating code from table: lib/Target/AArch64/AArch64.td @llvm//:aarch64_target_gen__gen_fast_isel_genrule failed: bash failed: 
    error executing command 
    (cd /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/execroot/tensorflow && \
    exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; 
    bazel-out/host/bin/external/llvm/llvm-tblgen -I external/llvm/include -I external/llvm/tools/clang/include -I    
    $(dirname external/llvm/lib/Target/AArch64/AArch64.td) -gen-fast-isel external/llvm/lib/Target/AArch64/AArch64.td -o bazel-out/local-opt/genfiles/external/llvm/lib/Target/AArch64/AArch64GenFastISel.inc'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.
    bazel-out/host/bin/external/llvm/llvm-tblgen: relocation error: bazel-out/host/bin/external/llvm/llvm-tblgen: symbol _ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm, version GLIBCXX_3.4.21 not defined in file libstdc++.so.6 with link time reference
    Target //tensorflow/tools/pip_package:build_pip_package failed to build
"
277,10749,1,"Numpy.fft.fft2() gives different result than tf.fft2d(). as mentioned in the issue #6401, the tf.fft2d() gives different result compared to np.fft.fft2(). Is there a reason for this ?

Note : numpy gives proper fourier transform after np.fft.fftshift(), and I have taken care of that in my code.
The differences are not visible here, but the mean squared error is significant.

![image1](https://user-images.githubusercontent.com/18488880/27207200-84637a88-5202-11e7-9d49-d22cf9a8057d.png)![image2](https://user-images.githubusercontent.com/18488880/27207198-8462f2b6-5202-11e7-88c2-7114b36c696a.png)    ![image3](https://user-images.githubusercontent.com/18488880/27207199-8462fe50-5202-11e7-8d7b-bca01bba063b.png)   ![image4](https://user-images.githubusercontent.com/18488880/27207631-f2a41284-5205-11e7-91a6-336d75755787.png)

Second image is the fft using tensorflow, and third one is using numpy. You can see the difference in the corners. The fourth image is the difference between the two images times 10.

 (tf has some features while numpy does not)

I am working on an application which uses fft in backpropagation and thus it is of absolute importance that the fft in numpy are same as fft by tf.

My question is - Why is there a difference and how can I get the same fft as numpy ?


"
1188,23410,0,"UnboundLocalError: local variable 'a' referenced before assignment. hi there, i have install phyton 3.7 with the following build : v1.11.0-rc2-4-gc19e29306c 1.11.0

here the phyton sample code: 

keras_file = ""keras_model.h5""
tf.keras.models.save_model(model, keras_file)

# Convert to TensorFlow Lite model.
converter = tf.contrib.lite.TocoConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

when it try to execute : converter = tf.contrib.lite.TocoConverter.from_keras_model_file(keras_file)

and throw the following error:

converter = tf.contrib.lite.TocoConverter.from_keras_model_file(keras_file)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 354, in from_keras_model_file
    _keras.backend.clear_session()
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/keras/backend.py"", line 335, in clear_session
    False, shape=(), name='keras_learning_phase')
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5148, in placeholder_with_default
    ""PlaceholderWithDefault"", input=input, shape=shape, name=name)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1144, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 228, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 207, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/Users/htan/venv/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 542, in make_tensor_proto
    append_fn(tensor_proto, proto_values)
  File ""tensorflow/python/framework/fast_tensor_util.pyx"", line 134, in tensorflow.python.framework.fast_tensor_util.AppendBoolArrayToTensorProto
  File ""/Users/htan/venv/lib/python3.7/site-packages/numpy/lib/type_check.py"", line 489, in asscalar
    return a.item()
UnboundLocalError: local variable 'a' referenced before assignment"
709,23389,0,"projector.tensorflow.org - ""Load bookmarks"" broken in Chrome 70. The ""Load bookmarks"" button (below) in projector.tensorflow.org doesn't seem to be working: 

![screen shot 2018-10-30 at 2 38 16 pm](https://user-images.githubusercontent.com/8292856/47752506-dd6a6800-dc51-11e8-9ac9-3fb93a6bf599.png)




Below is the error as reported in Chrome JS console:



**System information**
Chrome Version 70.0.3538.77 (Official Build) (64-bit)
macOS High Sierra

"
989,17400,0,"Eager: Allowing GPU memory growth. Since we don't have session in eager mode, how can we allocate only as much GPU memory as needed in our program?

Or can we set the fraction of the overall amount of memory that each visible GPU should be allocated?

When will this feature be supported?"
1299,3195,0,"Documentation of Session arguments seems outdated. In [README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/README.md) a session is created as . At the same time, the [docstring](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session.py#L888) says ""no value other than an empty string is supported"". This is clearly an inconsistency.
"
387,33152,0,"TPU has XLA compilation issue on TF 1.15rc3. **System information**
python: 
tensorflow version: 
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3704020/tf_env.txt)

**Describe the current behavior**

Error message I get is the following:



**Describe the expected behavior**
If I used tf.train.GradientDescentOptimizer instead it would work, I expect using momentum optimizer to work as well.

**Code to reproduce the issue**


**Other info / logs**

Issue occurs when using keras  layer alongside an optimizer that is not sgd.
The issue can be fixed by modifying keras implementation by changing:

to


The issue is in the weights being an IndexedSlice which causes any non-sgd optimizer to use the  operation which is unsupported by XLA. However, if you do  it makes that expression the IndexedSlice and weights itself doesn't become an IndexedSlice but rather a tensor so the optimizer implementation in tensorflow doesn't call .
"
193,29558,1,"model.fit() does not reshuffle the dataset between epochs. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION='2.0.0-dev20190606'
GIT_VERSION='v1.12.1-3447-g5a0f1bbfb7'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When calling  with a finite shuffled dataset, the model is trained on the same dataset order at each epoch.

**Describe the expected behavior**
I expect the dataset to be reshuffled after each epoch. Right now, it's not, even when I use  in the dataset's  method. This argument seems to only shuffle between iterations within one epoch.

**Code to reproduce the issue**



**Other info / logs**
The output is as follows (I've added comments):



As you can see the order of the data is perfectly identical during the 1st and 2nd epochs. It is only reshuffled at each iteration within the same epoch.

So the only way to ensure that the data will be reshuffled at each epoch is to use , then . It feels like unnecessary complexity."
276,26620,1,"No improvement in performance of mobilenet_v1_1.0_224 on TFLite for GPU. **System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 8.0.0
- Mobile device: Samsung S9+, Sony XPeria XZ2
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): - (used pretrained *.tflite file)
- Python version: - (used pretrained *.tflite file)
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: Exynos 9810 Octa (Samsung), Sanpdragon 845 (Sony)

**Describe the current behavior**

No differences in performance for Samsung S9+ between CPU and GPU versions.

**Describe the expected behavior**

According to [this TensorFlow's article on Medium](https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7) expected at least 3x improvement.

**Code to reproduce the issue**
TFLite archive for MobileNet v1 (224x224) has been downloaded from link from [article, mentioned above](https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7)

Code:

**Other info / logs**

***Samsung (Exynos GPU)***

Version with GPUDelegate is a little bit slower (170 ms per run vs 140 ms)

***Sony XPeria (Sanpdragon GPU)***

Version with GPUDelegate is a little bit faster (102 ms per run vs 125 ms).

The warning  **does not** appear in LogCat during the execution, so it doesn't look like GpuDelegate couldn't handle some operation.
"
1333,6201,0,"Undefined symbols for architecture x86_64. I builded a sample of ios app from https://github.com/yjmade/ios_camera_object_detection, but:



Is that any problem of my tensorflow?
Mac pro  
Intel Iris Graphics 6100 1536 MB
64bit"
961,35008,0,"convert to tflite failed using tf.lite.TFLiteConverter.from_saved_model and validate failed. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
when i convert saved lstm tensorflow model to the tflite file and meets the error.
this is piece of code :
def create_LSTM_model(inputs):
    W = {
        
        'hidden': tf.Variable(tf.keras.backend.random_normal([N_FEATURES, N_HIDDEN_UNITS])),
        'output': tf.Variable(tf.keras.backend.random_normal([N_HIDDEN_UNITS, N_CLASSES]))
        'output': tf.Variable(init_w_output)
    }
    biases = {
        'hidden': tf.Variable(tf.keras.backend.random_normal([N_HIDDEN_UNITS], mean=1.0))
        'output': tf.Variable(tf.keras.backend.random_normal([N_CLASSES]))
    }

    X = tf.transpose(inputs, [1, 0, 2])
    X = tf.reshape(X, [-1, N_FEATURES])
    hidden = tf.nn.relu(tf.matmul(X, W['hidden']) + biases['hidden'])
    hidden = tf.split(hidden, N_TIME_STEPS, 0)

    def get_a_cell(lstm_size, keep_prob):
        lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_size)
        drop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)
        return drop

    # lstm_layers = [tf.compat.v1.nn.rnn_cell.BasicLSTMCell(N_HIDDEN_UNITS, forget_bias=1.0) for _ in range(2)]
    lstm_size = N_HIDDEN_UNITS
    keep_prob = 0.9
    num_layers = 2
    # lstm_layers = tf.compat.v1.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size, keep_prob) for _ in range(num_layers)])
    lstm_layers = tf.compat.v1.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size, keep_prob) for _ in range(num_layers)])

    outputs, _ = tf.compat.v1.nn.static_rnn(lstm_layers, hidden, dtype=tf.float32)
    # keras.layers.RNN  outputs, _ = tf.keras.layers.RNN(lstm_layers, hidden, dtype=tf.float32)

    # Get output for the last time step
    lstm_last_output = outputs[-1]

    return tf.matmul(lstm_last_output, W['output']) + biases['output']


tf.compat.v1.reset_default_graph()

X = tf.compat.v1.placeholder(tf.float32, [None, N_TIME_STEPS, N_FEATURES], name=""input_X"")
Y = tf.compat.v1.placeholder(tf.float32, [None, N_CLASSES], name='input_Y')

pred_Y = create_LSTM_model(X)
tf.identity(pred_Y, name=""pred_Y"")

pred_softmax = tf.nn.softmax(pred_Y, name=""pred_softmax"")

train_count = len(X_train)
L2_LOSS = 0.0015

l2 = L2_LOSS * \
     sum(tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables())

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_Y, labels=Y)) + l2
# loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1,2])-tf.reshape(Y, [-1,2])))
tf.identity(loss, name=""saved_loss"")


global_step = tf.Variable(0, trainable=False, name='global_step')

learning_rate = tf.compat.v1.train.exponential_decay(
    LEARNING_RATE_BASE,
    global_step,
    train_count / BATCH_SIZE, LEARNING_RATE_DECAY)

tf.identity(learning_rate, name=""learning_rate"")

optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)
tf.identity(loss, name=""saved_optimizer"")

correct_pred = tf.equal(tf.argmax(pred_softmax, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32), name='output_accuracy')

history = dict(train_loss=[],
               train_acc=[],
               test_loss=[],
               test_acc=[])

sess = tf.compat.v1.InteractiveSession()
sess.run(tf.compat.v1.global_variables_initializer())

export_simple_path = 'model_simple/'
if os.path.exists(export_simple_path):
    shutil.rmtree(export_simple_path)

tf.compat.v1.saved_model.simple_save(sess,
            export_simple_path,
            inputs={""input_X"": X},
            outputs={""pred_softmax"": pred_softmax})
use this to save the model
and then i convert the .pb model to tflite with python API

import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
but got the error:
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, MUL, SOFTMAX, SPLIT, TANH. Here is a list of operators for which you will need custom implementations: RandomUniform.
Traceback (most recent call last):
  File ""c:\users\d\.conda\envs\ai\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\d\.conda\envs\ai\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\d\.conda\envs\ai\Scripts\toco_from_protos.exe\__main__.py"", line 7, in <module>
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""c:\users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, MUL, SOFTMAX, SPLIT, TANH. Here is a list of operators for which you will need custom implementations: RandomUniform.

and then i add the
converter.allow_custom_ops=True

then i can convert to tflite (lstm.tflite) successfully
but then i use below command to validate my model then i got the errors:

interpreter = tf.lite.Interpreter(model_path=""lstm.tflite"")

interpreter.allocate_tensors()

>>> interpreter = tf.lite.Interpreter(model_path=""D:\Feature\model_simple_1207\lstm_har_1207.tflite"")
INFO: Initialized TensorFlow Lite runtime.
>>> interpreter.allocate_tensors()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\lite\python\interpreter.py"", line 244, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""C:\Users\d\.conda\envs\ai\lib\site-packages\tensorflow_core\lite\python\interpreter_wrapper\tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
**R**untimeError: Encountered unresolved custom op: RandomUniform.Node number 14 (RandomUniform) failed to prepare.****

could anyone  hlep with the issue?

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**

"
659,10106,0,"Feature request: improve tensorboard embeddings search. Currently, when making a search query using a custom variable, let's say a relevance score, then results display relevance scores. This is not always helpful. In my application, I have word embeddings that I would like to filter by a custom variable (relevance score), and I can't see what word is corresponding to each relevance score (see screenshot).

It would be better to have a separate field so that users can define how they would like to see the results of their search query.
![screen shot 2017-05-22 at 11 54 44](https://cloud.githubusercontent.com/assets/12895366/26317930/3acefc0e-3ee7-11e7-8f17-5b9866679e5f.png)

"
742,21081,0,"Tensort RT Engine Object Detection. tensorflow.python.framework.errors_impl.ResourceExhaustedError: Requested batch size is not available and engine cache is full
	 [[Node: import/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data="""", fixed_input_size=true, input_shapes=[[?,1], [?,1], [?,1], [?,1]], max_cached_engines_count=1, output_shapes=[[?,4]], precision_mode=""FP32"", segment_funcdef_name=""my_trt_op_16_native_segment"", serialized_segment=""\2007\000\...00\000\000"", static_engine=true, workspace_size_bytes=65075264, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](import/MultiClassNonMaxSuppression/ClipToWindow/split, import/MultiClassNonMaxSuppression/ClipToWindow/split:2, import/MultiClassNonMaxSuppression/ClipToWindow/split:1, import/MultiClassNonMaxSuppression/ClipToWindow/split:3)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[Node: import/SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/strided_slice/_89 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1900_...ided_slice"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info."
1150,19989,0,"S3 backend request fails on small object (at least) against Minio S3 storage implementation. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9 (Stretch)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.8.0-5-gcfd0ea3bfb 1.8.0
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**:gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516 / clang version 4.0.1
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

### Describe the problem
At least when using [minio](https://minio.io) for S3 storage, small object requests initiated from TFRecordDataset() fail with following error:
2018-06-13 17:02:04.857500: W tensorflow/core/platform/s3/aws_logging.cc:57] Encountered Unknown AWSError 'InvalidRange': The requested range is not satisfiable
Reason is that TF is requesting the object with buffer size Range header and when the object is smaller than the buffer (256 kB) the system is not satisfied with the smaller response size, although response HTTP header clearly indicates that the object was received in full. Instead there's an attempt to try to fetch next chunk with Range that begins at the end of object and reaches further beyond end of the object. Naturally this request will fail. This will cause the entire request fail.

### Source code / logs
2018-06-13 17:02:04.857500: W tensorflow/core/platform/s3/aws_logging.cc:57] Encountered Unknown AWSError 'InvalidRange': The requested range is not satisfiable
"
817,27829,0,"Cannot create a stateful RNN with recurrent dropout. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tf.version.VERSION=2.0.0-dev20190413
tf.version.GIT_VERSION=v1.12.0-12481-gc7ce6f4cd9
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I get an exception when trying to use  in a stateful RNN:

var.assign(var * value)var = var * valuevar.assign(var * value)var = var * valueinitial_stateconstantsvar.assign(var * value)var = var * valuevar.assign(var * value)var = var * value"
551,22396,0,"[Feature Request]:Assign the name to SaprseTensor when build_tensor_info of it. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N.A.
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Tesla P100 16GB
- **Exact command to reproduce**:
A minimum reproduce example:

### Describe the problem
A normal Tensor has a name:

while the SparseTensor from the ctc Beam_Search_Decoder does not has a name:

Which cause the error when predict:


ValueError: The name '' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form ""<op_name>:<output_index>"".
and  also the above error, when export it into a saved_model and request it.

The reason is when building a TensorProto for the SaprseTensor, no name is assigned to it:
https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/saved_model/utils_impl.py#L46

"
766,2472,0,"distributed tensorflow mnist example hang at prepare_or_wait_for_session() . I tried both Distributed [tensorflow](https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html) and [minist_softmax.py.txt](https://github.com/tensorflow/tensorflow/files/271770/mnist_softmax.py.txt). They both hang at the prepare_or_wait_for_session() function with non-chief worker tasks.

task_index == 0(chief worker ) can run as expected.
I can not find out the reasion why it behaves like this. 
Can anyone please give some advice?
"
1131,22360,0,"Op type not registered 'TRTEngineOp' in binary running with C API. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.11
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**: 1080ti/11gb
- **Exact command to reproduce**: 
1. Clone tensorflow repo, checkout r1.11 branch
2. Build from source as directed from documentation by disabling everything except cuda, tensorrt(4.0.1.6)
3. Create a tensorrt .pb file using following:

4. Use Tensorflow C API to run infernce on the protobuf file

Full Error:
2018-09-18 14:21:41.085891: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: ""TRTEngineOp"" device_type: ""GPU""') for unknown op: TRTEngineOp
check_status: Caused by: Add graph to TF session: Not found: Op type not registered 'TRTEngineOp' in binary running on dhingratul-Workstation. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.)  should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.


Related issue: #22005 , except I am using the latest version of tensorflow and tensorRT. I have also tried the workarounds mentioned [here](https://stackoverflow.com/questions/50125889/c-tensorflow-api-with-tensorrt), but don't work.
 
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
31,33340,1,"Significant prediction slowdown after model.compile(). **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.0.0
- Python version: 3.7
- CUDA/cuDNN version: CUDA=10.0, cuDNN=7.6.4
- GPU model and memory: GTX 1060 6GB


**Describe the current behavior**
The prediction speed is slowed down a lot after  call.

**Describe the expected behavior**
Speed should not be affected. Predict function is used by users assuming that it will work fast because we use it all the time in production. It should not cause surprise to users.

**Code to reproduce the issue**
https://nbviewer.jupyter.org/github/off99555/TensorFlowExperiments/blob/master/test-prediction-speed-after-compile.ipynb?flush_cache=true

![image](https://user-images.githubusercontent.com/15215732/66762282-e3dc0900-eecf-11e9-8d93-82c8bcc5325b.png)
"
1285,13892,0,"Inconsistent Result of SyncReplicaOptimizer. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac Sierra 10.12.6
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: Python 3.5.2 |Anaconda custom (x86_64)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: Not used/
- **GPU model and memory**: Not used/
- **Exact command to reproduce**: python synchronous_sgd.py (see below)

### Describe the problem
Training a trivial model of 2-layer fully connected MNIST, with one parameter server thread and one worker thread to reproduce this issue.

The file is linked here. We run  and  one after one **in the same terminal so that they receive same random results** to recreate the bug.

The only difference in the two files below is: async comment out 10 trivial lines from sync. (Please diff)

https://github.com/heyucongtom/PGRD/blob/master/synchronized_sgd.py
https://github.com/heyucongtom/PGRD/blob/master/async_sgd.py

I make sure both trainer receive the exactly same data for each batch, and I also fixed the random seed. As a results, both model shall get exactly the same output. However, they don't.

The problem is, after the first step, the two models are in sync. At exactly the **second run of the train_op**, this train_op of the sync replica doesn't update the model, nor does it update the global step, resulting in output:




**As a comparison, let's take the output of the simple async version. With exactly**



I read through the source code of SyncReplicaOptimizer and find out that the train_op returned by that optimizer is a Assign operation, which could be only executed after the grads were applied and global steps enqueued. So sync and async with only one process should be exactly the same.

This behavior is mysterious to me now. Not sure if I got anything wrong.
"
1270,15046,0,"stop gradients for weights in tf.losses. In the case that the weights given to tf.losses.* depend in some way on the model parameters, 
the derivative of that loss also calculated with respect to the weights. 
(Stupid) minimal example:

results in 


I would expect the weights to be considered constant for the calculation of a loss. In case you agree with me, I can make a PR that adds  around the weights parameter.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A
"
589,28739,0,"Assigning to ressource variables with varying shapes violates the shape information of the tensor of which is assigned from.. Hi,
I ran into problems when assigning to variables from tensors with dynamic shape. In the end, it boiled down to using resource variables and having a strided slice tensor from which to assign from, see the upcoming MWE. When assigning to resource variables from a strided slice tensor, the shape information of the tensor becomes corrupt.

**System information**
- Unexpected behaviour in custom code (minimum working example see below)
- Windows 10
- TensorFlow installed from pip wheels
- TensorFlow version:
  - tf.version.VERSION = 1.13.1
  - tf.version.GIT_VERSION = b'unknown'
  - tf.version.COMPILER_VERSION = MSVC 190024215
  - Sanity check: array([1])
- Python version: Python 3.6.7 (default, Feb 28 2019, 07:28:18) [MSC v.1900 64 bit (AMD64)] on win32
- also occurs without GPU support


**Describe the current behavior**

Output is
> *results: [1 2] [array([1]), array([1, 2]), array([2])]
> var_value_before [3]
> var_value_after [1 2]

Note the contradiction that the tensor with elements [1, 2] has shape [1]

**Describe the expected behavior**

Expected output
> *results: [1 2] [array([**2**]), array([1, 2]), array([2])]
> var_value_before [3]
> var_value_after [1 2]

**Code to reproduce the issue**



**Note**
Either , leaving out the assign_op when running the session or leaving out the stride op  will yield the expected behaviour. This may be tested with the following routine:


Output:

> use_resource=0, additional_stride=0, add_assign_op=0: [array([2]), array([1, 2])], var = [3]
> use_resource=0, additional_stride=0, add_assign_op=1: [array([2]), array([1, 2])], var = [1 2]
> use_resource=0, additional_stride=1, add_assign_op=0: [array([2]), array([1, 2])], var = [3]
> use_resource=0, additional_stride=1, add_assign_op=1: [array([2]), array([1, 2])], var = [1 2]
> use_resource=1, additional_stride=0, add_assign_op=0: [array([2]), array([1, 2])], var = [3]
> use_resource=1, additional_stride=0, add_assign_op=1: [array([2]), array([1, 2])], var = [1 2]
> use_resource=1, additional_stride=1, add_assign_op=0: [array([2]), array([1, 2])], var = [3]
> use_resource=1, additional_stride=1, add_assign_op=1: [array([1]), array([1, 2])], var = [1 2]"
410,14699,0,"A very strange bug with tf.cond, update_ops and global_step. I'm using tf.Estimator with custom model_fn. When training, the estimator usually outputs log like:

INFO:tensorflow:loss = 1109.14, step = 1
INFO:tensorflow:loss = 937.876, step = 101 (6.245 sec)
INFO:tensorflow:loss = 632.192, step = 201 (6.195 sec)

By default, the printed steps should be 1, 101, 201... However, when I use the following function (which is simplified to reproduce the bug) in any place of the model:



The log becomes something like:

INFO:tensorflow:loss = 1130.58, step = 0
INFO:tensorflow:loss = 940.298, step = 0 (6.352 sec)

The global step is always 0. After some tests, I found that the global step is not updated if myfunc2 is not executed. For example, if I write

then the global step is always 1.

I suspect that this is caused by the tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, ...) in myfunc2, in which my intend is to update a variable when some condition holds. Maybe op created inside tf.cond can not be used as a dependency outside, but no error message is reported.

If I cannot add ops to UPDATE_OPS inside tf.cond, does this imply that stateful operations (like tf.layers.batch_normalization) can not be used inside tf.cond? So I cannot dynamically choose a network module from a set of network modules to execute if the network modules use any stateful operations like tf.layers.batch_normalization?

my tensorflow version: ('v1.4.0-rc0-10-g756a7fc', '1.4.0-rc1')"
204,3973,1,"cifar10 train runs 50% slower than r0.9 in r0.10rc0. GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of ):

If installed from binary pip package, provide:
1. Which pip package you installed.
2. The output from .

If installed from source, provide 
1. The commit hash ()
2. The output of 
### Steps to reproduce

1.
2.
3.
### What have you tried?

1.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
"
167,24386,1,"TensorForest: Performance issues in time and space. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary, pip install tensorflow==1.12.0
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No usage
- GPU model and memory: No usage
- Machine RAM and #Cores: **64GB RAM and 12 CPU Cores** 

**Describe the current behavior**

Currently, I trained TensorForest on a relatively large train data (1.5M examples, 1024 features) with 500 trees, max_nodes. Moreover, I compared its performance with Scikit-learn implementation.
It seems that the TensorForest is inferior to Scikit-Learn ExtraTrees implementation both in time and space.

**In terms of space costs**, training a TensorForest forest  consists of 500 trees with 10k nodes will throw the following exception in my machine (64GB RAM and 12 CPU Cores).

But Scikit-learn based extra trees classifier only costs up to **18GB** of memory.

This issue makes it unfeasible to train TensorForest on large datasets or large number of trees.
As the paper said, It seems that TensorForest should not have such a big performance gap compared with scikit-learn, right? 
Thus I wonder whether it is my mis-use?

**In terms of time costs**,  training a TensorForest forest  consists of **only 2** trees with 10k nodes will cost **1235 second** with 116 training steps on the machine. But Scikit-learn based implementation costs only **20** seconds.
This 60-fold gap is clearly unreasonable, right?

**Describe the expected behavior**

1. Isn't it an online algorithm? It seems that the memory consumption of the TensorForest is relatively large?
2. The original paper said that, in large dataset (like HIGGS, 11M examples, 28 features), TensorForest with 100 trees and 10k nodes per tree trains in about **one percent** of the time taken by scikit-learn, even without taking advantage of distributed training. But I cannot reproduce that fast training speed.

Thus I am sincerely looking forward to your help or explanations.
Great, Great, Great thanks : )
@yupbank @yongtang @tensorflower-gardener

**Code to reproduce the issue**

Complete Code Snippet, you can run it directely without any modification.
============================== Space Cost Comparison =========================
TensorForest:


Scikit-learn:


============================== Time Cost Comparison =========================
TensorForest:


Scikit-learn:


@yupbank 
"
760,26339,0,"Build r1.12 from source with CUDA10.0 sucessfully, but still need CUDA9.0 when importing. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source 
- TensorFlow version: r1.12
- Python version: py3.6
- Installed using virtualenv? pip? conda?: source and pip 
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.9.3
- CUDA/cuDNN version: CUDA10.0/cuDNN7.5.0
- GPU model and memory: TitanXP

**Describe the problem**
Since I want to use CUDA10.0, I tried to build TF1.12 from source with above versions.
The build is successfully done, and my .tf_configure.bazelrc is correct.
After install the pip wheel package, I import the tensorflow but it still look for CUDA9.0 which is not provided in environment path.

ERROR:


I tried the pip wheel that built by other people but still get the same error.
Seems somehow CUDA9.0 is needed by some components? 
Any hint will be welcome

Configuration:
"
1049,1148,0,"Issue and Pull Request Templates. Would help with organization and getting all the relevant info. Requires adding files issue_template.md and pull_request_template.md
"
955,16351,0,"TfLiteCameraDemo failed to work with NNAPI after commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1. I am testing NNAPI by forcing TfLiteCameraDemo to invoking libneuralnetworks.so. It worked correctly though slower. But since commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1, TfLiteCameraDemo crashes with error message like,

	01-24 03:39:36.393 19136 19153 E AndroidRuntime: FATAL EXCEPTION: CameraBackground
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 19136
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to run on the given Interpreter: NNAPI was requested, but dependent sized tensors being used.
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:123)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:104)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:114)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:558)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.handleCallback(Handler.java:790)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:99)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:164)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.HandlerThread.run(HandlerThread.java:65)
	01-24 03:39:36.396   626   871 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.CameraActivity

Here is my patch

	 index e44c5ae..1ed88eb 100644
	 ---a/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 +++b/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 @@ -91,7 +91,7 @@ public class ImageClassifier {
		
	   /** Initializes an {@code ImageClassifier}. */
	   ImageClassifier(Activity activity) throws IOException {
	-    tflite = new Interpreter(loadModelFile(activity));
	+    tflite = new Interpreter(loadModelFile(activity), true);
	     labelList = loadLabelList(activity);
	     imgData =
	         ByteBuffer.allocateDirect(
	diff --git a/tensorflow/contrib/lite/java/demo/build.gradle b/tensorflow/contrib/lite/java/demo/build.gradle
	index dd883d6..9361c71 100644
	--- a/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	+++ b/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	@@ -66,6 +66,13 @@ public final class Interpreter implements AutoCloseable {
	     }
	     wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	   }
	+  public Interpreter(@NotNull File modelFile, boolean nn) {
	+    if (modelFile == null) {
	+      return;
	+    }
	+    wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	+    wrapper.setUseNNAPI(nn);
	+  }
	
	   /**
	    * Initializes a {@code Interpreter} with a {@code MappedByteBuffer} to the model file.
	@@ -76,6 +83,10 @@ public final class Interpreter implements AutoCloseable {
	   public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer) {
	     wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	   }
	+  public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer,  boolean nn) {
	+    wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	+    wrapper.setUseNNAPI(nn);
	+  }
	 
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
"
597,28773,0,"TensorFlow 1.14 changes Keras callback order relative to model build. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): r1.14 branch built  from source
- TensorFlow version (use command below): r1.14
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
TensorFlow 1.14 changes the relative call order of building the model and the set_model callback in the tf.keras fit_generator path. (As of commit cd701ec in the r1.14 branch).

The keras/engine/training.py  adds the optimizer update ops / aka, the backward prop phase, and fills out the model's graph. Callbacks that need full graph visibility need their  function called after the full graph is created.

The order of the operations are as follows:

tf.keras fit():  1.   2. 
tf.keras fit_generator():  1.  2. 
keras_team Keras fit():  1.   2. 
keras_team Keras fit_generator():  1.   2. 

The tf.keras fit_generator() path stands out as being different from the rest.

Commit https://github.com/tensorflow/tensorflow/commit/a332fea0be8def4aa5985499ad807ef78d029142 fixed this for the non-eager mode case and added a test case to help prevent future regression.  Commit https://github.com/tensorflow/tensorflow/commit/615182adb3d01fd8357e574bd8920c0995c6bbb8#diff-6561418ac6882a842d78dad52731895b regressed this order, and also removed the test case that was intended to catch regressions.

Another side effect of the current code is that the  is called from  so in the fit_generator path this is now being called on every iteration. While the majority of the  is guarded by a check that won't do actual work, there are somethings in that method that will run and waste cycles on every iteration.

**Describe the expected behavior**
The Keras callback  method should be called after the whole model is populated. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1481,2174,0,"Unable to find a suitable algorithm for doing forward convolution. ### Environment info

Operating System: Ubuntu 14.04 LTS
Graphics card: GeForce GTX 750 Ti/PCIe/SSE2

Installed version of CUDA and cuDNN: CUDA 7.5, cuNN v5
(please attach the output of ): see cudalib.txt

If installed from sources, provide the commit hash:
I installed from source -> git clone --recurse-submodules https://github.com/tensorflow/tensorflow
Pulled it today: 04/29/2016
### Steps to reproduce
1.  From the tutorial section I copied the partial differential equations example exactly
2. I started a tensorflow environment: ""source ~/tensorflow/bin/activate""
3. I ran example from the command line: ""python pde_example.py""
4. I get this segmentation fault when I run it on a system using GPUs (details of this are in pde_error.txt) in the loop at:  step.run({eps: 0.03, damping: 0.04})
   NOTE: I've run this example on a system not using GPUs and it works splendidly.  Also, I've run other scripts just fine using using the GPUs, it's just this one that doesn't seem to be working.
### What have you tried?
1. I've tried running ./configure with the system default values and putting in them myself, then rebuilding.
2. Running other scripts using the GPU work fine.
3. Running this script using a CPU on other machines works fine.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

[pde_error.txt](https://github.com/tensorflow/tensorflow/files/243444/pde_error.txt)
[cudalib.txt](https://github.com/tensorflow/tensorflow/files/243447/cudalib.txt)
"
1003,5977,0,"split_v generates tensors of unspecified size for all dimensions, when one dimension is missing at the original one. The new split_v function introduced in 0.12rc0 does not correctly predict the shapes of the resulting tensors when the original tensor has an unknown dimension, resulting on the split dimension being of unknown size. For instance:



This results in three tensors, all with shapes [?, ?], instead of the expected [?, 6], [?, 4] and [?, 2]. While at runtime the resulting tensors are of the right size, this makes it more difficult to debug the code during graph definition.

EDIT: This also creates problems when combined with scan. Because scan expects fn to return tensors of the same shape as the initializers, it forces the use of a reshape just to clear the 'unknown' dimension."
1334,8082,0,"Slicing a tensor by an index tensor in Tensorflow. Hi the tensorflow team in Github,

I am not sure if it is appropriate to ask questions in Issue. If questions should not be here, please let me know so that I can close it (I already asked this question on [Stackoverflow](http://stackoverflow.com/questions/42597520/slicing-a-tensor-by-an-index-tensor-in-tensorflow) but I think It would be better to ask this right here in Tensorflow?)

I have two following tensors (note that they are both Tensorflow tensors which means they are still virtually symbolic at the time I construct the following slicing op before I launch a tf.Session()):

: has shape (64,784, 256)
: has shape (64, 784)
and I want to construct an op that returns the following tensor:

: has shape (64,784) where


What is the most efficient way in Tensorflow to do so?

Many thanks.
-Bests"
349,18677,0,"Tensorflow not respecting device placement for custom ops. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609, for the custom op
- **CUDA/cuDNN version**: nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176
- **GPU model and memory**: GTX 1080 Ti


### Exact command to reproduce

Folder for reproducing the issue:

https://github.com/proteneer/khan/tree/tf_bug_repro/gpu_featurizer/debug.py

0. Compile with instructions in README.build

1. Run debug.py

log device placement shows that the op is placed on a gpu:

2018-04-18 17:24:01.008389: I tensorflow/core/common_runtime/placer.cc:884] Featurize: (Featurize)/job:localhost/replica:0/task:0/device:GPU:0

however, it actually runs on the CPU:



2. Removing the reshape line (line 55) in debug.py:


Successfully triggers the custom op to be run on the GPU:



3. Removing the registration line in ani_op.cpp:



Restores the intended behavior as in runs properly on the GPU with or without the reshape. But cripples the op in that it only works on GPUs.

### Describe the problem

This should be a bug since we should be enforcing placement of the op onto the GPU. See steps to reproduce the bug. Oddly enough the custom Featurize op in addition to the Reshape op is shown being placed on the GPU:

2018-04-18 17:24:01.008389: I tensorflow/core/common_runtime/placer.cc:884] Featurize: (Featurize)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_3: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008404: I tensorflow/core/common_runtime/placer.cc:884] Reshape_3: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_2: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008418: I tensorflow/core/common_runtime/placer.cc:884] Reshape_2: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008432: I tensorflow/core/common_runtime/placer.cc:884] Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008446: I tensorflow/core/common_runtime/placer.cc:884] Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_3/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008462: I tensorflow/core/common_runtime/placer.cc:884] Reshape_3/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_2/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008476: I tensorflow/core/common_runtime/placer.cc:884] Reshape_2/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
Reshape_1/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008490: I tensorflow/core/common_runtime/placer.cc:884] Reshape_1/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0
Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-04-18 17:24:01.008505: I tensorflow/core/common_runtime/placer.cc:884] Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0


### Source code / logs

All the code required is inside here:

https://github.com/proteneer/khan/tree/tf_bug_repro/gpu_featurizer"
83,28844,1,"TF2.0 leaking memory when input_shape is specified in keras layer. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): there is a custom MWE code snippet included in the issue
- OS Platform and Distribution: Linux Elementary Loki (Ubuntu 16.04)
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0-alpha0
- Python version: 3.7.1

**Describe the current behavior**
Generating several models with no input shape specified results in normal memory occupancy:


![no_shape](https://user-images.githubusercontent.com/472900/57986674-e39a2780-7a6f-11e9-909f-ac244abbcc4b.png)

This is the behavior expected when the input shape is specified. However, when multiple models are generated within the same program life cycle, the specification of an input shape seems to produce a memory leak.

If the input shape is specified when the models are created, they pile up in memory, without being destroyed. Also the execution time seems to increase quite a lot.



![shape](https://user-images.githubusercontent.com/472900/57986711-67541400-7a70-11e9-9743-5528ece4417b.png)

**Describe the expected behavior**
Model generation should behave the same from a memory point of view regardless of whether the input shape is specified or not.

**Code to reproduce the issue**
It is enough to execute the code snippets included in the issue. In order to generate the memory occupancy, the  package can be used. Assuming the code snippet is pasted in a file called , run:



**Other info / logs**
I generated a list of all the objects for the 2 code snippets, using .



For the models generated without input shape, here is the result:



And here is the snapshot for the models generated with the input shape.



Edit: typo in 
Edit: typo in "
323,22514,0,"Assertion `d.nbDims >= 3' failed with int8 for Tensorflow TensorRT integration. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**:source 
- **TensorFlow version (use command below)**: r1.11
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**: 1080ti/11gb
- **Exact command to reproduce**:



You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

d.nbDims >= 3' failed.
Aborted (core dumped)
`
"
545,20342,0,"distributed training with SyncReplicasOptimizer got stuck after a number of iterations. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
I am running distributed training using SyncReplicasOptimizer, after about 10k iterations, the workers got stuck. CPU usage drops to 0 percent. 

The arguments for SyncReplicasOptimizer:
replicas_to_aggregate = 60, total_num_replicas = 64 (I have 64 workers)

It might also be worth noting that this happens after 27 workers finish their training data.

Connecting to one of the stuck worker processes using gdb I get the following backtraces:

#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38
#1  0x00007f5813609de4 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007f58136095b1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007f5813606af4 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007f5813607015 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007f58111a4b23 in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007f58111a54ab in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007f5811187256 in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007f581118767d in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, std::string const&) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007f5811187ceb in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007f5811468dba in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007f58114699b6 in TF_SessionRun () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007f5811119256 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007f581111939a in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007f58110d5b3e in _wrap_TF_SessionRun_wrapper () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so

Any ideas ? Thanks!
"
1266,22728,0,"while i was running the code          from keras.applications.vgg16 import VGG16 model = VGG16() i got the error Could not allocate ndarray. how to solve this?. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
1114,1392,0,"Minor Error in Tutorial. On [this tutorial page](https://www.tensorflow.org/versions/master/tutorials/mnist/download/index.html), it says that the MNIST data has been rescaled to [-.5, .5]:

""The image data is extracted into a 2d tensor of: [image index, pixel index] where each entry is the intensity value of a specific pixel in a specific image, rescaled from [0, 255] to [-0.5, 0.5].""

However, in [the code](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/input_data.py), the data is rescaled to [0,1] as per the comment on line 123 (and confirmed by loading a sample image and inspecting):



Just a minor error, but thought it was worth pointing out! I'm actually curious which is the better method of rescaling, but that's a topic for another discussion..
"
728,171,0,"Tensorboard creates unecessary loops in graph. I've followed the mnist expert tutorial and wanted to see the graph on the tensorboard. The dashboard looks great, except that in the graph unrelated nodes are grouped together, creating weird loops and making it harder to follow what's happening. Especially with Relu_1 & Relu_2, the merging of the arrows are quite confusing.

![screenshot tensorboard](https://cloud.githubusercontent.com/assets/725737/11115434/c009582e-892b-11e5-8d55-89666555b14e.png)

Code:


"
1152,28371,0,"Suggestion for Neural Machine Translation with Attention tutorial. <em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
2.0
- Doc Link:
https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention

**Describe the documentation issue**

1. max_length function is not needed. The keras.preprocessing.sequence.pad_sequences already pad the sequence to max length. One can just get max length by input_seqs.shape[1]
2. need to explain why we skip 0 for ""convert"" and ""loss_function"". I think it is because of padding
3. Validation set is not used.  This could be an good example to explain how we evaluate the model.
4.  GRU unit in encoder,  Dense in Attention weight calculation,  GRU unit in decoder all have the same units parameter. This is not necessary.  A note could be added to explain that same unit is used for convenience.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes

"
592,23615,0,"XLA does not know associative law. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Arch Linux**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **1.11.0**
- Python version: **3.7.1**

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
output: 


You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
output: 

**Describe the current behavior**
source code:
main.py

when run  and  , get 4.6s and 1.2s

**Describe the expected behavior**
In fact Z1 and Z2 should be the same, hope there is some method to optimize Z1, since in my program, there are many similar situation. consider order of tensor contract is too complicated"
808,33211,0,"Fail build librensorflow_cc.so with cuda support. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Debian 10
- TensorFlow version: 1.15.0
- Python version: 3
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): nvcc (gcc-7)
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2070 SUPER, 8192 MB



**Describe the problem**
When I try compile tensorflow I get error: . 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
While configuring I allow only: , compilation with  and cuda compiler as  (with complete path to it). After it I try run  for build  but, almost immediately I get the error: 


**Any other info / logs**


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
963,15383,0,Feature request: Use placeholders to specify the inputs of TFGAN model.
461,34228,0,"TFLite_Detection_PostProcess. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**



Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
267,32763,1,"tf.sparse.reduce_sum slower/less memory efficient than (unsorted_)segment_sum. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (below)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **ubuntu 16.04**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v1.12.1-9365-gff401a6 1.15.0-dev20190821**
- Python version: **3.6.9**
- CUDA/cuDNN version: **10.0** / ??
- GPU model and memory: **GTX-1070**

**Current behavior**
 (result of a custom kernel) **50-200x slower** and 2-5x less memory efficient compared to  implementation.

**Describe the expected behavior**
Custom kernel performance should be no worse than implementations in terms of other operations.

**Code to reproduce the issue**
A very basic implementation (without support for rank > 2 or multiple summation axes) is as follows.

Extending this to support full functionality should not be difficult.

Basic testing/benchmarking script:


**Other info / logs**

"
1086,17956,0,"Operation missing on iOS despite being added to tf_op_files.txt and ops_to_register.h. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.3
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.7.0-rc1
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: bazel release 0.11.1-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I believe it's a bug. We're trying to run a TensorFlow model on an iOS app by performing the steps described in the [official](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#optimization) [docs](https://www.tensorflow.org/mobile/prepare_models). I've previously successfully built and ran this graph and its related libraries on Android.

We build the libraries like this:

    tensorflow/contrib/makefile/build_all_ios.sh -a arm64 -g our_inference_graph.pb

We make sure the operations in question () are built:

1.  contains 

1.  in  contains 

1.  contains 

1. The fft object file is built as 

However, we get the following error when loading the graph:

    2018-03-23 13:04:06.618259: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: ""RFFT"" device_type: ""CPU""') for unknown op: RFFT
    2018-03-23 13:04:06.618616: E <App path>/TensorFlow/Adapter/TensorFlowUtils.mm:152] Could not create TensorFlow Graph: Not found: Op type not registered 'RFFT' in binary running on Foolish. Make sure the Op and Kernel are registered in the binary running in this process.
    2018-03-23 13:04:06.618827: F <App path>/TensorFlow/Adapter/TensorFlowProcessor.mm:73] Couldn't load model: Not found: Op type not registered 'RFFT' in binary running on Foolish. Make sure the Op and Kernel are registered in the binary running in this process.

Here's my question on SO (no repsonse): https://stackoverflow.com/questions/49452489/opkernel-op-rfft-device-type-cpu-for-unknown-op-rfft

Related issues: issues #15921 and #5518 suggest adding the operation to , which we've done (see above).

### Source code / logs
Here's how we load the graph: https://gist.github.com/nerthase/22f54c040a195f87b7a9b241536be2a1
"
1445,7620,0,"Tensorboard Embedding with t-SNE Crash. When I run t-SNE on my datasets (on smallest or bigger one) with tensorboard embedding , the app crash all the times (after thereabout 500 iterations). I can't re-run or stop the operation to change the parameters, the iteration number freeze and the other functionalities on embedding part crash also.

Anyone have the same problem ? Other curiosity is if I run on [online tensorboard](http://projector.tensorflow.org/) embedding I have the same problem.
"
374,32782,0,"Building TF 2.0 from source returns 1.14 wheel. - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0 (master branch)
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.1/7.6.3
- GPU model and memory: RTX2080TI

I am trying to build tensorflow 2.0 with GPU support. During the ./configure I enable CUDA (the locations are properly found). Afterwards I start the build with:

bazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package

I had to link python to python3, since Ubuntu 18 has no python 2 and bazel has problems, when no python link is available. The build finishes without any fails.

./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg

After building the wheel with the above line I can only build tensorflow 1.14

$ ls /tmp/tensorflow_pkg/
tf_nightly-1.14.0-cp36-cp36m-linux_x86_64.whl

Have been trying it multiple times now, even on a clean and fresh Ubuntu 18 setup.

Cheers
"
1153,34020,0,"Checkpoint.restore doesn't restore Dataset iterator state when Dataset contains shuffle(). **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.5.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: not installed
- GPU model and memory: no GPU

**Describe the current behavior**
The code at the end results in the following output.


**Describe the expected behavior**
After restoring from the checkpoint, I expect the iterator to return the same elements as it did after saving the checkpoint.  I.e.


**Code to reproduce the issue**

FWIW, I get the expected result if I remove .

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
925,8715,0,"RecordReader reads disk without buffer. tensorflow::io::RecordReader calls RandomAccessFile::Read() directly, without go through IoBuffer.
And RandomAccessFile::Read() will call pread(2) or ReadFile OS API.
If there is an IoBuffer between them, it could reduce a lot of syscalls. 

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None

### Environment info
Operating System:
Windows 10

Installed version of CUDA and cuDNN: 
(please attach the output of ):
None

if installed from source, provide 

1. The commit hash ()
c7b80d51da4fb6d51ea54a0bdf2601afa379d60c

2. The output of 
(compiled by cmake and vs 2017)

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
None


### What other attempted solutions have you tried?
None

### Logs or other output that would be helpful

"
537,35493,0,"tf.recompute_grad() throws dimension mismatched error when concatenating tensors with different number of channels . 
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 / 7.6
- GPU model and memory: GTX 1080 / 8GB

**Describe the current behavior**

My model is a segmentation model with DenseNet like structure. There are many concatenate operations between the encoder and decoder tensors. I want to recompute the gradients on these concatenate layers during backpropagation to limit the amount of GPU memory usage. A similar approach can be found here : https://github.com/joeyearsley/efficient_densenet_tensorflow.  I tried to use tf.recompute_grad() on a wrapper function which has a concatenate layer inside but it would raise an error when the channel dimensions of input tensors are not matched.

**Describe the expected behavior**
The concatenate layer should not raise an error when concatenating inputs with different number of channels.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://colab.research.google.com/drive/1d7zSGbYmocnupUpDIJLnlXt-3vH5bi83#scrollTo=xcPT-MWZAuvK&uniqifier=1

**Other info / logs**
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1609   try:
-> 1610     c_op = c_api.TF_FinishOperation(op_desc)
   1611   except errors.InvalidArgumentError as e:

InvalidArgumentError: Dimension 3 in both shapes must be equal, but are 16 and 32. Shapes are [?,30,30,16] and [?,30,30,32].
	From merging shape 0 with other shapes. for 'packed_7' (op: 'Pack') with input shapes: [?,30,30,16], [?,30,30,32].

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
16 frames
<ipython-input-24-689917ca63e3> in <module>()
----> 1 model = test_model()
      2 history = model.fit(train_images, train_labels, epochs=10, 
      3                     validation_data=(test_images, test_labels))

<ipython-input-23-caa5b725d514> in test_model()
      4     x_list.append(layers.Conv2D(16, (3,3), activation='relu')(x_in))
      5     x_list.append(layers.Conv2D(32, (3,3), activation='relu')(x_in))
----> 6     x = efficient_concat(x_list)
      7     x = layers.Flatten()(x)
      8     x = layers.Dense(10, activation='softmax')(x)

<ipython-input-3-fa43d4abcee2> in efficient_concat(input_list)
      4         return x
      5     wraper = tf.recompute_grad(wraper)
----> 6     return wraper(input_list)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in decorated(*args, **kwargs)
    164     """"""Decorated function with custom gradient.""""""
    165     if context.executing_eagerly():
--> 166       return _eager_mode_decorator(f, *args, **kwargs)
    167     else:
    168       return _graph_mode_decorator(f, *args, **kwargs)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in _eager_mode_decorator(f, *args, **kwargs)
    333 
    334   input_tensors = [ops.convert_to_tensor(x) for x
--> 335                    in list(args) + list(variables)]
    336   arg_count = len(args)
    337   def actual_grad_fn(*result_grads):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in <listcomp>(.0)
    332   flat_result = [gen_array_ops.identity(x) for x in flat_result]
    333 
--> 334   input_tensors = [ops.convert_to_tensor(x) for x
    335                    in list(args) + list(variables)]
    336   arg_count = len(args)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
   1182   preferred_dtype = deprecation.deprecated_argument_lookup(
   1183       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
-> 1184   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
   1185 
   1186 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1240       name=name,
   1241       preferred_dtype=dtype_hint,
-> 1242       as_ref=False)
   1243 
   1244 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)
   1294 
   1295     if ret is None:
-> 1296       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1297 
   1298     if ret is NotImplemented:

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)
   1276   elif dtype != inferred_dtype:
   1277     v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)
-> 1278   return _autopacking_helper(v, dtype, name or ""packed"")
   1279 
   1280 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)
   1182     # checking.
   1183     if all(ops.is_dense_tensor_like(elem) for elem in list_or_tuple):
-> 1184       return gen_array_ops.pack(list_or_tuple, name=name)
   1185   must_pack = False
   1186   converted_elems = []

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py in pack(values, axis, name)
   6302   axis = _execute.make_int(axis, ""axis"")
   6303   _, _, _op = _op_def_lib._apply_op_helper(
-> 6304         ""Pack"", values=values, axis=axis, name=name)
   6305   _result = _op.outputs[:]
   6306   _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    791         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,
    792                          input_types=input_types, attrs=attr_protos,
--> 793                          op_def=op_def)
    794       return output_structure, op_def.is_stateful, op
    795 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in create_op(***failed resolving arguments***)
    546     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access
    547         op_type, inputs, dtypes, input_types, name, attrs, op_def,
--> 548         compute_device)
    549 
    550   def capture(self, tensor, name=None):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
   3427           input_types=input_types,
   3428           original_op=self._default_original_op,
-> 3429           op_def=op_def)
   3430       self._create_op_helper(ret, compute_device=compute_device)
   3431     return ret

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1771           op_def, inputs, node_def.attr)
   1772       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
-> 1773                                 control_input_ops)
   1774     # pylint: enable=protected-access
   1775 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1611   except errors.InvalidArgumentError as e:
   1612     # Convert to ValueError for backwards compatibility.
-> 1613     raise ValueError(str(e))
   1614 
   1615   return c_op

ValueError: Dimension 3 in both shapes must be equal, but are 16 and 32. Shapes are [?,30,30,16] and [?,30,30,32].
	From merging shape 0 with other shapes. for 'packed_7' (op: 'Pack') with input shapes: [?,30,30,16], [?,30,30,32]."
943,28811,0,"Tensorflow matmul of float64's behaves like numpy float32 matmul. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: Titan Xp


**Describe the current behavior**
Multiplying two tf.float64s together is giving me a substantial error over multiplying two np.float64s together. 

**Describe the expected behavior**
np.matmul(a, b) should be very close to tf.matmul(a, b) when a, b are both float64s. This issue is compounding over a spectrogram preprocessing pipeline making tensorflow unusable for pushing my signal preprocessing from numpy into tensorflow. 

**Code to reproduce the issue**

Out:

This has the same issue whether the code is run on GPU or CPU. 

This issue does not appear on tf 1.12. "
392,16388,0,"java.lang.InternalError: Cannot find requested resource bundle for locale en_US. bazel version  0.9.0
java version ""1.8.0_161""
os：Ubuntu 16
编译之后出现这个错误，
 Building tensorflow/examples/android/libtensorflow_demo.jar (24 source files) failed (Exit 1)
java.lang.InternalError: Cannot find requested resource bundle for locale en_US

应该如何修改呢，谢谢啦
"
14,19992,1,"The DepthwiseConv2dNative() function ignores the dilations argument. Hi!

I am using Tensorflow v1.7.0.

I am invoking the DepthwiseConv2dNative() function with a dilations argument that is [1, 2, 2, 1]. Despite of this, the dilations value is being ignored.

Looking at the tensorflow source code, it is evident that the dilations argument is presumably being disregarded (depthwise_conv_op.cc, around line 400) in both CUDNN and non-CUDNN scenarios. Yet, there is no mention of this in the documentation.

Thanks!"
1367,25442,0,"dynamic_decode reports error under eager_execution ? “ValueError: The inequality of unknown TensorShapes is undefined."". - TensorFlow version (use command below): 1.2


I have encountered a weird problem when transforming a usual seq2seq code into eager execution mode. After changing the placeholder input to numpy array,  by calling 
gives error “ValueError: The inequality of unknown TensorShapes is undefined.""

Without activating eager execution error, everything is fine.

**Code to reproduce the issue**

The code requires two files from github: 
https://github.com/udacity/deep-learning/blob/master/seq2seq/data/letters_source.txt
and 
https://github.com/udacity/deep-learning/tree/master/seq2seq/data/letters_target.txt


[letters_source.txt](https://github.com/tensorflow/tensorflow/files/2823741/letters_source.txt)
[letters_target.txt](https://github.com/tensorflow/tensorflow/files/2823742/letters_target.txt)

"
1243,25224,0,"ModuleNotFoundError: No module named 'tensorflow.core.framework'. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
951,15942,0,"build_all_ios.sh:  x86_64 compilation failed.. when i compile build_all_ios.sh:   

****tensorflow 1.1 :**   is OK.... build success....**

when i load ""xxxx.pb"" model,  error:

Invalid argument: No OpKernel was registered to support Op 'Mul' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]
	 [[Node: UpSample2D_2/mul = Mul[T=DT_INT32](UpSample2D_2/mul/x, UpSample2D_2/Const)]]

How to solver ?

**tensorflow 1.4 :**   

make: *** [/Users/open/Downloads/tensorflow-1.4/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/random/distribution_sampler.o] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'i386 compilation failed.'
i386 compilation failed.
+ exit 1
## 
**tensorflow 1.5 :**   

ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [/Users/open/Downloads/tensorflow-1.5/tensorflow/contrib/makefile/gen/bin/ios_X86_64/benchmark] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'x86_64 compilation failed.'
x86_64 compilation failed.
+ exit 1


How to solver ?

**python:3.6.3  mac OS:  10.12.6**
"
271,24559,1,"[Perfomance]Dilated/Atrous Conv implementation with cudnn. **System information**
- TensorFlow version (TF 1.12.0):
- Are you willing to contribute it (Yes):


**Describe the feature and the current behavior/state.**
The [doc](https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d) and the [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L947) shows that dilated conv is implemented with traival ops(space_to_batch->regular conv->batch_to_space in python level. And the python code does not pass dilations parameters to lower level code. However both the registered [dilated conv ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L265), [gpu code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2901) and [cpu code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions.h#L1568) are ready to handle with dilatied conv. It is easy to implement dialted conv with cudnn by modifying the python level code.

**Will this change the current api? How?**
NO.

**Who will benefit with this feature?**
who deploy the model which has dilated conv

**Any Other info.**
time of my model reduces from 350ms to 216ms on 1080ti.
"
26,29746,1,"Inference Time TensorFlow C++ API vs Python API. Hi,
In the inference mode, Is C++ API faster than Python API? In the some issues, I see the C++ slower than python, why? Is this right?
[Issue 1](https://github.com/tensorflow/tensorflow/issues/22852 )
[Issue 2](https://github.com/tensorflow/tensorflow/issues/15552)"
66,23182,1,"Different results got using different batch sizes (cannot fully fit into GPU memory therefore evaluating different batch each time). **System information**
- Have I written custom code: **Yes**
- OS Platform and Distribution: **Linux Ubuntu 18.04.1** 
- Mobile device: N/A
- TensorFlow installed from: **binary, tensorflow-gpu from anaconda**
- TensorFlow version: **b'unknown', 1.11.0**
- Python version: **3.6.6**
- Bazel version: N/A
- GCC/Compiler version: N/A
- CUDA/cuDNN version: **10.0, Driver Version: 410.66**
- GPU model and memory: **TITAN Xp, 12195MiB**

**Describe the current behavior**
I'm trying to batch evaluate MLP (with same structure but different parameter) on GPU. Since the whole batch is too big to fit into GPU memory, I have to create smaller batches from the whole batch. Since the size of whole batch is not divisible by the size of little batch, the last little batch will be smaller than usual. It turns out the results in the last small batch is way smaller than previous batches.

**Describe the expected behavior**
The result should be the same since batch size should not affect the result itself.

**Exact command to reproduce**
Put any of the following examples in a  cell, and run it
Put any of the following examples in , and run  should also be fine

**Code to reproduce the issue**
A small example:

    import tensorflow as tf
    import numpy as np
    with tf.Session():
        x = tf.Variable(np.random.rand(43, 60000).astype('f2'))
        w = tf.Variable(np.ones((58624, 43), 'f2'))
        b = tf.Variable(np.ones((58624, 1), 'f2'))
        v1 = w @ x + b
        w = tf.Variable(np.ones((5376, 43), 'f2'))
        b = tf.Variable(np.ones((5376, 1), 'f2'))
        v2 = w @ x + b
        tf.global_variables_initializer().run()
        print(v1.eval(), v2.eval()) # they should have the same value while they dont

The actual example:

    import tensorflow as tf
    import numpy as np
    with tf.Session():
        x = tf.Variable(np.ones((43, 60000), 'f2'))
        y = tf.Variable(np.ones((8, 60000), 'f2'))
        w = tf.Variable(np.ones((916* 64, 43), 'f2'))
        b = tf.Variable(np.ones((916* 64, 1), 'f2'))
        v = tf.Variable(np.ones((916, 8, 64), 'f2'))
        c = tf.Variable(np.ones((916, 8, 1), 'f2'))
        r = tf.linalg.norm(tf.cast(v@tf.reshape(tf.tanh(w@x+b),(916,64,60000))+c,'float32'),axis=(1,2))# prevent overflow
        w = tf.Variable(np.ones((84* 64, 43), 'f2'))
        b = tf.Variable(np.ones((84* 64, 1), 'f2'))
        v = tf.Variable(np.ones((84, 8, 64), 'f2'))
        c = tf.Variable(np.ones((84, 8, 1), 'f2'))
        R = tf.linalg.norm(tf.cast(v@tf.reshape(tf.tanh(w@x+b),(84,64,60000))+c,'float32'),axis=(1,2)) # prevent overflow
        tf.global_variables_initializer().run()
        print(r.eval()[::100], R.eval()[::10]) # summary, they should have the same value while they dont

**Other info / logs**
I created a stack overflow question (https://stackoverflow.com/questions/52921884/tensorflow-result-inconsistent-across-batch-size) but since no body solved the problem, I submitted an issue here. It seems that this problem does not occur with small batches, it sometimes is not stably reproducible "
54,26736,1,"tflite's TRANSPOSE_CONV is much slower than tfmobile .... <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Zenfone 5Z
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below): nightly build 20190314
- Python version:3.6
- Bazel version (if compiling from source):0.21.0
- GCC/Compiler version (if compiling from source):5.4
- CUDA/cuDNN version:not used
- GPU model and memory:not used

**Describe the current behavior**

I measured performance of tf-mobile, tf-lite on Zenfone 5Z  / Snapdragon 845 / Android 8.0 by C++ benchmark model tool (arm64 build), and I find that the speed of tf-lite's TRANSPOSE_CONV is much slower than tf-mobile's one.

I used the attached custom model [models.zip](https://github.com/tensorflow/tensorflow/files/2970561/models.zip)  for benchmark .
The attached tflite is converted by toco_convert from the attached pb file.

**Summary**

The following table shows average computing time of 50 times predict.

|              | Threads|Conv2D     | TransposeConv2D | All        |
|:-------------|--------:|-----------:|----------------:|-----------:|
| TFMobile     | 1|251.561 ms |       35.585 ms | 310.380 ms |
| TFMobile     | 4|190.228 ms |       78.047 ms | 295.469 ms |
| TFMobile     | 16|87.586 ms |       20.264 ms | 122.102 ms |
| TFLite       | 1|294.214 ms |      562.609 ms | 880.674 ms |
| TFLite       | 4|75.783 ms |      560.368 ms | 659.156 ms |
| TFLite       | 16|55.597 ms |      561.441 ms | 641.541 ms |

**TFMobile 1 threads In:1x256x256x3 Performance**



**TFMobile 16 threads In:1x256x256x3 Performance**



**TFLite 1 threads In:1x256x256x3 Performance w/o NNAPI**



**TFLite 16 threads In:1x256x256x3 Performance w/o NNAPI**

"
1105,1196,0,"tensorflow using cpu+2 gpus turns out 3 times slower than theano using 1 gpu. 1. Does tensorflow automatically use all cpus and gpus to run the computational task?

2.I found that tensorflow use 1 5960X cpu and 2 Titan X gpus cost 300% time compared to theano use only 1 Titan X gpu, is there anything wrong with my configuration? I use the same model generated by keras, using different backend(theano and tensorflow)

I installed on Ubuntu 15.10, cuda 7.5, cudnn 4, gcc 4.9, installed from source code, built with GPU support.
"
163,13114,1,"Extremely slow first epoch.. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: PIP, tensorflow-gpu==1.2.1
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: Python 3.6.2
- **CUDA/cuDNN version**: CUDA Version 8.0.61 / cuDNN Version 5.1.10
- **GPU model and memory**: GTX 1080ti x 2, 11171MiB each.

### Describe the problem
When training model, ""**only First epoch after executing training script is extremely slow** (More specifically, It takes 12 hours while second and third epoch after executed takes 2.5 hours with my environment and dataset)""

I uploaded this issue because similar issue in stackoverflow is not handled long time.
(https://stackoverflow.com/questions/44966831/tensorflow-first-epoch-is-extremely-slow-maybe-related-to-pool-allocator)

In more detail,
my code uses model with  **1d convolution layers** (tf.nn.conv1d - tf.bias_add - tf.nn.relu) as model.
**tf.ctc_loss** as loss function.
Data is served with tf.PaddingFIFOQueue && tf.QueueBase.dequeue_many and ""**each data has different size**"".

Here is what I tried,
First I also assumed that pool allocator makes this problem like [above stackoverflow link](https://stackoverflow.com/questions/44966831/tensorflow-first-epoch-is-extremely-slow-maybe-related-to-pool-allocator) (**but now I think this issue is not from pool allocator**)
1. Uses tcmalloc
 which is suggested in [here](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/ja7FlGrvh-E)
2. Uses BFC allocator type.
- With 
- And I tried  also.
3. Build tensorflow from source with modified initial pool size limit (100 to 10000), (Which is hard coded in [here](https://github.com/tensorflow/tensorflow/blob/927f811b0303e51126531c135f8b093383de2d6d/tensorflow/core/common_runtime/gpu/process_state.cc#L187)) 

And all trying was ineffective.

Except extremely slow first epoch, everything was fine. Weights of model is trained well, save and load checkpoint and continuing training or inferencing is also fine without any warning and error.

Is it one of avoidable characteristic of Tensorflow or bugs?

Thank you for your reading.
If you need any more information, please notify me."
1323,7952,0,"[Java] Loading frozen graph causes a segfault. I froze then saved a tensorflow graph in python with [freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) producing a single model.pb file.

However, loading the graph in Java (where modelStream is an InputStream from reading the .pb file):

causes a segfault:

The same model.pb file loads and runs properly in Python.

### Environment info
Operating System: MacOS Sierra 10.12.3
Native library for OSX downloaded from [here](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-darwin-x86_64-1.0.0-PREVIEW1.tar.gz) by following the [readme on the Java Tensorflow repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md)"
917,442,0,"cannot find numpy/arrayobject.h. INFO: From Compiling tensorflow/python/client/tf_session_helper.cc:
In file included from tensorflow/python/client/tf_session_helper.cc:20:0:
./tensorflow/python/client/tf_session_helper.h:34:31: fatal error: numpy/                                                            arrayobject.h: No such file or directory
 #include ""numpy/arrayobject.h""
                               ^
compilation terminated.
ERROR: /root/download/tensorflowgit/tensorflow/tensorflow/python/BUILD:70                                                            4:1: C++ compilation of rule '//tensorflow/python:tf_session_helper' fail                                                            ed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-                                                            D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parame                                                            ter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ff                                                            unction-sections ... (remaining 51 argument(s) skipped): com.google.devto                                                            ols.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
"
414,32026,0,"Tensorflow r2.0 will not build successfully on skylake machines. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): not installed (attempting to build from source)
- TensorFlow version: attempting to build from branch r2.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: Anaconda conda virtual environment 
- Bazel version (if compiling from source): 0.26.0
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: CUDA 10, cuDNN 7.6.2
- GPU model and memory: NVidia RTX 2080 Ti (MSI Sea Hawk X, 11 GB)

**Describe the problem**
bazel build is unsuccessful yielding error in tensorflow/lite/experimental/ruy/kernel_avx512.cc
Please see [Issue 31187](https://github.com/tensorflow/tensorflow/issues/31187) for background leading to the same bug being resolved on the master branch.  I don't know how they fixed it on master.  The fix needs to be ported from master to r2.0 branch.

Please also note that you can bypass using AVX-512 and get a successful build by changing:
~/tensorflow/tensorflow/lite/experimental/ruy/platform.h
Comment out:
// TODO(b/138433137) Select AVX-512 at runtime rather than via compile options.
// #if defined(__AVX512F__) && defined(__AVX512DQ__) && defined(__AVX512CD__) && \
    defined(__AVX512BW__) && defined(__AVX512VL__)
// #define RUY_DONOTUSEDIRECTLY_AVX512 1
// #else
#define RUY_DONOTUSEDIRECTLY_AVX512 0
// #endif

**Provide the exact sequence of commands / steps that you executed before running into the problem**

git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r2.0
./configure
Accept defaults initially
When asked for CUDA support, type ""y""
When asked for TensorRT support, type ""y""
When asked for compute capabilities, type 7.0,7.5
Accept defaults thereafter
bazel build --explain=verbose_explanations.txt --verbose_explanations --verbose_failures --subcommands=pretty_print --config=opt --config=cuda --config=v2 --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
This error does not occur on machines that do not have Skylake support for AVX-512 (I know because I can build it fine on my laptop and other users report this bug is particular to Skylake).

I have attached a dump of the terminal output of the bazel build command (cleaned-capture.txt) and verbose_explanations.txt:

[cleaned-capture.txt](https://github.com/tensorflow/tensorflow/files/3548002/cleaned-capture.txt)
[verbose_explanations.txt](https://github.com/tensorflow/tensorflow/files/3548006/verbose_explanations.txt)

Full error message:
ERROR: /home/daniel/tensorflow/tensorflow/lite/experimental/ruy/BUILD:271:1: C++ compilation of rule '//tensorflow/lite/experimental/ruy:kernel' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/daniel/.cache/bazel/_bazel_daniel/79db702fc9f94af7d11e11c5d64854d0/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/extras/CUPTI/lib64 \
    PATH=/home/daniel/anaconda3/envs/tfgpu/bin:/home/daniel/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/lite/experimental/ruy/_objs/kernel/kernel_avx512.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/lite/experimental/ruy/_objs/kernel/kernel_avx512.pic.o' -iquote . -iquote bazel-out/host/bin -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -c tensorflow/lite/experimental/ruy/kernel_avx512.cc -o bazel-out/host/bin/tensorflow/lite/experimental/ruy/_objs/kernel/kernel_avx512.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
In file included from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:
./tensorflow/lite/experimental/ruy/kernel.h:613:9: warning: multi-line comment [-Wcomment]
 #endif  // (RUY_PLATFORM(NEON_64) || RUY_PLATFORM(NEON_32) || \
         ^
In file included from external/gemmlowp/fixedpoint/fixedpoint.h:895:0,
                 from ./tensorflow/lite/experimental/ruy/kernel.h:22,
                 from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:
external/gemmlowp/fixedpoint/./fixedpoint_sse.h:43:39: warning: ignoring attributes on template argument ‘__m128i {aka __vector(2) long long int}’ [-Wignored-attributes]
 struct FixedPointRawTypeTraits<__m128i> {
                                       ^
In file included from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:
./tensorflow/lite/experimental/ruy/kernel.h: In function ‘void ruy::MakeKernelParamsFloat(const ruy::PackedMatrix<float>&, const ruy::PackedMatrix<float>&, const ruy::BasicSpec<float, float>&, int, int, int, int, ruy::Matrix<float>*, ruy::KernelParamsFloat<LhsCols, RhsCols>*)’:
./tensorflow/lite/experimental/ruy/kernel.h:456:53: warning: typedef ‘using Params = struct ruy::KernelParamsFloat<LhsCols, RhsCols>’ locally defined but not used [-Wunused-local-typedefs]
   using Params = KernelParamsFloat<LhsCols, RhsCols>;
                                                     ^
tensorflow/lite/experimental/ruy/kernel_avx512.cc: In function ‘void ruy::Kernel8bitAvx512(const ruy::KernelParams8bit<16, 16>&)’:
tensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: error: ‘_mm512_loadu_epi8’ was not declared in this scope
         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);
                                  ^~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: note: suggested alternative: ‘_mm512_add_epi8’
         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);
                                  ^~~~~~~~~~~~~~~~~
                                  _mm512_add_epi8
tensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: error: ‘_mm512_loadu_epi32’ was not declared in this scope
                                _mm512_loadu_epi32(&params.lhs_sums[row]));
                                ^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: note: suggested alternative: ‘_mm512_load_epi32’
                                _mm512_loadu_epi32(&params.lhs_sums[row]));
                                ^~~~~~~~~~~~~~~~~~
                                _mm512_load_epi32
tensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: error: ‘_mm512_loadu_epi32’ was not declared in this scope
                                _mm512_loadu_epi32(&params.rhs_sums[col]));
                                ^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: note: suggested alternative: ‘_mm512_load_epi32’
                                _mm512_loadu_epi32(&params.rhs_sums[col]));
                                ^~~~~~~~~~~~~~~~~~
                                _mm512_load_epi32
tensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: error: ‘_mm_storeu_epi8’ was not declared in this scope
             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: note: suggested alternative: ‘_mm_store_epi64’
             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
             _mm_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: error: ‘_mm_storeu_epi8’ was not declared in this scope
             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: note: suggested alternative: ‘_mm_store_epi64’
             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
             _mm_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: error: ‘_mm256_storeu_epi16’ was not declared in this scope
             _mm256_storeu_epi16(tmp_ptr,
             ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: note: suggested alternative: ‘_mm256_store_epi64’
             _mm256_storeu_epi16(tmp_ptr,
             ^~~~~~~~~~~~~~~~~~~
             _mm256_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: error: ‘_mm512_storeu_epi32’ was not declared in this scope
             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);
             ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: note: suggested alternative: ‘_mm512_store_epi32’
             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);
             ^~~~~~~~~~~~~~~~~~~
             _mm512_store_epi32
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 35.925s, Critical Path: 10.04s
INFO: 282 processes: 282 local.
FAILED: Build did NOT complete successfully"
942,8298,0,"Broken initialization when graph has data dependencies. . As per conversation at OpenAI (cc @alextp @yaroslavvb). 

Here's an example of broken variable initialization and the super-slow fix:
https://gist.github.com/nivwusquorum/551b502e1cf36a09c9c05385ccea5eb5

Let me know if you need more details"
1339,17393,0,"import tensorflow failed, ""ImportError: DLL load failed"". Even after install visual studio 2015, Microsoft Visual C++ 2015 Redistributable Update 3.. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win10 x64
- **TensorFlow installed from (source or binary)**:  pip install --upgrade tensorflow
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
I was trying to install tensorflow **cpu version** on windows10, but always got error when import tensorflow.

I read the [common_installation_problems](https://www.tensorflow.org/install/install_windows#common_installation_problems) , tried many solution I found on github, stackoverflow, etc. I install visual studio 2015, visual studio 2017, [Microsoft Visual C++ 2015 Redistributable Update 3](https://www.microsoft.com/en-us/download/details.aspx?id=53587)(both 32 and 64),  msvcp140.dll can find in both System32 and SysWow64 folder. But still can't import tensorflow.

Is there something I missed out?


### Source code / logs
>import tensorflow error info:




>run tensorflow_self_check.py result:


"
1064,24835,0,"Windows 10/Anaconda3: Missing File on install. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Installed from pip
- TensorFlow version: 2 Preview
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip

**Describe the problem**
I created a virtual environment and tried to install the preview that Martin Wicke suggested on twitter. It failed with a missing file.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Commands from Anaconda3 prompt:
conda create -n tf_daily python=3.6
activate tf_daily
pip install tf-nightly-2.0-preview

Error Message:
(tf_daily) C:\Users\nurl_>pip install tf-nightly-2.0-preview
Collecting tf-nightly-2.0-preview
  Using cached https://files.pythonhosted.org/packages/a6/d9/0499db98422207eb3d7643ee3b8152dd503a85dc2f958f77834aa0a3fcde/tf_nightly_2.0_preview-1.13.0.dev20190108-cp36-cp36m-win_amd64.whl
Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\nurl_\\AppData\\Local\\Temp\\pip-install-r8cg1kry\\tf-nightly-2.0-preview\\tf_nightly_2.0_preview-1.13.0.dev20190108.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'

"
815,2566,0,"Android Example utils fail on models > 64MB. I've been trying to get started with tensorflow and I wanted to use an image recognition model I trained in the android example, but when I tried to replace the models in the example with my model, it would crash without a lot of information.

I finally figure out that ReadFileToProto in jni_utils.cc does not support reading files larger than 64MB: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/jni/jni_utils.cc#L107

Adding these lines seemed to fix the issue of not being able to load my graph at all:



I'm probably not the first to stumble into this problem, but I feel like I just wasted hours of my life on a trivially avoidable problem and I hope no-one else has to waste time on that little bit of protobuf trivia.
"
1309,13601,0,"java API had no bool tensorflow，how to add it to the session in java. i am  doing transer the facenet to android,the input is the img,and the phase_train is a bool data,but the java api had no bool to be feed to session,as i had down this:


and then the c++ write :
m_phase_tensor = tensorflow::Tensor(tensorflow::DT_BOOL, tensorflow::TensorShape());
m_phase_tensor.scalar<bool>()() = false;
how to writen in java,can somebody help me!
"
863,22761,0,"Error building on Windows, patch command failing. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: 0.17.2
- **GCC/Compiler version (if compiling from source)**: Visual C++ Build Tools 2015
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Follow build instructions on https://www.tensorflow.org/install/source_windows

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When following the build instructions on https://www.tensorflow.org/install/source_windows I am unable to build tensorflow from source. I'm running on a clean Windows 10 install (nothing else installed but the requirements listed on the homepage). 

When running the bazel build step (CPU) with the command: $ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package I get two different errors, both when the build script tries to patch some files. The commands that fail are:



Copying the patch commands into the cmd.exe prompt makes the patch command ""hang"" until I manually terminate it. If I add ' ' around the command part of the bash invocation (e.g. bash -c 'patch ...') it works just fine. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


"
52,33382,1,"Bugs about dequantize operator calculation formula in SCALED mode. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: Python 2.7.12
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
Using numpy to simulate the calculation formula in SCALED mode of the dequantize operator in tensorflow, you can't get the correct result.


**Describe the expected behavior**
the code written by me should get the same result as the tensorflow c++ code.
However, I got much different result.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

In my code, the _input_min_range_ param is redundant.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
819,14703,0,"tf.layers uses wrong variable scope. TF 1.4.


From what I can see, tf.layers is trying to create the variables under a wrong variable scope name, making variable sharing impossible if used under the root scope.

I found that this works:

But the first line seems redundant and counter-intuitive.

//UPDATE:
It also works if I don't use tf.layers:
"
923,6875,0,"Tensorflow require Protobuf 3.1 while attempting to use incompatible 3.0 function calls. **Environment:**
Arch Linux with CUDA 8.0 and cuDNN 5.1



Tensorflow was succesfully compiled from source for a C++ pipeline using
**Bazel** 0.4.3
**Protobuf** 3.1.x (also tried 3.2.x)
**Tensorflow** r1.0 (as well as master <9830ed87d46245a72a9d0c2dce854e6732c5542a>)

with the command: 


In _my_ CPP pipeline, the following include breaks the compile process




----------------------------------------------

Digging in the documentation, I see for the python part that Protobuf 3.1.x is required, however 3.1.x had an API change from 3.0.X for the ArenaStringPtr::Get(), where the former takes a string as argument while the newer doesn't. I could downgrade to Protobuf 3.0.X, but macro's in e.g. error_codes.pb.h specifies a requirement for 3.1.X "
969,22042,0,"Could not use adagrad or adadelta optimizer in eager mode(gpu). https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/text_generation.ipynb
Change to use tf.train.AdagradOptimizer then will get below error 
NotFoundError: No registered 'ResourceSparseApplyAdagrad' OpKernel for GPU devices compatible with node ResourceSparseApplyAdagrad = ResourceSparseApplyAdagrad[T=DT_FLOAT, Tindices=DT_INT32, update_slots=true, use_locking=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)
	.  Registered:  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
 [Op:ResourceSparseApplyAdagrad]"
743,17888,0,"tf.nn.softmax_cross_entropy_with_logits_v2 loss function so big. ### Describe the problem
I am trying to build up a binary image classifcation project (2 classes are balanced) . There are 2 nodes in output layer. However, my loss function is starting from a high point(3000). Is my loss function okay? Should I change my loss function ? I want to use a metric similar to Sklearn's log-loss. At the end of 1st epoch, it reaches to 80% accuracy and after 10 epochs and its accuracy becomes 82 %. During 10 epochs, it decreases from 3000 to 20. The graph of _my loss function_ is as following(1 epoch = 175 steps) .
![loss](https://user-images.githubusercontent.com/33747602/37697942-0eb62702-2cf1-11e8-8ebe-cfdac4d34750.png)

The graph of _accuracy_ for 10 epochs is below:
![acc](https://user-images.githubusercontent.com/33747602/37697954-28500084-2cf1-11e8-93c6-3f514531a8f8.png)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10 Home
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**:1.6.0
- **Python version**: 3.6.0(no anaconda) 
- **CUDA/cuDNN version**:9.0 - 7.0
- **GPU model and memory**:GTX 1050
- **Bazel Version**:N/A

### Source code / logs
cross_entropy=tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true,logits=y_pred ))
"
1194,566,0,"suppressing ""I"" log messages in tensorflow. I am using TF 0.6.0 with python 2.7.10 on both ubuntu linux and osx 10.9.5.  TF base functionality is working fine.  The issue is logging - I want to suppress the following messages that are emitted whenever I construct the first tensorflow Session:

I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 2
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 2

They do not appear to be transmitted through the normal python logging mechanism.  Any ideas?  Did I just miss something?

I don't want to turn off all log messages, I just want TF to stop generating these two specific messages - and perhaps if you can tell me how to turn off all INFO messages from this particular (seemingly non-pythonic) source, that would be fine too.
"
135,22803,1,"Model converted to TFLite always returns NaN as output.. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: --
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  v1.10.0-12-g4dcfddc5d1 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: --
- **GCC/Compiler version (if compiling from source)**: --
- **CUDA/cuDNN version**: --
- **GPU model and memory**: Intel Iris Plus Graphics 650 1536 MB
- **Exact command to reproduce**: 

### Describe the problem
I have been trying to convert a frozen graph trained using [this repo](https://github.com/GeorgeSeif/Semantic-Segmentation-Suite) for using on android with TFLite. Trained model uses MobileNetV2 as frontend and [Mobile UNet for Semantic Segmentation](https://arxiv.org/abs/1704.04861) as the model. The problem I am facing is: the frozen pb graph segments the image correctly but TFLite converted model returns all  for the output. To try the problem I wrote the following script. The model is converted without any errors or warnings, but the output is not correct. Do you have any idea what might be causing this?

Note: converted model is also returning NaNs on android device.

**Frozen graph**: [output_graph.pb](https://drive.google.com/file/d/1qGwD8h5ub0HjtO-Cc8Zd-HU6Uv-t9apF/view?usp=sharing)

### Source code / logs

**test.py**


**output**

"
1214,26547,0,"Tutorial: TF 2.0 - Text generation with keras.layer.LSTM - Fails to Generate Sensible Text. **System information**
- TensorFlow version: 
- Doc Link: https://www.tensorflow.org/alpha/tutorials/sequences/text_generation#generate_text

**Describe the documentation issue**

The model, after 10 epochs (the default), generates ""unreadable"" text. This is through running the default Colab notebook via the docs/as loaded from GitHub: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb




Training output w/ loss:



I've validated:

* The source data on GCS is OK - https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt
* The char2idx mapping is correct / reversible (as expected; no typos)
* Reduced the temperature - predictions are still invalid (as expected; they aren't really close to what we attempted to train)
* Ran for 30 epochs (loss: 0.6435)

Continuing to debug (far from an expert) but filing this as a heads-up."
351,18789,0,"can 1*1 kernel conv2d optimized using neon? the source code treat 1*1 kernel conv2d as mat-mul, but did not use neon optimization. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
605,23900,0,"No clear_devices in BestExporter. 
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, please see this [Colab](https://colab.research.google.com/drive/1GKAqEo7qSr6kMAgxPrOrudA5eLndQ6Ub)

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): n/a
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): n/a
- TensorFlow version (use command below): 1.10+
- Python version: 3.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: whatever is on Colab


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

One can not train a distributed- and export the model, then load it on a non-distributive device because if we look at the TensorFlow docs for BestExporter


it is apparent that  is not an option.

**Describe the expected behavior**

Let me easily export and import 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

 see this [Colab](https://colab.research.google.com/drive/1GKAqEo7qSr6kMAgxPrOrudA5eLndQ6Ub)
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
386,2497,0,"Setting up TensorFlow for Development broken on Linux. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md

TLDR;
instead of

I had to do

Ubuntu 15.04, bazel-0.2.3, week-old TensorFlow from head

On MacOS, Bazel 0.2.1-homebrew, the command that works is 
"
1260,3611,0,"Feature request: Support for non-minibatch data for intrinsic functions. Some functions in the TensorFlow standard library makes an assumption that we're always feeding minibatches. 

One example is  which assumes a 2-D input of shape . It would be nice to have a second API to such subroutines that work also without an assumption of a minibatch dimension in the input tensor.
"
1384,21199,0,"Documentation issue . ![screenshot 25 _li](https://user-images.githubusercontent.com/25861787/43352882-816db4d2-9244-11e8-8f0a-3af1eef9b29f.jpg)
It's a Fashion MNIST dataset Tesorflow tutorial.What are digits doing here?"
711,26584,0,"Tensorflow Lite Java Interpreter methods throw Exceptions, but this is not in the method declaration. Methods such as  throw Java exceptions originating from native, but the method declaration does not include . It would be helpful to beginners to include this. Thanks!"
378,17209,0,"Expected float32, got range(0, 3) of type 'range' instead.. ### System information
- **What is the top-level directory of the model you are using**:
      C:\Users\Administrator\Documents\Projects\models
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Because my previous [issue,](https://github.com/tensorflow/tensorflow/issues/17208) I disable the argument.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
     Windows 10
- **TensorFlow installed from (source or binary)**:
      Binary
- **TensorFlow version (use command below)**:
      1.5
- **CUDA/cuDNN version**:
     CUDA v9.0
- **GPU model and memory**:
      NVDIA GeForce GT 730

### Describe the problem
If my description or log is not clear enough, please tell me. Thanks in advance!

### Source code / logs
  File ""C:/Users/Administrator/Documents/Projects/models/research/object_detection/my_train.py"", line 164, in main
    worker_job_name, is_chief, FLAGS.train_dir)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\trainer.py"", line 255, in train
    train_config.optimizer)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\builders\optimizer_builder.py"", line 50, in build
    learning_rate = _create_learning_rate(config.learning_rate)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\builders\optimizer_builder.py"", line 108, in _create_learning_rate
    learning_rate_sequence)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\object_detection-0.1-py3.5.egg\object_detection\utils\learning_schedules.py"", line 155, in manual_stepping
    tf.constant(range(num_boundaries), dtype=tf.int32),
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 214, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 433, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 344, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected float32, got range(0, 3) of type 'range' instead."
281,30212,0,"error C2280: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': attempting to reference a deleted function. **System information**
- OS Platform :   Windows 10 Pro
- TensorFlow version: 2.99
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Installed using virtualenv: conda
- Bazel version :  0.24.1
- MSVC17
- NO CUDA

configuration : 
Please specify the location of python. [Default is C:\Users\zen2_microsoft\Anaconda3\envs\tensorflow-1.13.1-without-mkl\python.exe]:
Found possible Python library paths:
  C:\Users\zen2_microsoft\Anaconda3\envs\tensorflow-1.13.1-without-mkl\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\zen2_microsoft\Anaconda3\envs\tensorflow-1.13.1-without-mkl\lib\site-packages]
Do you wish to build TensorFlow with XLA JIT support? [y/N]: N
No XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:
Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: N

**Describe the problem**
Build fails with below error .
compilation error log :

  C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.12.25827/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/genfiles/external/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/genfiles/external/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/core/grappler/_objs/grappler_item/grappler_item.obj /c tensorflow/core/grappler/grappler_item.cc
Execution platform: @bazel_tools//platforms:host_platform
external/com_google_absl\absl/meta/type_traits.h(141): error C2280: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': attempting to reference a deleted function
.\tensorflow/core/framework/function.h(334): note: see declaration of 'tensorflow::FunctionLibraryDefinition::operator ='
external/com_google_absl\absl/meta/type_traits.h(121): note: see reference to alias template instantiation 'IsCopyAssignableImpl<tensorflow::FunctionLibraryDefinition>' being compiled
external/com_google_absl\absl/meta/type_traits.h(150): note: see reference to class template instantiation 'absl::type_traits_internal::is_detected<absl::type_traits_internal::IsCopyAssignableImpl,T>' being compiled
        with
        [
            T=tensorflow::FunctionLibraryDefinition
        ]
external/com_google_absl\absl/meta/type_traits.h(432): note: see reference to class template instantiation 'absl::is_copy_assignable<T>' being compiled
        with
        [
            T=tensorflow::FunctionLibraryDefinition
        ]
external/com_google_absl\absl/types/internal/optional.h(173): note: see reference to class template instantiation 'absl::is_trivially_copy_assignable<tensorflow::FunctionLibraryDefinition>' being compiled
tensorflow/core/grappler/grappler_item.cc(118): note: see reference to class template instantiation 'absl::optional<tensorflow::FunctionLibraryDefinition>' being compiled
.\tensorflow/core/framework/function.h(334): note: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': function was explicitly deleted
external/com_google_absl\absl/meta/type_traits.h(144): error C2280: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': attempting to reference a deleted function
.\tensorflow/core/framework/function.h(334): note: see declaration of 'tensorflow::FunctionLibraryDefinition::operator ='
external/com_google_absl\absl/meta/type_traits.h(121): note: see reference to alias template instantiation 'IsMoveAssignableImpl<tensorflow::FunctionLibraryDefinition>' being compiled
external/com_google_absl\absl/meta/type_traits.h(155): note: see reference to class template instantiation 'absl::type_traits_internal::is_detected<absl::type_traits_internal::IsMoveAssignableImpl,T>' being compiled
        with
        [
            T=tensorflow::FunctionLibraryDefinition
        ]
external/com_google_absl\absl/types/internal/optional.h(328): note: see reference to class template instantiation 'absl::is_move_assignable<T>' being compiled
        with
        [
            T=tensorflow::FunctionLibraryDefinition
        ]
external/com_google_absl\absl/types/optional.h(120): note: see reference to class template instantiation 'absl::optional_internal::assign_copy_traits<T>' being compiled
        with
        [
            T=tensorflow::FunctionLibraryDefinition
        ]
.\tensorflow/core/framework/function.h(334): note: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': function was explicitly deleted
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 11024.399s, Critical Path: 6294.48s
INFO: 2398 processes: 2398 local.
FAILED: Build did NOT complete successfully 

Please help me out ."
157,32052,1,"Linear RAM memory increase with Dataset and Estimator with epoch loops. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below):  tensorflow-gpu==2.0.0-rc0
- Python version: 3.7.1
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: Cuda 10.1 / cuDNN 7.6
- GPU model and memory: GeForce GTX 1060 Mobile 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
When using Dataset with Estimator, the memory foot print of RAM keeps raising when estimator's train and evaluate APIs are called in loop.

**Describe the expected behavior**
RAM usage should not increase with epochs.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Please find the source code @ https://gist.github.com/Mageswaran1989/facc3fc2a003807d029a914c721629db

[Update] My latest test case @ https://github.com/dhiraa/tf_issue_32052

StackOverflow Rereference :  https://stackoverflow.com/questions/55211315/memory-leak-with-tf-data

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[Updated the graph]
![tf_memory_test](https://user-images.githubusercontent.com/3304549/63917881-30bb6b80-ca59-11e9-8598-ebdca798dff5.png)
"
508,7637,0,"iris_monitors.py broken in release r1.0 due to inacurate MetricSpec namespace. NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

 should be changed to  in iris_monitors.py in release 1.0.0

No need to import MetricSpec in iris_monitors.py it's already imported on line 24 in commit [fa4ba830f437fdb9dc1085b4d68a3bab41a16e20](https://github.com/tensorflow/tensorflow/blob/fa4ba830f437fdb9dc1085b4d68a3bab41a16e20/tensorflow/examples/tutorials/monitors/iris_monitors.py):


### Environment info
Operating System:
>tensorflow/tensorflow/examples/tutorials/monitors$ lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.1 LTS
Release:	16.04
Codename:	xenial
>tensorflow/tensorflow/examples/tutorials/monitors$ uname -a
Linux panchito 4.4.0-62-generic-tuxonice #83~ppa1-Ubuntu SMP Thu Feb 2 23:17:45 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

Installed version of CUDA and cuDNN: 
(please attach the output of ):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from .

If installed from source, provide 

1. The commit hash ()
2. The output of 

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

repositories/tensorflow/tensorflow/examples/tutorials/monitors$ python iris_monitors.py
Traceback (most recent call last):
  File ""iris_monitors.py"", line 116, in <module>
    tf.app.run()
  File ""/home/chidochipotle/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""iris_monitors.py"", line 42, in main
    tf.contrib.learn.metric_spec.MetricSpec(
AttributeError: module 'tensorflow.contrib.learn' has no attribute 'metric_spec'

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
408,10855,0,"Differentiate Variable (Model Parameters) and Mutable Tensor. In the current tensorflow, the concept of variables is about same as model parameters and mutable tensors. I would like to see them separate out. For example, it seems perfect legit to me to use tf.scatter_nd_update to update a tensor. However all scatter APIs require ""mutable tensor"". That made operations on tensors with ""batch"" parameter close to impossible.  Having the concept of mutable tensor will help a lot.






"
1115,24257,0,"ImportError: DLL load failed: The specified module could not be found.. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6.7(also tried 3.7.1)
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): Na
- GCC/Compiler version (if compiling from source): Na
- CUDA/cuDNN version: 9.0
- GPU model and memory: GeForce GTX 1060 6GB



**Describe the problem**

I executed ""pip install tensorflow"" command and then wanted to test it by opening python on command prompt. I typed ""python"" and ""import tensorflow"" then I got this error message below: 
 
Python 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\(User)\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
1460,32375,0,"tf.concat not accepting arbitrary dtype with functional API model. **System information**
- Have I written custom code: yes
- OS Platform and Distribution: Windows 10 Pro, version 1903, OS build 18362.295
- TensorFlow installed via: pip
- TensorFlow version: v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0
- Python version: Python 3.6.9 |Anaconda, Inc.| [MSC v.1915 64 bit (AMD64)] on win32

**Describe the current behavior**
 only accepts  unless explicitly passing  arg to .

**Describe the expected behavior**
 implicitly accepts any dtype (as long as input dtypes match).

**Code to reproduce the issue**


**Full traceback**
"
540,22119,0,"XLA commonly aborts with nvidia error. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.15
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.2 / 7.2.1.38
- **GPU model and memory**: Titan
- **Exact command to reproduce**: N/A

### Describe the problem
When running XLA via the session config we commonly hit the following error:



However, sometimes we get further than this error and see significant speed up with XLA and the loss decreasing.

What could be causing this error?

The docker image is located at: https://hub.docker.com/r/joeyearsley/tf-builds/

I can't diverge the exact script, however I do have the following:
- NCCL for multi-gpu reduction like TF-Benchmark's Variable Manager
-  APIs including the string handles to alter between train and validation datasets.

"
375,1171,0,"Undefined reference to symbol 'ceil@@GLIBC_2.2.5' at build time. I'm working on compiling Tensorflow from source, using non-standard GCC/etc. installations.  Environment info: RHEL 6.7, GCC 5.2.1, Bazel 0.1.5.  I'm installing Tensorflow from HEAD (commit f82ad36).  I'm using a non-CUDA configuration.  I've followed the steps @sethbruder suggests in his comment on [bazel#649](https://github.com/bazelbuild/bazel/issues/649), including copying the contents of  from bazel into  and into .  This is possibly related to #332, as I'm getting the same error, but at build time as opposed to API usage.

I've tried to set up the relevant paths for my non-standard system resource install using the following settings before invoking bazel:



(there may be some leftover settings; this is adapted from what I used for building bazel in [bazel#925](https://github.com/bazelbuild/bazel/issues/925))

When I invoke bazel with



I get the following error output:



It seems like I may be missing an ""-lm"" flag in the invocation to gcc to the protobuf target; I've tried including it in LDFLAGS as shown above, but it doesn't seem to be appearing in the gcc invocation.
"
643,29072,0,"Documenting the Ref keyword from operations input/output types. Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## Description of issue (what needs changing):

### Clear description

Some operations, like , use a  keyword for their input and/or output. (example: tensorflow/core/ops/state_ops.cc). This keyword is not documented anywhere.

Additionally, I haven't found any way to combine  with  (i.e. make an operation take a list of mutable tensors as input).
"
50,19548,1,"Inference accuracy depends on batch size. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: r1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 4.9
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: 16GB
- **Exact command to reproduce**: pip3 install tensorflow_gpu-1.8.0-cp35-cp35m-manylinux1_x86_64.whl

### Describe the problem
The inference accuracy of a trained model for image recognition, e.g. ResNet-18, Incpetion_v3, depends on the batch size. For example, if the batch size of the input is set to one, the accuracy is around 0.2. However, if feed 32 images at ones, the accuracy becomes over 0.82... If feed even more images at ones, the accuracy can even increase... This is really strange.

### Source code / logs
The model will classify 16 categories.  Images for training and validating are subsets of imagenet. (Over 223,918 training images and 22, 016 validating images)

1. The input size for training is set as:


2. data is feeded by dataset  from tfrecord. 


3. model loading from pb file


4. Computation of accuracy


Feeding one image for inference:
> 2018-05-24 21:14:36 Step 22016 : Validation Accuracy:0.21825 

Feeding 16 images for inference：
> 2018-05-24 21:16:22 Step 22016 : Validation Accuracy:0.80069

Feeding 32 images for inference :
> 2018-05-24 21:18:14 Step 22016 : Validation Accuracy:0.81486

Feeding 128 images for inference:
> 2018-05-24 21:22:42 Step 22016 : Validation Accuracy:0.82744

"
1066,30815,0,"Feedback on Tensorflow 2.0 beta - documentation issues?. Hi,

Please accept the following as feedback on my experience of Tensorflow 2.0 (beta):
1. Read the [migration guide](https://www.tensorflow.org/beta/guide/migration_guide) and figured I would give it a go.
2. Installed without issues on my Mac Conda environment.  This was to be the highlight of my successes.
3. The [documentation site](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf) isn't searchable by version and given the frequency and amount of change here it would be nice to find a way to locate where various things have moved to.
4.  By way of example, PhasedLSTMCell, I knew the contrib module is gone in 2.0 but its still in the GitHub 2.0 branch which is misleading.  Further misleading is the comments [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/__init__.py): 
> Created in contrib, eventual plans to move to core.

No indication of where one might currently find it.
5. Figured it might be in in [Addons](https://github.com/tensorflow/addons) but no luck there.
6. I wanted to use tf.keras for the first time.  Maybe its my Pip/Conda environment but no matter what I did I could not get it to import unless I did import tensorflow._python._keras.  Did I miss this in the docs because I swear I didn't read it anywhere.  
After some more poking around I decided I had enough exposure to 2.0 and went back to 1.14 - it did give me some minutes of excitement.

However, I would love 2.0 to be speedily and widely adopted.  The API looks a lot cleaner (from what I read of it, I didn't get to use any ultimately) and I think some improvements around how the documentation is accessible would help uptake.  I'm willing to contribute to help this along if I can.
"
1160,3492,0,"Dilated Convolution, Dilated Pooling and 2D/3D Sliding Window CNNs. Could you please implement 2D/3D dilated convolution and 2D/3D dilated pooling in tensorflow?

Please see http://arxiv.org/pdf/1511.07122.pdf for a reference on dilated convolution.

Dilated max-pooling is simply regular max-pooling but the pixels/voxels you use in each ""application"" of the max-pooling operation are exactly the same pixels/voxels you would select with dilated convolution.

Dilated convolution/pooling are useful for connectomics and 3D shape datasets (3D deep learning).
"
888,6158,0,"installation issues on Ubuntu 16.04 GPU version. Dear developers:
I encountered one error with fine setting configuration
All the settings is right, and it said that 

                                           all external dependencies fetch successful 
                                           configuration finished 

all the thing right until I generate the pip_package

(bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package), 

process like below:


can you kindly tell me where is wrong?

Thank you!"
274,32248,1,"Performance regression in sparse_dense_matmul. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.14.0
- Python version:
3.7

**Describe the current behavior**

Performance of  is significantly slower compared to 1.12
 
**Describe the expected behavior**

Performance does not degrade between versions. If there is a regression, it should be included in the release notes.

**Code to reproduce the issue**

**Other info / logs**
"
1229,3008,0,"NotImplementedError for learn.TensorFlowEstimator.restore. I had saved a model using tensorflow.contrib.learn and am currently trying to restore it. However, I am getting a .



I realized that this is indeed not implemented in the [latest commit](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/base.py).

Until this is implemented, are there any workarounds? I tried pickling the model but was not able to do so, as it is a Module object.
"
1156,17579,0,"streaming curve points bug. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9/7
- **GPU model and memory**: titan x 12g
- **Exact command to reproduce**: eval_train_classifier.py

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

In the metrics_ops, there is streaming_curve_points. I added it to evaluation below code.
Everything else works flawless, but the ROC curve doesn't.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Source code:


Logs:

> 2018-03-09 01:26:50.313306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties:                                                                                                                 [157/1802]
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:03:00.0
totalMemory: 11.92GiB freeMemory: 11.80GiB
2018-03-09 01:26:50.313398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0
2018-03-09 01:26:50.674278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11431 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci b
us id: 0000:03:00.0, compute capability: 5.2)
INFO:tensorflow:Restoring parameters from /tmp/glaucoma-models/resnet_v2_152_new/model.ckpt-3013
INFO:tensorflow:Evaluation [1/2]
INFO:tensorflow:Evaluation [2/2]
eval/FNs[21]
eval/FPs[11]eval/TNs[44]
eval/TPs[34]

eval/Accuracy[0.709090889]
Traceback (most recent call last):
  File ""eval_image_classifier.py"", line 196, in <module>
    tf.app.run()
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""eval_image_classifier.py"", line 192, in main
    variables_to_restore=variables_to_restore)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.py"", line 212, in evaluate_once
    config=session_config)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/evaluation.py"", line 212, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 651, in __exit__
    self._close_internal(exception_type)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 683, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.py"", line 311, in end
    summary_str = session.run(self._summary_op, self._feed_dict)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1344, in _do_run
    options, run_metadata)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1363, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: tags and values not the same shape: [] != [200,2] (tag 'eval/ROC_curve')
         [[Node: eval/ROC_curve = ScalarSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](eval/ROC_curve/tags, curve_points/stack_1/_5417)]]

Caused by op u'eval/ROC_curve', defined at:
  File ""eval_image_classifier.py"", line 196, in <module>
    tf.app.run()
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""eval_image_classifier.py"", line 168, in main
    op = tf.summary.scalar(summary_name, value, collections=[])
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/summary/summary.py"", line 100, in scalar
    val = _gen_logging_ops._scalar_summary(tags=tag, values=tensor, name=scope)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 402, in _scalar_summary
    ""ScalarSummary"", tags=tags, values=values, name=name)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1617, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [200,2] (tag 'eval/ROC_curve')
         [[Node: eval/ROC_curve = ScalarSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](eval/ROC_curve/tags, curve_points/stack_1/_5417)]]
"
1054,714,0,"when do we support cudnn4?. cudnn4 natively support batch normalization, when do we support cudnn4?
Actually, theano acts very fast, it support cudnn4 only several days after it comes out.
"
412,10493,0,"Typo in text. Typo at this address: https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/Graph
I thought it primes in our mind that TensorFlow for Java is not yet mature. Fixing it might help new comers!

WARNING: Resources consumed by the Graph object **msut** be explicitly freed by invoking the close() method then the Graph object is no longer needed.

"
585,7397,0,"`tf.dynamic_stitch` gradient is incorrect. ### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


gives output


The numeric gradient correctly shows that  has no impact on  (since the value of  is completely overwritten by a constant in the ).  The analytic gradient is incorrect; it seems like the gradient calculation in  does not handle the case where there are duplicate indices being merged.
"
65,12419,1,"Non-fused batch norm with NCHW is mush slower than with NHWC. ### Describe the problem
I noticed that in my environment, non-fused batch norm with ""NCHW"" format  run significantly slower 
### Environment info

### Source code / logs
Here is my source code


And my tested results:

 data format |  fused/non-fused batch norm  |  images/sec
------------ | ------------- | --------------
NHWC | fused |  1085.5 
NHWC | non-fused |  969.1
NCHW | fused |  1315.6
NCHW | non-fused | **301.1**

I test benchmarks with model resnet50 on P40 and get similar results

 data format |  fused/non-fused batch norm  |  images/sec
------------ | ------------- | --------------
NHWC | fused |  65.7 
NHWC | non-fused |  51.0
NCHW | fused |   84.4
NCHW | non-fused | **7.5**


### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I found issue https://github.com/tensorflow/tensorflow/issues/7551 similar with my problem but  with opposite result"
974,32111,0,"ImportError: cannot import name 'dense_features' from 'tensorflow.python.feature_column'. 

tensorflow==1.14.0
Python==3.7.3

What is the issue in this case?"
1289,32073,0,"build error tensorflow lite for ARM64. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- TensorFlow version (or github SHA if from source):master
- GCC/Compiler version (if compiling from source): 7.4.0

**problem:**
i want to build the label_image,and i change the makefile,add label_image like minimal:













i got a build error：label_image.cc
undefined reference：""tflite::evaluation::CreateGPUDelegate(tflite::FlateBufferModel)""
undefined reference：""tflite::evaluation::CreateNNAPIDelegate()"""
895,22449,0,"[bazel] Update bazel to 0.17.1. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: PR #19461
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.17.1/0.17.2
- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: build of PR #19461

### Describe the problem

This is a placeholder for updating bazel to 0.17.1 so that PR #19461 could pass CI tests. Before bazel 0.17.1, bazel had some issues with fetching http_archive (See https://github.com/bazelbuild/bazel/issues/5932). The issue has been fixed in 0.17.1.

The update of bazel 0.17.1 requires additional efforts than bumping versions in the repo (See https://github.com/tensorflow/tensorflow/pull/22281#issuecomment-421414201). Pushing bleeding edge bazel immediately caused some issues before, so it is preferred to wait until a full release cycle before making the change.

At the moment, bazel 0.17.2 has been released (See https://github.com/bazelbuild/bazel/issues/6164#issuecomment-423524449) which is a minor release increment to 0.17.1.

/cc @gunan 

Note The bazel 0.18.0 release is not far away (See https://github.com/bazelbuild/bazel/issues/5963), as far as I could see. so wait until 0.18.0 is rebased, then update to 0.17.1 is also reasonable I think."
1432,29946,0,"How to make remote worker load custom op?. I write a custom op and compile it to .so file. When I use it in to train model locally, it works fine. But when I use it to run in standalone mode, worker server alway tells me OP not registered, because the worker server started before, it have not load my custom op.

So is there anyway I can do to tell remote worker to load lib dynamic like tf.load_op_library in local mode?"
98,23766,1,"TfLite performance is way worse comapred to Tensorflow mobile. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S8
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): On PC: current tf-nightly (14.11.2018), on phone current org.tensorflow:tensorflow-lite:0.0.0-nightly
- Python version: 3.6.6
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: - 
- GPU model and memory: CPU only

**Describe the current behavior**
I build a model with this code:


Then I froze the model. In the first try I converted it to TFlite directly, in a second try I set the  option in the conversion process to quantize my model and in a second try I optimized my model for inference before converting and quantizing.
The code for the tflite conversion is the following:

The code for optimizing for inference was the following terminal call: 

All my models, including the plain, unconverted model is uploaded here for testing: [models.zip](https://www.dropbox.com/s/gyu9cfkn3yn07oc/models.zip?dl=0)

**Describe the expected behavior**
I deployed those models to my phone. I get runtime of around 290 ms for the unconverted model, while having runtimes of about 420 ms for the quantized tflite, the optimized and quantized tflite and the plain tflite model. That can't be right, can it?

**Code to reproduce the issue**
The code for running inference on the phone for the tflite models is the following:


The code to run inference for the unconverted model is:


**Other info / logs**
Output behavior is as expected for all models. Activating the useNNAPI option doesn't really do anything performance wise aswell as setting the number of threads.
"
1326,9566,0,"Documentation pages are unnecessarily large. 
Each page in the documentation now contains a HUGE left navbar contributing over 99% of the size.

1. This would waste a lot of network traffic on loading identical navbar over and over again.
2. It creates a big trouble when I tried to build an offline version of the doc. The whole html documents used to be <100MB, now they are 2.5GB."
37,31307,1,"Performance decline after updating from 1.13.1 to 1.14.0. I have observed an obvious performance decline after I update TF from 1.13.1 to 1.14.0, even if the code has never changed. 
I am using CUDA 10.0 and tf.contrib.opt.AdamWOptimizer"
33,13376,1,"MatMul in TensorFlow is slower than dot product in numpy. I am observing that on my machine tf.matmul in tensorflow is running significantly slower than dot product in numpy. I have GTX 1080 GPU, and expecting tf.matmul to be at least as fast as when running the code using CPU (numpy).

**Environment Info

Operating System**


**Installed version of CUDA and cuDNN:**


**TensorFlow Setup**


**Code:**


"
831,11755,0,"how to assign a tensor to a sub part of another tensor. Hi, dear all,

I am new to tensorflow. Previously I use theano. Now I want to assign a tensor to a sub part of another tensor. e.g.:

a = 
[[0,0,0,0],
[0,0,0,0],
[0,0,0,0],
[0,0,0,0]]
b = 
[[1],
[3],
[4].
[5]]
I would assign to a[0,:], like this
a = 
[[1,0,0,0],
[3,0,0,0],
[4,0,0,0],
[5,0,0,0]]
in theano we use set_subtensor, what should I do with tensorflow? Thank you very much!"
646,20468,0,"Failing to load h5 model using tf-gpu?. 




I've tried--fresh reinstall, change float

Any fixes? No idea why Im getting this. P.S: I trained the model on a titan v and I am trying to now open it on a computer with a GeForce GT 530 Gpu."
316,15839,0,"Problem in ""/tensorflow/examples"" project to build an android app. ## As we know, there will build three APP and one TF speech app in the 'tensorflow/examples'. 
## But, I just want to use the  detectorActivity to build an app, aim at to use this app to do a porject named 'Image Recognition'.
##  So I need to delete three other java file like ClassifierActivity、StylizeActivity、SpeechActvity and so on, and download the model that detectorActivity need. like this

![1](https://user-images.githubusercontent.com/33651882/34554975-6605fa0e-f16a-11e7-889a-418d99ac1498.png)

## But, when I choose to build apk, there was a problem that ""Execution failed for task: downloadFile  org.apache.http.conn.ConnectTineoutException:......."" 
## I use the windows and android studio to build this project. 
#  **Is anyone can help me to separate this example just retain the  detectorActivity  and use our's model in android:assets , to help me build an app to do 'Image Recognition' Project. Thx every friends**
  
  
  "
1146,27384,0,"How to use estimator with graph creation routines in model_fn using placeholders?. It seems like tf.estimator.Estimator is worth using even for optimization of custom graphs. I am falling down a pattern of creating graph constructors using placeholders for the input. The particluar graph chunk will appear in multiples places in the large graph with the same (shared) weights but different inputs. 

It is not clear from the estimator documentation what the ""right"" pattern inside the model_fn is. I have a feeling running a session with a feed_dict in there is wrong. 

Are there any more examples that handle something like this? "
1142,19625,0,"Raw pointer member variables of tflite::Interpreter are not initialized or released properly.. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.13.1
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)
- **CUDA/cuDNN version**: 9/7
- **GPU model and memory**: K80
- **Exact command to reproduce**:


output:


There are two raw pointer member variables within tflite::Interpreter.
- [](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/interpreter.h#L576) is not initialized. 
[It may cause segmentation fault](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/profiling/profiler.h#L134) if  is initialized with a nonzero value and users don't set it by .

- [](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/interpreter.h#L545)
The default error reporter pointed by  will not be released.
"
1290,24814,0,"TensorFlow binary was not compiled to use : AVX2 AVX512F FMA. Using the virtualenv install procedure (https://www.tensorflow.org/install/pip)
(venv) user@ubuntu:~$ pip install --upgrade tensorflow

Installing collected packages: tensorflow
Successfully installed tensorflow-1.12.0



I don't know if this is a warning or that TensorFlow is in fact installed.
Ubuntu 18.04



### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
275,24059,1,"The performance of transpose_conv op in TensorFlow Lite. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
**Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Linux Ubuntu 14.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
**Meizu 16th**
- TensorFlow installed from (source or binary):
**benchmark_model is built form source**
- TensorFlow version (use command below):
**master**
- Python version:
**Python 3.5.3**
- Bazel version (if compiling from source):
**0.16.1**
- GCC/Compiler version (if compiling from source):
**4.9.4**
- CUDA/cuDNN version:
**No**
- GPU model and memory:
**N/A**

**Describe the current behavior**
I wanted to deploy a trained model on Android devices with TensorFlow Lite, but it was quite slow.
Then, I profiled the model(.tflite) with the benchmark_model tool, and found that the transpose_conv op took too much time. The summary by node type shown below:
![screenshot from 2018-11-30 11 11 48](https://user-images.githubusercontent.com/21071150/49267677-8f39c600-f496-11e8-9e91-1da5dd9f4603.png)

When I profiled the same model(.pb) used to convert to .tflite, I found that the transpose_conv achieving fast inference speed. The summary by node type shown below:
![screenshot from 2018-11-30 11 04 33](https://user-images.githubusercontent.com/21071150/49269045-d9727580-f49d-11e8-8d4c-84e965350420.png)

It seems like the transpose_conv op in TensorFlow Lite is much slower than that in TensorFlow Mobile?
"
343,23003,0,"Problem on running tensorflow. Traceback (most recent call last):
  File ""retrain.py"", line 132, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
1174,2225,0,"InvalidArgumentError: No OpKernel T=DT_INT32 for log, exp, etc.. ### Environment info

Operating System: Mac/Ubuntu
Installed version of CUDA and cuDNN: None
commit b8883a237b71e877759327fb4b9077847d4cb16c
### Steps to reproduce

Thanks to http://stackoverflow.com/questions/37027762:


### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

https://www.tensorflow.org/versions/r0.8/api_docs/python/math_ops.html#log says ""x: A Tensor. Must be one of the following types: float32, float64, int32, complex64, int64"", but int32 and in64 throws InvalidArgumentError:

InvalidArgumentError: No OpKernel was registered to support Op 'Log' with these attrs
     [[Node: Log_4 = Log[T=DT_INT32](Log_4/x)]].

I haven't checked all, but ,  and  have the same issue.

I added a simple test:



which clearly reveals this issue:


"
1087,24120,0,"TFLite Speech recognization app is getting crashed with setUseNNAPI(true). **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):[NO]
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):
- Python version: 3.0
- Bazel version (if compiling from source):[bazel release 0.16.1]
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


Description: 
Im using the source code from below repo:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app
Compiled using the readme in above link
The app works fine without any changes.
However when i enable setUseNNAPI(true) for speechActivity the crashes with below logs:

======================================================================
### tflite  : Custom operations are not supported when using NNAPI.
12-03 14:10:16.545  7947  7969 E tflite  : Returning error since TFLite returned failure nnapi_delegate.cc:745.
12-03 14:10:16.545  7947  7969 E tflite  : Failed to build graph for NNAPI
12-03 14:10:16.545  7947  7969 E AndroidRuntime: FATAL EXCEPTION: Thread-3
12-03 14:10:16.545  7947  7969 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 7947
12-03 14:10:16.545  7947  7969 E AndroidRuntime: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: 
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:140)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.demo.SpeechActivity.recognize(SpeechActivity.java:339)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.demo.SpeechActivity.access$100(SpeechActivity.java:67)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at org.tensorflow.demo.SpeechActivity$3.run(SpeechActivity.java:290)
12-03 14:10:16.545  7947  7969 E AndroidRuntime: 	at java.lang.Thread.run(Thread.java:764)
==============================================================================
**Other info / logs**
Note:
Image clasification works fine with same changes setUseNNAPI(true)

"
422,11052,0, standard_tensorboard_wsgi() missing 1 required positional argument: 'plugins'. how set plugins ? thanks
453,9774,0,"Cifar-10 link inside Tensorflow webpage report 404. Not sure if this is the right place for an 404 error...

The link to CIFAR-10 source code on Tensorflow webpage reports 404 since yesterday afternoon (08/May/2017), kindly help to fix it.

This is the page found contains error:
[https://www.tensorflow.org/tutorials/deep_cnn](url)

This is the link found to reports 404:
[https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/image/cifar10/](url)

Thanks"
612,33599,0,C binding for tensorflow 2.0. Is there a release date for  for ?
730,1117,0,"embedding_lookup on multiple dimensions with AdagradOptimizer throwing exception. I am completing the Udacity course on Tensorflow, and noticed that when embedding_lookup is used in 3 dimensions with AdagradOptimizer, the optimizer throws an error:

CODE (with error):



Error message:



Just change it to get embedding look up once for each time for the third dimension:



The code runs smoothly!
"
1151,21750,0,"SwappingPass technology problem in grappler. System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device : N/A
TensorFlow installed from (source or binary): source
TensorFlow version (use command below):1.8.0
Python version: 3.5
Bazel version (if compiling from source):0.10.0
GCC/Compiler version (if compiling from source): c++11
CUDA/cuDNN version: 9/7
GPU model and memory: gtx 1080ti, 11G
Exact command to reproduce: N/A
Describe the problem:

Describe the problem:
1.
if I use this config "" gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5)"", why the prop.memory_size() is not following the ""per_process_gpu_memory_fraction=0.5"" size ? 

In IdentifySwappingCandidates (memory_optimizer.cc:530) 
 if (mem_usage.used_memory <= prop.memory_size()) {
      continue;
    }
 int64 required_savings = mem_usage.used_memory - prop.memory_size()

The size is determined by the user is also let the tensorflow know to calculate the demand.

2.
Although the tensorflow has a swap function, I found it only for kernel tensors not feature tensors.
If I want to add a new swap data function for feature tensors, can it be added in memory_optimizer.cc file?
"
1178,31040,0,"TFLite conversion fails when using BatchNorm after Reshape (Check failed: dim_x == dim_y). **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 & Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v1.12.1-6931-g2b5ece29d3 1.15.0-dev20190724**
- Python version: **3.6.8**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
Trying to convert a graph containing a reshape layer followed by batch normalization appears to trigger an incorrect op reordering. In particular, the toco converter relocates the mul operation from the BN layer to before the reshape layer, at which point the layers do not have compatible dimensions. This results in the  error. 

From looking at the Graphviz video, this change is introduced in frame 38.

**Describe the expected behavior**
The converter should not reorder operations across reshape if it would cause the dimensions to no longer match.

In addition, the check failure message should provide more information about the location of the error, such as the originating layer names (which are visible in the Graphviz outputs).

**Code to reproduce the issue**


**Other info / logs**
This occurs on all version of TF I've tested (1.13.1, 1.14.0, tf-nightly). It's possible that this sequence of operations just isn't supported, but this should be explicitly stated if so. The error message is quite vague and made the problematic sequence of ops extremely difficult to track down in a large graph.

Relevant log messages:

"
912,26809,0,"[TF 2.0] Cannot load Keras Sequential model witn InputLayer from h5.. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian Stable**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **CPU, both TF-2.0.0a0 and tf-nightly-2.0-preview-2.0.0.dev20190315**
- Python version: **3.5.3**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
When a  Keras model contains  and it is saved, it cannot be loaded and fails with a message


**Describe the expected behavior**
The model can be loaded correctly.

**Code to reproduce the issue**


**Other info / logs**
The problem is in the fact that  does not return the . To quote comments from the method:
sequential.layersaddInputLayerTrackable_layers
The problem is that if  was added manually with a specified , then it is an error not to serialize it -- because then the following layers do not know what the input shape is.

Workarounds:
- if instead of  you add  to the  layer, it works. However, input  cannot be specified in this way and when you pass  on input, using an  is required
- in Functional API the  _is_ serialized in the model (and, funnily, as a ).

Solutions:
- when serializing a  model, all layers need to be serialized (and no filtering of  performed)."
530,24134,0,"optimizing for mac users to use AVX2 FMA. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac 10.13.6
- TensorFlow installed from (source or binary): 1.12.0 binary

**Describe the problem**

according to the docs,  , however, when running that version, I still see 

I wanted to know if there were plans to optimize the mac build to take full advantage of the CPU?

**Provide the exact sequence of commands / steps that you executed before running into the problem**






**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
679,12792,0,"Feature request: let transform_graph use summarize_graph inputs/outputs guess as default flags.  is very useful for listing names of input and output nodes, however it seems like you still have to set  and  with  explicitly. Would it be possible to have  assume the same nodes from  by default?"
805,28613,0,"Issue with transformer guide. ## URL(s) with the issue:
https://www.tensorflow.org/alpha/tutorials/text/transformer
## Description of issue (what needs changing):
format and normalization layer.

### Clear description
1. when describe the multi-head attention, there is this text in one line:  ""Multi-head attention consists of four parts: * Linear layers and split into heads. * Scaled dot-product attention. * Concatenation of heads. * Final linear layer. ""
  I guess the * mean bullet items. It is not formatted correctly.
2. The EncoderLayer and DecoderLayer use LayerNormalization. There is no LayerNormalization in keras.layers. There is only BatchNormalization.
3. Evaluate step creates mask. No mask is needed since a) there is no padding for a single sentence. b) there is no look_ahead since we are trying to predict next words. 
"
393,2115,0,"Tutorial yields ""tensorboard: Command not found"". GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System: Mac OS X Yosemite, 10.10.5
MacPorts
python 2.7

Installed version of CUDA and cuDNN:  None
(please attach the output of ):

If installed from binary pip package, provide:
1. Which pip package you installed.
   % sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl
   Collecting tensorflow==0.8.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl
   Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl (19.3MB)
     100% |████████████████████████████████| 19.3MB 53kB/s 
   Requirement already up-to-date: six>=1.10.0 in /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from tensorflow==0.8.0)
   Collecting protobuf==3.0.0b2 (from tensorflow==0.8.0)
   Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB)
     100% |████████████████████████████████| 327kB 2.0MB/s 
   Collecting wheel (from tensorflow==0.8.0)
   Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)
     100% |████████████████████████████████| 71kB 10.1MB/s 
   Collecting numpy>=1.10.1 (from tensorflow==0.8.0)
   Downloading numpy-1.11.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.9MB)
     100% |████████████████████████████████| 3.9MB 265kB/s 
   Collecting setuptools (from protobuf==3.0.0b2->tensorflow==0.8.0)
   Downloading setuptools-20.10.1-py2.py3-none-any.whl (509kB)
     100% |████████████████████████████████| 512kB 1.9MB/s 
   Installing collected packages: setuptools, protobuf, wheel, numpy, tensorflow
   Found existing installation: setuptools 19.2
     Uninstalling setuptools-19.2:
       Successfully uninstalled setuptools-19.2
   Found existing installation: numpy 1.10.4
     DEPRECATION: Uninstalling a distutils installed project (numpy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.
     Uninstalling numpy-1.10.4:
       Successfully uninstalled numpy-1.10.4
   Successfully installed numpy-1.11.0 protobuf-3.0.0b2 setuptools-20.10.1 tensorflow-0.8.0 wheel-0.29.0
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".
   % python -c ""import tensorflow; print(tensorflow.**version**)""
   0.8.0

If installed from sources, provide the commit hash:
### Steps to reproduce

1: Read  the documentation at https://www.tensorflow.org/versions/r0.8/how_tos/summaries_and_tensorboard/index.html:
""Launching TensorBoard

To run TensorBoard, use the command

tensorboard --logdir=path/to/log-directory""

2: Do what it says


### What have you tried?

1: Try rehash



2:



3:



4:

Check Google, find StackExchange post ""How to install tensorboard""
http://stackoverflow.com/questions/33634008/how-to-install-tensorboard
Comments suggest that if one installs via pip (as I did), then tensorboard is available on the command-line, but it is not.
Alternate invocation of tensorboard via python is not the same as in the Tutorial but does work....



and everything works.

...Conclusion is that either online Tutorial documentation is incorrect and needs revision, or pip installation does not perform as advertised, or...user error.

Workarounds:

1.



2.

echo '#! /usr/bin/env python' > tmp.txt ; cat tmp.txt /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/tensorboard/tensorboard.py > tensorboard ; chmod u+x tensorboard ; rm -f tmp.txt; sudo mv tensorboard /opt/local/bin/
`
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
"
17,10270,1,"Performance degradation with large lookup tables - optimizer._apply_sparse_duplicate_indices  (TF V1.0.1). Hi,

I ran into this performance issue while trying to upgrade tensorflow from version 0.12.1  to 1.X.

We ran a network with large embedding lookup tables:
- 100K X 32 (for example, word embedding -  with 100K unique words)
- 300K X 128 (for example, categorical feature with cardinality of 300K unique items)

 After upgrading TF version to 1.0.1,  GPU usage dropped in from 60% to 30%.
Training time went up in 50%-200% (depends on how big is the embedding lookup table). 


This is the commit that caused the performance degradation:
https://github.com/tensorflow/tensorflow/commit/f9f56f9dc7fe41ef1128290a77ac88e889ea5229

The handling of unique indexes is very slow and does not run in parallel with others operations. 
Please note the big unique blocks in the middle.
![trace_unique](https://cloud.githubusercontent.com/assets/8734262/26542969/ab0f3740-4464-11e7-9dcb-f3ccd58dfc8a.png)

Here is a work around (not handling unique indexes ):



Thanks,
Erez
"
723,5942,0,"Update Cmake README Doc Reflecting 0.12r. Now with 0.12r would be interesting to update Readme as current status, instructions or simply reference the release? "
749,17485,0,"tf.contrib.image.transform crashes under Windows when CUDA is enabled. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. A minimal example reproducing the bug is provided below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 (x64)
- **TensorFlow installed from (source or binary)**: 
Binary installed using 
- **TensorFlow version (use command below)**: 
b'unknown' 1.6.0 (also tested on b'unknown' 1.4.0)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**:  
  CUDA 9.0.176 / cuDNN 7.0.5
  CUDA 8.0.61 / cuDNN 6.14.11
- **GPU model and memory**:
(device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2, memory: 3.00GiB)
(device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0, memory: 4.00GiB)
- **Exact command to reproduce**: python example.py (see below)


### Describe the problem
The function tf.contrib.image.transform crashes when CUDA is enabled under Windows. It produces the following errors:  on tensorflow 1.6 or  on tensorflow 1.4.
However, it functions correctly when CUDA is disabled (by setting _CUDA_VISIBLE_DEVICES_ to -1). 

I tested a variation of different parameters (such as varying the batch size, image sizes, and number of channels), but the behavior stays the same. In addition I reproduced the same error on a different machine with an older tensorflow version.

### Source code / logs
- **Code**:

- **Console Output**:

  Output with tensorflow 1.6 (with GeForce GTX 970M,) :



  Output with tensorflow 1.4 (with Quadro M1200):

"
1390,35068,0,"Unable to use optimisation when modifying gradients returned by tf.gradienttape. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI 2018.03
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0
- Python version: 3.6.8
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla K80

My network has multiple outputs and I wanted to be able to use separate loss functions on each output branch.

Initially, I had it setup like this:



The loss functions were giving None gradients for the branch of the prediction that was not included in that particular loss, so I replaced those None gradients with zeros_like.

When I run this as written, it takes an extremely long time (10min+) to run a single training step, and I see the following message in the logs:




When I remove the @tf.function decorator, it runs in about 10% of the time, and I do not see this log warning, but GPU utilisation is still 0%, and it still runs more slowly than I would expect (i.e. much slower than (number of output branches)*(time for updating just one branch))

The way that I had to resolve this, was to stop overwriting the None gradients, which I was able to do like this:


This is fine, and now training at the expected rate, however I can't see why one shouldn't be able to modify the gradients in this way? The zero gradients that I was passing had the correct dtype, which isn't reflected in the warning message that says it was receiving ints, and that was the reason it was failing to optimise.
"
8,10765,1,"[Performance] contirb.seq2seq.attention_wrapper slower due to using matmul instead of reduce_sum. tf version '1.2.0-rc0' contirb.seq2seq.attention_wrapper is great, it make using attention much easier.
However I found using attention_wrapper will be much slower then tf version 1.0.
After some experiment I found it is due to using matmul instead of reduce_sum.

from attetntion_wrapper.py 731
      
      expanded_alignments = array_ops.expand_dims(alignments, 1)
      attention_mechanism_values = self._attention_mechanism.values
      context = math_ops.matmul(expanded_alignments, attention_mechanism_values)
      context = array_ops.squeeze(context, [1])

Using above code for one of my application got 2.2 batch/s, after changing to use reduce_sum(as tf version 1.0 did), the speed is 3.4 batch/s, improve a lot.

      expanded_alignments = array_ops.expand_dims(alignments, 2)
      attention_mechanism_values = self._attention_mechanism.values
      context = math_ops.reduce_sum(expanded_alignments * attention_mechanism_values, [1])"
857,19469,0,"C:\Users\Lenovo\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
645,9382,0,"Tensorboard not (correctly) displaying histograms; unreliably displaying graphs (V1.1rc2,win7,chrome57). When I went from tensorflow V1.0 to V1.1rc2, histograms stopped working correctly: the data isn't drawn unless that trace is highlighted via mouseover, and even then it's not using the right plot boundaries:
![image](https://cloud.githubusercontent.com/assets/8495647/25305145/f3be2926-2743-11e7-8ed1-589d4b17452e.png)
is an example (cursor is not displayed but is at the dot in top left of graph; otherwise graph is blank) generated from this code:

I haven't tried this toy repro with v1.0, but other models that I've done in 1.0 and 1.1rc2 exhibit the same behavior. The distributions look fine, and scalars also display fine.

Another problem that's new to me between 1.0 and 1.1rc2 is that graphs sometimes display fine, sometimes are blank until I reload browser page a few times. (Apologies if this should have been a separate issue; wanted to keep spam volume down). I don't have a strong idea of what triggers this, but the frequency is pretty high, roughly half the time.

the console running tensorboard emits the following warning periodically, the timing of which I haven't correlated with either problem mentioned above:

> WARNING:tensorflow:path ../external\data/plugin/text/runs not found, sending 404

### System Information
- using custom code, copied above
- Windows 7 64 bit, fully patched
- tensorflow 1.1rc2 downloaded from https://pypi.python.org/packages/0d/cb/25f2cdd8905070373945c1f57edbe9d5f51a4482aa7097e5613cbdc4a41f/tensorflow_gpu-1.1.0rc2-cp35-cp35m-win_amd64.whl#md5=80ccd71614b438ffe8af3cd0dd572d3b and installed via pip
- CUDA 8.0 CUDNN 5.1
- GTX 1080 ti, 11gb ram
- generate histogram's data (eg, with code above), start tensorboard, (fail to) view histogram; view graph, experience intermittent success
"
1022,25007,0,"FAILED: Build did NOT complete successfully. **System information**
- OS Platform and Distribution: Ubuntu 16.04.5 LTS
- TensorFlow installed from: Docker image 
- TensorFlow version:  branch
- Python version: 2 and 3
- Installed using: Docker


**Describe the problem**

I want to submit pull requests and am unable to run the TensorFlow unit tests. I will be happy to add directions to the documentation once I solve the problem.

I set up an Azure virtual machine with Tesla M60 GPUs (the most recent that my academic subscription allows) with the image  on a . I ssh into it, run these commands, and get the error in the title.



Then, inside the Docker container:



I attach the whole console output, which ends with


[failed_build_log.txt](https://github.com/tensorflow/tensorflow/files/2770095/failed_build_log.txt)


How can I set up a machine that runs TensorFlow's unit tests?"
226,30126,1,"Memory leaking in tf.data.Dataset in eager mode tensorflow1.12.0. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (MacBook Pro10.14):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (pip install tensorflow-gpu):
- TensorFlow version (1.12.0):
- Python version:3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
At the beginning of the program, the memory is only 190MB. After running for 1 minute, the memory reaches 6GB. Memory leak is very serious

**Describe the expected behavior**

The program should always be the initial state of 190MB without memory leaks.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


class RecordDataset:
    def __init__(self, configs):
        assert(isinstance(configs, dict))
        self.norm_h = 32
        if 'norm_h' in configs:
            self.norm_h = int(configs['norm_h'])
        self.expand_rate = 1.0
        if 'expand_rate' in configs:
            self.expand_rate = float(configs['expand_rate'])
        self.file_list = []
        if 'file_list' in configs:
            self.set_files(configs['file_list'])
        self.num_parallel=4
        if 'num_parallel' in configs:
            self.num_parallel = int(configs['num_parallel'])
        self.batch_size = 32
        if 'batch_size' in configs:
            self.batch_size = int(configs['batch_size'])

        self.max_txtlen = 32
        if 'max_txtlen' in configs:
            self.max_txtlen = int(configs['max_txtlen'])
        self.max_imglen = 1024
        if 'max_imglen' in configs:
            self.max_imglen = int(configs['max_imglen'])
        self.min_imglen = 16
        if 'min_imglen' in configs:
            self.min_imglen = int(configs['min_imglen'])
        self.BUFFER_SIZE = 4096
        if 'BUFFER_SIZE' in configs:
            self.BUFFER_SIZE = int(configs['BUFFER_SIZE'])

        self.char_dict = configs['char_dict']
        self.model_type = configs['model_type']
        self.charset = Charset(self.char_dict, self.model_type)

    def set_files(self, file_list):
        assert(isinstance(file_list, (list, tuple)))
        self.file_list = [file for file in file_list if os.path.isfile(file) and os.path.getsize(file)>0]

    def get_idstr_by_charstr(self, charstr):
        if isinstance(charstr, bytes):
            charstr = charstr.decode('utf-8')
        idxstr = self.charset.get_idxstr_by_charstr(charstr)
        idxlen = len(idxstr.split(','))
        return idxstr, idxlen

    def parse_example(self, serial_example):
        norm_h = self.norm_h
        expand_rate = self.expand_rate
        debug = False

        feat_dict = tf.parse_single_example(serial_example,features={
                                            'img_raw' : tf.FixedLenFeature([], tf.string), \
                                            'height'  : tf.FixedLenFeature([], tf.int64),  \
                                            'width'   : tf.FixedLenFeature([], tf.int64),  \
                                            'channel' : tf.FixedLenFeature([], tf.int64),  \
                                            'img_path': tf.FixedLenFeature([], tf.string), \
                                            'coord'   : tf.FixedLenFeature([], tf.string), \
                                            'label'   : tf.FixedLenFeature([], tf.string)})

        img_raw = feat_dict['img_raw']
        height = feat_dict['height']
        width = feat_dict['width']
        channel = feat_dict['channel']
        img_path = feat_dict['img_path']
        coord = feat_dict['coord']
        img_text = feat_dict['label']
        txt_index, txt_len = tf.py_func(self.get_idstr_by_charstr, [img_text], [tf.string, tf.int64])
        txt_len = tf.to_int32(txt_len)

        coord_val = tf.string_split([coord], ',').values
        coord_val = tf.string_to_number(coord_val, out_type=tf.int32)
        offset_w = coord_val[0]
        offset_h = coord_val[1]
        target_w = coord_val[2] - coord_val[0]
        target_h = coord_val[3] - coord_val[1]

        img_raw = tf.decode_raw(img_raw, tf.uint8)
        orig_img = tf.reshape(img_raw, (height, width, channel))
        crop_img = tf.image.crop_to_bounding_box(orig_img, offset_h, offset_w, target_h, target_w)

        ratio = tf.to_float(norm_h / tf.to_float(target_h))
        norm_w = tf.to_int32(tf.to_float(target_w) * expand_rate * ratio)
        norm_img = tf.image.resize_images(crop_img, (norm_h, norm_w))

        if debug:
            norm_img = tf.cast(norm_img, tf.uint8)
        else:
            # convert RGB-->BGR
            mean = [102.9801, 115.9465, 122.7717]
            norm_img = norm_img[:, :, ::-1]
            norm_img = norm_img - mean
        return img_path, norm_img, img_text, txt_index, txt_len, coord, norm_w

    def filter(self, img_path, norm_img, img_text, txt_index, txt_len, coord, norm_w):
        img_len = tf.cast(norm_w, dtype=tf.int32)
        txt_len = tf.cast(txt_len, dtype=tf.int32)
        txt_len_logical = tf.logical_and(txt_len <= self.max_txtlen, txt_len >= 0)
        img_len_logical = tf.logical_and(img_len <= self.max_imglen,img_len >= self.min_imglen)
        return tf.logical_and(txt_len_logical, img_len_logical)

    def data_reader_v0(self, repeat=0):
        padded_shapes = ([], [self.norm_h, None, 3], [], [], [], [], [])
        padding_values = ('', 0.0, '', '', 0, '', 0)
        dataset = tf.data.TFRecordDataset(self.file_list)
        if repeat != 0:
            dataset = dataset.repeat(repeat)
        dataset = dataset.map(map_func=self.parse_example, num_parallel_calls=self.num_parallel)
        dataset = dataset.padded_batch(self.batch_size, padded_shapes, padding_values)
        return dataset

    def data_reader(self, repeat=0):
        padded_shapes = ([], [self.norm_h, None, 3], [], [], [], [], [])
        padding_values = ('', 0.0, '', '', 0, '', 0)
        fileset = tf.data.Dataset.list_files(self.file_list)
        dataset = fileset.apply(
                    tf.data.experimental.parallel_interleave(
                        lambda filename: tf.data.TFRecordDataset(
                            filename, num_parallel_reads=self.num_parallel),
                        cycle_length=32))

        if repeat != 0:
            dataset = dataset.repeat(repeat)
        else:
            dataset = dataset.repeat()
        dataset = dataset.map(map_func=self.parse_example, num_parallel_calls=self.num_parallel)
        dataset = dataset.filter(self.filter)
        dataset = dataset.cache()
        dataset = dataset.shuffle(self.BUFFER_SIZE).padded_batch(self.batch_size, padded_shapes, padding_values)
        dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)
        #dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
        return dataset


def RecordDatasetTest():
    tf.enable_eager_execution()

    configs = {}
    configs['norm_h'] = 32
    configs['expand_rate'] = 1.0
    configs['file_list'] = ['tfrecord_dir/tfrecord.list.0', 'tfrecord_dir/tfrecord.list.1']
    configs['num_parallel'] = 4
    configs['batch_size'] = 1
    configs['model_type'] = 'ctc'

    configs['char_dict'] = 'char_dict.lst'
    dataset = RecordDataset(configs)
    dataset = dataset.data_reader()
    total_count = 0
    for line in dataset:
        img_path, norm_img, img_text, txt_index, txt_len, coord, norm_w = line
        norm_img = norm_img + [102.9801, 115.9465, 122.7717]
        image = np.array(norm_img.numpy(), np.uint8)
        img_path = img_path.numpy()
        img_text = img_text.numpy()
        txt_index = txt_index.numpy()
        txt_len = txt_len.numpy()
        total_count = total_count + txt_len
    print(total_count)


if __name__=='__main__':
    RecordDatasetTest()

"
1402,2403,0,"""Total error: nan"" for neural network model. Hi 

I have randomly generated exponential signal. I am training the network to predict y values depending on the random time constant (tau). I have added one hidden layer and used gardient descent for optimization. The code runs fine, but it doesnt print error properly. I get this - ('Total Error: ', nan)

here is my code-


"
108,30431,1,"Significantly reduced validation accuracy when switching from alpha0 to beta0/1. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Code is strongly based on: [Stock example](https://www.tensorflow.org/beta/tutorials/images/transfer_learning ), but uses kaggle aerial-cactus-identification dataset. 
- OS Platform and Distribution: Google Colab
- TensorFlow installed from: binary via 
- TensorFlow version: 2.0.0-beta0
- Python version: 3.6.7
- CUDA/cuDNN version: N/A (Google Colab as of 05.07.2019)
- GPU model and memory:  N/A (Google Colab as of 05.07.2019)

**Describe the current behavior**
With beta 0 and 1 the training/validation history is as follows:
![Beta 0/1](https://puu.sh/DOY9H/8ec1a1a7f5.png)
This is a very low validation accuracy. This can of course happen, even with code based on an example, the issue is that this only appears with beta0 and beta1 builds, not with alpha0 (see below) when using the exactly same code and training data.

**Describe the expected behavior**
With alpha0 and the same code the history looks as follows:
![Alpha 0](https://puu.sh/DOYss/b14cc7f979.png)

An upgrade of the tensorflow version should not affect the resulting accuracy in such a manner.

**Code to reproduce the issue**
1. Open this [Google Colab](https://colab.research.google.com/drive/14oaC63n1n3yymRyB90uMx3GNvzt-rGMT)
2. Run the code with Alpha0
![Choose Version](https://puu.sh/DOYdz/3c3efe344c.png)
3. Make note of the training history plot.
4. Restart the runtime.
5. Run the code with Beta0 or Beta1
![Choose Version](https://puu.sh/DOYeP/6a24c7c681.png)
6. Make note of the training history plot.
7. Observe that with no change to the code but using the beta0 instead of alpha0 the validation accuracy goes down from > 95% to < 90%, with beta1 even to < 80%

**Other info / logs**
My best guess is that changes have been made to the pretrained MobileNetV2 or to the Adam optimizer, otherwise the drastic loss in accuracy is hard to explin.

"
1187,30073,0,"model = MyModel() #model1 = MyModel() model1 = model. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
652,31982,0,"Can't build 1.14.0 on Nvidia Xavier AGX. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04, 
- Device : Nvidia Xavier AGX
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0.326-1
- GPU model and memory: GV10B, 16GB shared

The build process can't start out of the box. Problem is missing instructions for the toolchain.
The following two patches are needed to be able to build:

and

It would be nice if these get incorporated into the mainline. Probably related to #22629"
618,27449,0,"[Enhancement] Automatically chose either memory or storage to cache in tf.data API. **System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
At the moment, one has to keep in mind what the systems resources will be when using the tf.data pipeline API. Specifically, when using the https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache operation, I have to ask myself whether it's worth caching the data and whether to do so in memory or storage.

As I both switch systems and datasets a lot I often chose wrong and end up with filled swap memory or extremely slow epochs because I have caching disabled.

What I would like to see, although I understand the complexities involved, is some sort of dynamic cache where the systems itself decides what to use (memory, storage or no caching), depending on available resources and the computations to cache.

A intermediate solution might be one where the user can select how much memory (percentage) to use to cache and for the remaining required cache to use storage so we get some hybrid memory/storage solution.

**Will this change the current api? How?**
It will either add to it or change the current cache() parameters. How and what's best here is not up to me.

**Who will benefit with this feature?**
Anyone using the tf.data API looking to improve performance."
583,9201,0,"Tensorflow Still Trying to use CUDA even when Session Created with device_count={'GPU': 0}. ### System Information
Using the  Docker image.

Host: , 

### Issue
If I  on the Nvidia device (), then even though I tell the  not to use GPUs (), Tensorflow attempts to use the GPU resulting in an inability to create session:
__enter__
when another process is using CUDA and the exclusive process mode is set.

If exclusive process mode is _not_ set, then the session is created but using , I see that the process is using GPU ram (and CUDA):


The issue seems limited to TF trying to lock the CUDA device (an allocate ~61MB memory). Subsequent computations do happen correctly on the CPU."
683,11063,0,"TensorFlow 1.2.0 depends on a 5 year old release of the Markdown package. ### System information

- This is an installation problem
- Windows 10 64 bit
- Tensorflow installed from pip
- Version 1.2.0
- Bazel version: None
- CUDA v8 CuDNN v6
- GTX980Ti and Pascal TITAN X
- python -m pip install tensorflow-gpu==1.2.0

### Describe the problem

TensorFlow 1.2.0 has added a new dependency on the markdown==2.2.0 package. This package is being actively maintained, and the latest version of it is 2.6.8. However, Tensorflow 1.2.0 has a strict dependency on version 2.2.0, which was released July 2012; almost 5 years ago. As such, the package no longer installs cleanly on all modern distributions of python.

I am attempting to use the Windows python-3.6.1-embed-amd64 release, but when attempting to install either Tensorflow==1.2.0 or markdown==2.2.0 on this environment, installation fails due to a syntax issue in the Markdown package. I haven't delved deep into the issue, but have included log output from trying to use pip to install tensorflow on this system.

However, the most recent version of markdown 2.6.8 installs fine under this environment.

Is there a good reason for why Tensorflow specifically requires the old version? Can the dependency be upgraded to a later version that will support the embedded 3.6.1 Windows release of python?

Surely taking on a legacy dependency like this is not ideal, and should hopefully be fixed.

### Source code / logs

"
1144,32702,0,"Keras casts targets to incorrect dtype. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc1
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Keras tries to cast targets to the dtypes of the model outputs. Currently it assumes that every model output has a corresponding target, so when doing this casting it just matches outputs and targets up one-to-one. But in the case where some outputs are not part of the loss function (i.e. they were missing from the loss dictionary passed to ), this may match outputs to the wrong targets. Then it casts targets to the wrong dtype, causing errors.

**Describe the expected behavior**

Keras should match up targets with the correct output when casting, according to the loss dictionary defined in .  If a model output is not part of the loss function, then it should be ignored when casting targets.

**Code to reproduce the issue**



This results in the error

"
846,26616,0,"Dependency on absl_py needs to be updated for compatibility with Bazel version >= 0.25. In Bazel 0.23,



has a breakage in . I submitted an upstream fix that should appear on github today, with a release occurring tomorrow. When that lands, [workspace.bzl](https://github.com/tensorflow/tensorflow/blob/a4c589d5c754493c86f10cc71022fe38d184001a/tensorflow/workspace.bzl#L323-L336) will need to be updated to reference the new abseil-py release.

The fix relies on a feature introduced in Bazel 0.22. The incompatible change flag should be flipped in Bazel 0.25, at which point versions of TF without this fix will break by default.

For more info see bazelbuild/bazel#7308."
467,9233,0,"Hello All, i am using Floyd, yet I keep getting the error -  ImportError: No module named 'tensorflow'. Please go to Stack Overflow for help and support: http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
1425,6319,0,"Occasional ""Connection timed out"" in distributed TensorFlow. We are occasionally observing crashes like below (training on AWS, seeing couple of crashes per day). It looks like in gRPC, recvmsg returns with ""Connection timed out"". The error points to [this line](https://github.com/grpc/grpc/blob/a98778f33dc602ad474b19e2f5243972722e758e/src/core/lib/iomgr/tcp_posix.c#L224). Default behavior in TensorFlow seems to be ""wait forever"", is there way to have same behavior for slow grpc connections?

@mrry

"
308,34385,0,"GPU device not found in Colab. My code worked well with GPU in Colab yesterday. But this morning it became very slow. So I suspect that CPU is used despite hardware accelerator is set to GPU in “change runtime type” explicitly. So I check the availability of GPU following the tutorial:
https://colab.research.google.com/notebooks/gpu.ipynb

code chunk:


Indeed, I got SystemError: GPU device not found. I tried this with different sessions or google user accounts, the same results. You should be able to replicate this in a new Colab session.
Maybe some tensorflow updates are involved? It could be that Colab session provide GPU, but tensorflow can’t see it, e.g. tf.test.gpu_device_name() or tf.test.is_gpu_available(). 
Please help, thank you!"
799,35482,0,"non_max_suppression is full of bugs!. **System information**
The bugs are not related to my system informations. They are caused by a bad coding style in the https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cu.cc and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cc. Just to be sure those bugs could be easily reproduced on google colab. 

**Describe the current behavior**
I ll mention two major bugs.
The first one is a garbage output when the boxes coordinates are not logical. In fact given a box(y1,x1,y2,x2)  coordinates with y1 = y2 or x1 = x2 (the coordinates of a line) or even worst the coordinates of a point ( x1 = x2 = y1 = y2) the algorithme will only output the line when reaching it. 
**Code to reproduce the issue**
Here is the code to reproduce the behaviour on google colab: 



output:


Now I ll explain brievely how the algorithm is coded in tensorflow. Given a list of candidate boxes containing in the beginning all the user boxes and a list of chosen boxes empty, if the box is chosen it will not immediately delete it from the candidate box. In fact, this box will be again processed as a candidate box in the next iteration. But because it is already in the chosen boxes it wont be chosen again . The reason for that is that the IOU of a box with himself is 1 which is always above the threshold. Unfortunately, the IOU of a line or a point with any box is 0. This is applied even when the IOU is calculated of the line with iteself. This will result in adding the line to the chosen boxes again and again. This behaviour is mentioned in this issue but wasn't clearly explained. https://github.com/tensorflow/tensorflow/issues/29628

**Describe the expected behavior**
The expected behaviour must be decided by the tensorflow programer. He can chose between putting it only once in the result : 
-<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)> 
or deleting the line box
 -<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 2, 3], dtype=int32)>

- The second bug is really inexplainable. Why there is only the gpu specialisation of non_max_suppression_v2 ? Did the developer forgot about it? This was mentioned in several issues under the name : non max suppression work only on cpu. This is completely understandable because the default version of non_max_suppression is v3 which dosent have a gpu specialisation. 

**Code to reproduce the issue**
on colab u can just copy this code:



output:

> Executing op NonMaxSuppressionV3 in device /job:localhost/replica:0/task:0/device:CPU:0
> tf.Tensor([0 1 1 1 1 1 1 1], shape=(8,), dtype=int32)

**Describe the expected behavior**

> Executing op NonMaxSuppressionV3 in device /job:localhost/replica:0/task:0/device:GPU:0

Correcting this issue is quite simple and straightforward. Gpu specialisation must be made for non_max_suppression_v3 at least."
113,24572,1,"The performance with profiler seems much slower than normal. 
    I found something uninterpretable when profiling with tensorflow profiler.
    The time measured with profiler seems much slower than without it. 
            eg: I translate a sentence with the model Transformer, it takes 70ms normally, but it takes 220ms with profiler. code as follows:
             with tf.contrib.tfprof.ProfileContext('/home/work/tmp/profile') as pctx
        Does profiler take too much overhead? Which part costs so much time?? Does the overhead take place in every op or between ops??
"
1271,5324,0,"Small documentation error in iOS Example. I'm following the wonderfully comprehensive instructions here (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/ios_examples/README.md)  for building an iOS project with tensorflow with the makefile system. 

The instructions, in the section ""creating your own app"" suggest adding 
tensorflow/contrib/makefile/downloads/eigen-latest
to the header search paths; however this folder does not exist after running the make scripts.
I believe it should be changed to 
tensorflow/contrib/makefile/downloads/eigen


(I have not seen anything suggesting tensorflow can be built for iOS using bazel, so I assume that this remains the recommended approach.)"
1325,2680,0,"Loading model in Android and No OpKernel was registered to support Op error. I encountered a problem when using a self-trained face-recognition model to make inference on android platform (using c++ api, just like the android demo). The error says something like this:



It is similar to the issue #1269 

I don't understand why it causes an error? 
All the other layers ( from incept3a  to incept5a) have almost the same structures, but there's no error....

Could anyone give me some advice?
Thanks a lot!

The structure of the model I use is like this:


"
1071,26789,0,"[TF2.0] Optimizer for Linear models. 
**System information**
- TensorFlow version: 2.0.0-alpha0
- Doc Link: 
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearEstimator
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearClassifier
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearRegressor


**Describe the documentation issue**

The examples in these documents use tf.train.FtrlOptimizer which is not available in TF2.0.


I tried to use tf.optimizers.Ftrl instead, but it gives the following error:



I think the documentation should be updated, but I am not sure the correct way of defining optimizer. Could please someone take a look at this?

Thanks,"
941,22237,0,"Graph_def is invalid at node u'ExpandDims': Input tensor 'image_ph:0' Cannot convert a tensor of type float32 to an input of type int32. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
498,7716,0,"Upgrade Tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl on Windows and get warning. I upgrade Tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl, CPU version on Windows server 2012 R2

pip install --upgrade   
http://ci.tensorflow.org/view/Nightly/job/nightly-win/85/DEVICE=cpu,OS=windows/artifact
/cmake_build/tf_python/dist/tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl

It shows 
""Successfully installed tensorflow-1.0.0rc2 werkzeug-0.11.15""

But I get the following warning when I run the hello example, please advise what's the problem.

>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow')
>>> sess = tf.Session()
2017-02-20 16:53:15.825086: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE instructions, but these are available on your m
achine and could speed up CPU computations.
2017-02-20 16:53:15.826037: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE2 instructions, but these are available on your
machine and could speed up CPU computations.
2017-02-20 16:53:15.827289: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE3 instructions, but these are available on your
machine and could speed up CPU computations.
2017-02-20 16:53:15.828478: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE4.1 instructions, but these are available on you
r machine and could speed up CPU computations.
2017-02-20 16:53:15.829644: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use SSE4.2 instructions, but these are available on you
r machine and could speed up CPU computations.
2017-02-20 16:53:15.830997: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use AVX instructions, but these are available on your m
achine and could speed up CPU computations.
2017-02-20 16:53:15.832161: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use AVX2 instructions, but these are available on your
machine and could speed up CPU computations.
2017-02-20 16:53:15.833336: W c:\tf_jenkins\home\workspace\nightly-win\device\cp
u\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow li
brary wasn't compiled to use FMA instructions, but these are available on your m
achine and could speed up CPU computations.
>>> print(sess.run(hello))
b'Hello, TensorFlow'"
1244,7152,0,"Library not loaded: @rpath/AdobeCreativeSDKCore.framework/AdobeCreativeSDKCore. I just completed migrating my project from swift 2 to swift 3. I had installed AdobeCreativeSDKCore and AdobeCreativeSDKImages as pod file. The code builds but I get a ""**_Thread 1: signal SIGABRT_**"" error in the ""**_0_abory_with_payload_**"" file and ""**_10_dyld_start_**"" 

__abort_with_payload:
    0x1063966f8 <+0>:  movl   $0x2000209, %eax          ; imm = 0x2000209 
    0x1063966fd <+5>:  movq   %rcx, %r10
    0x106396700 <+8>:  syscall 
->  0x106396702 <+10>: jae    0x10639670c               ; <+20>
    0x106396704 <+12>: movq   %rax, %rdi
    0x106396707 <+15>: jmp    0x106396014               ; cerror_nocancel
    0x10639670c <+20>: retq   
    0x10639670d <+21>: nop    
    0x10639670e <+22>: nop    
    0x10639670f <+23>: nop    

dyld
I've been checking the podfile, I seem to have the framework in the ""Pods"" droplist but not in the products drop list 

![screen shot 2017-01-30 at 4 23 24 pm](https://cloud.githubusercontent.com/assets/20662833/22442415/493d67ca-e709-11e6-81ef-de57964aa741.png)

![screen shot 2017-01-30 at 4 25 32 pm](https://cloud.githubusercontent.com/assets/20662833/22442284/cdca07ce-e708-11e6-895d-4cedfea7da17.png)

Im also not able to add it to the build phases cause Xcode is not giving me that option 

![screen shot 2017-01-30 at 4 28 21 pm](https://cloud.githubusercontent.com/assets/20662833/22442380/259b7f0a-e709-11e6-8d4a-fe54256d465c.png)

I'm sure it's a simple fix, probably somewhere in the settings but I'm don't know where. I am still new to iOS development. Any response is greatly appreciated "
655,15497,0,"Image Input for GridLSTM. I would like to use GridLSTM for a handwriting recognition task. Unfortunately, the documentation is lacking info on how to input images into GridLSTMs. 
"
534,34723,0,"2.1.0rc0 cudart64_101.dll not found. **System information**
- OS Platform: Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1.0rc0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: virtualenv
- CUDA/cuDNN version: 10.0
- GPU model and memory: rtx 2060



**Describe the problem**

When I tried to install tensorflow 2.1.0rc0, It didn't recognize my gpu and gave me this error:

Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found

Is that mean 2.1.0rc0 support cuda 10.1 now? It doesn't mention on release notes?


"
35,24643,1,"Tensorboard histogram_freq is not using GPU: slows down training. **System information**
- No custom code
- Ubuntu 18.04
- TensorFlow version 1.11.0
- tf.COMPILER_VERSION = v1.11.0-0-gc19e29306c
- Python version 3.6
- CUDA/cuDNN version 9.0
- GPU model and memory: GTX 1060 6GB

When I run Tensorboard with  where , it uses the CPU at the end of each epoch when Tensorboard writes to logs. 

Usually the GPU is used for the entirety of training and writing to logs and this is also true when . 

This [issue] (https://github.com/keras-team/keras/issues/3358) has been mentioned in previous questions but it is dissimilar in that their issue occurs when validation data is passed via a data generator and that histograms are not created - no reference to CPU or GPU. 

I do not use a data generator for validation and histograms appear to be created. This is an issue as writing to logs using CPU seems to take an eternity making the training process up to five times longer.

Notes: 

Training carried out by Keras .

Training dataset passed by .

Validation dataset is passed without data generator.

    tensorboard = TensorBoard(log_dir='../logs/vv2', histogram_freq=1, write_graph=True, write_images=True)
    model.fit_generator(train_generator, 
                                  steps_per_epoch=batch_size, 
                                  epochs=epochs, 
                                  validation_data=(x_test,y_test),
                                  callbacks=[tensorboard])
"
1403,30009,0,"TensorFlow 1.14 docker images lacking. This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

---

There doesn't seem to be any docker images published along side the resent tf 1.14 release. Is this intentional, or can we expect them in the near future?"
562,27355,0,"Dataset c++ extend documentation is outdated for tf 2.0 & DatasetV2. **System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/guide/extend/formats

**Describe the documentation issue**

The C++ code to extend Dataset (especially since DatasetV2) is outdated.

Here is a working version of files needed in the documentation : https://github.com/vrince/tensorflow_addons/tree/master/tensorflow_addons/dataset

NOTE: the only part I am not really sure about is this one : https://github.com/vrince/tensorflow_addons/blob/master/tensorflow_addons/dataset/cc/my_dataset.cpp#L76 ... basically let it as it was but I don't see the point.

There is also and external test running from python and bazel files.

Not sure where or if I even can do a pull request to change the doc."
1136,30679,0,"can not convert model to tflite. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
699,11367,0,"saver will cause crash,error message:""InvalidArgumentError: Shape [xx] has negative dimensions"". ### It should be a bug of tensorflow
add the saver will cause the procedure crash. error message is strange, as following:
_### InvalidArgumentError (see above for traceback): Shape [-1,32,32,3] has negative dimensions
	 [[Node: x = Placeholder[dtype=DT_FLOAT, shape=[?,32,32,3], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]_

the added part is:
            if (i % 1000 == 0) or (i == num_iterations - 1):
                # Save all variables of the TensorFlow graph to a
                # checkpoint. Append the global_step counter
                # to the filename so we save the last several checkpoints.
                saver.save(sess,
                           save_path=save_dir,
                           global_step=train_step)

### System information
== cat /etc/issue ===============================================
Darwin zhangdeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64
Mac OS X 10.12.5

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.6.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin zhangdeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.0)
protobuf (3.3.0)
tensorflow (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================

== cuda libs  ===================================================

Tensorflow version:
v1.2.0-5-g435cdfc 1.2.1




### Source code / logs
source code:





"
1007,24940,0,"NNAPI doesn't support tensors with rank 0 - Mobilenet. **System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10 for retraining, Linux Ubuntu 16.04 for converting to tflite model
- Mobile device: Sony Xperia Z1 - Android 9 (AICP ROM)
- TensorFlow installed from : binary
- TensorFlow version: b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
- Python version: 3.6

**Describe the current behavior**
I retrained ""https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/quantops/classification/1"" using this script: https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py.

Then I converted the output to tflite format using the following command:

I moved graph.tflite (the output of the above command) to assets folder of this project: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo

I did all the necessary changes (such as changing the graph name, input size, etc.) in the ImageClassifierQuantizedMobileNet class.

Everything is right as long as CPU is selected from the device list, but as soon as I select NNAPI, the app crashes.

I think the problem is from the pre-trained models that are available on tfhub. Since when I convert the stock mobilenet_v1_1.0_224.pb from ""http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz ""
the problem is gone.

Log:

"
154,8497,1,"Tensorflow Retrain Model performance. I am using [Tensorflow for poet](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0) guide for train own model. I have create retrained_graph.pb and retrained_labels.txt. While I use it in application then I get error that 




After That  further train model for application use [Tensorflow for mobile ](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/)  blog and create **optimized_graph.pb, rounded_graph.pb, mmapped_graph.pb** files.

optimized_graph.pb and rounded_graph.pb file work in android application without any error.
While use mmapped_graph.pb I get error that **Failed to initialize: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef**

Performance of application is not good while use optimized_graph.pb and rounded_graph.pb file.While application camera screen not contain any flower photos otherwise random flower name show with high confidence rate. Any way to detect only flower and remain blank when not flowers.

![screenshot](https://cloud.githubusercontent.com/assets/25680329/24044730/8edab30c-0b42-11e7-8209-a5fab382b81c.png)
"
431,7524,0,"[Bug] TypeError: Input 'points' of 'NearestNeighbors' Op has type float64 that does not match expected type of float32.. Hi, I am trying to implement KMeansClustering using tensorflow.contrib.learn.python.learn.estimators.kmeans

But I am getting the following error while using the code :-
def cluster_data(X, num_clusters) :
    kmeans = KMeansClustering(num_clusters=num_clusters)
    kmeans.fit(X)
    y = kmeans.predict(X)

During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""C:\Users\#####\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-57-5df4c7e34540>"", line 1, in <module>
    kmeans.fit(X)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py"", line 191, in new_func
    arg_spec: Output from inspect.getargspec on the called function.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 355, in fit
    """"""Initializes a BaseEstimator instance.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 699, in _train_model
    '2016-09-23',
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1052, in _get_train_ops
    training hooks.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1021, in _call_model_fn
    def __init__(self,
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\kmeans.py"", line 201, in _model_fn
    kmeans_plus_plus_num_retries=self.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\factorization\python\ops\clustering_ops.py"", line 295, in training_graph
    # Implementation of kmeans.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\factorization\python\ops\clustering_ops.py"", line 195, in _infer_graph
    # nearest_neighbors op.
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\factorization\python\ops\gen_clustering_ops.py"", line 90, in nearest_neighbors
    centers=centers, k=k, name=name)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 508, in apply_op
    raise TypeError(""%s expected type of %s."" %
TypeError: Input 'points' of 'NearestNeighbors' Op has type float64 that does not match expected type of float32.
"
1411,7616,0,"Change device Placement of existing variable. How do I change the device placement of a tf.Variable() ?
I tried two methods 

I tried enforcing variable reuse by using tf.get_variable_scope().reuse_variables()

This time, I get an error saying the variable 'a' did not exist in the gpu.

Any help on changing-device-placement or lazy-device-assignment would be appreciated. Thanks



"
958,26073,0," no such attribute 'downloaded_file_path' in 'http_file' rule. 
**System information**
- OS Platform and Distribution :Linux Ubuntu 16.04
- TensorFlow installed from :source code
- TensorFlow version:https://github.com/tensorflow/tensorflow
- Installed using bazel(bazel build tensorflow/cc:cc_ops):
- Bazel version (0.15.0):
- GCC/Compiler version (5.4.0):
**Describe the problem**
when i use bazel(bazel build tensorflow/cc:cc_ops) to buid the tensorflow source code from ""https://github.com/tensorflow/tensorflow"",there is some error as below:
WARNING: while reading option defaults file '/home/luwei/ML/tensorflow/tensorflow-master/.bazelrc':
  invalid command name 'try-import'.
WARNING: while reading option defaults file '/home/luwei/ML/tensorflow/tensorflow-master/.bazelrc':
  invalid command name 'try-import'.
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:101:9: //external:bazel_gpg: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:110:9: //external:docker_gpg: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:119:9: //external:gcloud_gpg: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:128:9: //external:launchpad_openjdk_gpg: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:138:9: //external:golang_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:147:9: //external:debian8_clang_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:155:9: //external:ubuntu16_04_clang_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:164:9: //external:debian8_libcxx_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:172:9: //external:ubuntu16_04_libcxx_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0141_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0150_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0152_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0161_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0171_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0172_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0180_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0181_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0190_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0192_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0200_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0210_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:196:9: //external:azul_open_jdk: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:204:9: //external:azul_open_jdk_src: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': error loading package 'external': Could not load //external package
ERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': error loading package 'external': Could not load //external package
INFO: Elapsed time: 0.051s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)


**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build tensorflow/cc:cc_ops

**Any other info / logs**
  no!"
1288,29,0,"minimum req: Cuda compute capability 3.5. Tensorflow seems to require a cuda compute capability of 3.5, thereby excluding amazon G2 instances which use a GRID K520 with 3.0, might be good to know:


"
769,1500,0,"beam search in translation model. Do we have plan to support beam search in the decoding process of translation model, since the raw paper shows better result with beam search?
"
1219,3038,0,"Incorrect tf.truncated_normal results on gpu. I've narrowed the behavior I'm seeing to this small example



When DEVICE is '/cpu:0' all is as expected.  With DEVICE as '/gpu:0', the output will be as expected for one execution, but output two identity matrices for all subsequent executions.  Running again with DEVICE='/cpu:0' resets the gpu behavior.
### Environment info

Operating System: Fedora 23

Installed version of CUDA and cuDNN: 
I'm using CUDA 7.5 and cuDNN v5 with Driver Version: 367.27 on a gtx780
-rw-r--r--. 1 root    root      322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx. 1 root    root          16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx. 1 root    root          19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x. 1 root    root      383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r--. 1 root    root      720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx. 1 jlovitt jlovitt       13 Apr 22 20:52 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx. 1 jlovitt jlovitt       17 Apr 22 20:52 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5
-rwxrwxr-x. 1 jlovitt jlovitt 59909104 Apr 22 18:15 /usr/local/cuda/lib64/libcudnn.so.5.0.5
-rw-rw-r--. 1 jlovitt jlovitt 58775484 Apr 22 18:15 /usr/local/cuda/lib64/libcudnn_static.a

If installed from sources, provide the commit hash: 84225a2b612fe748c9c923f0c1cb8471911c3b77
I've added the following lines to third_party/gpus/crosstool/CROSSTOOL to compile with gcc 4.9.3


### Output from execution producing incorrect results


## Update:

Recompiling with CUDA 7.0 and cuDNN v4 fixes the problem.
"
145,9498,1,"tf.metrics.accuracy maintains a running accuracy?. I use the , however it is a bit **counter-intuitive** in that it maintains a **running** accuracy (the [doc](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) agrees with this).  The following simple script illustrates the situation



My concerns are
1. the use of accuracy is bit **surprising**, are we supposed to manually construct the normal accuracy?
2. IMHO, it is better to 
    1. implement the normal accuracy behavior or
    2. provide a clean way to **reset** the local variables created by , i.e., the  and ."
1075,6644,0,"Error: Data loss: file is too short to be an sstable. Hi there,

I'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm

I am getting a ""Data loss"" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.

Currently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues. 

"
1456,354,0,"Tutorials are difficult to read. Greyish text on white background causes eye fatigue. Especially with longer text, and sanserif fonts.
"
1135,8129,0,"No event files found within logdir in tensorboard. Dear Sir/mam

I confuse how to display using tensorboard. here is my program
graph = ""d://""
import tensorflow as tf

# Build our graph nodes, starting from the inputs
a = tf.constant(5, name=""input_a"")
b = tf.constant(3, name=""input_b"")
c = tf.multiply(a,b, name=""mul_c"")
d = tf.add(a,b, name=""add_d"")
e = tf.add(c,d, name=""add_e"")

# Open up a TensorFlow Session
sess = tf.Session()

# Execute our output node, using our Session
output = sess.run(e)

# Open a TensorFlow SummaryWriter to write our graph to disk
writer = tf.summary.FileWriter(graph, sess.graph)

# Close our SummaryWriter and Session objects
writer.close()
sess.close()

the events file has been created in drive D (i'm using windows 8.1, and tensorflow 1.0). and i found it when using explorer

When I type 
tensorboard --inspect --logdir =""d:\\""
tensorboard --inspect --logdir =""d:\""
tensorboard --inspect --logdir =""d://""
tensorboard --inspect --logdir =""d:/""

tensorboard --inspect --logdir ='d:\\'
tensorboard --inspect --logdir ='d:\'
tensorboard --inspect --logdir ='d://'
tensorboard --inspect --logdir ='d:/'

the result are : No event files found within logdir, 

pls tell me why this happened.

Thx
"
230,33201,1,"Tensorflow 2.0 SavedModel format with GPU acceleration slowdown. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): tensorflow==2.0.0-rc1 (2.0.0rc0-gpu-py3 docker image)
- Python version: 3.5.0
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: TeslaV100 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
I have a SavedModel trained with Tensorflow 2.0 which I use to make an inference as follows: 


The model predicts fast on a CPU but when using a GPU device, it scales linearly and GPU does not offer any advantage. When logging the device placement, I see that GPU is used.

To compare, I have the same model trained in Tensorflow 1.13 saved in frozen graph format. I use the model as follows:


The timing of this model on GPU in the same environment is much faster than SavedModel format. I think this is a performance issue.

Here are the timing details:
TF2.0 SavedModel
Batch of 64 images CPU: 23 seconds
Batch of 64 images GPU: 14 seconds

TF13.0 Frozen Graph:
Batch of 64 images CPU: 25 seconds
Batch of 64 images GPU: 5 seconds

**Describe the expected behavior**
My expected behaviour is that Tensorflow 2.0 version with SavedModel would use GPU acceleration. Maybe I am missing on how to import the SavedModel in python. Please let me know what could be the route cause of it.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1422,11312,0,"tf.gfile.FastGFile(filename, 'r').read()  error: 'utf-8' codec can't decode byte 0xff. 
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
TensorFlow 1.2.0

image_data = tf.gfile.FastGFile(filename, 'r').read()   
python2.7 is good，but Python3.5 error ：  'utf-8' codec can't decode byte 0xff in position 0: invalid start byte

why???"
809,31520,0,"yet another windows 10 build fail ( 2.0 rc.0 ). <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0-rc.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.26.0 x64
- GCC/Compiler version (if compiling from source):  Visual Studio 2019
- CUDA/cuDNN version: 10.1 / 7.6.2
- GPU model and memory: RTX 2080Ti GDDR6 11GB


**Describe the problem**
build failed 

**Provide the exact sequence of commands / steps that you executed before running into the problem**




**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


"
1014,31266,0,"undeclared inclusion(s) in rule '//tensorflow/lite/toco/python:toco_python_api'. 
**System information**
- OS Platform and Distribution: 19.04
- TensorFlow installed from (source or binary): 2.0beta1
- TensorFlow version: 2.0beta1
- Python version: 3.7.3
- Bazel version: 0.26.0
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1/7.6.2
- GPU architecture: 5.2 

**Describe the problem**


Any further suggestions?"
705,6742,0,"`tensorflow.examples.tutorials.mnist` not currently downloadable. ### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)



raises , if you don't already have the data cached. I believe this is because the underlying data [SOURCE_URL](http://yann.lecun.com/exdb/mnist/) is currently [down](http://downforeveryoneorjustme.com/http://yann.lecun.com/exdb/mnist/)."
535,5757,0,"CPU slowdown with Quantized Eight bit graphs. In attempts to highly optimize my Tensorflow client application (and TF install) for consumer desktop hardware i've noticed (and noticed in other bug reports) that quantized eight bit graphs appear to run very slow.  My goal is to match the realtime 1 batch (1 x 299 x 299 x3 ) iOS performance that the Camera Example gets, yet I can't get a Desktop CPU compile of TF to get lower than roughly 150ms per frame, where in reality close to 16ms per frame is needed for roughly 60hz, or 33ms for 30hz performance. It appears somehow the iOS / ArmV7 build is able to achieve this performance unless I am missing something!

From the discussion group, I was asked by @petewarden to start a bug based on findings

Thread here: https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/PJwgfoeNIKs/jiegynxLBAAJ

Briefly, I've added Stat Tracing to the Image Label example.  Modified source is here:

https://gist.github.com/vade/18d7e72f633f9479c5080a251661ebd9

Ive downloaded compiled tensor flow with the following bazel build commands, referenced from the makefile for iOS which speeds things up just a bit more than the standard compile:



I then compiled Image Label via the standard bazel command:



And ran it with 4 graphs:

* Standard InceptionV3
* InceptionV3 run through Inference Optimizer Script
* InceptionV3 run through Inference Optimizer and Quantizer in Weighted Rounding mode
* InceptionV3 run through Inference Optimizer and Quantizer in eight bit mode

The output of the runs are documented here, in order:

* https://gist.github.com/vade/a7d95da155c25dc8134f7cda8168e540
* https://gist.github.com/vade/71af1cbd38864cb176ce64bcafb934de
* https://gist.github.com/vade/e1923d7e7a9abfe1d8c912cc5d36a763
* https://gist.github.com/vade/1d9d5e102878cfb42f9c02a4200b3a50

Note the time for the Quantized Eight bit mode is roughly 2x longer than previous runs.

As a second set of data, my custom C++ app which uses the same lib_tensorfow_cc.so nets similar results to the benchmark : 


* InceptionV3 (no optimizations or quantizations) 
* 222 frames took 32.598143 seconds
* https://gist.github.com/vade/77a9314a5c7a5bda9b4a2c90f691a98e


* InceptionV3 (Inference Optimizations - no quantizations)
* 222 frames took 28.129690 seconds
* https://gist.github.com/vade/1c5dc51015f5a0fa24f4e0a7209cabf9


* InceptionV3 (Inference Optimizations & Quantizations Rounded)
* 222 frames took 25.201791 seconds
* https://gist.github.com/vade/ad8a2c42c5fbcf9be9f074c95d2e95ae


* InceptionV3 (Inference Optimizations & Quantizations Eightbit)
* 222 frames took 63.174700 seconds
* https://gist.github.com/vade/d6dcce06861bf8932446ae5ed33d93bb


Operating System:


If installed from source, provide 

1. The commit hash



2. The output of



### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)`

See above for source code for minimally modified label image source.

### What other attempted solutions have you tried?

Attempted to run cuda but am targeting consumer desktop systems and would like similar performance to iOS targets for realtime or better than realtime performance for InceptionV3 / pool_3 feature vector determination and possibly labelling / classification.

For my system, cuda compilation netted similar results to CPU, although admittedly I did not enable batch sizes larger than 1. However, I think this is moot because iOS appears to be able to get realtime labelling and desktop cant, (is roughly 10x slower)

### Logs or other output that would be helpful
Logs and links provided in preamble  / description"
611,26259,0,"Any plan to add SoftClipping and SmoothMax. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):
Yes.
**Describe the feature and the current behavior/state.**
SoftClipping for activation. [https://en.wikipedia.org/wiki/Activation_function]
SmootMax for pooling. [https://en.wikipedia.org/wiki/Smooth_maximum]
**Will this change the current api? How?**
No. This is new feature.
**Who will benefit with this feature?**
Everybody. 
SmoothClipping is smoot function and can not suffer from vanishing/exploding gradients. 
So with this we will have faster learning. https://github.com/tiny-dnn/tiny-dnn/pull/1014.  Only 10 layers achieve 99.35% on MNIST set, Comparing to original 12 and 99.0%
**Any Other info.**
"
164,6460,1,"Slow Adam sparse updates in distributed TF. I am trying to train a model with the **tf.nn.embedding_lookup_sparse** operation. Small example: https://gist.github.com/Bobrosoft98/2d639d3924dfbc4ec7bc620fd5a4d480

When I run this code with NUM_WORKERS = 1, the output is as follows



However, when I increase the number of workers to 30, every single process works more than 30 times slower:



The CPU load is only ~50%, so there is a lot of resources available for the computation. This makes me think that sparse updates use locking, even though the **use_locking** flag is set to False by default. There is no such problem with other optimizers (I tried GradientDescentOptimizer and AdadeltaOptimizer). Also if I exclude sparse operations from the graph (commented lines), the problem disappears.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/464 - there was mentioned that Adam was slower on sparse updates in general

### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: I used the CPU version

If installed from binary pip package, provide:

1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0-cp27-none-linux_x86_64.whl
2. The output from .
"
27,5683,1,"Synchronous distributed training is too slow. While trying to train Imagenet using Inceptionv3, I noticed the training was proceeding too slow. I then tested with Alexnet and Resnet and I saw the same slow training speed. 

Here is the images/sec I'm able to achieve on various models:


Is it possible there is some bug causing this slow performance?

I'm using EC2 P2 instances (p2.16xlarge). Each instance has 16 GPUs. Each GPU is a Tesla K80. I'm running one worker per GPU and one PS per host. Here is more info on P2 instances: https://aws.amazon.com/ec2/instance-types/p2/

I use randomly generated synthetic data for images and label. I've written some scripts to reproduce this issue in case it helps: https://github.com/indhub/tfperftest/tree/master/perftest

- Inception model is based on https://github.com/tensorflow/models/tree/master/inception. I've done some changes to use randomly generated synthetic data for images and labels. Modified code here: https://github.com/indhub/tfperftest/tree/master/inception
- Resnet model is based on https://github.com/tensorflow/models/tree/master/resnet. I've done some changes to build Resnet 152. Modified code here: https://github.com/indhub/tfperftest/tree/master/resnet
- Alexnet model is based on https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/image/alexnet/alexnet_benchmark.py. I've added the fully connected layers at the end. Modified code here: https://github.com/indhub/tfperftest/blob/master/alexnet/alexnet.py

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/40411597/synchronous-distributed-training-is-slow

### Environment info
Operating System: Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of ):


If installed from binary pip package, provide:

1. A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl
2. The output from .
ubuntu@ip-172-31-52-161:~$ python -c ""import tensorflow; print(tensorflow.__version__)""



### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Just running https://github.com/tensorflow/models/tree/master/inception reproduces the problem. 
"
198,24957,1,"native tf and tf.keras optimizer and gradient calculation problem. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Win 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
pip install 
- TensorFlow version (use command below):
1.6 and 1.10
- Python version:
3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
9.0/7.05
- GPU model and memory:
GTX 1080 Ti

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
train model using tf and tf.keras
**weights updates** and **gradients** are different

**test result as following chart:**
![gradients](https://user-images.githubusercontent.com/39117820/51234690-7a62b480-19a8-11e9-8f46-94b247d91273.JPG)


**Describe the expected behavior**
Since tf.keras using tf as backend operation, weights updates and gradient calculation should be identical.
**Code to reproduce the issue**





**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1250,1663,0,"Nightly debian CPU build fails during docker build . For example, see:
http://ci.tensorflow.org/view/Nightly/job/nightly-debian-cpu/28/console

The error happens during the step: RUN /install/install_deb_packages.sh
"
342,6937,0,"Confusing arg name for sequence_loss. Sorry if this is noise, but it's just a minor suggestion to make the API better (before the release of TensorFlow 1.0 where it will be much more difficult to change)

For the function [sequence_loss](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py) on the master branch, the name of the parameter  seems to imply that only softmax based loss are compatible which is not true (for instance  works too). Maybe the name was that just to indicate the default behavior but I think it is misleading.

Why not simply call the parameter  ? It's more representative of what the parameter really does."
986,31104,0,"W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at iterator_ops.cc:988 : Invalid argument: Input shape axis 0 must equal 4, got shape [3] . Epoch: [93/10] step: [186/2] time: 0.2719242572784424s, mse: 0.03679807484149933
2019-07-28 12:23:43.405599: W tensorflow/core/framework/op_kernel.cc:1431] OP_REQUIRES failed at iterator_ops.cc:988 : Invalid argument: Input shape axis 0 must equal 4, got shape [3]
[[{{node crop_to_bounding_box_1/unstack}}]]
Traceback (most recent call last):
File ""train.py"", line 380, in
train()
File ""train.py"", line 178, in train
for step, (lr_patchs, hr_patchs) in enumerate(train_ds):
File ""/home/xyh/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 556, in next
return self.next()
File ""/home/xyh/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 585, in next
return self._next_internal()
File ""/home/xyh/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 577, in _next_internal
output_shapes=self._flat_output_shapes)
File ""/home/xyh/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1954, in iterator_get_next_sync
_six.raise_from(_core._status_to_exception(e.code, message), None)
File """", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input shape axis 0 must equal 4, got shape [3]
[[{{node crop_to_bounding_box_1/unstack}}]] [Op:IteratorGetNextSync]
2019-07-28 12:23:43.507127: W tensorflow/core/kernels/data/generator_dataset_op.cc:79] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
[[{{node PyFunc}}]]
"
282,22007,0,"Newly included absl headers are missing from the include path. [Newly included](https://github.com/tensorflow/tensorflow/commit/3cb3a450ed845c4602080f43d7bb6cfade298a22)  headers are present in the  Python package but do not seem to be part of the include path, and now custom op plugin compilation fails.

This has been broken between  and .



cc @yunxing "
140,23833,1,"The same compilation target results in different compilation results. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Linux Ubuntu 16.04
- TensorFlow version:1.8.0
- Python version:3.5
- Bazel version (if compiling from source):0.17.2
- GCC/Compiler version (if compiling from source):gcc 5.4

**Describe the problem**

Initially, I used tensorflow1.8.0 as my external dependency for development, mainly using core:lib core:all_kernels core:core_cpu as the compilation target, and I wanted to use these three library files to generate my.so dynamic library files

It does works and Everything seems normal, but I find that if I compile the same goal in tensorflow's self workspace, The generated binaries will be much smaller than the ones I compiled using external dependencies.

In the beginning, I thought this was a problem caused by bazel compilation, so I consulted on bazle's community. Here is the issue I initiated

https://github.com/bazelbuild/bazel/issues/6677

But after analysis by the developers, it is likely that tensorflow controls itself.

This problem has been bothering me for a long time. I hope you can give me some ideas or Suggestions to solve the problem. Thank you very much!

Here are the issues and validation process I mentioned in issue.


The situation became clear, and I simplified my work the most, remove BUILD file and modified WORKSPACE file to the following：


I then modified the BUILD file in the tensorflow source directory （../tensorflow），

I tested three scenarios：
- In my workspace dir

This creates a 700M .so file

- in the tensorflow source directory

This creates a 125M .so file

- in the tensorflow source directory

This also creates a 125M .so file

### Here if I don't add the three ""_impl"" dependencies and run  in the source directory of tensorflow, I will fail in the link phase!  But even without the three ""_impl"" dependencies, I can still compile successfully in my own workspace!

I don't know what the problem is there, it looks like the content is similar when linked, but when I executed under the source directory of tensorflow, bazel didn't compile enough targets?
"
440,5163,0,"AttributeError: 'module' object has no attribute 'histogram'. ### Environment info

Operating System: MacOS 10.12

Running tensorflow (0.11.0rc1), I'm trying to retrain the Inception's final layer with my own set of images, and I'm running into this error:

<img width=""749"" alt=""screen shot 2016-10-24 at 12 56 46 am"" src=""https://cloud.githubusercontent.com/assets/8280282/19637602/c7ea232e-9984-11e6-8d9d-5e0ba56a7b47.png"">
"
916,8044,0,Broken link in Adding a New Op to zero_out_op_1.py. The link in [Adding a New Op](https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/docs_src/extend/adding_an_op.md#L244) to  is broken. It points to: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/how_tos/adding_an_op/zero_out_op_1.py.
978,32077,0,"The batch_size argument must not be specified when using dataset as an input & Batch size: 32 is not divisible by num_workers: 3. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14.0
- Python version :3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: TITAN Xp, 3 x 12G

---

i'm using  to train on 3 GPUs, and use a tf  as the model inputs. when calling , if i specify , it shows  and if i don't specify , it shows .

i know that by calling ,  is 32 by default, but when using dataset, i did call  and i can't change  in .

i thought i could use 1, 2, 4, etc. GPUs that can devide 32, but why 3 GPUs is not working.


"
121,11514,1,"tf.assign is much slower than tf.assign_add on CPU. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v0.8.0rc0-16474-gac98d11', '1.2.1')
- **Python version**: Python 2.7.13 :: Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**:cuda8.0/cudnn6.0
- **GPU model and memory**:old titan x / 12GB
- **Exact command to reproduce**: a python script

### Describe the problem
 is much slower than  on CPU when  set to 10. Although tensorflow treats these two operations a little different(maybe because  allows uninitialized tensor, accepts more tensor types), they all use a same class in eigen. I find this problem when doing some test about #11411.

### Source code / logs
script is on [gist](https://gist.github.com/suiyuan2009/24315b35915bddbe2d53b764164bb8fb),

set  to 1, 

on GPU,
"
156,23860,1,"Poor memory performance of K.batch_dot under tensorflow backend relative to batched tf.matmul . [ x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

[ x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found here.

[x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am performing batch matrix multiplies of two tensors of size (batch, N, M) and (batch, M, K) to get a tensor of size (batch, N, K), with the matrix products. This behavior can be done with both tf.matmul and K.batch_dot with the default axis arguments.

However in K.batch_dot, the elementwise multiplication in the line

keras/keras/backend/tensorflow_backend.py

Line 1248 in 75a3503

 result = tf.reduce_sum(x * y, 1) 
eats up a lot of memory. The elementwise multiplication followed by summing over an axis is of course mathematically equivalent to the matrix multiply, but in the two-step implementation, Tensorflow assigns memory to the intermediate very large tensor.
In this simple example, my small GPU (Nvidia 970) is able to perform the calculation using tf.matmul, but using K.batch_dot Tensorflow fails with an OOM error.

import numpy as np
import tensorflow as tf
from keras import backend as K

a = np.random.normal(size=(100, 500, 10000)).astype(np.float32)
b = np.random.normal(size=(100, 10000, 32)).astype(np.float32)

a_t = K.placeholder(a.shape)
b_t = K.placeholder(b.shape)

td = tf.matmul(a_t, b_t)
bd = K.batch_dot(a_t, b_t)

sess = K.get_session()
sess.run(td, feed_dict={a_t: a, b_t: b})
sess.run(bd, feed_dict={a_t: a, b_t: b})
This fails when it tries to assign a tensor of size (100, 10000, 500, 32) in the elementwise multiply in batch_dot (the dimension of 10000 not being strictly necessary in this case since we are only interested in the sum)."
680,12905,0,"bazel error related to python . bazel build tensorflow/examples/image_retraining:retrain
ERROR: /home/dile/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 310
		_create_local_python_repository(repository_ctx)
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 268, in _create_local_python_repository
		_get_python_bin(repository_ctx)
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 166, in _get_python_bin
		_get_env_var(repository_ctx, _PYTHON_BIN_PATH, No..., ...)
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 49, in _get_env_var
		_python_configure_fail((""'%s' environment variable is n...))
	File ""/home/dile/tensorflow/third_party/py/python_configure.bzl"", line 37, in _python_configure_fail
		fail((""%sPython Configuration Error:%...)))
Python Configuration Error: 'PYTHON_BIN_PATH' environment variable is not set
 and referenced by '//third_party/py/numpy:headers'
ERROR: Analysis of target '//tensorflow/examples/image_retraining:retrain' failed; build aborted
INFO: Elapsed time: 4.555s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/core ... (3 packages)



Operating system is ubuntu
and i dont know how to run configure..what and all i have to select in that
"
757,27539,0,"ValueError: Input 0 of layer dense is incompatible with the layer: its rank is undefined, but the layer requires a defined rank on Colab script downloaded to local machine. As a companion issue to #27538 with tensorflow 2.0.0a. Script fails with

> ValueError: Input 0 of layer dense is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.

in the line


**System information**
- TensorFlow version:
> ('v1.12.0-9492-g2c319fb415', '2.0.0-alpha0')

using an updated script



which also works for tensorflow 1.
- Doc Link: https://www.tensorflow.org/tutorials/images/hub_with_keras


**Describe the documentation issue**
The downloaded .py script (see #27538) fails to run (it's not clear-cut if this is a documentation issue or simply a bug) at


**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
If it's a doc issue, I'd love to. It could go deeper than that. The same issue appears for the MCVE at stackoverflow: https://stackoverflow.com/questions/55490885/error-converting-keras-model-to-tfjs-duplicate-weight-name-variable
"
1302,17457,0,"realtime bounding box . i use tensorflow project for android

I trained my custom datsets by yolo and then i made pb file.

This detected bounding box where i used picture not realtime

But when i put yolo file  in the android project, this cannot detect.

So i screenshot the scene of android project and i trained this screenshot in pc by darkflow.

It detected well. I don't know why. I think option value is correct. plz help me T.T

"
1434,24518,0,"Keras crossentropy with logits as loss function. **System information**
- TensorFlow version (you are using): 1.13rc0 (Windows GPU build)
- Are you willing to contribute it (Yes/No): **Yes**

**Current behavior/state.**
In the keras api for tensorflow there does not seem to be a function for crossentropy with logits, (without sigmoid or softmax activations). This is quite confusing for those coming from the estimators (like me). I see that a  argument is being added in the current release, but it needs to be used with a proxy function or a lambda.



**Will this change the current api? How?**
Instead of introducing argument, I think it's better to have a new loss function like 

**or**
loss name as string:


**Who will benefit with this feature?**
- Using logits to calculate loss is actually efficient than activation then loss, so tf should encourage using crossentropy with logits, as it did in estimator API.
- Those who switch from estimators to Keras API won't have any confusion.
"
1338,3089,0,"400 bad request when creating pip package. GitHub issues are for bugs / installation problems / feature requests.  
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### Environment info

Operating System:
CentOS 6
Installed version of CUDA and cuDNN: 7.5, 4.0

(please attach the output of ):
CUDA and cuDNN files are both present

If installed from binary pip package, provide:
1. Which pip package you installed.
2. The output from .
   NA

If installed from sources, provide the commit hash:

### Steps to reproduce
1. Build package from source
2. Run 
### What have you tried?
1. Have tried looking at #1817 which had same error. It is not intermittent as described in that issue, it fails every single time but works if you just try to clone the repo directly.
2. However, running  results in:


### Logs or other output that would be helpful

(If logs are large, please upload as attachment)


"
1185,34787,0,Bug: tensors built by tf.keras layers cannot use numpy() to obtain the array. . Bug: tensors built by tf.keras layers cannot use numpy() to obtain the array. So can i get the value of such tensors??
792,1281,0,"ImportError: cannot import name tensorboard_server . I've successfully built tensorflow ""version: 0.6.0 "" and tensor board from source on Mac OS X. The problem is I am getting this error when I try to run the tensorboard module. 

command: 


the produced error 


"
1058,15071,0,"Tensorflow build fails with --config=sycl. System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 17.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.3
Python version: 2.7
Bazel version (if compiling from source): 0.5.1
GCC/Compiler version (if compiling from source): 6.0.3
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce:

> bazel build -c opt --config=sycl //tensorflow:libtensorflow_cc.so

**Logs**

> 
> ERROR: /home/ashok/Ashok/tensorflow-c++/tensorflow/core/kernels/BUILD:3355:1: C++ compilation of rule '//tensorflow/core/kernels:sendrecv_ops' failed: computecpp failed: error executing command external/local_config_sycl/crosstool/computecpp -fPIE -fno-omit-frame-pointer -Wall -msse3 -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF ... (remaining 119 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1
> In file included from tensorflow/core/kernels/sendrecv_ops.cc:16:
> In file included from ./tensorflow/core/kernels/sendrecv_ops.h:19:
> In file included from ./tensorflow/core/framework/op_kernel.h:19:
> In file included from /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:55:
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:1404:14: error: no matching constructor for initialization of 'tuple<const tensorflow::Status &&, const tensorflow::Rendezvous::Args &&, const tensorflow::Rendezvous::Args &&, const tensorflow::Tensor &&, bool &&>'
>     { return tuple<_Elements&&...>(std::forward<_Elements>(__args)...); }
>              ^                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:992:13: note: in instantiation of function template specialization 'std::forward_as_tuple<const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>' requested here
>               std::forward_as_tuple(std::forward<_Args>(__args)...),
>                    ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:1731:2: note: in instantiation of function template specialization 'std::_Bind<(lambda at tensorflow/core/kernels/sendrecv_ops.cc:155:7) (std::function<void ()>, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>)>::operator()<const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool, void>' requested here
>         (*_Base::_M_get_pointer(__functor))(
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:2115:33: note: in instantiation of member function 'std::_Function_handler<void (const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool), std::_Bind<(lambda at tensorflow/core/kernels/sendrecv_ops.cc:155:7) (std::function<void ()>, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>)> >::_M_invoke' requested here
>             _M_invoker = &_My_handler::_M_invoke;
>                                        ^
> tensorflow/core/kernels/sendrecv_ops.cc:154:38: note: in instantiation of function template specialization 'std::function<void (const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool)>::function<std::_Bind<(lambda at tensorflow/core/kernels/sendrecv_ops.cc:155:7) (std::function<void ()>, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>)>, void, void>' requested here
>   Rendezvous::DoneCallback done_cb = std::bind(
>                                      ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:600:18: note: candidate template ignored: disabled by 'enable_if' [with _Dummy = void]
>                  _TCC<_Dummy>::template
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:611:18: note: candidate template ignored: disabled by 'enable_if' [with _Dummy = void]
>                  _TCC<_Dummy>::template
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:628:5: note: candidate template ignored: disabled by 'enable_if' [with _UElements = <const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]
>                   _TC<sizeof...(_UElements) == 1, _Elements...>::template
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:641:5: note: candidate template ignored: disabled by 'enable_if' [with _UElements = <const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]
>                   _TC<sizeof...(_UElements) == 1, _Elements...>::template
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:737:19: note: candidate template ignored: disabled by 'enable_if' [with _Alloc = tensorflow::Rendezvous::Args, _UElements = <const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]
>         enable_if<_TMC<_UElements...>::template
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:748:19: note: candidate template ignored: disabled by 'enable_if' [with _Alloc = tensorflow::Rendezvous::Args, _UElements = <const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]
>         enable_if<_TMC<_UElements...>::template
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:579:17: note: candidate constructor template not viable: requires 0 arguments, but 5 were provided
>       constexpr tuple()
>                 ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:589:26: note: candidate constructor template not viable: requires 0 arguments, but 5 were provided
>       explicit constexpr tuple()
>                          ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:670:19: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided
>         constexpr tuple(const tuple<_UElements...>& __in)
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:682:28: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided
>         explicit constexpr tuple(const tuple<_UElements...>& __in)
>                            ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:694:19: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided
>         constexpr tuple(tuple<_UElements...>&& __in)
>                   ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:705:28: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided
>         explicit constexpr tuple(tuple<_UElements...>&& __in)
>                            ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:721:2: note: candidate constructor template not viable: requires 7 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a,
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:732:11: note: candidate constructor template not viable: requires 7 arguments, but 5 were provided
>         explicit tuple(allocator_arg_t __tag, const _Alloc& __a,
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:711:2: note: candidate constructor template not viable: requires 2 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a)
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:759:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a, const tuple& __in)
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:763:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a, tuple&& __in)
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:772:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a,
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:784:11: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         explicit tuple(allocator_arg_t __tag, const _Alloc& __a,
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:796:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         tuple(allocator_arg_t __tag, const _Alloc& __a,
>         ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:808:11: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided
>         explicit tuple(allocator_arg_t __tag, const _Alloc& __a,
>                  ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:654:17: note: candidate constructor not viable: requires 1 argument, but 5 were provided
>       constexpr tuple(tuple&&) = default; 
>                 ^
> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:652:17: note: candidate constructor not viable: requires 1 argument, but 5 were provided
>       constexpr tuple(const tuple&) = default;
>                 ^
> 1 error generated.
> Target //tensorflow:libtensorflow_cc.so failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 1003.058s, Critical Path: 53.74s
> FAILED: Build did NOT complete successfully
"
10,22033,1,"Accuracy oscillates between ~0% and ~70% when creating multiple models. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: PyPi (pip)
- **TensorFlow version (use command below)**: b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0
- **GPU model and memory**: GTX 1060 6 GB
- **Exact command to reproduce**:
  - Download bug.py and training.csv from this Gist: https://gist.github.com/ChrisSwinchatt/97304761e9f875dfd34e3339891a5475
  - Run python bug.py

### Describe the problem

**I've found that the problem doesn't occur when TensorFlow is forced to use the CPU**

Full disclosure: I also opened an issue with Keras (https://github.com/keras-team/keras/issues/11070) because I'm not sure whether the bug is with TensorFlow or Keras. I am using tf.Keras, though, rather than just Keras with TensorFlow as a backend.

I'm using a sequence-to-sequence model based on the Keras blogpost which I've wrapped into a fairly complicated object (although the issue also occurs with a simplified version linked below). When I create a new model (which I have to do for gridsearch and for clearing the TF session when the graph gets too big and slows down training) it starts with accuracy of either 0% or 70%.

Here are a pair of screenshots that show what I mean:

Good: https://i.imgur.com/7mT5Siv.png
Bad: https://i.imgur.com/MZ3NdCB.png

You can see in the first screenshot that the accuracy is low but trending upwards. In the second, the accuracy of two models starts at 70% and doesn't increase (another model starts at 3% and also doesn't increase).

This happens whether I create new, blank models or load pretrained weights into new models with model.load_weights().

### Source code / logs

Source code and screenshots are above."
758,3543,0,"boolean_mask failed in iOS: Running model failed:Invalid argument: No OpKernel was registered to support Op 'Gather' with these attrs. ### Environment info

Operating System:
Building on Mac OS X
Running on iOS 9.2.1
1. The commit hash ()
   71f6bb336e5e11d6da2cedac6ba1c992ad9992bd
2. The output of 


### Steps to reproduce
1. I compile tf for iOS with the makefile
2. I frozen a model with last step as  a boolean_mask (that prozen pb file runs perfectly with TF 0.9.0 python api )
3. I run it in iOS, log report 



so it happens with the gather op inside the boolean_mask, when I removed this operation, it runs without problem in iOS.
Is that means gather op is not currently support in iOS? 
Do I have a work around for now? I need to filter the data with porbs > threhold.



I checked the  tf_cc_files.txt, tensorflow/core/kernels/gather_op.cc do exist.
"
86,6333,1,"Very low GPU usage. I try to use single dynamic_rnn to process very long sequence for classification task.
Here are some parameters:
rnn_size=500, seq_max_length=2500, batch_size=50, embedding_size=64, softmax_size=1600.

the code is as below:



The usage of GPU on TITAN is only 5%.
The usage of CPU  is about 150%.
I am not sure what's the problem."
1208,5659,0,"How to trought cuda/lib path. Hi 
I am very Struggling without going through the path of cuda/lib.



~/.bash_profile




but, In the following case I Successful ""import tensorflow"" 


"
134,30995,1,"LSTM prediction is numerically inconsistent for the last few instances.. The predictions you get may differ slightly depending on input length and position within it.  E.g., if you have 11 instances of input, you get one answer for the first 8, and a different answer for the last 3.
I write ""may"" as it happens to me with probability around 0.4. ""Slightly"" means in the order of the least significant bits of the float32 mantissa.



**System information**
- Yes, I have written custom code, supplied below as a reprex in R using keras.
- Tried on two platforms, with identical results.
Platform A:
- Linux Ubuntu Ubuntu 16.04.5 LTS
- TensorFlow version:
      VERSION                        ""1.7.0""               
      GIT_VERSION                    ""v1.7.0-3-g024aecf414""
      COMPILER_VERSION               ""4.8.4""               
- Python version: 2.7.12

Platform B:
- Linux Ubuntu 14.04.6 LTS
- TensorFlow version: tried both
      VERSION                        ""1.12.0""
      GIT_VERSION                    ""v1.12.0-0-ga6d8ffae09""
      COMPILER_VERSION               ""4.8.5""                
- Python version: 2.7.6

both:
- TensorFlow installed from binary.
- Not a mobile device.
- CUDA/cuDNN version: Not used.
- GPU model and memory:  Not used

**Describe the current behavior**
If the first dimension of  is , ""row""  will get one value if , but a possibly different value for . (These C++/Python style 0-based indices.  For R, 1-based, it's  versus .)

**Describe the expected behavior**
Reproducible prediction from same input instance, independent of row number or input length.  I use generalized ""row"" for a slice of a tensor with a given fixed first index, e.g.,  or .

**Code to reproduce the issue**
This is a reprex written in R.  I'd be happy to port to other languages if that's preferable.


<sup>Created on 2019-07-25 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1.9000)</sup>

**Other info / logs**"
1028,4969,0,"Messy in Python API Docs - state_ops.html#constant_initializer. https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#constant_initializer

On Constant Initializer  -> Examples:

The very large gray box contains the html of other initializer docs. Hope it can be fixed. 
"
632,25228,0,"TF Lite works wrong on quantized TF Hub retrained Inception V3 mnist model. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android Emulator
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly 1.13.0.dev20190123
- Python version:3.7.1
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory:NA

**Describe the current behavior**
- Using TF Hub to retrain Inception V3 for mnist images
- Converting the retrained model to quantized model
- Deploy the converted quantized model to Android App
- The quantized TF Lite works wrong like below:
![image](https://user-images.githubusercontent.com/2778945/51802538-1385ab00-2286-11e9-8297-cd9c7c3c5350.png)


**Describe the expected behavior**
- The mnist images are well classified.

**Code to reproduce the issue**
https://github.com/dpinthinker/TFGrocery/tree/master/MnistClassifier
"
443,14364,0,"What is the instruction of checking which version of cuda and cudnn the tensorflow is running on?. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:1.4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:8.0/6.0
- **GPU model and memory**:Tesla P100-PCIE-16GB
- **Exact command to reproduce**:



### Describe the problem

What is the instruction of checking which version of cuda and cudnn the tensorflow is running on?
like the tensorflow version could be checked by the instruction tf.__version__ .
I want to check my tensorflow 1.4 running on the 8.0 cuda and 6.0 cudnn, not running on the 9.0 cuda and 7.0 cudnn.


"
1372,30600,0,"TFTRT: Can I specify the computation precision for a specific layer?. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
1.14

**Describe the feature and the current behavior/state.**
Support blacklist of ops that cannot use lower precision when converting a graph into tftrt with precision mode of fp16 or int8. Or provide a way to specify a subgraph that cannot be optimized by tftrt.

**Who will benefit with this feature?**
All users that need a mixed precision of their model and want to optimize the model by tftrt.

**Any Other info.**

In my product I'm developing a model that needs full precision for a few layers (which include elementwise ops and conv ops). The layers need full precision because they are doing some math computation that cannot sacrifice precision. I also want to use tftrt to optimize the graph because most of other layers can use precision as low as fp16 or int8. However, i cannot find a way in tftrt to specify a blacklist of ops that cannot use lower precision. And there is no way to specify a subgraph that cannot be optimized by tftrt. Tensorrt supports specifying a precision for a layer but my model uses many ops that don't exist in tensorrt so I cannot convert it into tensorrt.

Thank you.

@trevor-m "
430,29864,0,"tflite invoke function crash. I came cross a strange issues only occurred on HuaWei Phone. 
For phone infomation with this image.
![image](https://user-images.githubusercontent.com/17869361/59587579-26572a00-9118-11e9-8b20-67a14a5c8f08.png)

At first time run inference no crash, It is always crash at second time. below is crash info.
![image](https://user-images.githubusercontent.com/17869361/59587879-e3498680-9118-11e9-8fe6-bf97e2e2301b.png)

Below is processed crash info with ndk-stack, but unable to locate in tensorflow source as build tflite from source with no debug symbols and i do not know how to build with debug symbol
![image](https://user-images.githubusercontent.com/17869361/59588620-9a92cd00-911a-11e9-9fb9-ba68baa06f52.png)

I have tried many methods for building tflite with debug symbol but not successed
For example
1、 bazel build -c dbg --strip=never --compilation_mode=dbg --per_file_copt=//tensorflow/lite/.*\.cc@-g,-O0  //tensorflow/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11""
with -c dbg --strip=never  --compilation_mode=dbg

![image](https://user-images.githubusercontent.com/17869361/59588889-345a7a00-911b-11e9-9424-c3cfcdd87d99.png)
Crash occurred at 62 line
It crashed at second time only on HuaWei Phone. Other phones and ios has no crash.


ps:This issue finally crashed at 277 line with below image
![image](https://user-images.githubusercontent.com/17869361/59751454-217aad80-92b3-11e9-8a98-155da7e58c92.png)

I guess the bias_data address is unavaible
![image](https://user-images.githubusercontent.com/17869361/59751423-10ca3780-92b3-11e9-971c-99ec5e074624.png)
"
15,23578,1,tensorflow performance bottleneck o IteratorGetNext
1104,31245,0,"gRPC: terminate called after throwing an instance of 'std::bad_alloc'. System information
    • OS Platform and Distribution: CentOS Linux release 7.4.1708 (Core)
    • TensorFlow version : Tensorflow-1.12.0 built from source

Describe the problem:
Running a model parallel implementation results in the following error:

The code runs fine on a single node i.e. with a single worker, but distributing across 2 nodes/2 workers results in the above error. Suspect it is related to grpc.

Source code / logs
On the chief worker:


On the non-chief worker (on another node with a different IP address):



"
781,7295,0,"tf.get_collection to extract variables of one scope . Hi,

I have (e.g: n=3) scopes and  (e.g: x=4) no of Variables defined in each scope. 
The scopes are:

        model/generator_0
        model/generator_1
        model/generator_2

Once I compute the loss, I want to extract and provide all the variables from only one of the scope based on a criteria during run-time. Hence the index of the scope  that I select is an argmin tensor cast into  

        <tf.Tensor 'model/Cast:0' shape=() dtype=int32>

I have already tried: 

        train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'model/generator_'+tf.cast(idx, tf.string)) 

which obviously did not work. 
Is there any way to get all the  Variables belonging to that particular scope using  to pass into the optimizer.
Forgive me if this question doesnt fit into tensorflow issues. 

Thanks in advance!
Vignesh Srinivasan"
1211,34427,0,"Segfaults and SIGABRTs on nightlies since 2019-11-19. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v1.12.1-18736-g229a46c', '2.1.0-dev20191119')
- Python version: Python 2.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

TensorFlow version  experiences core
dumps on some simple summary operations. I can’t reproduce this on my
normal workstation, but I have a consistent repro on a Cloud VM (which
does not have Cuda installed; maybe relevant?).

Repro steps:

Create a new virtual machine running Ubuntu 14.04:
<https://console.cloud.google.com/compute/instancesDeploySolution?solution=ubuntu-os-cloud:ubuntu-trusty>

Prepare the system:



Create a virtualenv:



Attempt to reproduce the failure:



About half the time, the last command will SIGABRT. On Python 2.7, the message is:

python': free(): invalid pointer: 0x0000000001ecaad0 ***
Aborted (core dumped)

(ve3.5a)wchargin@trusty-travis-debug-37:~$ python -c '__import__(""tensorboard"").summary.scalar(""test"", 1)'
2019-11-19 21:06:03.550539: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-11-19 21:06:03.550578: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2019-11-19 21:06:03.550601: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (trusty-travis-debug-37): /proc/driver/nvidia/version does not exist
2019-11-19 21:06:03.550931: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-19 21:06:03.559645: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz
2019-11-19 21:06:03.560529: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a31d20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-19 21:06:03.560558: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
*** Error in 

Note that though this code is invoked through TensorBoard APIs, the
error is actually in TensorFlow (TensorBoard does not have any C++ code
of its own), and the TensorBoard nightlies did not change substantially
between the last good nightly and the current broken nightly.
Downgrading TensorBoard to nightly  or to
stable  does not fix the problem.

If you instead install the following dependencies—

<details>
<summary> output; install with </summary>



</details>

—then the following script consistently segfaults after completing each
command (except rarely when it SIGABRTs instead):




Example double-free/invalid-free in failed build log:
<https://travis-ci.com/tensorflow/tensorboard/jobs/258256205>

Example segfault in failed build log:
<https://travis-ci.com/tensorflow/tensorboard/jobs/258256206>

**Describe the expected behavior**

Running TensorFlow code should never SIGSEGV or SIGABRT.

**Code to reproduce the issue**

See above.

**Other info / logs**

This previously occurred with TensorFlow’s 20191024 nightlies:



See <https://github.com/tensorflow/tensorboard/pull/2834> for context.

This is blocking TensorBoard CI. We’ll probably just pin yesterday’s
nightlies in the short term, but last time we did that (see above PR)
the underyling issue was evidently never actually resolved.
"
1081,18913,0,"Accessing CuDNN autotuner in built-in Keras. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9.1, CuDNN 7.0.5
- **GPU model and memory**: Tesla V100:

- **Exact command to reproduce**: N/A

We would like to be able to have access to the CuDNN autotuner in  module to access optimal algorithms for a given hardware (or, perhaps passing a custom convolutional algorithm from config). In TensorFlow, I can specify to use the CuDNN autotuner by setting: 

(currently enabled by default), which improves performance significantly on Volta GPUs and especially with FP16.

However, I am unable to access this performance improvement when running pure , where setting this environmental variable does not have any effect. 

### Source code / logs

Following simple example could be used to reproduce the issue: https://gist.github.com/ASvyatkovskiy/8d1dd622e447d9d8de1ec4e238e0dbaa"
252,31706,1,"TF 2.0 Dataset.shuffle() is deterministic. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): tf-nightly-gpu-2.0-preview==2.0.0.dev20190815
- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview==2.0.0.dev20190815
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
 returns results in the same order across invocations and across runs.

**Describe the expected behavior**
Previously,  would return randomized results.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

(TF 2.0)




(TF 1.14)




**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

cc @martinwicke"
1295,14523,0,[Fetaure Request] Layer normalization for NCHW/NHWC with fast gpu kernel. tf.contrib.layers.layer_norm is slow and only supports NHWC layout. It is beneficial to have a fast gpu kernel for layer normalization that supports both NCHW and NHWC. I think layer normalization is quite useful when the minibatch is extremely small or comprises only one sample (in which case batch norm/renorm doesn't work) and when local receptive fields is desired (in which case instance norm is bad).
640,6108,0,"[TensorBoard] Charts in TensorBoard in Safari don't display correctly. macOS Sierra, latest Safari (Version 10.0.1). Works good in Chrome.

Steps to repro:
1. Run https://github.com/openai/universe-starter-agent

2. Observe the following graphs:
<img width=""423"" alt=""screen shot 2016-12-05 at 6 32 12 pm"" src=""https://cloud.githubusercontent.com/assets/823890/20910888/30ec783a-bb19-11e6-8514-5c1027ca068a.png"">

3. Console contains these errors:
<img width=""505"" alt=""screen shot 2016-12-05 at 6 32 58 pm"" src=""https://cloud.githubusercontent.com/assets/823890/20910904/4936486c-bb19-11e6-8032-417677a78d2e.png"">"
501,22993,0,"TOCO failed see console for info. . INFO:
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Anaconda 
TensorFlow version (use command below): 1.11
Python version: 3.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

Hi, 
 I've used the example given in [converter python api guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/convert/python_api.md)

import numpy as np
import tensorflow as tf
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(2, input_shape=(3,)))
model.add(tf.keras.layers.RepeatVector(3))
model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))
model.compile(loss=tf.keras.losses.MSE,
              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
              metrics=[tf.keras.metrics.categorical_accuracy],
              sample_weight_mode='temporal')
x = np.random.random((1, 3))
y = np.random.random((1, 3, 3))
model.train_on_batch(x, y)
model.predict(x)
keras_file = ""keras_model.h5""
tf.keras.models.save_model(model, keras_file)
converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()`


I ran this code and this is the error I get:

> RuntimeError: TOCO failed see console for info.
b'Traceback (most recent call last):\r\n  File ""C:\\Users\\sgavvala\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module(\'_tensorflow_wrap_toco\', [dirname(__file__)])\r\n  File ""c:\\users\\sgavvala\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\imp.py"", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named \'_tensorflow_wrap_toco\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""c:\\users\\sgavvala\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\runpy.py"", line 193, in _run_module_as_main\r\n    ""__main__"", mod_spec)\r\n  File ""c:\\users\\sgavvala\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""C:\\Users\\sgavvala\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\Scripts\\toco_from_protos.exe\\__main__.py"", line 5, in <module>\r\n  File ""C:\\Users\\sgavvala\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\toco_from_protos.py"", line 22, in <module>\r\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\r\n  File ""C:\\Users\\sgavvala\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 28, in <module>\r\n    _tensorflow_wrap_toco = swig_import_helper()\r\n  File ""C:\\Users\\sgavvala\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 20, in swig_import_helper\r\n    import _tensorflow_wrap_toco\r\nImportError: No module named \'_tensorflow_wrap_toco\'\r\n'
None

My plan is to convert a keras model with my custom layers into tflite quantized version. Figured I would start with a given example but that doesn't seem to run."
1437,35299,0,"About bias quantization of hexagon delegate. As far as I know, Tensorflow Lite uses input_scale * weight_scale as bias scale to quantize bias tensor to int32. However, hexagon use int32_min/in32_max as min/max to quantize bias tensor.
So how do you handle this difference to hexagon delegation? Below is the conv_2d builder of the hexagon delegation. I don't see where the transformation is.
https://github.com/tensorflow/tensorflow/blob/539b7642a928c7fbfb4d896f650f7e6d79c2a5e0/tensorflow/lite/experimental/delegates/hexagon/builders/conv_2d_builder.cc#L234"
871,35122,0,"undeclared inclusion(s) in rule '@nccl_archive//:device_lib'. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux centos 6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 10.0.130/7.6.4
- GPU model and memory:



**Describe the problem**



**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
221,30715,1,"Low GPU usage of RNN layers under MirroredStrategy. **System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 14.04
- TensorFlow installed from: binary
- TensorFlow version: 
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / 
- GPU model and memory: TITAN X

**Describe the current behavior**

RNN layers have poor performance and low GPU usage when used with . By monitoring , part of the execution seems to run sequentially on each GPU.

**Describe the expected behavior**

With , the model is expected to be run in parallel.

**Code to reproduce the issue**

Consider the dummy training code below. It generates examples with random shapes and apply a stack of  on batches of sequences on 3 GPUs. If you replace the RNN layer by e.g. a stack of Dense layers, the parallelism is visibly improved.



cc @jkamalu."
194,29049,1,"Big bug, tf.keras is slower then keras.. I just change all keras api to tf.keras. But training become too slower, gpu is usable
"
1400,23552,0,"if i set the inputshape with (None, None, featdim),how can I split the input by second dimention(axis=1)?. I want to have a dynamic input in my net, so, I set the input with 2 unknown dim, (1:batch, 2:sequencelength, 3:feature dims),and I defined another variable to specify the real shape of input in forwarding.
i have a test as followings:

    a = tf.placeholder(tf.float32, shape=[None, None, 40], name = ""tensor_a"") 
    b = tf.placeholder(tf.int32, shape=[2], name = ""tensor_b"")  # b[0] indicate the sequence length

    split_tensor_a = split_a(a, b)
    sess = tf.Session()

    array_a = np.logspace(1.0, 2.0, num = 120).reshape(1,3,40)
    array_b = np.array([3, 40])
    feed_dict = {a: array_a, b: array_b}
    split_b_value = sess.run(split_tensor_a, feed_dict = feed_dict)


and the py_func is:

    def split_a(tensor_a, tensor_b):
        sp_tensor =tf.py_func(_split_a, [tensor_a, tensor_b], tf.float32, ""split"")
        return sp_tensor

    def _split_a(a, b):
        tensor_tup = ()
        newsp_a = np.reshape(a, (-1, b[0], a.shape[-1]))
        for n in range(b[0]):
            tensor_tup = tensor_tup + (newsp_a[:, n, :],)
        print(tensor_tup)
        return tensor_tup

In _split_a I can get the value of tensor_tup, but when I run the sess, I got an error:

I want to know how to split the unknown dim of tensor?
If the net has 2 unknown dim,how to deal with the input data by one timestep?
The net has cnn layer and lstm layer, I donot want to change the input shape to (-1, featdim) in cnn layer, but it can only do forward by one time step,  the follwing lstm layer must have shape of (batch, numstep, featdim), so, what should I do ?
Can anyone help me?
Thanks~"
366,22757,0,"keras.model.predict also needs two inputs (one for sample, and one for label) when used with tf.data together. I use tf.data.TFRecordDataset as the input of the tf.keras.model. The training and evaluation are OK. When I used the model for inference, I call the function tf.keras.model.prediction. However, it shows me an error 

> Please provide model inputs as a list or tuple of 2 elements: input and target pair.

I check the source code, and found that the function ""_standardize_user_data"" (line 988) in ""./tensorflow/python/keras/engine/training.py"" check the input number as the training and evaluation. I think it may be a bug, because when I does prediction, I don't know the true label.

tensorflow version: 1.11.0 gpu.

Thanks for your notice. I am looking forward your reply."
61,20607,1,"Performance drop for CPU graph calculation after upgrading from 1.3 to newer versions. - **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: 1.4 and newer for CPU
- **Python version**: 2.7.5 3.5.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: python test1.py

### Describe the problem

Initially I found training using CPU in TensorFlow 1.8 is slow and it appeared only one thread was used. I can confirm multi-threading for individual operations works well (like multiplying two large matrices). But it seems only one operation can be done at the same time. Specifying inter_op_parallelism_threads doesn't help. For version 1.8 and 1.9rc2 and the following test code I wrote, the CPU usage was almost consistently 100% (1 core) according to ""top"". For version 1.3 and 1.0, it was obvious that the following code used multiple cores and ran significantly faster. I reproduced most results with both Python versions. Here are my test results:
- Version time printed
- 1.0, 1.3: 20s
- 1.4, 1.5: 50s
- 1.6: 60s
- 1.7, 1.8: 80s
- 1.9 rc2: 130s

The above results were based on a 64GB, 20-core (2x E5 without hyperthreading) machine. I also tried on an AWS t2.2xlarge instance (32GB, 8-core) with Ubuntu 16.04, Python 3.5.2, TensorFlow 1.3 and 1.9 and got similar results.

### Source code / logs


Appended:
I just tried to collect the timeline in 1.3, 1.8 and 1.9rc. They all look similar and reported 5-10 seconds with good parallelism. Maybe the issue is caused by overhead from other parts like scheduling?
Timeline:
<img width=""943"" alt=""timeline"" src=""https://user-images.githubusercontent.com/10559772/42405897-56f3c342-81d0-11e8-9a8b-bf4a08a6a6c7.png"">
Timeline enlarged:
<img width=""886"" alt=""timeline_enlarged"" src=""https://user-images.githubusercontent.com/10559772/42405908-6ec434c0-81d0-11e8-871b-a25a559dc745.PNG"">
"
63,34435,1,"Significantly slow training with TFRecords when the labels are one-hot encoded. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: K80

**Describe the current behavior**

I am incorporating TFRecords along with all the  legacy functions in my projects. I primarily work with images. I am trying to develop a data input pipeline for an image classification project with the **Flowers-17** dataset. My aim is to first serialize the entire dataset in form TFRecords with multiple shards. I have been able to do that. I am then reading those TFRecords, preprocessing them, augmenting them and finally, I am feeding them to a model. 

Now, I ran two versions of the above experiment:
- One where the image labels were simply encoded using indexing
- Another one where the image labels were one-hot encoded

While compiling the model for the above two scenarios I supplied the loss functions accordingly as well. It now seems that the first experiment where the model **was not** supplied one-hot encoded class labels is training _much faster_ than the other one. 

**Describe the expected behavior**

Model training time should be closely equal in both the cases I mentioned. 

**Code to reproduce the issue**
- [Colab notebook](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/tf.data%20exploration/TFRecords_with_tf_data_Advanced.ipynb) for where one-hot encoding wasn't done. 
- [Colab notebook](https://colab.research.google.com/drive/1CuOZJNlWsXdGlhCCfZ14PvSEU2Gtcrdf) with one-hot encoding. "
601,17930,0,"Memory Leak in SavedModelBundle.load() in the TensorFlow Java API. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 2.7.12
- **Java version**: OpenJDK 1.8.0_151
- **Bazel version (if compiling from source)**: 0.6.1
- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.0
- **GPU model and memory**: name: TITAN Xp, compute capability: 6.1, total memory: 11.90GiB
- **Exact command to reproduce**: SavedModelBundle.load() and Runner.run() from Java API

### Describe the problem

There seems to be a memory leak when using the Tensorflow Java API when executing the load static method of SavedModelBundle class and the run method of the Runner class.  In the process of debugging, I've attached the VisualVM profiler and a remote Java debugger to step through code execution.  Additionally, I've been using  to monitor the memory usage from active processes running on the server hosting the API.  

From this, I've been able to observe
- each time the inference is invoked, it increases my total memory consumption by about 0.18G - 0.19G, which is approximately the size of my saved model files being loaded.  
- my Java heap memory is not permanently increasing; after the inference has finished all heap memory returns to the amount before the inference started.

I've been sure to invoke the close method of each class that extends AutoCloseable that I've used.  I do not explicitly invoke the close method on the Session and Graph managed by the SavedModelBundle, as its close method invokes the close methods of its Session and Graph.  This is reflected in the code example provided below.

This memory leak occurs whether I'm building with GPU acceleration or not.

To build the native libraries and Java API from source with GPU acceleration, I'm using the following bazel command (from the 1.6.0 release tag):



To build the native libraries and Java API from source without GPU acceleration, I'm using the following bazel command (from the 1.6.0 release tag):



In an attempt to get more information about the execution of the native code, I've tried to enable debug mode while building the native libraries by repliacing  with , however this causes the following exception:

cudaGetLastError() == cudaSuccess' failed.
-g--compilation_mode=dbg-gstdin`, to demonstrate the memory being used not released while the application is still running.

This code is used for inference against a trained version of Object Detection model architecture found [here](https://github.com/tensorflow/models/tree/master/research/object_detection).  I've prepared a custom dataset and performed training similar to the [Quick Start guide for Training a Pet Detector](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md).  Following this guide through the section titled **Exporting the Tensorflow Graph** should produce a saved model that can be used with this code sample.  You'll need to adjust the path to the saved model, the labels to those used for the guide, and the file path to the image to detect objects in, which are all captured as static final variables towards the top of the class.



The instructions to compile and run this are similar to those found in the [TensorFlow for Java Readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md).


"
236,29108,1,"Gradient clipping by norm has different semantics in tf.keras.optimizers against keras.optimizers. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0 + cuDNN 7.4
- GPU model and memory: NVIDIA Tesla V100 32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

The gradient clipping mechanism is implemented in the abstract class, tf.keras.optimizers.Optimizer (keras.optimizers.Optimizer) so that every optimizer inherited from the class supports it.
We find different semantics of the implementations between tf.keras and keras.io.

In tf.keras, the gradient norm is calculated and clipped **per gradient tensor**.

https://github.com/tensorflow/tensorflow/blob/bb8cf258bc87f68612a5e032d5b4def0d3c52566/tensorflow/python/keras/optimizers.py#L98-L99

But in keras.io, the gradient norm is calculated globally **across all gradient tensors**.


Code link: https://github.com/keras-team/keras/blob/eab1b5bcdf105746ede02d2eb8a5cb3ca359b1b5/keras/optimizers.py#L96-L98

IMO, tf.keras should adopt the latter method since
(1) The paper introducing gradient clipping (https://arxiv.org/abs/1211.5063) suggests the latter one, and
(2) tf.keras should be compliant with keras.io.

"
1336,16001,0,"fail to convert resnet_v1_50 to tflite. I downloaded resnet_v1_50 model from https://github.com/tensorflow/models/tree/master/research/slim

when I tried to convert this model to tflite, I got below error:
2018-01-10 14:54:19.491608: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Mean, Squeeze.

I use below command to convert:
bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=./resnet_v1_50_frozen.pb \
  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=./resnet_v1_50.tflite --inference_type=FLOAT \
  --input_type=FLOAT --input_arrays=input \
  --output_arrays=resnet_v1_50/predictions/Reshape_1  --input_shapes=1,224,224,3


Does it means that  tflite do not support op Mean and Squeeze  ?

  "
483,34815,0,"[bug] Lambda multiple-layers-different-shapes (ValueError: Dimensions must be equal). Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux tfbug 4.9.0-11-amd64 #1 SMP Debian 4.9.189-3+deb9u2 (2019-11-11) x86_64 GNU/Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
v1.12.1-19580-gc397ed9 2.1.0-dev20191203
(also tried in 2.0.0-stable)
- **Python version**:
Python 3.5.3
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
When there are multiple layers wrapped in a Lambda, where the unit of the 1st layer is not the same as the inputs, error occurs (ValueError: Dimensions must be equal).
If there are multiple layers wrapped in a Lambda, where the unit of the 1st layer is the same as the inputs, no error occurs (refer to #bug#fine).
If there is only a single layer wrapped in a Lambda, no error occurs (refer to model_lambda_single).
If layer(s) are not wrapped in a Lambda, no error occurs (refer to model_function_,model_bare_).
As a prototype counterpart of subclassed layer, Lambda should be able to wrap multiple layers
Hence, it is convincing that Lambda is not relaying the shape correctly.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

>SOURCE CODE: lambda_bug.py
import tensorflow as tf
from tensorflow.keras import Input, layers
D=123
C=4
#bare
def model_bare_single():
    model_in = Input((D,),name='model_in')
    model_out = layers.Dense(C,name='lyr_0')(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_bare_single')
    return model
def model_bare_multiple():
    model_in = Input((D,),name='model_in')
    model_io = layers.Dense(C,name='lyr_0')(model_in)
    model_out = layers.Dense(C,name='lyr_1')(model_io)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_bare_multiple')
    return model
#function
def function_single(ins):
    outs = layers.Dense(C,name='lyr_0')(ins)
    return outs
def model_function_single():
    model_in = Input((D,),name='model_in')
    model_out = function_single(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_function_single')
    return model
def function_multiple(ins):
    ios = layers.Dense(C,name='lyr_0')(ins)
    outs = layers.Dense(C,name='lyr_1')(ios)
    return outs
def model_function_multiple():
    model_in = Input((D,),name='model_in')
    model_out = function_multiple(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_function_multiple')
    return model
#lambda
def lambda_single(ins):
    outs = layers.Dense(C,name='lyr_0')(ins)
    return outs
def model_lambda_single():
    model_in = Input((D,),name='model_in')
    model_out = layers.Lambda(lambda_single,name='lambda_single')(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_lambda_single')
    return model
def lambda_multiple(ins):
    ios = layers.Dense(C,name='lyr_0',input_shape=(D,))(ins)#bug
    #ios = layers.Dense(D,name='lyr_0')(ins)#fine
    outs = layers.Dense(C,name='lyr_1')(ios)
    return outs
def model_lambda_multiple():#bug
    model_in = Input((D,),name='model_in')
    model_out = layers.Lambda(lambda_multiple,name='lambda_multiple',output_shape=(C,))(model_in)
    model = tf.keras.Model(inputs=model_in,outputs=model_out,name='model_lambda_multiple')
    return model
def main():
    model_bare_single().summary()
    model_bare_multiple().summary()
    model_function_single().summary()
    model_function_multiple().summary()
    model_lambda_single().summary()
    model_lambda_multiple().summary()
    print(tf.__version__)
    return
if __name__ == '__main__':
    main()

>LOGS
...
Traceback (most recent call last):
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 1619, in _creat
e_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 4 and 123 for 'lamb
da_multiple/lyr_1/MatMul' (op: 'MatMul') with input shapes: [?,4], [123,4].
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""lambda_bug.py"", line 69, in <module>
    main()
  File ""lambda_bug.py"", line 65, in main
    model_lambda_multiple().summary()
  File ""lambda_bug.py"", line 56, in model_lambda_multiple
    model_out = layers.Lambda(lambda_multiple,name='lambda_multiple',output_shape=(C,))(model_in)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 773, 
in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/core.py"", line 827, in cal
l
    return self.function(inputs, **arguments)
  File ""lambda_bug.py"", line 52, in lambda_multiple
    outs = layers.Dense(C,name='lyr_1')(ios)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 773, 
in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/core.py"", line 1089, in ca
ll
    outputs = gen_math_ops.mat_mul(inputs, self.kernel)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 5626, in mat
_mul
    name=name)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 742,
 in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/func_graph.py"", line 595, in 
_create_op_internal
    compute_device)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 3314, in _creat
e_op_internal
    op_def=op_def)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 1786, in __init
__
    control_input_ops)
  File ""/home/johnght/venv/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 1622, in _creat
e_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 4 and 123 for 'lambda_multiple/lyr_1/MatMul' (op: 'MatMul') with inpu
t shapes: [?,4], [123,4]."
590,23713,0,"Audio recognition on 8,000 samples per second audio data, label_wav.py. I followed the Simple Audio Recognition tutorial given on tensorflow website. Everything went good. I downloaded the google speech command data set, trained a model using default setting, got 87% test accuracy, and then tested on some other simple audio clips using  label_wav.py code given as tutorial.

But I need to decode audio files of 8,000 sps. The results of this model is not good on 8000 sps audios. reading from tensorflow audio recognition tutorial I got to know that I need to use 8000 sps data for training.

I downsampled all audio files in data set and trained the model again with this downsampled data using the following command:

python tensorflow/examples/speech_commands/train.py --data_url= --data_dir=C:\tmp\speech_dataset --sample_rate=8000

The accuracy of this model is good i.e. Final test accuracy = 87.2% . But the problem is when I am testing (labeling) any audio file using this model and by using the code label_wav.py I am not getting the right results even for the audio files from the data set.

I am not getting where the problem is? I searched alot but didn't find any solution. I also read the complete tensorflow tutorial on audio recognition.

Kindly Help me!!!! How to decode 8k sps audio clips using the model trained on 8k sps audio data and by using the given label_wav.py Python code?"
509,35485,0,"apply_gradients Error after do weight modification. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 1.15.0
- Python version: 3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
I want to do some modification of gradient, weight separately. When I do modification of gradient, it works, but when I use similar code to do modification of weight, it does not work.




**Describe the expected behavior**
Actually, when I use pdb to debug the v, it is a variable, it should be updated.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1202,19110,0,"Add to documentation: how to install Tensorflow Lite onto Raspberry Pi 3B+ Raspian Stretch. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Raspberry Pi 3B+, Raspian Stretch
- **TensorFlow installed from (source or binary)**:
What I'm asking about. 
- **TensorFlow version (use command below)**:
Tensorflow Version 1.7.0
- **Python version**: 
Python 3.5
- **Bazel version (if compiling from source)**:
What I'm asking about. 
- **GCC/Compiler version (if compiling from source)**:
What I'm asking about. 
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
Broadcom VideoCore IV @ 250 MHz (BCM2837: 3D part of GPU @ 300 MHz, video part of GPU @ 400 MHz)
- **Exact command to reproduce**:
Addition to documentation: Installation of Tensorflow Lite on Raspberry Pi 3B+ Raspian Stretch. 

### Describe the problem
I would like instructions on how to install Tensorflow Lite onto Raspberry Pi 3B+ with Raspbian Stretch OS. To the best of my knowledge, the documentation doesn't yet cover installation for Raspbian Stretch. I posted on the Raspberry Pi Stack Exchange (https://raspberrypi.stackexchange.com/questions/83498/how-do-i-install-tensorflow-lite-on-raspbian-stretch) and I was directed here. My goal is to deploy a Tensorflow neural network onto the Raspberry Pi 3B+ with Tensorflow Lite. "
556,24738,0,"float16 matmul is way slower than float32 matmul on CPU. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.2

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
float16 matmul is way slower than float32 matmul on CPU

**Code to reproduce the issue**
       
output         
"
663,19510,0,"The name 'softmax:0' refers to a tensor which does not exist.. Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (use command below): v1.8.0-0-g93bc2e2072 1.8.0
Python version: 3.5.2
CUDA/cuDNN version: N/A
GPU model and memory: N/A

I have this error: 


Here's a part of my code:

I've made some research and found out that this can be a tensorflow version problem. But I installed a latest version using pip.
Thank you for your help"
206,30596,1,"Very bad performance using Gradient Tape. System information

Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 18.04.2
TensorFlow installed from (source or binary): binary pip
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.8
CUDA/cuDNN version: 10.0/7
GPU model and memory: Tesla K80

I'm trying to learn Tensorflow 2.0, so I build a toy model and trained it using keres .fit 
method, everything worked well. 
But when I tried to implement the training loop from scratch, the training is happening very very slowly. Keras .fit method trained the model in 1 min 41 secs while the training code I've written taking more than 8 mins to train!!!

Below is my model definition:

Below I'm defining loss, optimizer and accuracy:


And Below is my training loop:

I'm runnig my notebook in Google Colab"
674,29279,0,"[TF 2.0 API Docs] tf.histogram_fixed_width_bins. ## URL(s) with the issue:

https://www.tensorflow.org/versions/master/api_docs/python/tf/histogram_fixed_width_bins

## Description of issue (what needs changing):

### Raises listed and defined

Raises are not defined and listed

### Submit a pull request?

No"
532,32625,0,"ImportError: DLL load failed: The specified module could not be found.. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not application
- TensorFlow installed from (source or binary): PIP Command
- TensorFlow version:1.14.0
- Python version:Python 3.7.3 (default, Apr 24 2019, 15:29:51)
- Installed using virtualenv? pip? conda?: PIP
- Bazel version (if compiling from source): Not applicable
- GCC/Compiler version (if compiling from source): Not applicable
- CUDA/cuDNN version: cudnn-10.1-windows10-x64-v7.6.3.30
- GPU model and memory:NVIDIA GetForce GTX 1660 Ti anf 6GB



**Describe the problem**
When I am try to import the tensorflow I am getting the error ImportError: DLL load failed: The specified module could not be found.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Followed the installation guide provided for CUDA Installation Guide for Microsoft Windows

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

>>> import tensorflow as t
Traceback (most recent call last):
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Anaconda\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
1373,15992,0,"android demo ,How to switch the horizontal screen?. tensorflow 1.4
TF-OD-API model



I have some problems now, and I want to ask how to do the rotation.
  "
456,10838,0,"Compiling TensorFlow gives 3k lines of warnings!. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 (4.8 kernel)
- **TensorFlow installed from (source or binary)**: Compiling TF from Source
- **TensorFlow version (use command below)**: 1.2.0
- **Bazel version (if compiling from source)**: 0.5.1
- **CUDA/cuDNN version**: CUDA 8.0, CuDNN 5.1
- **GPU model and memory**: 1050Ti 4GB (Notebook version) (Intel i5-7300hq CPU)
- **Exact command to reproduce**: Compiling from source [following documentation](https://www.tensorflow.org/install/install_sources).

### Describe the problem
I compiled tensorflow from source. The process finished successfully and the binary managed to install and run successfully. What seems strange is that during the process I got ~3k lines of warnings. I am linking to them at the end of the issue. I am wondering if that's expected behavior or indication of a (small or _not_so_small_?) problem.

One thing that may affect this is bazel installation. I followed [Bazel Installation Instructions](https://bazel.build/versions/master/docs/install.html) and used the recommended apt method. This led to me running into [this](https://github.com/tensorflow/tensorflow/issues/8092) issue. Installing openjdk-8-jdk on top of the ibm-java80-jdk as suggested in a [comment](https://github.com/tensorflow/tensorflow/issues/8092#issuecomment-304957009) solves the problem (although I am not sure how much technical debt this solution caries, which may have manifested in some of the warnings produced during compilation).

### Source code / logs
**Configuration Script options:**

[Console output of compilation command](https://github.com/Iolaum/CompileTF/blob/master/CompilationOutput.txt)
Because the output is too big to be placed within the issue I 've put it in it's own repository."
286,24789,0,"TensorRT's create_inference_graph() produced output with too big size. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.7
- CUDA/cuDNN version: 9.0
- GPU model and memory: Titan V

**Describe the current behavior**

I have tried to optimize my custom frozen model to run on TensorRT using , however, the output was larger than the original model (my model is around 200MB, but after converting it's more than 2GB). When I increased  to 30 or 40, the size was samller but still slightly bigger than the original one.

**Describe the expected behavior**
Smaller size of output graph

**Code to reproduce the issue**



**Other info / logs**
Because the model was way too big, I couldn't serialize it to .pb file, so that I had this error:

"
171,34632,1,"[Performance Problem] The tf.image.crop_and_resize op costs much time during training. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.6.4
- GPU model and memory: GTX 1080Ti, 11Gb

**Problem Description**
Hi Dear experts, my question is about the performance problem on the  op. My code is as follows:

    def roi_pooling(featuremap, boxes, box_inds, crop_size, data_format='channels_last'):
        if data_format == 'channels_first':
            shp2d = tf.shape(featuremap)[2:]
            featuremap = tf.transpose(featuremap, [0, 2, 3, 1])
        else:
            shp2d = tf.shape(featuremap)[1:3]
        feat_h, feat_w = tf.cast(shp2d[0], tf.float32), tf.cast(shp2d[1], tf.float32)
        xmin, ymin, xmax, ymax = tf.split(boxes, num_or_size_splits=4, axis=1)
        xmin /= feat_w
        ymin /= feat_h
        xmax /= feat_w
        ymax /= feat_h
        normalized_boxes = tf.concat([ymin, xmin, ymax, xmax], axis=1)
        normalized_boxes = tf.stop_gradient(normalized_boxes)
    
        box_features = tf.image.crop_and_resize(image=featuremap,
                                                boxes=normalized_boxes,
                                                box_ind=box_inds,
                                                crop_size=crop_size)  # nhwc
        box_features = tf.layers.average_pooling2d(inputs=box_features, pool_size=2,
                                                   strides=2, padding='valid',
                                                   data_format='channels_last')
    
        if data_format == 'channels_first':
            box_features = tf.transpose(box_features, [0, 3, 1, 2])  # nhwc --> nchw
    
        return box_features
    
And I've take the full trace of the training procedure using the  object and got the following tracing graph. 
![A0587B57-0D3A-49ba-81C9-56DC9332CB57](https://user-images.githubusercontent.com/43327429/69691790-01c19e00-110b-11ea-8eb3-85e3988eb209.png)

From this graph, we can see that the  ops costs much time during one training step. So is this reasonable? Could you please give some advice on how to improve the performance? Thanks."
821,19795,0,"Tensorflow logs everything twice while training. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS Sierra version 10.12.6
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.8.0
- **Python version**: 
3.6.5
- **Bazel version (if compiling from source)**:
0.13.0
- **GCC/Compiler version (if compiling from source)**:
GCC 4.2.1

### Describe the problem
I'm training an object detection model using the new  and it's configuration file  and tensorflow installed from source. When I launch the training tensorflow starts printing the same info twice. 

This problem didn't happen while training the same network I'm trying to get, with a different model (checkpoint)   and the configuration file  and with tensorflow installed from pip (I tested with version 1.6.0 and 1.8.0) 

NOTE : I didn't change the code in both cases and I wonder what's the cause of this.

### Source code / logs


"
1352,7290,0,"""SAME"" padding for ConvNet doesn't work as expected !. Hi folks,
When I want to reimplement the paper [VDCNN for Text Classification](https://arxiv.org/abs/1606.01781), I need to do padding=1 with filter_kernel=3 to keep the same input's dimension between ConvNet in the model.  When I use padding 'SAME', the result isn't well expected. There's a problem in the height_size_output. It seems doesn't do element-wise product of block convnet 72x3 between the input & the filter. 
By the way, there's no problem of height_size_output using the padding 'VALID'. It seems that it did. However, its width_size_output is reduced by 2. 
In my intuition, padding 'VALID' behave much more properly like in the paper, but how can I pad 1 to both the left & right of the 2D input before doing ConvNet ?

-----------------------------------------
import tensorflow as tf

data = tf.placeholder(""float"", shape=[1, 72, 1014, 1])
#
filter_shape = [72, 3, 1, 64]	
W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=""W"")
#
conv = tf.nn.conv2d(data, W, padding='SAME')
print(conv.get_shape())  # prints (1, **72**, 1014, 64), but should be (1, **1**, 1014, 64)

conv = tf.nn.conv2d(data, W, padding='VALID')
print(conv.get_shape())  # prints (1, **1**, **1012**, 64), but should be (1, **1**, **1014**, 64)"
1171,35252,0,"Issue - Blas GEMM launch failed. - Have I written custom code (as opposed to using a stock example script provided in TensorFlow):



- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 Ultimate 64bit
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7
- CUDA/cuDNN version:

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Wed_Oct_23_19:32:27_Pacific_Daylight_Time_2019
Cuda compilation tools, release 10.2, V10.2.89

- GPU model and memory:

GTX 1050Ti 4GB

GPU Engine Specs
CUDA Cores
768
Graphics Clock (MHz)
1290
Processor Clock (MHz)
1392
Graphics Performance
high-6747
Memory Specs
Memory Clock
7 Gbps
Standard Memory Config
4 GB
Memory Interface
GDDR5
Memory Interface Width
128-bit
Memory Bandwidth (GB/sec)
112
Feature Support
Supported Technologies
CUDA, 3D Vision, PhysX, NVIDIA G-SYNC™, Ansel
Thermal and Power Specs
Maximum GPU Temperature (in C)
97
Maximum Graphics Card Power (W)
75
Minimum System Power Requirement (W)
300

**Describe the current behavior**

Testing keras with hyperopt on my model (total shape approx. [1200000, 150]

Receving error below (nvidia-smi below error code):

What I have tried so far:
-installing different CUDAA/cuDNN/tenserflow configurations
-updating packages
-checking for additional nvidia-smi processes
- batch_size manimulation range(1, 64)

DenseDense(input_dim=126, units=114)DenseDense(units=88, kernel_initializer=""glorot_uniform"")nb_epochfitepochs

**Other info / logs**
Using hyperopt setup for keras NN from https://github.com/keras-team/keras/issues/1591
"
1246,26686,0,"When using tf.gradients in the forward pass, my codes stop at `compute_gradients`. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary (conda install)
- TensorFlow version (use command below): 1.9.0
- Python version: 3.5
- CUDA/cuDNN version: 390.25
- GPU model and memory: GTX1080, 11GB


### 1. What I did:
My codes like this example: [tensorpack faster rcnn example](https://github.com/tensorpack/tensorpack/blob/f417c49fe45759fb2c69cafcabe6613ea85ec469/examples/FasterRCNN/train.py#L98)

I used DetectModel as the base class to implement the detection model of the SSD architecture. The training, optimizer, and get_inference_tensor_names are unchanged, the build_graph is reimplemented in the subclass, and the preprocess is modified. Other modules (such as anchor_generator, target_assign), I used the code in tensorflow offical models] (https://github.com/tensorflow/models/tree/master/research/object_detection).

my codes as follow

### 2. What I observed:
(1) **Include the ENTIRE logs here:**
But there are not any errors. The runtime log as follows， and and I added some extra logs.


The code seems to be stuck at this line: https://github.com/tensorpack/tensorpack/blob/master/tensorpack/train/tower.py#L261
The function is "
596,30468,0,"tensorflow 2.0 variable slice assign_add not supported. Could anybody tell me in tf 2.0 rc1, why is variable slice assignment (varaible[...].assign function) works well but the assign_add is not supported?
e.g. :






"
1026,28096,0,"Distributed Training - Device Incarnation mismatch . **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): we are running a docker container with these benchmarks: https://github.com/mkulaczkowski/benchmarks
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Docker container: tensorflow/tensorflow:1.13.1-gpu-py3
- TensorFlow version (use command below): 11.13.1
- Python version: python 3
- CUDA/cuDNN version: 9.1
- GPU model and memory:  K80 


**Describe the current behavior**
We are attempting to run a distributed job. 8 workers and 1 parameter server. We are using this benchmarks repository https://github.com/mkulaczkowski/benchmarks inside the  tensorflow/tensorflow:1.13.1-gpu-py3 container. 

We are running the workers with this invocation: 
python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --num_gpus=8 --model=resnet50 --variable_update=distributed_replicated --batch_size=16

We are running the parameter server like this:
""python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model=resnet50 --variable_update=distributed_replicated --batch_size=16""

We are having some workers fail due to this error message:
113tensorflow.python.framework.errors_impl.AbortedError: RecvTensor expects a different device incarnation: 881627422748727503 vs. 7505616095957727320. Your worker job (""/job:worker/replica:0/task:4"") was probably restarted. Check your worker job for the reason why it was restarted.

I have done some debugging and I do not see a restart of the running container. We also do not necessarily see any networking issues. I will attach a full log to the end of this issue

**Describe the expected behavior**
All workers progress thru the benchmark and finish successfully. Right now it seems a a minimum of one and a max of 4 fail with the error above. 

**Code to reproduce the issue**
The code that is running can be found here: https://github.com/mkulaczkowski/benchmarks
The commands that run this code are listed above

**Other info / logs**
Full output of a failed worker:
https://pastebin.com/HzPQHEDQ
"
1284,24849,0,"GradientTape return none for variable not being present in expression. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
Currently, Gradient tape return none for derivative respect to variables not being present in expression. Semantically, the gradient should return zero for variables. The current behavior make programmer have to checkout whether the output is none and replace it with zero.

**Will this change the current api? How?**
no change to current api

**Who will benefit with this feature?**
people who use gradient tape will get intuitive result. 

**Any Other info.**
"
396,1996,0,"Distributed tensorflow on Mesos. In the [distributed howto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md): ""We are working on tools for launching tasks programmatically, e.g. using a cluster manager like Kubernetes. If there are particular cluster managers for which you'd like to see support, please raise a GitHub issue.""
It could be interesting to have CPU and GPU dockefiles ready for distributed than can run in a scalable way on Mesos (with Marathon and Kubernetes)
"
934,3680,0,"error: can't copy 'tensorflow/models/embedding/gen_word2vec.py': doesn't exist. I was installing TF from source like I always did but I'm having the following issue this time when running  I got



plus, I can't find that file anywhere... did anything change or am I missing something?
"
761,12571,0,"tf.subtract doesn't work for uint8 and uint16 images (such as PNG). It seems that tf.subtract doesn't support uint8 and unit16 image.
Could someone please add a PR to enable the uint16 and int16 support for subtraction.

Thanks,"
752,31607,0,"Keras Models with tf.sparse.sparse_dense_matmul can't be saved - Not JSON Serializable . **System information**
- OS Platform and Distribution: Windows 10 Version 1607
- TensorFlow version: 2.0.0-beta1
- Python version: 3.7.3

**Describe the current behavior**
Error:


**Describe the expected behavior**
Save without error


**Code to reproduce the issue**
"
1073,359,0,"ADAM optimizer creates trainable variable. ADAM optimizer crates two variables that are marked as trainable for no good reason (at least not apparent to me). 

I sent the following path but no one seems to have looked at it:
https://tensorflow-review.googlesource.com/#/c/1141/
"
763,24946,0,tensorflow/third_party/toolchains/gpus/cuda/BUILD. tensorflow/third_party/toolchains/gpus/cuda/BUILD file contains hard coded paths which would not necessarily true for all systems. Anyway I thought these files were generated through repository gen rules in . Isn't it the case anymore?
1310,15976,0,"iOS Camera Example not able to fit to iphone 6 screen . ### System information
- **OS Platform and Distribution macOS High Sierra **:
- **TensorFlow installed from source**:
- **TensorFlow version 1.4**:

### Describe the problem
When I run the tensorflow camera example on xcode it runs fine but only in dimensions that fit an iphone 4. I changed the build settings, have turned on autolayout, and even tried messing around with the constraints. It seems like no matter what I try the viewcontroller won't get any larger.
"
731,8154,0,"Error while tf.image.crop_and_resize . I am trying to crop and resize an image with a list of co-ordinates using tf.image.crop_and_resize() 
but am getting the following error:
### TypeError: Expected binary or unicode string, got 960, 
Below is the code which I am using



and I am getting the above said error. The complete stack trace is given below.



I am passing a string returned by tf.image.encode_jpeg still it is not working. Can I get any help ? "
739,13624,0,"speech demo is not working on duration different from 1000ms. hi

when i try to use a different data-set (different duration) on the speech recognition demo. the training works well.but the freeze part crash with the message

**""Assign requires shapes of both tensors to match ""**

tried almost everything:
- delete the temp directory
- pass the same param from train to freeze
- make sure that all wav file are with the same size
- used the nightly docker
- used the latest python

nothing worked!!!!!

my guess is that the problem is in the loading part

any idea what next? 
"
896,21898,0,"AWS lib is verbose when using S3. I'm using latest version of Tensorflow with AWS S3 as a backend for models and I have those annoying logs:

[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.183378: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.183471: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.194855: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.194963: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.206484: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.206714: I external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing. 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.212817: E external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404 
[tensorflow-server-pr-14-585f869b5f-2wln8] 2018-08-27 13:51:41.212852: W external/org_tensorflow/tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer. 

The server is working as expected though. I set the log level to ERROR only but still the 404 error is happening more than every second.

I saw that there is an issue open in  but this seems to be core/3rd party issue.

https://github.com/tensorflow/serving/issues/615


**Have I written custom code:** No
**OS Platform and Distribution:** Kubernetes 1.8 running on CENTOS
**TensorFlow installed from:** Docker image tensorflow/serving:1.10.1
**TensorFlow version:** 1.10.1
**Bazel version**: ??
**CUDA/cuDNN version**: N/A
**GPU model and memory:** N/A
**Exact command to reproduce:** 

Example of model_config_file:

**Mobile device:** None

Thank you
"
254,19933,1,"Using feed_dict is more than 5x faster than using dataset API?. ------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.4 (on Linux Ubuntu 16.04 you get similar results)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7 cpu
- **Python version**:  3.6.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I created a dataset in TFRecord format for testing. Every entry contains 200 columns, named C1 - C199, each being a strings list, and a label column to denote the labels. The code to create the data can be found here: https://github.com/codescv/tf-dist/blob/8bb3c44f55939fc66b3727a730c57887113e899c/src/gen_data.py#L25

Then I used a linear model to train the data. The first approach looks like this:


The full code can be found here: https://github.com/codescv/tf-dist/blob/master/src/lr_single.py

When I run the code above, I get 0.85 steps/sec (batch size being 1024).

In the second approach, I manually get batches from Dataset into python, then feed them to a placeholder, like this:


The full code can be found here: https://github.com/codescv/tf-dist/blob/master/src/lr_single_feed.py

When I run the code above, I get 5 steps/sec. That is 5x faster than the first approach. This is what I do not understand, because theoretically the second should be slower due to the extra serialization/deserialization of data batches.

Is this possibly a bug or am I using it mistakenly ?

Thanks!

### Source code / logs
I have also included some profile traces below:

(using tf Dataset API)
![lr_single](https://user-images.githubusercontent.com/124190/41281915-3b049342-6e65-11e8-8b04-4d4611a96fa8.png)

[profile-101.json.zip](https://github.com/tensorflow/tensorflow/files/2093402/profile-101.json.zip)


(using feed_dict)
![lr_single_feed](https://user-images.githubusercontent.com/124190/41281927-4186bcae-6e65-11e8-80a6-e19dcd6b5c53.png)

[profile-101.json.zip](https://github.com/tensorflow/tensorflow/files/2093405/profile-101.json.zip)



"
359,362,0,"Some wrong code in get_started document. On your get started website http://www.tensorflow.org/get_started/basic_usage.html#basic-usage
I found a small piece of wrong code:
In the sample code of Feeds section:



I tried them and python gave the AttributeError: 'module' object has no attribute 'types'
After reading the document http://www.tensorflow.org/resources/dims_types.html
I think it should be  instead of .  Probably it's a version issue?
"
677,5763,0,"The TensorFlow operation concat_offset is undocumented. This shows up in some protocol buffers.  A vigorous search through the source code produced a short comment in some C++ code that gives some hints about its behaviour.  In particular, the word 'cumsum"" was used.  There appears to be, at times, a second output argument produced, although I have no idea what that could be except a copy of the first output.

A lot of searching turned up nothing useful.

TensorFlow version .11"
444,6378,0,"WhereOp: Race condition between counting the number of true elements and writing them. I have the same problem as issue #4033 and [tensorflow/models#486](https://github.com/tensorflow/models/issues/486) happening to me in my own project with the nightly wheel. It is running fine on CPU, the problem only happens on GPU (titan x pascal).

The code that's causing the problem is this:

(iou is a tensor, params.thresh is a python float)

WIth the error message: 


Cuda:


cuDNN:




"
1398,9685,0,"[Java][Suggestion] Add Enum with all Operations. Currently it is hard for beginners to start with the Java API of TF. Mostly because the function names staed in the python tutorial do not apply to the Operation names used in Java. It would be nice if we had an Enum Containing all standart possible Operations.



I am currently struggling to find out what the correct name of ""Multiply"" is.
I have digged my way thorgh the code and most of the operations are registered trought  and I could trace down the java methods to  in c_api_internal.h. But I absolutly dont know were the operations are storred. 

Also nice would be a table ""python function name"" to ""internal operation name"""
902,32376,0,"[TF2.0]Shuffle function on tf.data fills up RAM memory.. In TF2.0rc, using  on  type dataset, it fills up shuffle buffer which also fills up RAM memory so if the data is huge and RAM is not enough then it hangs up the system which leads to system restart. This memory filling happens before every epoch. It of course releases the memory once the epoch is done. To reproduce this problem, I followed [this](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) TF tutorial but with more amount of data.
Since  is for handling the dataset  that can not be filled up in the memory then why does shuffle function shows this behaviour and fills up the memory.?"
1409,4138,0,"seq2seq model is not efficient when using sampled softmax. Hi, 
my commit number is
70de76e696c21da617fd2e6435cf7fedab220db8

I want to try sampled softmax to speed up my lm training. I'm changing slightly on the ptb training example code. I'm working on a CPU machine.
First, on ptb(vocab 10k), I'm using the ""small"" config, I see some speed up gain:
normal softmax 896wps

sample_softmax h256L1ba20 sample512 1975wps

Then, I move to a larger set, and using 100k vocab, I'm still setting the sample number to 512, I think the speed should be similar to the ptb case. But I got the speed to be about 300 wps, even if I set the sample number as small as 4. I don't understand why it should be so slower than the ptb case. Do you know how can I make it faster?

Here's some of the related code:



Thanks!
Goose
"
459,28245,0,"[feature request] Bidirectional with explict cell_fw & cell_bw parameters. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): all version with 
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

For bidirectional rnns, the TF native API EXPLICITLY specifies the forward & backward rnn cells as in , while the high-level keras API  creates forward and backward layers IMPLICITLY, which may cause confusion. 

> Just recall how much confusion  has caused with TF 0.12: At that time, a list of the **SAME** RNNCell instance is passed but different parameters are created for each RNN layer. Later this behaviour is fixed with a list of **DIFFERENT** RNNCell instances, and a list of the **SAME** RNNCell instance will lead to parameter sharing across layers.

My point is,  should accept two RNNCell instances (one forward and one backward). If both  and  are exposed to users, it will be more consistent with the TF native API and users will know they are using two different RNN cells for forward and backward purpose.

This new interface design also enables more complicated use cases, e.g.: using a 2-layer forward LSTM with hidden size 1024 each layer & a 3-layer backward GRU with hidden size 512 each layer, or sharing the parameters between forward & backward RNN if users wish.

**Will this change the current api? How?**
No. Due to compatibility concern, the  and  can be designated as key word arguments, making sure normal users are not influenced while higher level users gain more control.

**Who will benefit with this feature?**
Advanced RNN users. Or users familiar with the original  API.
"
1342,990,0,"GPU device not found on Ubuntu 14.04. I have installed tensorflow python3 version using virtual environment on Ubuntu 14.04. There are three GPUs (Tesla K20c) available on the server that i work on.

On the tensorflow webpage it says: ""In general you do not have to specify CPUs or GPUs explicitly. TensorFlow uses your first GPU, if you have one, for as many operations as possible.""

When i checked the examples provided on Using GPUs about using multi-GPUs 

import tensorflow as tf
# Creates a graph.

c = []
for d in ['/gpu:1', '/gpu:2']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
    c.append(tf.matmul(a, b))
with tf.device('/cpu:0'):
  sum = tf.add_n(c)
# Creates a session with log_device_placement set to True.

sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.

print(sess.run(sum))

the following error was displayed:

I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:93] Couldn't open CUDA library libcud                                   a.so. LD_LIBRARY_PATH: :/usr/local/cuda/lib64
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: dmlserver
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported vers                                   ion is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file c                                   ontents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  346.96  Sun Aug 23 2                                   2:29:21 PDT 2015
GCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported versi                                   on is: 346.96
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1060] LD_LIBRARY_PATH: :/                                   usr/local/cuda/lib64
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1061] failed to find libc                                   uda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dl                                   error: libcuda.so: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcurand.so.7.0 locally
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op paral                                   lelism threads: 16
E tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUD                                   A_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagn                                   ostic information for host: dmlserver
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: dmlserver
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported vers                                   ion is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file c                                   ontents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  346.96  Sun Aug 23 2                                   2:29:21 PDT 2015
GCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported versi                                   on is: 346.96
I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA:
I tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op p                                   arallelism threads: 16
Device mapping: no known devices.
I tensorflow/core/common_runtime/direct_session.cc:134] Device mapping:

Traceback (most recent call last):
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py"", l                                   ine 428, in _do_run
    target_list)
tensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: Cannot assign                                    a device to node 'Const_3': Could not satisfy explicit device specification '/g                                   pu:2'
         [[Node: Const_3 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape:                                    [3,2] values: 1 2 3...>, _device=""/gpu:2""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""checking_2.py"", line 15, in <module>
    print(sess.run(sum))
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py"", l                                   ine 368, in run
    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py"", l                                   ine 444, in _do_run
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device                                    to node 'Const_3': Could not satisfy explicit device specification '/gpu:2'
         [[Node: Const_3 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape:                                    [3,2] values: 1 2 3...>, _device=""/gpu:2""]()]]
Caused by op 'Const_3', defined at:
  File ""checking_2.py"", line 8, in <module>
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/ops/constant_op.py"",                                    line 165, in constant
    attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", li                                   ne 1834, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", li                                   ne 1043, in __init__
    self._traceback = _extract_stack()

Don't understand what the problem is? Need URGENT help!!! 
"
1275,2482,0,"Tensor board installing issue with docker. Hi I'm a new tensor flow installer~
I do not have any experienced with docker or tensor flow and i followed this web-cite 

http://www.netinstructions.com/how-to-install-and-run-tensorflow-on-a-windows-pc/

https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#requirements

![1](https://cloud.githubusercontent.com/assets/19249239/15487824/30c4a332-218b-11e6-8017-6b51cc276439.PNG)

[this screen stopped after showing this image]

![2](https://cloud.githubusercontent.com/assets/19249239/15487829/398ded3e-218b-11e6-8ac8-3bb39792301a.PNG)

[I do not know what they are doing in actual cmd line but it does not work]

Those are the stops I got stock.  can anyone help?
"
382,30413,0,"gradient back propagation in tf.contrib.image.transform. **System information** Ubuntu 16.04
TensorFlow version 1.10.1

tf.contrib.image.transform applies the given transform(s) to the image(s). But gradients are not backpropagated into transformation parameters https://www.tensorflow.org/api_docs/python/tf/contrib/image/transform. 
Affine transformations can be constructed using a series of translations, scales, rotations and shears. It could calculate the gradient and back propagation, at least in rotation,scale and translations. I noticed that tf.contrib.image.rotate and tf.contrib.image.translate do not emphasize that gradients cannot propagate backwards.
I'd like to know how to back propagation into the transformation parameters especially in affine transform. Do I need to explicitly define the gradient using gradient_map_override?（#17442）

Any suggestions would be helpful. Thanks

This may change the current api."
701,20834,0,"SyncReplicasOptimizer + MonitoredTrainingSession go through network many times with one session.run. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: 16G
- **Exact command to reproduce**: None

### Describe the problem
training model by  combined with ,
I find the model network will be run twice with only once  calling, and many NaN values will be raised. More, this will update the batch normalization parameters.
I do the initialization by . By the way, code will be right without .


### Source code / logs


Problem A:
I add  line when define in network, I find this line executing twice with once sess.run calling.

Problem B: 
when , the BatchNorm Parameter update right, but when ,  and  turn to NaN, so does the loss.

Everything is OK when training with async optimizer."
1239,30635,0,"Java version upgrade from 1.13.1 to 1.14.0 got error. <em>Java version upgrade from 1.13.1 to 1.14.0 got error</em>

**System information**
- Ubuntu 18.04.2 LTS (GNU/Linux 4.18.0-1013-azure x86_64):
- TensorFlow 1.14.0:
- Java 1.8:

Log with 1.13.1(Worked):
  | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.1.3.RELEASE)

2019-07-12 07:15:28.366  INFO 21194 --- [           main] com.springWeb.App                        : Starting App v0.0.1-SNAPSHOT on RaTceUbt2019 with PID 21194 (/home/RaTce2019/od/springWeb-0.0.1-SNAPSHOT.jar started by RaTce2019 in /home/RaTce2019/od)
2019-07-12 07:15:28.375  INFO 21194 --- [           main] com.springWeb.App                        : No active profile set, falling back to default profiles: default
2019-07-12 07:15:31.058  INFO 21194 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8765 (http)
2019-07-12 07:15:31.143  INFO 21194 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2019-07-12 07:15:31.143  INFO 21194 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.16]
2019-07-12 07:15:31.171  INFO 21194 --- [           main] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib]
2019-07-12 07:15:31.362  INFO 21194 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2019-07-12 07:15:31.362  INFO 21194 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 2885 ms
2019-07-12 07:15:31.940  INFO 21194 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'
2019-07-12 07:15:32.424  INFO 21194 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8765 (http) with context path ''
2019-07-12 07:15:32.432  INFO 21194 --- [           main] com.springWeb.App                        : Started App in 4.883 seconds (JVM running for 5.79)
2019-07-12 07:15:36.232  INFO 21194 --- [nio-8765-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2019-07-12 07:15:36.233  INFO 21194 --- [nio-8765-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2019-07-12 07:15:36.249  INFO 21194 --- [nio-8765-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 16 ms
2019-07-12 07:15:38.465576: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: models/pepsi/ssd/saved_model
2019-07-12 07:15:38.574596: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2019-07-12 07:15:38.614832: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2019-07-12 07:15:38.621862: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-07-12 07:15:38.623574: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7fe1c435d720 executing computations on platform Host. Devices:
2019-07-12 07:15:38.623614: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-12 07:15:38.716123: I tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.
2019-07-12 07:15:38.716231: I tensorflow/cc/saved_model/loader.cc:192] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: models/pepsi/ssd/saved_model/variables/variables.index
2019-07-12 07:15:38.716265: I tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 250701 microseconds.

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _
This is my pom.xml:
<?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0""
	xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
	xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
	<modelVersion>4.0.0</modelVersion>

	<groupId>springWeb</groupId>
	<artifactId>springWeb</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<packaging>jar</packaging>

	<name>springWeb</name>
	<url>http://maven.apache.org</url>

	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
		<java.version>1.8</java.version>
		<maven.compiler.source>1.8</maven.compiler.source>
		<maven.compiler.target>1.8</maven.compiler.target>
	</properties>

	<parent>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-parent</artifactId>
		<version>2.1.3.RELEASE</version>
	</parent>

	<dependencies>
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-web</artifactId>
		</dependency>
		<dependency>
			<groupId>org.tensorflow</groupId>
			<artifactId>tensorflow</artifactId>
			<version>1.14.0</version>
		</dependency>
		<dependency>
			<groupId>org.tensorflow</groupId>
			<artifactId>proto</artifactId>
			<version>1.14.0</version>
		</dependency>
		<dependency>
			<groupId>org.json</groupId>
			<artifactId>json</artifactId>
			<version>20180813</version>
		</dependency>
	</dependencies>

	<build>
		<plugins>
			<plugin>
				<groupId>org.springframework.boot</groupId>
				<artifactId>spring-boot-maven-plugin</artifactId>
			</plugin>
		</plugins>
	</build>
</project>



And I tried official Java start up got the same error:
**System information**
- macOS High Sierra Version 10.13.6 (17G6030):
- TensorFlow 1.14.0:
- Java 1.8:

pom.xml:
<project xmlns=""http://maven.apache.org/POM/4.0.0""
	xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
	xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
	<modelVersion>4.0.0</modelVersion>

	<groupId>tftest</groupId>
	<artifactId>tftest</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<packaging>jar</packaging>

	<name>tftest</name>
	<url>http://maven.apache.org</url>

	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
	</properties>

	<dependencies>
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>3.8.1</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.tensorflow</groupId>
			<artifactId>tensorflow</artifactId>
			<version>1.14.0</version>
		</dependency>
		<dependency>
			<groupId>org.tensorflow</groupId>
			<artifactId>libtensorflow</artifactId>
			<version>1.14.0</version>
		</dependency>
	</dependencies>
</project>


App.java:


Log:
"
482,27246,0,"Build Issue on windows - cd command does not change drive without /d flag. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10:
- TensorFlow installed from (source or binary): building
- TensorFlow version: Master
- Python version: 3.6
- Bazel version (if compiling from source):23.0
- GCC/Compiler version (if compiling from source):VS2015
- CUDA/cuDNN version:7.1
- GPU model and memory:GTX970



**Describe the problem**

When building with bazel the it fails with the following 



The cause of this is that the tensorflow source directory is on the D drive, but the temporary files are created in the windows user directory that is on the C drive. On windows the cd command does not change drive by default. Thus the line 

will not change the directory if the current working directory is on the D drive. This can be fixed by making the above line cd /d <path> which will then change drive. Alternatively you can insert cd D:
This command appears to be autogenerated but i cannot easily see how to change it.
"
1319,4202,0,"undefined reference to `tensorflow::CanUseDeepConv2D(...)' building with Makefile method. Building tensorflow with makefile method gets stuck at 

tensorflow::LaunchDeepConvOp<Eigen::ThreadPoolDevice, float>::Run(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, int, int, int, int, int, int, int, int, int, int, int, int, int, tensorflow::Tensor*, tensorflow::TensorFormat)':
conv_ops.cc:(.text._ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE[_ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE]+0x58): undefined reference to tensorflow::functor::DeepConv2D<Eigen::ThreadPoolDevice, float>::operator()(tensorflow::OpKernelContext*, tensorflow::Conv2DArgs const&, float const*, float const*, float*)'
collect2: error: ld returned 1 exit status
tensorflow/contrib/makefile/Makefile:485: recipe for target '/home/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark' failed
make: *** [/home/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1
`
"
486,19270,0,"Different dynamic rnn output depending on batch_size. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, own code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 6.7 and Windows 10
- **TensorFlow installed from (source or binary)**:  by pip install
- **Bazel version**:  N/A
- **TensorFlow installed from (source or binary)**:  by pip install
- **TensorFlow version (use command below)**: ('v1.4.0-rc0-21-g1e25994', '1.4.0-rc1')
 on CentOS ; 1.8.0 on Windows
- **Python version**: 2.7 on CentOS ; 3 on Windows
- **CUDA/cuDNN version**: CUDA 8.0/cuDNN 6 on CentOS; CUDA 9.0/cuDNN 7.1 on Windows
- **GPU model and memory**: NVidia Tesla K80 on CentOS ; GeForce GTX 1050 Ti 4GB on Windows
- **Exact command to reproduce**: Example below, tested in jupyter-notebook from anaconda 5.1 (i think so)
ipykernel==4.8.2
ipython==5.6.0
jupyter==1.0.0
jupyter-client==5.2.3
jupyter-console==5.2.0
jupyter-core==4.4.0
jupyter-tensorboard==0.1.6


### Describe the problem
**Context**:  I'm trying to create a syntactic parser with NN classifier which defines state of next parsing step. Batch size is dynamic and equals to parsing steps count during training, but on prediction i feed into NN only 1 state per parsing step, so batch size=1. When i found that i lost some prediction accuracy i started to dig and that's what i found.
**Problem**: hidden states of dynamic LSTM  are a bit different when batch size is 1 and >1. The difference between them is small, about 0.0000001, but since i have several LSTM in NN, it affects the output of network. And interesting that if batch size is 2, 3 or more, the outputs are equal, but they are not if batch size is 1. And last one, it's ok with input with small dimensions, like [batch_size, 4, 4], but not when i have [batch size, 4, >10]
I wrote some test example to represent this. It worked for me on 2 systems.

### Source code / logs



"
451,30684,0,"Compiling libtensorflow-core.a using NDK16b or later?. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Snapdragon 820 , Android
- TensorFlow installed from (source or binary):
Source
- TensorFlow version (use command below):
1.13

- Python version:
3
- Bazel version (if compiling from source):
NA
- GCC/Compiler version (if compiling from source):
4.9
- CUDA/cuDNN version:
NA
 GPU model and memory:
NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
Compilation Error
Due 

e/gen/obj/android_arm64-v8a/tensorflow/contrib/boosted_trees/proto/learner.pb.o
In file included from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:44:0,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/postypes.h:40,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/char_traits.h:40,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/string:40,
                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.h:7,
                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.cc:4:
/opt/google/android-ndk-r16b/sources/android/support/include/wchar.h:32:24: fatal error: wchar.h: No such file or directory
 #include_next <wchar.h>



**Describe the expected behavior**

**Code to reproduce the issue**
Checkout r1.13 and export NDK_ROOT=/opt/google/r16b
**Other info / logs**
nuc2@nuc2-NUC7i5BNH:~/alok/tf-static-android/tensorflow$ export e/gen/obj/android_arm64-v8a/tensorflow/contrib/boosted_trees/proto/learner.pb.o
In file included from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:44:0,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/postypes.h:40,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/char_traits.h:40,
                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/string:40,
                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.h:7,
                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.cc:4:
/opt/google/android-ndk-r16b/sources/android/support/include/wchar.h:32:24: fatal error: wchar.h: No such file or directory
 #include_next <wchar.h>

"
1457,8515,0,"Gradients Tutorial feature requests.. 
I need to modify the tensorflow source for specific ops. When add one more input to the  from

    .Input(""feature: T"")
    .Input(""labels: T"")
to 

    .Input(""feature: T"")
    .Input(""labels: T"")
    .Input(""my_input: T"")

the forward version of corresponding codes do work, but the backpropagation version of corresponding codes seem to need further modification. Since the organization of TF source codes seems too complicated to me, I don't what to modify for the next step.

Here is the Error I got:
  
        grads = optimizer.compute_gradients(total_loss)
      File ""/home/jake/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 345, in compute_gradients
        colocate_gradients_with_ops=colocate_gradients_with_ops)
      File ""/home/jake/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 488, in gradients
        _VerifyGeneratedGradients(in_grads, op)
      File ""/home/jake/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 255, in _VerifyGeneratedGradients
        ""inputs %d"" % (len(grads), op.node_def, len(op.inputs)))
    ValueError: Num gradients 2 generated for op name: ""softmax_cross_entropy_loss/xentropy""
    op: ""SoftmaxCrossEntropyWithLogits""
    input: ""softmax_cross_entropy_loss/Reshape""
    input: ""softmax_cross_entropy_loss/Reshape_1""
    input: ""Reshape_3""
    attr {
      key: ""T""
      value {
        type: DT_FLOAT
      }
    }
     do not match num inputs 3

Any help will be appreciated. Thanks.
"
778,21643,0,"Tensorflow Lite Custom op have no effect even source code *.cc has modified. System information
== cat /etc/issue ===============================================
Linux master 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

== uname -a =====================================================
Linux master 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy 1.14.5
protobuf 3.6.0
tensorflow 1.8.0
tensorflow-tensorboard 0.4.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/dist-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64/:/usr/local/cuda/lib64/:/usr/local/cuda-9.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Mon Jul 30 11:49:23 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.45 Driver Version: 396.45 |
|-------------------------------+----------------------+----------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
|===============================+======================+======================|
| 0 Quadro P600 Off | 00000000:03:00.0 On | N/A |
| 34% 43C P8 N/A / N/A | 574MiB / 1997MiB | 1% Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes: GPU Memory |
| GPU PID Type Process name Usage |
|=============================================================================|
| 0 1152 G /usr/lib/xorg/Xorg 237MiB |
| 0 2213 G compiz 79MiB |
| 0 2589 G ...-token=224D99F526E00AE3A3C0EF4D5E6D103A 113MiB |
| 0 5305 G ...-token=53BA9EE005E85645FEB6537828E37D64 141MiB |
+-----------------------------------------------------------------------------+

== cuda libs ===================================================
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148


### Describe the problem
I converted caffe ssd model to tensorflow/tensorfow lite. There are small differences in caffe and tensorflow detection layer (i.e., decode box and coordinates difference). 
Tensorflow lite does not support all ops of tensorflow.
Detection layer in tensorflow lite is implemented in detect_postprocess.cc as custom op.
I changed detect_postprocess.cc to apply caffes' decode box and other functions. Then, I compiled tf with ""bazel build -c opt --jobs=8 //tensorflow/tools/pip_package:build_pip_package"" and also tried with ""bazel build -c opt ""//tensorflow/contrib/lite/kernels:builtin_ops"",
However, problem is when I load detect.tflite on android and/or PC (with tensorfow lite python api <<tf.VERSION = 1.8.0 does not work,tf version:: 1.10.0 workd>>), the detection results return the same result as what detect_postprocess.cc supposed to work even I changed functionality of detect_postprocess.cc.

In other words, tensorflow lite always refers very first version of detect_postprocess.cc

What is the problem?

"
401,22245,0,"Could not open .\linear_regression.pbtxt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. Please go to Stack Overflow for help and support:


If you open a GitHub issue, here is our policy:

------------------------

### System information
TensorFlow 1.9
Windows 10
Python 3.6 64

You can collect some of this information using our environment capture script:






### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
"
1268,14398,0,"name_scope is indistinguishable from string type . variable_scope yields a class VariableScope when entered, while name_scope yields a string. 



I think string might be unsafe. Moreover, sometimes we have to hack a ""/"" suffix if we want to reenter the current name scope. 

Especially, it's difficult to check the type when  is an argument for a function.


Will tensorflow plan to introduce a class like VariableScope, say NameScope, in the future? Thanks."
1222,26138,0,"TensorFlow v1.12.0 Test failures on s390x. **System information**
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, 18.04 s390x
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): source
TensorFlow version: v1.12.0
Python version: 2.7.x
Installed using virtualenv? pip? conda?: Building from source
Bazel version (if compiling from source): v0.15.0
GCC/Compiler version (if compiling from source): 7.3.0 (Ubuntu 18.04), 5.4.0 (Ubuntu 16.04)
CUDA/cuDNN version: NA
GPU model and memory: NA

**Describe the problem**
We have build TensorFlow v1.12.0 from source on Ubuntu 16.04, 18.04 on s390x platform. Observed test failures from contrib and python module. 




Most of the tests are failing with an error: 
Could you please let us know the severity of these failures? Are these covering/affecting any critical functionality of TensorFlow?
Accordingly, we can start debugging the test failures.


"
471,28232,0,"Warning recommends using unexisting tf.keras.layers.CuDNNLSTM layer. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 1080 Ti, 11175MiB

**Describe the current behavior**
In TF-GPU 2.0 

**Describe the expected behavior**
Either have a  or remove the warning.

**Code to reproduce the issue**
This to raise the warning

    import tensorflow as tf
    import tensorflow.keras.layers as ll
    input_ = ll.Input((100,50))
    x = ll.LSTM(100)(input_)

This to try to use CuDNNLSTM 

    import tensorflow as tf
    import tensorflow.keras.layers as ll
    input_ = ll.Input((100,50))
    x = tf.keras.layers.CuDNNLSTM(100)(input_)

**Other info / logs**

The warning message:

    W0428 17:18:46.256715 140569873639168 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fd8c75a9940>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.

When trying to call CuDNNLSTM:

    AttributeError: module 'tensorflow.keras.layers' has no attribute 'CuDNNLSTM'"
77,14496,1,"Building with MKL reduces CPU performance. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: both
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: Tesla P100-PCIE-16GB
- **Exact command to reproduce**:

### Describe the problem
Building tensorflow with mkl (--config=mkl) prevents the system from using all its cores.
CPU load remains always below 20% in my testcase. Using the same build flags but without mkl achieve 100% CPU load and a nearly 10 times faster execution.

While playing with the MKL flags described here https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu
i noticed some strange behavior:

Running the MKL-build with 

results in the following print:



If I use the same execution flags with a build without MKL or with the pip version I get the same ouput up to 

Afterwards no OMP prints are created. It seems like, if I build with mkl, tensorflow continues to create more and more threads but cant utilize them. 

Is this a configuration issue or a bug? 
If its a known issue, please expand the performance guide :) 

pinging @skye because of its help with the performance issue with while_loop 

"
329,23177,0,"in mnist_with_summaries.py,the function act(), what mean,,where define. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py#L97
      activations = act(preactivate, name='activation')

"
660,33277,0,"INT8 calibration error using TrtGraphConverterV2 in TensorFlow2.0. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda10.0
- GPU model and memory: TITAN V, 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
Using TensorRT version in: 5.1.5
When using TrtGraphConverterV2 to convert a saved model and trying to calibrate with INT8 support, it will raise the error shown as follow.
Should be mentioned that, the FP16 and FP32 conversation is work fine but only the INT8 is failed.

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1303,35055,0,"Post-training quantization leaves quantization nodes. As reported [here](https://towardsdatascience.com/hacking-google-coral-edge-tpu-motion-blur-and-lanczos-resize-9b60ebfaa552), post-training quantization with representative dataset leaves quantization and dequantization nodes in the graph. 
In the post,  works but the model expects , which is suboptimal. 
Also all the tensors have   instead of .
Is this the expected behavior? Or a known issue?

 Is there a robust workaround? 

I can work on an example with DNNs if needed but I just wanted to ask first.
"
1,10245,1,"Implement Focused Online Learning which converges faster than SGD. ![image](https://cloud.githubusercontent.com/assets/9004594/26520117/ec115938-42fe-11e7-9350-8da77dc6488e.png)
Shai Shalev-Shwartz and Yonatan Wexler. Minimizing the Maximal Loss: How and Why?. ICML, 2016.
http://proceedings.mlr.press/v48/shalev-shwartzb16.pdf
http://arxiv.org/abs/1602.01690
https://www.cs.huji.ac.il/~shais/talks/FOL_talk.pdf"
524,3658,0,"API for getting hidden layer state. I'm using  and I'd like to get the hidden layers state after forward propagation (calling predict()). There doesn't seem to be an API to do this. Will one be provided in the future, or would I need to avoid ?
"
517,20955,0," the quantized form of Shape operation is not yet implemented. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.9.0
- **Python version**:2.7.3
- **Bazel version (if compiling from source)**:0.12.0
- **GCC/Compiler version (if compiling from source)**:c++11
- **CUDA/cuDNN version**:7.5.18
- **GPU model and memory**:TITAN,12GB
- **Exact command to reproduce**:
 ./bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=~/deeplabv3_mobinetv2/frozen_inference_graph.pb   --output_file=~/deeplabv3_mobinetv2/foo.cc   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=QUANTIZED_UINT8   --input_shape=1,513,513,3   --input_array=ImageTensor   --output_array=logits/semantic/BiasAdd   --default_ranges_min=0   --default_ranges_max=6   --mean_value=127.5   --std_value=127.5

### Describe the problem
I want to use dummy quantization to quantize deeplabv3_mobilenetv2 model ""mobilenetv2_coco_voc_trainaug"" from https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md.
But I got the shape operation is not yet implemented.
Do you have plan to implement it?

### Source code / logs
2018-07-19 13:49:26.114180: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:459] Unimplemented: this graph contains an operator of type Shape for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Aborted (core dumped)
"
1251,3370,0,"Tests for examples/learn. A little while ago I caught a bug with the [resnet example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/resnet.py) that could have been caught by an automated test.

What do people think about bringing in some kind of automated tests to examples/skflow? What sort of tests should these be? I think the first step would be to simply run the code regularly. That would have caught the bug with the resnet example.
"
293,33422,0,"Specifying output_shape is not working in tf.keras Lambda Layer. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.3
- CUDA/cuDNN version: 10.0/7

**Describe the current behavior**
When creating a keras Model using a lambda function with specified output shape, the shape is not assigned to the resulting tensor: 
dense_net from the example below:

If used before another layer like Dense the error appears:
DenseNone

**Describe the expected behavior**
dense_net should have shape information:


**Code to reproduce the issue**

"
492,600,0,"Translate (Seq2Seq) Tutorial Expectations. The output I am getting from the translate.py tutorial looks corrupted.
hello -> G0

I followed the translate tutorial here:
https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html

I wanted to create a small test model, limiting to 1M records
python translate.py --data_dir data --train_dir train --en_vocab_size=40000 --fr_vocab_size=40000 --size=256 --num_layers=2 --steps_per_checkpoint=50 --max_train_data_size=1000000

No errors during the training.

After 15 hours, I stopped the training when the perplexity was below 10 points.
global step 19500 learning rate 0.2655 step-time 2.76 perplexity 8.63
  eval: bucket 0 perplexity 13.15
  eval: bucket 1 perplexity 10.71
  eval: bucket 2 perplexity 12.78
  eval: bucket 3 perplexity 14.38

After stopping the training I deleted the last corrupted training file.
rm train/translate.ckpt-19500

If I try to translate simple words I get junk.
python translate.py --decode --data_dir data --train_dir train
Created model with fresh parameters.
-> hello
G0 Processing Processing Processing Processing Processing Processing Processing Processing Processing
-> house
G0 G0 pâturage d’infrastructures d’infrastructures Twin Twin Twin Twin Twin
-> Who is the president of the United States?
expédiées expédiées expédiées m0 m0 m0 m0 m0 m0 m0 m0 m0 m0 m0 m0

Other translation technologies (es. Moses) will provide a decent translation with that training data.

If there a glitch somewhere or did I do something wrong?
"
823,26020,0,"MirroredStrategy doesn't use GPUs. 
**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.12
- TensorFlow version (use command below): 1.12
- Python version: 3.5.2
- CUDA/cuDNN version: CUDA 9 / cuDNN 7.3
- GPU model and memory: 2 x Nvidia Titan X

**Describe the current behavior**


I was working on rewriting a script from the queue/threading approach to tf.data.Dataset approach of providing data. I got really nice throughput of data with over >90% util of GPUs.  Now that I have rewritten it, when starting the training with MirroredStrategy the GPUs are not used at all and I get the following output:



At this point I am thinking there is some issue with TF1.12.

**Code to reproduce the issue**

Here is basically the structure of my code. I tried out different things like train directly with tf.keras.fit with multi_gpu_model() but it didn't work out. Basically I am trying to reproduce the functionality of my RandomShuffleQueue() I had before with multiple threads filling up the queue. 





Any help would be greatly appreciated since I'm stuck on it since a while now.

"
1360,11048,0,"tensorflow-gpu aborts with CuDNN v6. ### System information
- **The code can be found from the notebook [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb)**:
- **OS Platform and Distribution: Linux Ubuntu 16.04**:
- **TensorFlow installed from pip**:
- **TensorFlow version ('v1.2.0-rc2-21-g12f033d', '1.2.0')**:
- **Bazel version : None**:
- **CUDA/cuDNN version : CUDA v8, CuDNN v6**:
- **GPU:  GeForce GTX 950M (2GB)**:
- **Exact command to reproduce: Run the notebook**:

I was trying out some code for a convolutional network using tensorflow-gpu but the following messages appear.  

The code runs smoothly if I'm only using CPU.  Is it a bug or feature of tensorflow? How do I get around it?"
721,3420,0,"Running Bidirectional RNN cells on parallel. is there any method to run Bidirectional RNNs on parallel as they should (they have no dependency on each other)
I noticed that stacking 2 LSTM layer has the exact same performance as bidirection LSTM (BLSTM)
even 4 stacked LSTM has same performance as 2 parallel BLSTMs (which simply means  that TF runtime doesn't parallelize BLSTMs at all, I also notice the same behavior on both the CPU and the GPU)

Note: I am using small cells that are trivial to fit simultaneously

Thanks,
"
1016,11569,0,"Why use memory_layer in all cases?. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2

In current AttentionWrapper, both [BahdanauAttention](https://github.com/tensorflow/tensorflow/blob/c996c7b381a8eb54f9c7d7b298b24b1715645b68/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L416-L417) and [LuongAttention](https://github.com/tensorflow/tensorflow/blob/c996c7b381a8eb54f9c7d7b298b24b1715645b68/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L295-L296) enforce to use a memory_layer. According to my understanding, it is needed only when the depth of memory is not matched with that of query_layer. Is it intended to be in this manner?

@ebrevdo , would you mind having a look at this?"
775,15938,0,"An easy problem about tensorflow tf.reduce_mean op. I know... there might not be a suitable palce to ask this question, but I really hope someone cloud help me.

i want to use the ""tf.reduce_mean"" to obtain the mean of an array (ignore the zeros element)
eg:
    data = [[1,2,3],[4,5,6],[0,0,0]]   
    i want to obtain mean= [2.5, 3.5, 4.5]  
    but  tf.reduce_mean op gets the mean=[1.6, 2.3, 3]

Thank you very much!
  "
202,13187,1,"parse_example is awfully slow. @skearnes
you have indicated in this post of yours https://github.com/tensorflow/tensorflow/issues/390 way back in 2015 that parse_example is about 30 times faster than parse_single_example.
I have tried different options to modify my simple training script which only prints about 100000 tfrecords batched in 1000 and just does a print of feature and label after session.run(feature, label). Feature is a sparseTensor BTW.
Can you please put a test sample which proved that parse_example was that fast. parse_single_example was taking ~320 secs, now parse_example takes ~240 secs.

@Admin, please do not close this issue and refer to stackoverflow, as I don't think this is something to do with API usage or wrong parameters.
This is to do with the performance of queues (enqueue, dequeue) & threads"
768,6261,0,"text_classification_cnn dataset error. The dbpedia_csv.tar.gz file downloaded seems to be corrupt and the link for the dataset, i.e.
https://googledrive.com/host/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M/dbpedia_csv.tar.gz

is giving 404 error. The file dbpedia_csv.tar.gz seems to be downloaded but upon running tensorflow/tensorflow/examples/learn/text_classification_cnn.py, an error is produced giving

Successfully downloaded dbpedia_csv.tar.gz 1657 bytes.
Traceback (most recent call last):
  File ""main.py"", line 122, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 88, in main
    'dbpedia', test_with_fake_data=FLAGS.test_with_fake_data, size='large')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py"", line 64, in load_dataset
    return DATASETS[name](size, test_with_fake_data)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py"", line 48, in load_dbpedia
    maybe_download_dbpedia(data_dir)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py"", line 40, in maybe_download_dbpedia
    tfile = tarfile.open(archive_path, 'r:*')
  File ""/usr/lib/python2.7/tarfile.py"", line 1678, in open
    raise ReadError(""file could not be opened successfully"")
tarfile.ReadError: file could not be opened successfully
"
215,23455,1,"Excessive MKL memory consumption with variable sized tensors. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Both source and binary
- TensorFlow version (use command below): 1.11.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): gcc 5.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
During inference of variable sized examples with Tensorflow MKL system memory consumption gradually increases.
If a server application is running long enough to go through a lot of examples of same rank but different shapes it would eventually run out of system memory.
After further investigation setting the [TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/util/mkl_util.h#L2041) to either 0 or 1 does not help.
Most MKL primitive factories ignore TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE and even some that know about it may choose to ignore it based on other heuristics like [batches smaller than 32](https://github.com/tensorflow/tensorflow/commit/86d9ce130c5691cdba16024f7cc7987082acd294#diff-192dfeafbdd684934bdb0dfa8983674a) for example (I am not sure this is intended behavior).
These [""do_not_cache"" heuristics](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/core/kernels/mkl_conv_ops.cc#L902
) on the other hand don't seem to follow the described logic in the comments above replacing ""or"" for ""and"".

**Describe the expected behavior**
There should be a way to disable MKL primitive memory reuse globally with an environment variable be it TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE or better yet something else.
Primitive memory reuse does not play nice with unknown tensor sizes as most of the time there are no cache hits and allocated memory is simply piled up in the cache.

**Code to reproduce the issue**
I've attached a small example.
[min-leak-example.txt](https://github.com/tensorflow/tensorflow/files/2542973/min-leak-example.txt)

**Other info / logs**
I've attached Valgrind results of the example running on TF version 1.11.0 with and without MKL.
[min-leak-example-mkl.txt](https://github.com/tensorflow/tensorflow/files/2542977/min-leak-example-mkl.txt)
[min-leak-example-no-mkl.txt](https://github.com/tensorflow/tensorflow/files/2542978/min-leak-example-no-mkl.txt)

I've been working on a solution I can propose.
Should you find it reasonable I've extended the solution to full refactoring of the MKL primitive factories."
794,23252,0,"Tensorflow Docker files for 1.12 don't have XLA built binaries included.. Using  and running the [example script](https://www.tensorflow.org/performance/xla/jit) shows that the tensorflow included in the docker image isn't built with XLA.

However, if you manually pip install tensorflow:  then run the example script it shows XLA support is built into the binary.

Please can someone update the Docker files, I would do but it's far from obvious where the arg would be inserted.

cc @angersson 

"
1287,8391,0,"tf.lbeta() error when fed with placeholder. Hi, there seems to be a bug in .


"
81,24232,1,"tensorflow.keras: evaluate with custom loss and metric gives wrong output. This issue is about tensorflow.keras

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip virtual env
- TensorFlow version (use command below):('v1.11.0-0-gc19e29306c', '1.11.0')
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Geforce 940mx

**Describe the current behavior**
I have a custom metric

while training I get the following output:

> Train on 161532 samples, validate on 40385 samples
> Epoch 1/5
>  - 132s - loss: 3.6781 - root_mean_squared_error: 3.6781 - val_loss: 3.6463 - val_root_mean_squared_error: 3.6463
> Epoch 2/5
>  - 129s - loss: 3.6528 - root_mean_squared_error: 3.6528 - val_loss: 3.6287 - val_root_mean_squared_error: 3.6287
> Epoch 3/5
>  - 143s - loss: 3.6352 - root_mean_squared_error: 3.6352 - val_loss: 3.6210 - val_root_mean_squared_error: 3.6210
> Epoch 4/5
>  - 134s - loss: 3.6223 - root_mean_squared_error: 3.6223 - val_loss: 3.6369 - val_root_mean_squared_error: 3.6369
> Epoch 5/5

after running 
I get the following output:
> [3.028181584421266, 3.028181584421266]
for  
> ['loss', 'root_mean_squared_error']

after running

I get
> 3.7112323807806966def my_mean_squared_error(y_true, y_pred):
    return K.mean(K.square(y_pred - y_true))model.compile(optimizer=my_opt, loss=my_loss,
                  metrics=[my_mean_squared_error,""mse""])mean_squared_error(val_label,np.squeeze(model.predict(val_data, batch_size=256)))`
it works fine.

so it seems, that there is something wrong with K.sqrt()"
153,23345,1,"how to use DATASET to replace function feed_dict,because it is so slow. when I learn a tensorflow project,find one line code:



But, this line code It took a lot of time. (use CPU need 15 seconds...┭┮﹏┭┮)

By consulting information, I find use function 'dataset' could solve this problem which took a lot of time, How should I use it?

source of 'blob':



source of 'output_cls_prob'&'output_box_pred'&'input_img':



Parameter type：

> blob：type 'numpy.ndarray'
> 
> output_cls_prob：class 'tensorflow.python.framework.ops.Tensor'
> 
> output_box_pred：class 'tensorflow.python.framework.ops.Tensor'
> 
> input_img：class 'tensorflow.python.framework.ops.Tensor'

These codes are used for model computation.The model has been trained.

If you see the same problem on Stack Overflow，Yes, yes, that's what I asked myself.\(^o^)/~
"
915,14944,0,"[BUG]embedding_lookup can't convert out of index into zeros vector when embedding matrix is placed on CPU . ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:  8.0 / 6
- **GPU model and memory**: nvidia geforce gtx 1080 ti
- **Exact command to reproduce**:

### Describe the problem

embedding_lookup can't convert out of index into zeros vector when embedding matrix is placed on CPU.

when the matrix is placed on GPU, embedding_lookup method automatically convert out_of_index components into zeros vector, but it doesn't work when the matrix is placed on CPU


### Source code / logs

import tensorflow as tf

inputs = tf.placeholder(tf.int32, [None, None])
with tf.device('/cpu'):  ### when tf.device('/gpu') it's okay
    embedding_matrix = tf.get_variable('embedding_matrix', [5, 2],
                                            dtype=tf.float32,
                                            initializer=tf.contrib.layers.xavier_initializer())
    embedded = tf.nn.embedding_lookup(embedding_matrix, inputs)



inputs_test = [[1],[2],[10]


]

sess = tf.Session()
sess.run(tf.global_variables_initializer())
res = sess.run(embedded,feed_dict = {inputs:inputs_test})
print(res)
"
445,15689,0,"TensorFlowInferenceInterface: readNodeFloat error. This is part of my Tensorflow frozen graph, I have named the input and output nodes.

    >>> g.ParseFromString(open('frozen_graph.pb','rb').read())
    >>> g
    node {
      name: ""input""
      op: ""Placeholder""
      attr {
        key: ""dtype""
        value {
          type: DT_FLOAT
        }
      }
      attr {
        key: ""shape""
        value {
          shape {
            dim {
              size: -1
            }
            dim {
              size: 68
            }
          }
        }
      }
    }
    ...
    node {
      name: ""output""
      op: ""Softmax""
      input: ""add""
      attr {
        key: ""T""
        value {
          type: DT_FLOAT
        }
      }
    }

I ran this model by the following code
(CELL is name of directory where my file is located)

    final String MODEL_FILE = ""file:///android_asset/"" + CELL + ""/optimized_graph.pb"" ;
    final String INPUT_NODE = ""input"" ;
    final String OUTPUT_NODE = ""output"" ;
    final int[] INPUT_SIZE = {1,68} ;
    float[] RESULT = new float[8];
    
    inferenceInterface = new TensorFlowInferenceInterface();
    inferenceInterface.initializeTensorFlow(getAssets(),MODEL_FILE) ;
    inferenceInterface.fillNodeFloat(INPUT_NODE,INPUT_SIZE,input);

and finally

    inferenceInterface.readNodeFloat(OUTPUT_NODE,RESULT);


But I get this error in Log

    12-28 16:42:48.622 9890-12178/com.getfocus.signalsimilarity I/native: tensorflow_inference_jni.cc:151 Initialization done in 52.275ms
    12-28 16:42:51.048 9890-12178/com.getfocus.signalsimilarity E/native: tensorflow_inference_jni.cc:170 Output [output] not found, aborting!


I have searched a lot for the solution but nothing seems to solve this. Thanks in advance"
1256,21585,0,"Freezing a graph from a .pb file. Hello!
This is not a bug or a feature request but I cannot find answers on stackoverflow and have tried multiple ways of fixing this. This is my stack-overflow question: https://stackoverflow.com/questions/51826706/tensorflow-load-a-pb-file-and-then-save-it-as-a-frozen-graph-issues
I went to the discussion forums and after finding some bugs in that code got redirected here. 
This is the code that I have tried:


The error I get:


Thank you for your time!"
1238,12912,0,"T-SNE Suddenly Not Working in Projector. Hello,

I'm sorry this isn't a technical question, but just this week I've noticed that T-SNE in the tensorboard projector is no longer clustering data properly. I am running the same vectors I did a few weeks ago, with the same perplexity. While before I saw distinct clusters, now the points are all forming an indistinct ball. Has anyone else noticed this?"
673,20431,0,"'VBN' object has no attribute 'dtype'. Its broken or I do wrong something?

version 1.9.0-rc1

i = tf.layers.conv2d_transpose(i, filters, 4, strides=2, activation=tf.nn.leaky_relu, padding='same')
i = tf.contrib.gan.features.VBN(i)"
1223,15957,0,"Switching branch and run ./configure does not regenerate spec.json. When building from source with TensorFlow and switch to another branch, error returned even if I rerun :




I think the issue is that  is not updated when running 




------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) 
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:



You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

  "
906,6071,0,"[Feature request] To add the ability to use own data during the training a RNN network for translation by the translate.py script. Right now the script [translate.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py) does not allow to use own data for training. At the moment it works only with the data, that the script downloads from the Internet (en/fr). Also there is not any documentation on the format of the input data that it expects in the input. In order to have the ability to train the model with own data 2 changes need to be made to the script:
* to document the format of the input data;
* enable the ability to pass an own input file path via the script arguments, the file path should be used for training instead of downloading the data from the internet."
1475,35221,0,"Crash on Hexagon Delegate. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Mi A2, Pixel3
- TensorFlow version (use command below): 1.15.0
- Python version: 
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source):

**Describe the current behavior**
I've built the dsp delegate aar.
But on my miA2, I always got the following crash in native code.
Is there anything I can do to debug with the cc files?
Also, I tried on Pixel3, it returned this device does not support hexagon delegate.
It doesn't seem to be normal for a snapdragon 845 device.

"
278,27135,1,"TFLite got same result whatever feed any image. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 1+6T
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1050 Ti


**Describe the current behavior**
I used tf.keras built a FaceNet model and try to deploy it in Android TFLite, the processing of the transforming(from tf.keras to tflite) was smoothed, but when I try to deploy it in Android TFLite, whatever I feed any image, the Interpreter always send me the same result
**Describe the expected behavior**

**Code to reproduce the issue**


[converted_model.zip](https://github.com/tensorflow/tensorflow/files/3006839/converted_model.zip)




**Other info / logs**
The FaceNet has it own loss function, so I set compile=False in the tflite conventor source code.

@wangtz "
1149,13707,0,"cd /. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
212,23957,1,"Inconsistent results between estimator and graph/session. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.17134
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.12.0
- Python version:3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0
- GPU model and memory: GTX1070, 8Gb


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I implemented the same model (2-layer fully-connected neural network + batch norm to reduce initialization effects) using 
1. Low level APIs (graphs and sessions), and
2. High level APIs (estimators)
I tested the two implementation and tested it on the MNIST dataset. The Estimator shows a much slower training and a significantly lower final recognition error. (Estimator: First epoch~92%, Final~96%,  Sess+Graph: First epoch~96%, Final>98%)
**Describe the expected behavior**
The performance should be similar.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

The High-Level API

The Low-Level API:
"
232,19892,1,"distributed estrimator hang forever.. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  2.7.13
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I ran the mnist code with estimator, It's ok when running in single process, but hang when running in 3 processes for distributed mode. The code (PS) is shown below. Notice that, worker should change the task.type in TF_CONFIG to 'worker', and chief should change to 'chief'.



### Source code / logs
import tensorflow as tf

import numpy as np

import pandas as pd

from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt

import matplotlib.cm as cm

tf.logging.set_verbosity(tf.logging.INFO)

def load2():
    train = pd.read_csv('/tmp/train.csv')

    test = pd.read_csv('/tmp/test.csv')

    labels = train['label']

    images = train.iloc[:, 1:]

    image_size = 28

    train_ds, valid_ds, train_labels, valid_labels = train_test_split(images, labels, test_size=0.33, random_state=42)

    print len(train_ds)
    print len(valid_ds)
    print len(train_labels)
    print len(valid_labels)

    data = images

    train_ds, train_labels = reformat(train_ds, train_labels)
    valid_ds, valid_labels = reformat(valid_ds, valid_labels)
    test_ds, test_labels = reformat(test, train_labels[:len(test)])

    print('Training set', train_ds.shape, train_labels.shape)
    print('Validation set', valid_ds.shape, valid_labels.shape)
    print('Test set', test_ds.shape, test_labels.shape)


def reformat(dataset, labels):
    num_channels = 1
    num_labels = 10
    dataset = dataset.values.reshape(
        (-1, image_size, image_size, num_channels)).astype(np.float32)
    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)
    return dataset, labels


def cnn_model_fn(features, labels, mode):
    input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])
    conv1 = tf.layers.conv2d(
        inputs=input_layer,
        filters=32,
        kernel_size=[5, 5],
        padding=""same"",
        activation=tf.nn.relu
    )

    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)  # 14*14*32

    conv2 = tf.layers.conv2d(
        inputs=pool1,
        filters=64,
        kernel_size=[5, 5],
        padding='same',
        activation=tf.nn.relu
    )

    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)
    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])

    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

    logits = tf.layers.dense(inputs=dropout, units=10)
    print '####', logits

    predictions = {
        'classes': tf.argmax(logits, axis=1),
        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')
    }

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

    if mode == tf.estimator.ModeKeys.TRAIN:
        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
        train_op = optimizer.minimize(
            loss=loss,
            global_step=tf.train.get_global_step()
        )
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

def main(argvs):
    mnist = tf.contrib.learn.datasets.load_dataset(""mnist"")
    train_data = mnist.train.images
    print train_data.shape
    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
    eval_data = mnist.test.images
    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)
    mnist_classifier = tf.estimator.Estimator(
        model_fn=cnn_model_fn, model_dir=""/tmp/mnist_convnet_model"")
    # Set up logging for predictions
    # Log the values in the ""Softmax"" tensor with label ""probabilities""
    tensors_to_log = {""probabilities"": ""softmax_tensor""}
    logging_hook = tf.train.LoggingTensorHook(
        tensors=tensors_to_log, every_n_iter=50)
    print 'labels: ', train_labels
    # Train the model
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": train_data},
        y=train_labels,
        batch_size=100,
        num_epochs=None,
        shuffle=True)
    mnist_classifier.train(
        input_fn=train_input_fn,
        steps=20000,
        hooks=[logging_hook])
    eval_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": eval_data},
        y=eval_labels,
        num_epochs=1,
        shuffle=False)


if __name__ == '__main__':
  import os,json
  os.environ['TF_CONFIG']=json.dumps({
      ""cluster"": {
        ""ps"": [
          ""127.0.0.1:34567""
        ],
        ""chief"": [
          ""127.0.0.1:34568""
        ],
        ""worker"": [
          ""127.0.0.1:34569""
        ]
      },
      ""task"": {
        ""index"": 0,
        ""type"": ""ps"" # optional: chief, ps, worker
      }
  })
  tf.app.run()`




WARNING:tensorflow:From ps_cnn_mnist.py:94: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data.
WARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST-data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST-data/train-labels-idx1-ubyte.gz
Extracting MNIST-data/t10k-images-idx3-ubyte.gz
Extracting MNIST-data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
(55000, 784)
INFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'127.0.0.1:34567'], u'chief': [u'127.0.0.1:34568'], u'worker': [u'127.0.0.1:34569']}, u'task': {u'index': 0, u'type': u'ps'}}
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': u'ps', '_train_distribute': None, '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff7169d49d0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_master': u'grpc://127.0.0.1:34567', '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/mnist_convnet_model', '_global_id_in_cluster': 2, '_save_summary_steps': 100}
labels:  [7 3 4 ... 5 6 8]
INFO:tensorflow:Calling model_fn.
#### Tensor(""dense_1/BiasAdd:0"", shape=(100, 10), dtype=float32, device=/job:ps/task:0)
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized."
1486,21472,0,"RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7. **Environment:**
-  MacOS (High Sierra)
-  Python 3.7
-  Tensorflow 1.9.0

After I've imported tensorflow, I've got warning message link this


And I've tried to run demo.py from this repo https://github.com/mystic123/tensorflow-yolo-v3
still got error message
 


"
1443,31005,0,"""Not found: Resource does not exist"" exception thrown in runtime. I am facing a similar error mentioned above. I will try my best to help resolve this issue as it benefits my work as well. It is the same problme with issue #22631, 

OS Platform and Distribution: Linux Ubuntu x86_64 - 4.15.0-52-generic (kernel)
TensorFlow installed from: conda 4.7.5
TensorFlow version: 1.13.1
Bazel version: N/A
CUDA/cuDNN version: 10.0
GPU model and memory: Tesla V100-SXM2-16GB
Exact command to reproduce:
Mobile device: N/A

[tf_error.log](https://github.com/tensorflow/tensorflow/files/3415129/tf_error.log)
"
352,8034,0,"SparseTensor constructor change not noted as a breaking change in 1.0. The  keyword argument of the  constructor changes its name to  between Tensorflow 0.12 ([relevant API document](https://www.tensorflow.org/versions/r0.12/api_docs/python/sparse_ops/sparse_tensor_representation#SparseTensor)) and Tensorflow 1.0 ([relevant API document](https://www.tensorflow.org/versions/r0.11/api_docs/python/sparse_ops/sparse_tensor_representation#SparseTensor)).  However, this is neither documented on [the migration page](https://www.tensorflow.org/install/migration) nor handled by the migration script.

"
388,20833,0,"Coordinator stopped with threads still running. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: NV-P40
- **Exact command to reproduce**: N/A

### Describe the problem
Running distributed tensorflow using estimator in sync mode, there is always a exception after the last training step, as followed:
Session::Close()Session::Close()"
211,28563,1,"TFLite GPU delegate produces very different results. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu 16.04)
- Mobile device if the issue happens on mobile device: LG V30 Android 8.0.0
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 0.0.0-gpu-experimental
- Python version: 3.6.5

**Describe the current behavior**

When DeepLab [mobilenetv2_coco_voc_trainaug](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz) is run on a sample image using GPU delegate, the result is significantly different to that generated using CPU. The TFLite model was converted using:


And, here's the [TFLite model](https://drive.google.com/file/d/1rlD4uBxKegUuWVPBzllFVTQXlJTNb1g0/view?usp=sharing) used.

CPU gives the following (using random colours):

![Screenshot_2019-05-10-13-30-19](https://user-images.githubusercontent.com/14197204/57501503-2438c900-732b-11e9-930d-0f9ba51451fe.png)


GPU delegate gives the following (using random colours):

![Screenshot_2019-05-10-13-29-13](https://user-images.githubusercontent.com/14197204/57501509-2733b980-732b-11e9-824c-bb9c360926ee.png)

Basically, all pixels here are (wrongly) identified to be of the same class (at index 0) because the scores for class 0 are always the highest.

Similarly, if [mobilenetv2_ade20k_train](http://download.tensorflow.org/models/deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz) is used, the two also give very different results:

CPU gives the following (using random colours):

![Screenshot_2019-05-10-04-10-59](https://user-images.githubusercontent.com/14197204/57530071-c4b5da00-7379-11e9-94f1-2d87282eb9eb.png)


GPU delegate gives the following (using random colours):

![Screenshot_2019-05-10-04-09-41](https://user-images.githubusercontent.com/14197204/57530083-cc757e80-7379-11e9-859e-41804569b851.png)

Here's the [TFLite model](https://drive.google.com/file/d/1RdHs85WPxL1u0TUlPHwCqdY6KOn5XuYe/view?usp=sharing) used.

Indeed, the models use operations like , but as far as I know these operations should only affect speed performance.

**Describe the expected behavior**

CPU and GPU delegate should produce the same masks."
272,12789,1,"Zero accuracy if shuffle is False in TF Keras ImageDataGenerator. If I use the TF Keras reimplementation (tensorflow.contrib.keras) and set the ImageDataGenerator's shuffle param to False, I get zero accuracy every time. Also this:

I've just used for the first time the ModelCheckpoint from function to save the best model (best_model = True) and wanted to test its performance. When the model was saved it said that the ""val_acc"" was at 83.3% before saving. I loaded the model and used the evaluate_generator on validation_generator but the result for ""val_acc"" was 0.639. I got confused and used it again and got 0.654 and then 0.647, 0.744 and so on. Questions are:

1. Am I loading the model correctly, and if not what did I miss? Why is the result so much different?
2. Why are the results between different evaluate_generator executions different (no retraining is happening, just shear execution of predict_generator multiple times in a row)?
important part of the code (ResNet50 fine-tuning):


"
229,35249,1,"Wrong Accuracy value for Training data in tutorial.... ## URL(s) with the issue:
https://www.tensorflow.org/tutorials/keras/classification/

## Description of issue (what needs changing):

Under ""Train the model"" in ""Build the model"", the accuracy of the model on training data after 10 epochs is 0.91(91%) while it is mentioned as 0.88(88%).

### Clear description

Since, it is already mentioned in the tutorials that the model overfits the training data, thus the accuracy on training data should be more than that on testing data(88.3%).
### Submit a pull request?

If this issue is alright, I'll be glad the submit a PR right away...
Thanks for the help!
"
550,32761,0," keras.metrics.Accuracy != keras.metrics.accuracy. The following test 


does not pass because 
https://github.com/tensorflow/tensorflow/blob/3d5e79e08ae299812e0eaf6183f4886591e932bd/tensorflow/python/keras/metrics.py#L576-L577

seems to be doing two casts they should not.

Also it is unclear to me what is the expected value for  (which I would have understood to be 1, but currently returns 1/3).




"
104,13530,1,"Pandas_input_fn slow, starving CPU/GPU. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: It is a customized version of the Deep & Wide example code. Fairly close to original code.
- **OS Platform and Distribution: Windows Server 2012 R2
- **TensorFlow installed from: nightly build WHL through pip (this was tried after numerous other versions, including install through pip)
- **TensorFlow version (use command below)**: b'unknown' 1.4.0-dev20170926
- **Python version**: 3.5 and 3.6
- **Bazel version (if compiling from source)**: Not compiling
- **CUDA/cuDNN version**: CUDA 8, CUDnn 6.1
- **GPU model and memory**: Tesla M60 GPU 8GB
- **Exact command to reproduce**:  See attached Script.
-For the record, the server vm has 8 xeon physical cores and 240 gb ram allocated. The CPU only machine is a new skylake i7 with 32gb ram.

### Describe the problem
To start, I submitted to stack overflow (https://stackoverflow.com/questions/46457476/tensorflow-pandas-input-fn-slow-starving-cpu-gpu) and have not been able to garner assistance after multiple edits to make sure it was framed correctly. I truly believe this is a bug since I am sticking so close to the example code, but if I have made a mistake I am deeply sorry to all of you.

I am working on a wide and deep model following the framework in the Tensorflow Wide and Deep tutorial (https://www.tensorflow.org/tutorials/wide_and_deep). Model works fine when built the old way (load entire dataset from pandas, convert to tensors, feed in input_fn) which is ok for running on a CPU. 

However, to make it work on the GPU the dataset is too large to fit into GPU memory, so batching is necessary. I tried using the pandas_input_fn to batch data to the video card and noticed I get spikes of activity followed by long lulls while the next batch is prepared. The odd thing is, this happens even if I run it on a machine with CPU only. The lulls are almost the exact same length, so it isn't simply the video card crushing through a simple model faster than the proc can deliver it. It seems like it is always waiting to begin loading the next batch until the last one is done training. 

(If this function simply cannot be used in this way, can we get an example of Deep and Wide using the dataset API? or a manual build of deep and wide using layers and queues? At the moment, the example code for the dataset api using make_one_shot_iterator for canned estimators doesn't run.)

I increased the complexity of the model to make sure it wasn't too easy to compute and still have the same issue. I have tried increasing the number of threads allocated to pandas_input_fn, I have tried increasing the queue size to far larger than seems reasonable (10x dataset size) which helps a bit, but not much. I am not sure if the slowdown is when it is queueing or de-queueing, but I have been unable to solve the issue after two weeks of troubleshooting. The data I am working with is 117 columns, 400k rows.

I have created a generic script that generates fake values to simulate the problem. However, there are far fewer fake columns than real ones, so the gap between steps is not nearly as long, but still noticeable. Code attached.


--

### Source code / logs
attached
[pandas_input_example.txt](https://github.com/tensorflow/tensorflow/files/1363335/pandas_input_example.txt)
"
1195,18205,0,"Distribution Strategy not working with tf-nightly-gpu for Python3.5. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I am using the script https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_estimator_example.py provided as an example for DistributionStrategy API
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  4.13.0-37-generic #42~16.04.1-Ubuntu SMP Wed Mar 7 16:03:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux ""16.04.4 LTS (Xenial Xerus)""
- **TensorFlow installed from (source or binary)**: binary through pip3 install tf-nightly-gpu
tf.VERSION = 1.8.0-dev20180402
tf.GIT_VERSION = v1.7.0-rc1-1091-gc7a04561fb
tf.COMPILER_VERSION = v1.7.0-rc1-1091-gc7a04561fb
Sanity check: array([1], dtype=int32)
- **TensorFlow version (use command below)**: v1.7.0-rc1-1091-gc7a04561fb 1.8.0-dev20180402
- **Python version**: 3.5
- **CUDA/cuDNN version**: 
CUDA version: 9.0, V9.0.176
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 0
#define CUDNN_PATCHLEVEL 5
- **GPU model and memory**: Quadro M6000 24GB
- **Exact command to reproduce**: python3 

### Describe the problem
I am trying to test the DistributionStrategy API. In order to do that I downloaded the tensorflow nightly build by executing  
pip3 install tf-nightly-gpu
Then I tried to execute the example provided here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_estimator_example.py
But it is not working, there is an exception shortly after the script starts to run ( I am copying the stack trace in the next section).
I have tried to do the same using the  tensorflow nightly build for python 2.7 downloaded by executing  
pip install tf-nightly-gpu and it works without any problem.
The issue here, is that I would like to integrate this API with my multi-gpu training and inference processes, which are complete written for python3. 
I would like to know, if DistributionStrategy API is already supported in for python 3.5 and the problem is that I am using a wrong example. Or, in case it is not supported yet, if there are plans to do it.

Thanks in advance.


### Source code / logs
(vtf_nightly_gpu) /ccrespo/mirrored_strategy/src$ python3 mirrored_strategy_test.py 
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmou3ft0t
2018-04-03 16:17:32.334622: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-04-03 16:17:32.686755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: Quadro M6000 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.114
pciBusID: 0000:03:00.0
totalMemory: 23.90GiB freeMemory: 23.35GiB
2018-04-03 16:17:32.956384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: 
name: Quadro M6000 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.114
pciBusID: 0000:04:00.0
totalMemory: 23.90GiB freeMemory: 23.78GiB
2018-04-03 16:17:33.215141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 2 with properties: 
name: Quadro M6000 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.114
pciBusID: 0000:a1:00.0
totalMemory: 23.90GiB freeMemory: 23.78GiB
2018-04-03 16:17:33.215663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2
2018-04-03 16:17:34.314104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-03 16:17:34.314172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 
2018-04-03 16:17:34.314181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y N 
2018-04-03 16:17:34.314185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N N 
2018-04-03 16:17:34.314190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   N N N 
2018-04-03 16:17:34.315429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 22663 MB memory) -> physical GPU (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0, compute capability: 5.2)
2018-04-03 16:17:34.802106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 23083 MB memory) -> physical GPU (device: 1, name: Quadro M6000 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2018-04-03 16:17:35.250349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 23083 MB memory) -> physical GPU (device: 2, name: Quadro M6000 24GB, pci bus id: 0000:a1:00.0, compute capability: 5.2)
2018-04-03 16:17:35.726606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2
2018-04-03 16:17:35.726819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-03 16:17:35.726838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 
2018-04-03 16:17:35.726849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y N 
2018-04-03 16:17:35.726858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N N 
2018-04-03 16:17:35.726867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   N N N 
2018-04-03 16:17:35.727458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22663 MB memory) -> physical GPU (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0, compute capability: 5.2)
2018-04-03 16:17:35.727626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 23083 MB memory) -> physical GPU (device: 1, name: Quadro M6000 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2018-04-03 16:17:35.727819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 23083 MB memory) -> physical GPU (device: 2, name: Quadro M6000 24GB, pci bus id: 0000:a1:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File ""mirrored_strategy_test.py"", line 86, in <module>
    tf.app.run()
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""mirrored_strategy_test.py"", line 70, in main
    estimator.train(input_fn=input_fn, steps=10)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 363, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 841, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 884, in _train_model_distributed
    self.config)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/distribute.py"", line 751, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 254, in _call_for_each_tower
    coord.join(threads)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 248, in _call_for_each_tower
    self, *merge_args, **merge_kwargs)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 667, in _distributed_apply
    reduced_grads = distribution.batch_reduce(""sum"", grads_and_vars)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/python/training/distribute.py"", line 796, in batch_reduce
    return self._batch_reduce(method_string, value_destination_pairs)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 295, in _batch_reduce
    value_destination_pairs)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 175, in batch_reduce
    return self._batch_reduce(method_string, value_destination_pairs)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 462, in _batch_reduce
    [v[0] for v in value_destination_pairs])
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 517, in _batch_all_reduce
    method_string)
  File ""/usr/local/share/methinks/vtf_nightly_gpu/lib/python3.5/site-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 276, in _ungroup_and_make_mirrored
    index[i][destinations[d]] = v
TypeError: 'dict_keys' object does not support indexing
"
30,21048,1,"High loss of accuracy after coverting "".pb"" to "".lite"" on Android. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I only modified the ImageClassifier.java to make it compatible with float and quant.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04.3 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
LG G4
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.7.1
- **Python version**:2.7.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

### Describe the problem
Hi,
I retrained a module refer to [tensorflow-for-poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0), and its retrain.py was replaced by [this one](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py).

I did two experiments:

**1. Choose the module: mobilenet_1.0_224_quant**
I retrained the module with the command below:

> python -m scripts.retrain \
  --architecture=mobilenet_1.0_224_quant \
  --bottleneck_dir=tf_files/bottlenecks \
  --how_many_training_steps=500 \
  --model_dir=tf_files/models/ \
  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224_quant \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --image_dir=tf_files/flower_photos

Then I converted "".pb"" to "".lite"" with this command:

> toco \
  --input_file=tf_files/retrained_graph.pb \
  --output_file=tf_files/optimized_graph_quant.lite \
  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape=""1,224,224,3"" \
  --input_array=input \
  --output_array=final_result \
  --std_value=128 --mean_value=128

I tested _optimized_graph_quant.lite_ with the _TfLiteCameraDemo_ which provided by tensorflow-for-poets.
And only once inference, I found that the accuracy is very good:

> 01-01 01:12:37.942 12117 12217 D TfLiteCameraDemo: Timecost to run model inference: 354
01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: textToShow = 354ms
01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: roses: 1.00
01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: sunflowers: 0.00
01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: tulips: 0.00


**2. Choose the module: mobilenet_1.0_224**
I retrained the module with the command below:

> python -m scripts.retrain \
  --architecture=mobilenet_1.0_224 \
  --bottleneck_dir=tf_files/bottlenecks \
  --how_many_training_steps=500 \
  --model_dir=tf_files/models/ \
  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224 \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --image_dir=tf_files/flower_photos

Then I converted "".pb"" to "".lite"" with this command:

> toco \
  --input_file=tf_files/retrained_graph.pb \
  --output_file=tf_files/optimized_graph.lite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --input_shape=""1,224,224,3"" \
  --input_array=input \
  --output_array=final_result \
  --inference_type=FLOAT \
  --input_data_type=FLOAT

I also tested optimized_graph.lite with the _TfLiteCameraDemo_ which provided by tensorflow-for-poets.
And after multiple inferences, the accuracy increased to an acceptable value :

> 01-01 01:29:23.221 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 510
01-01 01:29:23.226 13207 13224 D TfLiteCameraDemo: roses: 0.06
01-01 01:29:23.483 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 238
01-01 01:29:23.488 13207 13224 D TfLiteCameraDemo: roses: 0.18
01-01 01:29:23.741 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 221
01-01 01:29:23.746 13207 13224 D TfLiteCameraDemo: roses: 0.31
01-01 01:29:24.162 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 398
01-01 01:29:24.167 13207 13224 D TfLiteCameraDemo: roses: 0.45
01-01 01:29:24.527 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 336
01-01 01:29:24.533 13207 13224 D TfLiteCameraDemo: roses: 0.58
01-01 01:29:24.898 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 339
01-01 01:29:24.907 13207 13224 D TfLiteCameraDemo: roses: 0.68
01-01 01:29:25.274 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 326
01-01 01:29:25.280 13207 13224 D TfLiteCameraDemo: roses: 0.76
01-01 01:29:25.646 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 340
01-01 01:29:25.652 13207 13224 D TfLiteCameraDemo: roses: 0.83
01-01 01:29:26.032 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 354
01-01 01:29:26.041 13207 13224 D TfLiteCameraDemo: roses: 0.87
01-01 01:29:26.433 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365
01-01 01:29:26.438 13207 13224 D TfLiteCameraDemo: roses: 0.91
01-01 01:29:26.831 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365
01-01 01:29:26.838 13207 13224 D TfLiteCameraDemo: roses: 0.93
01-01 01:29:27.288 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 391
01-01 01:29:27.296 13207 13224 D TfLiteCameraDemo: roses: 0.95
01-01 01:29:27.658 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 333
01-01 01:29:27.663 13207 13224 D TfLiteCameraDemo: roses: 0.97
01-01 01:29:28.005 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 313
01-01 01:29:28.010 13207 13224 D TfLiteCameraDemo: roses: 0.97
01-01 01:29:28.381 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 341
01-01 01:29:28.385 13207 13224 D TfLiteCameraDemo: roses: 0.98

**Please help to analyze that why it needs multiple inferences to get an good accuracy here? Thanks!**
"
298,31965,0,"Crash-course issue. #31958  URL(s) with the issue:
https://developers.google.cn/machine-learning/crash-course/reducing-loss/video-lecture

## Description of issue (what needs changing):
On 1:50,it prompts me to do the gradient-descent practice，when i click the button,then redirect to the wrong page.

### Correct links

https://developers.google.cn/machine-learning/crash-course/reducing-loss/gradient-descent
"
777,29445,0,"Feature Columns stock example fails on GPU. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. this is a stock example, see collab notebook here to reproduce 
https://colab.research.google.com/drive/1O8dCWeYBVjFEax-ZK1XbJE_vfEzB2Ieq
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): '2.0.0-dev20190605'
- Python version: 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Collab

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
model.fit fails in the stock example with the following error:
InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Expected D2 of index to be 2 got 3 at position 1
	 [[node sequential/dense_features_6/age_bucketized_X_thal_indicator/SparseCross (defined at <ipython-input-20-bf1fb22dfeb0>:14) ]]
  (1) Invalid argument:  Expected D2 of index to be 2 got 3 at position 1
	 [[node sequential/dense_features_6/age_bucketized_X_thal_indicator/SparseCross (defined at <ipython-input-20-bf1fb22dfeb0>:14) ]]
	 [[sequential/dense_features_6/age_bucketized_X_thal_indicator/SparseToDense/_56]]
0 successful operations.
0 derived errors ignored. [Op:__inference_keras_scratch_graph_2134]

**Describe the expected behavior**

**Code to reproduce the issue**
https://colab.research.google.com/drive/1O8dCWeYBVjFEax-ZK1XbJE_vfEzB2Ieq


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
190,13171,1,"tf.reduce_*(mean/sum) runs very slow on GPU.  
I notice that tf.reduce_*(mean/sum) runs very slow on GPU in some cases, which can happen in the following simple examle:

x = tf.Variable(tf.ones([80, 80, 80, 80])) # 4-D tensor.
y = tf.reduce_sum(x, [0, 2, 3]) # Sum over all dims except the 2nd.

The execution time on GPU is very large and is approximate same as (or more than) the time on CPU, which probably means the GPU is not used at all. The same result can be obtained by choosing the other axes, except for the first and the last axes, in which case the execution on GPU is significantly faster than CPU.

Here is the [code](https://gist.github.com/vs-zhehangd/8a547094cfa0efc181b814bfb20b31ce) that reproduces the problem. You can run with  k=0,1,2,3 to select different axes.

I am using the following PC system:

* Kubuntu 16.04
* TensorFlow 1.2.1 ('v1.2.0-5-g435cdfc', '1.2.1')
* Python 2.7
* GeForce GTX 1080 Ti
* CUDA-8.0

The execution time on GPU, CPU, and NumPy is given as follows:

 exec time (s) | [1,2,3] | [0,2,3] | [0,1,3] | [0,1,2] |
|---------------|---------|---------|---------|---------|
| GPU           | 0.00918 | 0.40572 | 0.55388 | 0.01905 |
| CPU           | 0.05921 | 0.22461 | 0.56524 | 0.16172 |
| NumPy         | 0.24799 | 0.24847 | 0.24886 | 0.26601 |


Similar result was observed also on my laptop.

-------------------------------------------------------

I met this problem when I tried to implement Batch Normalization (initially when I ran tf.nn.moment(), then realized the key was tf.reduce_*). I wanted to collect means of different channels. Using NHWC data format the problem does not matter as C is the last dimension. However, slow execution occurs if I want to implement for NCHW format as C is the second dimension, this makes the execution time on Batch Normalization overwhelms all other ops such as convolutions. This is surely a nightmare for training and evaluation.

I hope it will be fixed if it is a bug. Or, if the reason is that the oprations are just not implemented for GPU by now, I wonder if there is a way to walk around."
1391,33298,0,"Not able to install tensorflow. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Windows 7):

- TensorFlow installed from (source or binary):
- TensorFlow version:1.13.1
- Python version:3.7.3
- Installed using virtualenv? pip? conda?: pip

- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

When i install tensorflow and i import tensorflow ,i get the folowing error:
Traceback (most recent call last):
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Admin\Anaconda3\envs\ptetensor\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
107,33498,1,"TFLite slower than Keras on RPi 4. When doing inference on a Raspberry Pi 4 with Keras and Tensorflow installed using , the inference time is slower using TFLite. 

The initial Keras model is about 20 mb - after converting it to TFLite it is about 2.4 mb. During inference the Keras model processes a sample in about 50 ms and TFLite does it in about 80 ms. 

Initially the pip-installed version of Tensorflow caused errors with TFLite, so I installed TFLite-runtime using this information: https://www.tensorflow.org/lite/guide/python



During inference in TFLite I use the following snippet:


Does anybody have any experience with TFLite on Raspberry Pi? Am I missing something in order accelerate inference further? It seems wrong that inference should be faster in Keras with 10x model size."
748,5407,0,"tf.contrib.metrics.streaming_precision doesn't accept predictions and labels of dtype tf.bool. According to documentation and comments in code, ""tf.contrib.metrics.streaming_precision"" should accept predictions and labels of boolean type, but it doesn't seem to be true.

To reproduce, modify testAllCorrect procedure in [this file](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/metrics/python/ops/metric_ops_test.py) by adding dtype=tf.bool to tf.constant, as below:


The test will fail:
`

This happens in _streaming_true_positives function in [this file](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/metrics/python/ops/metric_ops.py), when executing 

Can be repeated simply as:


### Environment info
Operating System: Ubuntu 14.04.1




"
243,25537,1,"TFLite model performance on Android is worse than the one on TFLite Model Benchmark Tool. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.13.0.dev20190126
- Android Studio version: 3.3
- TensorFlow Lite version on Android: 0.0.0-gpu-experimental

**Descrive the current behavior**
I tried running my custom classify model on Android Studio. This model works well, but the performance is bad. It takes about **300 msec** to classify a 224 x 224 image on Simulator. I measured the run time of  method.   
When I benchmark this model with TFLite Model Benchmark Tool, it takes about **34 msec** on desktop.  
What is the difference between these performances?

**The output of TFLite Model Benchmark Tool**



**Model file**
[classify.tflite.zip](https://github.com/tensorflow/tensorflow/files/2835017/classify.tflite.zip)
"
1438,32496,0,"[r2.0.0-rc1] Converting to TFLite format: <type == kTfLiteInt32/64 condition> was not true. ONE_HOT failed to prepare. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): **Source**
- TensorFlow version (use command below): **2.0.0-rc1 commit 59bf33**
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0.130
- GPU model and memory: NVidia 1080Ti / 11G

**Describe the current behavior**

Conversion of TF2.0 function containing  and  ops to TFLite format fails with the following RuntimeError. Source code of the program is listed in the  section below.



**Describe the expected behavior**

  finishes without errors

**Code to reproduce the issue**

File: 


**Other info / logs**
N/A"
118,28405,1," GPU utilization of these two Epoch is very low. I run https://www.tensorflow.org/alpha/tutorials/text/transformer in Calab with GPU.
The result is a strange phenomenon. The first two Epoch are very slow. I tried to run on my local machine and the result was the same. According to observations, the GPU utilization of these two Epoch is very low, almost zero. What happened to these two Epoch?


"
600,35010,0,"Memory leak with tf.py_function in eager mode using TF 2.0. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.3 64bit
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I customize a Keras layer and use  in the  of the layer. The used memory keeps increasing linearly with the number of calling the layer with inputs in eager mode. What's 

![image](https://user-images.githubusercontent.com/18071380/70585663-d5744a00-1bff-11ea-8d41-3649d035e9be.png)

**Describe the expected behavior**
The memory should keep stable.

**Code to reproduce the issue**


**other info**
We encounter this issue when developing a customized embedding layer in [ElasticDL](https://github.com/sql-machine-learning/elasticdl) and resolving the ElasticDL [issue 1567](https://github.com/sql-machine-learning/elasticdl/issues/1567)

"
733,32422,0,"[lite/micro] quantized dense layer is not supported.. The following error is thrown while running a quantized tf lite model converted from a tensorflow model using tf lite **micro**.



# Environment information
- The master branch is used to compile lite/micro.
- OS: Ubuntu 16.04
- Gcc version: 5.4.0

# tensorflow version
1.13.1

The script to convert the tensorflow model to a tflite model is as follows:



Output of the above script is:

"
1155,30341,0,"session still referable after sess.close(). From my intuition, a session will be removed from memory once a close() method is called. However, you can still refer to a session even after explicitly closing it. Here is a little example:



Above code gives me such result:



My question is, since a session is not supposed to be reopened, what's the point of preserving it in memory? Is it that the closed session is ready for reopening or is it because it's just a reference and occupys no resource so it makes no difference wether or not the session is removed?"
28,23604,1,"c_api.h Why GPU is slower than CPU. hello i tried the c_api .I have trained a lstm mode and i load the model in my c project to predict my data.I find the GPU is slowly than CPU.
gpu:nvidia TITAN X
CUDA：9.0
cudnn:7.0
cpu: intel E5
tensorflow: 1.11.0
i predicted about 200 data and every data call the function ：
TF_SessionRun(sess,
nullptr, // Run options.
&input_op, &input_tensor, 1, // Input tensors, input tensor values, number of inputs.
&out_op, &output_tensor, 1, // Output tensors, output tensor values, number of outputs.
nullptr, 0, // Target operations, number of targets.
nullptr, // Run metadata.
status // Output status.
);
every time GPU is slowly than CPU
Is my method wrong ?Is there a way to increase the speed? and can I enter data in batches for prediction?how to feed data in batches?
thanks"
1137,2594,0,"add tf.assert for GPU (maybe also host-memory Const[string] for GPU) was: dynamic_rnn GPU support error . When attempting to place dynamic_rnn on the gpu, I get the following error:



Full traceback:



It seems to have some problem with a tensor shape, but I can't deduce why. This error does not occur if I enclose the tf.nn.dynamic_rnn(...) call in a tf.device('/cpu:0') statement. I am on commit 0050a205bc5521a563ee66baa3b73373d4c0e62e from mid last week.

I am doing data parallel processing and am trying to keep as much as possible on each GPU.
"
834,7489,0,"how to use baidu wrap-ctc on tensorflow?. when i use tensorflow ctc, i find it is very slow,,,
so i want use baidu wrap-ctc on tensorflow,

but how to write code?

when i use tensorflow ctc, i write it:
    print logits.get_shape()
    logits = tf.reshape(logits, [batch_size, -1, nout])
    logits = tf.transpose(logits, (1, 0, 2))
    print 'logits shape is '
    print logits.get_shape()
    labels = tf.SparseTensor(indices=label_idx, values=label_vals, shape=label_shape)
    print 'init label'
    loss = ctc_ops.ctc_loss(inputs=logits, labels=labels, sequence_length=seq_len)
    print 'init loss'
    cost = tf.reduce_mean(loss)
    print 'init cost'
    decoded, log_prob = ctc_ops.ctc_greedy_decoder(logits, seq_len)
    print 'init decoded'
    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),
                                          labels))
    print 'init ler'"
1433,31500,0,"CUDA dll check not reporting correct version in comments. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
Line 76 in tensorflow/python/platform/self_check.py
        except OSError:
          raise ImportError(
              ""Could not find %r. TensorFlow requires that this DLL be ""
              ""installed in a directory that is named in your %%PATH%% ""
              ""environment variable. Download and install CUDA %s from ""
              ""this URL: https://developer.nvidia.com/cuda-90-download-archive""
              % (build_info.cudart_dll_name, build_info.cuda_version_number))

The URL directs you to version 9 but the current dll checks for version 10. 


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1365,17207,0,"TF Keras inference is way slower than Numpy. I'm working on a reinforcement learning model implemented with Keras and Tensorflow. I have to do frequent calls to model.predict() on single inputs.

While testing inference on a simple pretrained model, I noticed that using Keras' model.predict is WAY slower than just using Numpy on stored weights. Why is it that slow and how can I accelerate it? Using pure Numpy is not viable for complex models.

    import timeit
    import numpy as np
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import Dense
    
    w = np.array([[-1., 1., 0., 0.], [0., 0., -1., 1.]]).T
    b = np.array([ 15., -15., -21., 21.])
    
    model = Sequential()
    model.add(Dense(4, input_dim=2, activation='linear'))
    model.layers[0].set_weights([w.T, b])
    model.compile(loss='mse', optimizer='adam')
    
    state = np.array([-23.5, 17.8])
    
    def predict_very_slow():
        return model.predict(state[np.newaxis])[0]
    
    def predict_slow():
        ws = model.layers[0].get_weights()
        return np.matmul(ws[0].T, state) + ws[1]
    
    def predict_fast():
        return np.matmul(w, state) + b
    
    print(
        timeit.timeit(predict_very_slow, number=10000),
        timeit.timeit(predict_slow, number=10000),
        timeit.timeit(predict_fast, number=10000)
    )
    # 5.168972805004538 1.6963867129435828 0.021918574168087623
    # 5.461319456664639 1.5491559107269515 0.021502970783442876

I'm using Tensorflow for CPU, version 1.5.0 installed from pypi for python 3.5 on Windows 10."
1341,22193,0,"AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RaspberryPi stretch
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: https://www.tensorflow.org/install/install_linux
- **TensorFlow version (use command below)**: 0.11.0
- **Python version**: Python 3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I was trying to run facenet module on raspberrypi. and in facenet.py I am getting this error.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
[facenet.zip](https://github.com/tensorflow/tensorflow/files/2367106/facenet.zip)

"
1388,10268,0,"sparse_softmax_cross_entropy_with_logits gives NaN instead of error when using non-existent labels. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04

- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
v1.0.0-65-g4763edf-dirty
1.0.1

- **Bazel version (if compiling from source)**:
0.4.5

- **CUDA/cuDNN version**:
V8.0.61

- **GPU model and memory**:
NVIDIA GFORCE GTX 760 2GB

- **Exact command to reproduce**:
import tensorflow as tf
sess = tf.Session(0
sess.run(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=[ 100 ], logits=[[ 0.0, 1.0 ]]))
>>> array([ nan], dtype=float32)

### Describe the problem
Running above code gives NaN instead of an error when on Ubuntu but when I run the same code on Windows I correctly get an InvalidArgumentError error.

### Source code / logs
See code above."
725,11958,0,"[Bug?]Session Hang during training with 'mnist_replica.py' and learning rate as a placeholder. ### System information
 OS: Mac OS 
 TF: 1.2.0, pip install
 Python: 2.7.9
also tested on RHEL with TF 1.1 and Python 2.7.13, the problem still exists. I don't think it is system specific.


### Describe the problem
I am training the 'mnist-replica.py' provided on github. link is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py) 
and set up two workers and one parameter server on my laptop.
I made only one modification to the code:
 use learning_rate as a placeholder rather than a fixed value and feed 0.01 to it each step.
When I train it asynchronously, it is ok to go, but the session hangs in .run() after one or two steps when I train it in synchronous.

Can anyone figure out the reason of this?

### Source code / logs
the source code is 
[here](https://drive.google.com/open?id=0Bw4fA7bI0IScTVlsT3FidS0tMXM). just provide it for ease. It is almost the same as the original one."
364,27345,0,"[TF2.0] EstimatorV2 uses non existing export_savedmodel method. Dear tensorflowers,

as per the title, EstimatorV2's exporter.py calls a method that has been removed. The fix is pretty easy, but I'm not sure about the implications of the change. See more below. 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code, but using estimators
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.19.20-1rodete1-amd64 #1 SMP Debian 4.19.20-1rodete1 
- **TensorFlow installed from (source or binary)**: pip install --upgrade tensorflow==2.0.0alpha0
- **TensorFlow version (use command below)**: 2.0.0-alpha0
- **Python version**: 3.7.1

### Describe the problem
using  results in  being thrown.

2 small modifications to my local  file fix the issue. However, before submitting a PR i wanted to clarify that I had to:

1. Rename the method call to  and this poses no problem.
2. Remove the  [keyword argument](https://github.com/tensorflow/estimator/blob/d14b0dce35baea00f27e17d8a44690080abd7bce/tensorflow_estimator/python/estimator/exporter.py#L120), but I have no idea what this entails. I assume that, since you can't specify this argument anymore, it defaults to the default policy in TF 1.X of stripping the GraphDef default attributes. If that's the case, is there any action required?

#### Logs


##### Code

I'm trying to adapt the [CMLE template](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/cloudml-template) to TF 2.0 . I avoided putting the input_fns and model_fns since the model trains and evals correctly. The only problem is during the export.




Happy to provide any additional information!"
213,11160,1,"High variance in training convergence between keras (with tf backend) and tf.contrib.keras. 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.2
- **Python version**: 
3.6


### Describe the problem
I started to move my keras (Keras 2.0.4) scripts to tf.contrib.keras  (tf version 1.2) but I am achieving worse performance though the porting was seamless. Not sure why there is such huge discrepancy. in training performance

### Source code / logs
Original (Keras 2.0.4 code)


___This model achieved 96% accuracy in 1 epoch___ (which is expected)

Same code (tf.contrib.keras)



___This model struggles to achieve 58% accuracy after 1 epoch and 61% after 2 epochs__

Is there something different in terms of hyper-parameter settings that is required when switching between the 2 versions. 

"
681,29277,0,"[TF 2.0 API Docs] tf.nn.dropout. ## URL(s) with the issue:

https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/dropout

## Description of issue (what needs changing):

### Usage example

Usages are linked but none are detailed inline on the page

"
1118,17204,0,"HtoD takes 2.5x longer than D2H. I'm noticing that HtoD copies are taking significantly longer than DtoH.

IE, doing  vs 
100MB tensor takes 8ms on V100 to fetch (12.5GB/sec), but 21.67ms to feed (4.5 GB/sec)
 
Benchmark
https://github.com/diux-dev/cluster/blob/db10c890530e7ded9e4a933596803e3ae0de1db0/yuxin_numpy/square_minimize_cpu_pipeline.py

Is there a way to make it faster? (something to do with memory pinning?)

<img width=""1152"" alt=""screenshot 2018-02-22 17 47 07"" src=""https://user-images.githubusercontent.com/23068/36574018-6e853e74-17f8-11e8-80dd-7509e7c643a2.png"">

TensorFlow:
version: 1.5.0
__git_version__: v1.5.0-0-g37aa430d84
https://github.com/tensorflow/tensorflow/commit/37aa430d84
"
1070,21198,0,"'toco_from_protos: not found' error found using prebuilt tensorflow binary for Raspbian Stretch 9 on Raspberry Pi 3 Model B. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian Stretch 9
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Raspberry Pi 3 Model B
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:
tf.GIT_VERSION returns v1.9.0-0-g25c197e
- **Python version**:3.5.3
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:Please refer to Raspberry Pi 3 Model B specs, but I use the cpu only version of tensorflow.
- **Exact command to reproduce**:
use example code in tf-lite wiki... One sec...
# Converting a GraphDef from file. converter = lite.TocoConverter.from_frozen_graph( graph_def_file, input_arrays, output_arrays) tflite_model = converter.convert() open(""converted_model.tflite"", ""wb"").write(tflite_model)
This was taken from tf.contrib.lite.TocoConverter's API webpage.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
tl;dw2r run the script on the above specs and you get 'toco_from_protos: not found'
long story:
I am trying to convert a frozen pb file into a tflite format.  I have a Windows 10 system, but tflite isn't available yet on the Windows 10 binaries.  The only other system that I have available is a Raspberry Pi so I figured that its version of tensorflow should have access to the tflite segment of the api.  Sure enough, it was there.  But when I ran the script, the following error occurred.  I can try to use Ubuntu, but it's been a bit of a learning curve trying to manage all the systems I have built on my Windows computer with Linux alternatives.  If at all possible, I would like to have a solution on either of these devices.  

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
503,34375,0,"Run multi-worker with nccl error: NET/IB : collective mismatch error. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):   (docker image)
- TensorFlow version (use command below):  
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- NCCL version: 
- GPU model and memory:  Nvidia P40

**Describe the current behavior**

* worker A


* worker B


The error as follows:

"
411,15096,0,"java.lang.UnsatisfiedLinkError: No implementation found for void com.ppdai.tensorflow.tracking.ObjectTracker.initNative(int, int, boolean). recently i begin to learn and use tensorflow but some errors i don't know why,  help me please, error as follow:

Process: com.ppdai.tensorflow, PID: 4226
                                                 java.lang.UnsatisfiedLinkError: No implementation found for void com.ppdai.tensorflow.tracking.ObjectTracker.initNative(int, int, boolean) (tried Java_com_ppdai_tensorflow_tracking_ObjectTracker_initNative and Java_com_ppdai_tensorflow_tracking_ObjectTracker_initNative__IIZ)
                                                     at com.ppdai.tensorflow.tracking.ObjectTracker.initNative(Native Method)
                                                     at com.ppdai.tensorflow.tracking.ObjectTracker.init(ObjectTracker.java:257)
                                                     at com.ppdai.tensorflow.tracking.ObjectTracker.getInstance(ObjectTracker.java:220)
                                                     at com.ppdai.tensorflow.tracking.MultiBoxTracker.onFrame(MultiBoxTracker.java:211)
                                                     at com.ppdai.tensorflow.DetectorActivity.processImage(DetectorActivity.java:250)
                                                     at com.ppdai.tensorflow.CameraActivity.onPreviewFrame(CameraActivity.java:149)
                                                     at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1285)
                                                     at android.os.Handler.dispatchMessage(Handler.java:111)
                                                     at android.os.Looper.loop(Looper.java:194)
                                                     at android.app.ActivityThread.main(ActivityThread.java:5868)
                                                     at java.lang.reflect.Method.invoke(Native Method)
                                                     at java.lang.reflect.Method.invoke(Method.java:372)
                                                     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1019)
                                                     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:814)

1、firstly compile aar from jcenter 1.4.0
2、secondly add related so to jniLibs 
      libtensorflow_demo.so
      libtensorflow_inference.so
      libandroid_tensorflow_lib.lo
      benchmark_model
3、then use offical demo code to test tensorflow detect function, error comes.

what will i can do to solve this problem?

i have add related so "
3,12808,1,"tensorflow performance issue for map_fn and gather. I am trying to understand more about certain surprising results i see in implementing a tf graph .
The graph i am working with is just a forest (bunch of trees). This is just a plain forward inference graph , and nothing related to training. I am sharing the snippets for 2 implementation

code snippet 1: 

    with tf.name_scope(""main""):
       
        def get_tree_output(offset):
            loop_vars = (offset,)
            leaf_indice = tf.while_loop(cond,
                                        body,
                                        loop_vars,
                                        back_prop=False,
                                        parallel_iterations=1,
                                        name=""while_loop"")
            tree_score = tf.gather(score_tensor, leaf_indice, name=""tree-scores"")
            output = tf.add(tree_score, output)

        leaf_indices = tf.map_fn(get_tree_output,
                                 tree_offsets_tensor,
                                 dtype=INT_TYPE,
                                 parallel_iterations=n_trees,
                                 back_prop=False,
                                 name=""tree-scores"")

        tree_scores = tf.gather(score_tensor, leaf_indices, name=""tree-scores"")

        output = tf.reduce_sum(tree_scores, name=""sum-output"")
        output = tf.sigmoid(output, name=""sigmoid-output"")


code snippet 2:

    with tf.name_scope(""main""):
        tree_offsets_tensor = tf.constant(tree_offsets, dtype=INT_TYPE, name=""tree_offsets_tensor"")
        loop_vars = (tree_offsets_tensor,)
        leaf_indices = tf.while_loop(cond,
                                     body,
                                     loop_vars,
                                     back_prop=False,
                                     parallel_iterations=n_trees,
                                     name=""while_loop"")

        tree_scores = tf.gather(score_tensor, leaf_indices, name=""tree-scores"")

        output = tf.reduce_sum(tree_scores, name=""sum-output"")
        output = tf.sigmoid(output, name=""sigmoid-output"")



The rest of the code is exactly the same  : the constant tensors , variables, condition and body for the while loop. thread and parallelism was also the same in both case
code snippet2 :  takes about 500 micro sec to do inference 
code snippet 1 : take about  12 milli sec to do inference 

The difference is that in snippet 1 , I use  to operate on , where as in snippet 2 , I get rid of that , and just directly use that tensor, so as I understand in snippet1  method gets called with one element from , we are  having multiple  for each individual offset value, whereas in snippet 2 we just have one  that just takes multiple offset values (basically the offset_tensor). 

I also tried another variation for snippet , instead of using the map_fn  I write a hand written for loop

code snippet 1 (variation for loop) :

    output = 0
    with tf.name_scope(""main""):
        for offset in tree_offsets:
            loop_vars = (offset,)  # offset here is a scalar 
            leaf_indice = tf.while_loop(cond,
                                        body,
                                        loop_vars,
                                        back_prop=False,
                                        parallel_iterations=1,
                                        name=""while_loop"")
            tree_score = tf.gather(score_tensor, leaf_indice, name=""tree-scores"")
            output = tf.add(tree_score, output)

        #leaf_indices = tf.map_fn(get_tree_output,
        #    tree_offsets_tensor, dtype=INT_TYPE,
        #    parallel_iterations=n_trees, back_prop=False,
        #    name=""tree-scores"")

        #tree_scores = tf.gather(score_tensor, leaf_indices, name=""tree-scores"")

        #output = tf.reduce_sum(tree_scores, name=""sum-output"")
        output = tf.sigmoid(output, name=""sigmoid-output"")

This gives minor improvement :  9 millisec
The while condition does a bunch of gather operation , so if i use map_fn or the ""for loop"" the gather operates on a bunch of scalars instead of tensor of offset . Why is the code 20-40x slower , is the usage wrong or are there any caveats here ? Any help in understanding or optimizing this is appreciated"
97,35030,1,"High memory consumption with model.fit in TF 2.0.0 and 2.1.0-rc0. **System information**

- Have I written custom code: Yes
- OS Platform and Distribution: Linux Kubuntu 18.04, kernel 5.0
- Mobile device: Not verified on mobile devices
- TensorFlow installed from: binary via 
- TensorFlow version: , however affected are also  and , , 
- Python version: 3.6.9
- CUDA version: 10.1 for TF 2.1.0-rc0; 10.0 for the earlier versions of TF
- cuDNN version: 7
- GPU model and memory: Nvidia GeForce GTX 1050 Ti (4GB)
- CPU model: AMD Ryzen 7 1700

**Describe the current behavior**

Model training with the Keras API consumes high amount of system memory with TF  and , as well as in ,  and . It looks like the memory used by  is proportional to the size of the training data provided as numpy arrays, with the proportionality constant being approximately 1. In other words, if the numpy arrays  and  are, say, 8 GB in total, then  will use another 8 GB (plus some overhead). This may suggest that  creates unnecessary copies of the data arrays. This is in contrary to TF , ,  and , where  seems to use some amount of RAM independent of the data size (and much less than 8 GB, at least in the test code attached below).

The same concerns the validation data. If validation data are passed as numpy arrays to  via the argument , then the memory use of  seems to duplicate the size of the validation data arrays with TF from  to .

In the code attached below, one may change the variable  to vary the size of the data and test the above described behaviour. It is straightforward to estimate the data size: e.g. with  the data arrays in the below code should be ca. 7.32 GB in total. The whole Python process associated with this code uses approximately this much RAM plus some overhead when running with TF , ,  or . But with TF from  to  the Python process consumes twice that much RAM. One may comment out the line containing  to check that it is the point at which the high memory consumption starts.

**Describe the expected behavior**

The size of the memory used by  should not duplicate the size of the training and validation data passed as numpy arrays. It should be more or less independent of the size of the data arrays, similarly as in TF  and in the pre-releases ,  and . 

**Code to reproduce the issue**



"
21,7203,1,Noticable lag with different Inception model. Running the demo app on Android works absolutelly great. Then I tried retraining the inception5h model (as used by the app) with no success. I retrained Inception v3 model but then image recognition would lag. I tried quantizating the model but it lagged even more. So what I'm asking is for a direction or help on the issue or a retrainable Inception5h model with some instructions.
1361,27815,0,"Differential (Delta) Model Update. I'm new to tensorflow. My area of usage would be on device execution for item recognition.
As my use case requires frequent updates on the items to recognize, I was wondering if it is possible to update models in a delta way, e.g. to only transfer the updated data and not the whole model each time."
296,9411,0,"Cifar10 Tutorial Link to Example Code 404's. Following any of the links to the code for the CIFAR10 tutorial 404's.

Example link: https://www.tensorflow.org/versions/master/tutorials/deep_cnn#code_organization

Any from above."
207,6531,1,"tf.losses.softmax_cross_entropy is deviously inconsistent. There are at least three variants of  in TensorFlow:

1. 

Accepts  as the first argument, and  as the second argument.

2. 

Accepts  as the first argument, and  as the second argument. So far so good. Except this is deprecated, and displays the recommendation .

3. 

Decides to switch things around for fun and have  as the first argument and   as the second. Since  and  are identically shaped tensors, the call succeeds without any complaints (until something else fails as a consequence, like the gradient).

This inconsistency seems a bit error prone. If nothing else, perhaps the deprecation warning for   should include a heads up.

"
595,11791,0,"Feature request: equivalent of tf.nn.maxpooling_with_argmax for 3D maxpooling. It would be great to have the equivalent of tf.nn.maxpooling_with_argmax in 3D, in order to allow for 3D unpooling layers.

I am implementing a 3Dversion of the originally 2D Deconvolution network [Noh et al. 2015]. It has been implemented in 2D in Tensorflow [here](https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation/blob/master/DeconvNet.py) and in 3D in caffe [here](https://github.com/Microsoft/O-CNN/blob/master/caffe/examples/o-cnn/segmentation_5.prototxt). 

I need to use unpooling, and for that I need the indexes of the elements selected during pooling. This feature is implemented for 2D (tf.nn.maxpooling_with_argmax), but not for 3D.
"
273,29687,1,"HashTable lookup performance very low in comparison to plain Python dictionaries (~5,000x). ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.2 LTS (Bionic Beaver)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Don't know (Colab)
- **TensorFlow version (use command below)**: 1.13.1
- **Python version**: 3.6.7
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: - 
- **CUDA/cuDNN version**: 10.0.130
- **GPU model and memory**: 
- **Exact command to reproduce**:  tf.contrib.lookup.HashTable..lookup()




### Describe the problem
While writing a document ranking algorithm in TensorFlow we found out that TensorFlow s appear to be very slow (~5,000x) in comparison to plain Python ionaries. 

Looking into the TensorFlow source code in  (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lookup_table_op.cc) shows that the underlying object structure appears to be an . So we wonder, why the performance is so low? 

### Source code / logs
Here is the source code that is similar to a part of what we use and can be easily executed on any system. There is actually a __Colab__ notebook that can be used: https://colab.research.google.com/drive/1bB_sir7-sVd3bNrSgkcdT9UlU9eoyA2Q 


"
724,22040,0,"AttributeError: 'TPUInfeedOutfeedSessionHook' object has no attribute '_name'. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

Probably irrelevant for this bug
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux debian
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: not sure, it's baked into an image
- **TensorFlow version (use command below)**: b'v1.9.0-0-g25c197e' 1.9.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A


### Describe the problem

Running a TPUEstimator and passing initial_infeed_sleep_secs to the TPUConfig results in:

Exception in thread InfeedController:
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/opt/conda/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 421, in _run_infeed
    logging.info('%s thread sleeping for %d seconds.', self._name,
AttributeError: 'TPUInfeedOutfeedSessionHook' object has no attribute '_name'


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Usage of self._name here is invalid
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py#L421
"
321,33648,0,"Can save but not load custom metrics with a variable named 'weights' in the tf saved model format. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7

**Describe the current behavior**
AttributeError occurs when trying to load a tf saved model using  tf.keras.models.load_model with a custom metric with a variable named 'weights'.

**Describe the expected behavior**
Either an error gets thrown during assignment or saving that you are not allowed to save a variable with the name 'weights', or no attribute error occurs and load_model loads the metric successfully.

**Code to reproduce the issue**


**Other info / logs**


"
747,591,0,"tf.Print does not work on embedding gradients. Is this expected? because an embedding gradient is an IndexedSlices rather than a Tensor.
"
247,30162,1," tf.keras model.fit calls slow with TPU distribute strategy. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below):1.14
- Python version:3,6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
TPU distribution strategy does not support model.fit_generator, and repeated model.fit calls result in a 50x slowdown presumably because it adds operations to graph. 

**Describe the expected behavior**

**Code to reproduce the issue**

resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)
tf.contrib.distribute.initialize_tpu_system(resolver)
strategy =  tf.distribute.experimental.TPUStrategy(resolver)
with strategy.scope():
    model = ..... ## Your tf.keras model
    model.compile(loss = custom_loss,optimizer ='custom_optimizer)
 
for i in range(num_its):
      data,labels = = next(generator_fn()) 
      model.fit(data,labels)   

     





**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1346,27606,0,"CMake Error at tf_core_ops.cmake. 
**System information**
- OS Platform and Distribution : Windows 10 64bit
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.12.0
- Python version: python3.6 64bit
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): bazel-0.24.1-windows-x86_64.exe
- GCC/Compiler version (if compiling from source): vs2015
- CUDA/cuDNN version: cpu only



I have been trying to build a tensorflow c++ lib for a long time but never succeed.

This time, I tried to build it with cmake(gui), when I click ""generate"" some error occurred.

> CMake Error at tf_core_ops.cmake:73 (add_library):
>   Cannot find source file:
> 
>     D:/tensorflow/tensorflow/contrib/data/ops/dataset_ops.cc
> 
>   Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
>   .hpp .hxx .in .txx
> Call Stack (most recent call first):
>   tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)
>   CMakeLists.txt:512 (include)
> 
> 
> CMake Error at tf_core_kernels.cmake:221 (add_library):
>   Cannot find source file:
> 
>     D:/tensorflow/tensorflow/contrib/data/kernels/assert_next_dataset_op.cc
> 
>   Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm
>   .hpp .hxx .in .txx
> Call Stack (most recent call first):
>   CMakeLists.txt:514 (include)
> 
> 
> CMake Error at tf_core_ops.cmake:73 (add_library):
>   No SOURCES given to target: tf_contrib_data_dataset_ops
> Call Stack (most recent call first):
>   tf_core_ops.cmake:92 (GENERATE_CONTRIB_OP_LIBRARY)
>   CMakeLists.txt:512 (include)
> 
> 
> CMake Error at tf_core_kernels.cmake:221 (add_library):
>   No SOURCES given to target: tf_core_kernels
> Call Stack (most recent call first):
>   CMakeLists.txt:514 (include)
> 
> "
494,20370,0,"MirroredStrategy fails with ""no supported kernel for GPU devices is available"". - **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ML Engine default
- **TensorFlow installed from (source or binary)**: ML Engine default
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 2.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: ML Engine default
- **GPU model and memory**: NVIDIA Tesla K80
- **Exact command to reproduce**: N/A

MirroredStrategy fails with ""no supported kernel for GPU devices is available"". The same code works on a single GPU.

Traceback:
Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/main.py"", line 188, in <module> main(sys.argv) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/main.py"", line 184, in main start_training(output_dir, hparams, **otherargs) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/main.py"", line 131, in start_training tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 439, in train_and_evaluate executor.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 546, in run getattr(self, task_to_run)() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 601, in run_master self._start_distributed_training(saving_listeners=saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 739, in _start_distributed_training saving_listeners=saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 363, in train loss = self._train_model(input_fn, hooks, saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 841, in _train_model return self._train_model_distributed(input_fn, hooks, saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 977, in _train_model_distributed saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1056, in _train_with_estimator_spec log_step_count_steps=self._config.log_step_count_steps) as mon_sess: File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 405, in MonitoredTrainingSession stop_grace_period_secs=stop_grace_period_secs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 816, in __init__ stop_grace_period_secs=stop_grace_period_secs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 539, in __init__ self._sess = _RecoverableSession(self._coordinated_creator) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1002, in __init__ _WrappedSession.__init__(self, self._create_session()) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1007, in _create_session return self._sess_creator.create_session() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 696, in create_session self.tf_sess = self._session_creator.create_session() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 467, in create_session init_fn=self._scaffold.init_fn) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 285, in prepare_session sess.run(init_op, feed_dict=init_feed_dict) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call raise type(e)(node_def, op, message) InvalidArgumentError: Cannot assign a device for operation 'tower_3/Tile_7/input': Could not satisfy explicit device specification '/device:GPU:3' because **no supported kernel for GPU devices is available**. Registered kernels: device='CPU'; T in [DT_QINT32] device='CPU'; T in [DT_QUINT8] device='CPU'; T in [DT_QINT8] device='CPU'; T in [DT_VARIANT] device='CPU'; T in [DT_RESOURCE] device='CPU'; T in [DT_STRING] device='CPU'; T in [DT_BOOL] device='CPU'; T in [DT_COMPLEX128] device='CPU'; T in [DT_COMPLEX64] device='CPU'; T in [DT_DOUBLE] device='CPU'; T in [DT_FLOAT] device='CPU'; T in [DT_BFLOAT16] device='CPU'; T in [DT_HALF] device='CPU'; T in [DT_INT8] device='CPU'; T in [DT_UINT8] device='CPU'; T in [DT_INT16] device='CPU'; T in [DT_UINT16] device='CPU'; T in [DT_INT32] device='CPU'; T in [DT_INT64] device='GPU'; T in [DT_INT32] device='GPU'; T in [DT_BOOL] device='GPU'; T in [DT_INT64] device='GPU'; T in [DT_BFLOAT16] device='GPU'; T in [DT_DOUBLE] device='GPU'; T in [DT_FLOAT] device='GPU'; T in [DT_HALF] [[Node: tower_3/Tile_7/input = Pack[N=1, T=DT_INT16, axis=0, _device=""/device:GPU:3""](tower_3/Cast_4)]] Caused by op u'tower_3/Tile_7/input', defined at: File ""/usr/lib/python2.7/threading.py"", line 774, in __bootstrap self.__bootstrap_inner() File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner self.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 465, in run self.main_result = self.main_fn(*self.main_args, **self.main_kwargs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 831, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/model.py"", line 230, in model_fn iou_accuracy = box.compute_safe_IOU(target_rois, detected_rois, detected_rois_overflow, settings.TILE_SIZE) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/utils_box.py"", line 521, in compute_safe_IOU iou_accuracy = IOUCalculator.batch_intersection_over_union(detected_rois * tile_size, target_rois * tile_size, tile_size=tile_size) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/utils_box.py"", line 476, in batch_intersection_over_union linmap2 = cls.__iou_gen_linmap(batch, n2, tile_size) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/utils_box.py"", **line 428, in __iou_gen_linmap linmap = tf.tile([row], [tile_size, 1])** File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 8430, in tile ""Tile"", input=input, multiples=multiples, name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper preferred_dtype=default_dtype) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1104, in internal_convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1034, in _autopacking_conversion_function return _autopacking_helper(v, inferred_dtype, name or ""packed"") File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 997, in _autopacking_helper return gen_array_ops.pack(elems_as_tensors, name=scope) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 4517, in pack ""Pack"", values=values, axis=axis, name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__ self._traceback = self._graph._extract_stack() # pylint: disable=protected-access InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'tower_3/Tile_7/input': Could not satisfy explicit device specification '/device:GPU:3' because **no supported kernel for GPU devices is available**. Registered kernels: device='CPU'; T in [DT_QINT32] device='CPU'; T in [DT_QUINT8] device='CPU'; T in [DT_QINT8] device='CPU'; T in [DT_VARIANT] device='CPU'; T in [DT_RESOURCE] device='CPU'; T in [DT_STRING] device='CPU'; T in [DT_BOOL] device='CPU'; T in [DT_COMPLEX128] device='CPU'; T in [DT_COMPLEX64] device='CPU'; T in [DT_DOUBLE] device='CPU'; T in [DT_FLOAT] device='CPU'; T in [DT_BFLOAT16] device='CPU'; T in [DT_HALF] device='CPU'; T in [DT_INT8] device='CPU'; T in [DT_UINT8] device='CPU'; T in [DT_INT16] device='CPU'; T in [DT_UINT16] device='CPU'; T in [DT_INT32] device='CPU'; T in [DT_INT64] device='GPU'; T in [DT_INT32] device='GPU'; T in [DT_BOOL] device='GPU'; T in [DT_INT64] device='GPU'; T in [DT_BFLOAT16] device='GPU'; T in [DT_DOUBLE] device='GPU'; T in [DT_FLOAT] device='GPU'; T in [DT_HALF] [[Node: tower_3/Tile_7/input = Pack[N=1, T=DT_INT16, axis=0, _device=""/device:GPU:3""](tower_3/Cast_4)]]
"
905,35064,0,"tensorflow 1.14 not picking up GPU: CUBLAS_STATUS_INTERNAL_ERROR. I am having problems running tensorflow on my GPU.

My environment is as follows:

* OS: Linux Mint
* GPU: GeForce RTX 2070 Super
* Nvidia Driver Version: 435.21
* CUDA Version: 10.1
* gcc --version: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
* tensorflow-gpu 1.14
* keras 2.3.1

Both the keras and tensorflow packages are located in a conda environment with no other tensorflow or keras versions to avoid package conflict. First, I get conflicting GPU devices when I run the following quick test:


This doesn't make much sense as I thought keras uses tensorflow as the backend.

Then I tried running the below program (I got some hints from these two SO posts [[1]](https://datascience.stackexchange.com/a/41958/41929)[[2]](https://stackoverflow.com/a/52132342/4139143))



But got multiple errors


Am I doing something incorrect here? TF was certainly working with my GPU (the program above worked) just a few weeks ago. It seems every time a new TF or keras version is released, I install it and get GPU issues. Any help / advice is much appreciated. Thank you

"
844,9413,0,"Android ops - not supporting NCHW data format. Hi,

I'm using my custom tensorflow v1.0.1 model freezed and exported to Android arm-ABI-v7a (I compiled tensorflow 1.0.1 with selective registration for the models ops).

As a training time optimization we tried to convert the data format to NCHW format (as written in the tensorflow performance tutorial).

But now, when I run inference on Android, I get the following error in the logcat:

 E/native: tensorflow_inference_jni.cc:233 Error during inference: Invalid argument: CPU BiasOp only supports NHWC. [[Node: conv_layer_1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=""NCHW"", _device=""/job:localhost/replica:0/task:0/cpu:0""](conv_layer_1/conv_l11, bc11_init)]]


Are you going to add NCHW support for android CPU ops in the future versions?

Thanks,
Eran


### System information
- **Have I written custom code: Yes - custom model*:
- **OS Linux Ubuntu 16.04:
- **TensorFlow installed from source:
- **TensorFlow version 1.0.1:
- **Bazel version 0.4.5*:
- **CUDA/cuDNN version 5.1.10:
- **GPU model and memory GTX 1080 TI:

"
1453,14755,0,"Problem with assigning values to matrix indices in tensorflow. Hi,

I am constructing a NN with tensorflow that uses a custom stddev function. I have for a batch and indices i and j a function . Of course, .



This function seems to work, but then I would like to do the following:



My problem is that the function  in tensorflow is not differentiable, so  will outout a . Therefore, my network cannot be trained.

A test can be done with

"
428,31561,0,"TensorFlow Lite conversion. **System information**
- TensorFlow running in Google Colab

Text Output from TFLite convert

"
1386,34566,0,"Google Colab: InvalidArgumentError: Cannot update variable with shape [] using a Tensor with shape [32], shapes must be equal. 	 [[{{node metrics_26/acc/AssignAddVariableOp}}]]. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Google Colab
- TensorFlow installed from (source or binary): 2.00
- TensorFlow version (use command below): 2.00
- Python version: 3.6

**Describe the current behavior**
I'm working on an image classification project using Tensorflow and running the code on Google Colab. The dataset is hosted on my Google Drive.
Everything works as expected until the model begins to train and I get an error as shown below.


**Describe the expected behavior**
If I run the same code on my Windows 10 setup, I do not run into any errors.



**Code to reproduce the issue**



**Other info / logs**
"
507,33014,0,"tflite.allocate_tensors() fails after changing input size. (using tensorflow 1.14.0)

I'm trying to use a tflite model to do inference on a batch. I use the following code:



The code crashes and gives the following error:
`

When looking at the output details, it still has the shape  and not  as I would expect.

Any ideas?
"
1369,2920,0,"dso_loader on MacOS opens libcudnn.dylib instead of libcudnn.5.dylib. I'm getting error below after syncing. Was there something in last 24 hours that affected how libcudnn filename is resolved?


Instead I have these files



This worked for me as a work-around:



PS: my configure run is below. 



Note: one productivity sink is that  overwrites files that are written by  because they are checked in, so at each  you need to run  or [stash/merge](http://stackoverflow.com/questions/37758333/how-to-prevent-checked-in-files-from-overwriting-local-versions). Also, Bazel is not fully hermetic, so if you  with incorrect  once, you will then need to . This issue can be non obvious -- things run but give numerically incorrect results
"
627,25279,0,"tf.contrib.opt.ScipyOptimizerInterface error. Hello.

I tried to use tf.contrib.opt.ScipyOptimizerInterface with an example code from your documentation.




.

I am getting the following error: 

OS Platform and Distribution: Ubuntu 18.04.1
TensorFlow installed from: source
TensorFlow version: 1.12.0
Python version: 3.6
Bazel version: 0.21.0
GCC version: 7.3.0
CUDA and GPU: no running on GPU, because I don't have GPU.

Thank you!"
1209,34772,0,tf.pad is limited to six dimensions
469,25779,0,"keras and tf.keras behave differently with custom loss function and fit_generator. **System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 3.7.1
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0 / 7.4.2
- GPU model and memory: GTX 1080 ti
- Keras version: 2.2.4

**Describe the current behavior**
I am using Keras bundled with tensorflow. When I apply my own (rather obscure) loss I get a **NotFoundError**:

when I use , but when I use  everything works ok. However, when I use vanilla Keras, both methods work just fine.

As a side question: my loss is rather slow, can someone give me some pointers in how to improve performance?

**Describe the expected behavior**
I would expect  to work just the same as in the vanilla-keras version or in the normal  version.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


**Other info / logs**
Error message:


"
855,2062,0,"tf.cond not working with depedencies. tf.cond seems to have a bug if one of the condition have a dependency. (Dependencies are run, whatever tf.cond arg is True or False).

To illustrate:



Output:


"
1237,1141,0,"support different horizontal axises for different scalar summaries. Is it possible to have different horizontal axises for different scalar summaries?
For example, I want to have a scalar summary of loss for each global step, and a scalar summary of network outputs for each epoch.
Currently, I have the same horizontal axis showing the global step for both like the following image.
Is it possible to have the global step (0 ... 8.0k) for  and the epoch (0 ... 2.0k) for ?

![image](https://cloud.githubusercontent.com/assets/6128440/13111093/405a40b4-d5c7-11e5-9d03-566f19c76b2c.png)

If I set the epoch for , I get this:

![image](https://cloud.githubusercontent.com/assets/6128440/13112966/519aa52c-d5d0-11e5-8ead-552ac22898e2.png)
"
685,4485,0,"Unable to retrieve flower_photos.tgz using curl command. NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
### Environment info

Operating System:Ubuntu 12.04

Installed version of CUDA and cuDNN: 
(please attach the output of ):

I have installed TensorFlow using docker by running the following command $ docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow

If installed from binary pip package, provide:
1. A link to the pip package you installed:
2. The output from .

If installed from source, provide 
1. The commit hash ()
2. The output of 
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
### What other attempted solutions have you tried?
### Logs or other output that would be helpful

(If logs are large, please upload as attachment or provide link).
I am trying to retrieve the set of images for TensorFlow for Poets using the following command curl -O http://download.tensorflow.org/example_images/flower_photos.tgz but the below mentioned error is being thrown

~/tf_files$ curl http://download.tensorflow.org/example_images/flower_photo.tgz
<?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message></Error>
"
603,32122,0,"Cannot seek on write only tf.gfile.GFile. **System information**

-  Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.6

**Describe the current behavior**
Calling  on a  opened in write only mode raises .

**Describe the expected behavior**
GFile should support the Python IO semantics that supports seeking on a write only file.

More generally it would be preferable if GFile followed the API of Python's [](https://docs.python.org/3/library/io.html#io.IOBase).


**Code to reproduce the issue**



**Other info / logs**
"
732,22115,0,"Feature Request: fp16 support for tf.contrib.image.transform on gpu. ### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **Mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.10.0-0-g656e7a2b34', '1.10.0')
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**:  9.0  /  7.1.4.18-1
- **GPU model and memory**: GeForce GTX 1080 8097MB
- **Exact command to reproduce**:

### Describe the problem
 does not support fp16 on GPU. 

### Source code / logs

 

"
1206,21421,0,"Android: cannot find -lpthread for simple binary. I'm trying to compile a simple application for Android and getting linker errors: Bazel is linking in , even though Android doesn't support that.

The application code is as follows:

*tensorflow/demo-bug/main.cc*:


The BUILD file is as follows:

*tensorflow/demo-bug/BUILD*:


If you don't include  in the , the error doesn't happen. For some reason, that dependency pulls in the  from somewhere.

When you run the  command (see below), you get the following error:

Indeed, looking through the command that it uses to compile it, you see the flag:

(Full command below)


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see above
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.9.0 (git tag)
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: Android NDK r17b
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
"
1296,15155,0,"Input too short to compute filterbank. Hi,
I am new to tensorflow and i am trying to train model with my own data but i am getting below error

2017-12-06 18:56:38.030081: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030095: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030105: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030162: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030203: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030243: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030256: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030267: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030305: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030347: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030359: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030370: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030408: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030446: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030458: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030469: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030481: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030492: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030534: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030547: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030558: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030569: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030632: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030645: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030656: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030668: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030679: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030720: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030732: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030743: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030754: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030790: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030851: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030865: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030877: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030937: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030954: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.030967: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031028: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031043: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031054: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031066: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031078: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031118: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031132: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031144: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031155: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031211: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031227: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031238: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031249: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031309: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031325: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031338: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031349: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031408: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031425: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031437: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031474: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031512: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031525: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031537: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031573: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031608: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031620: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031631: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031643: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031687: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031699: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031710: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031769: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031799: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031812: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031823: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031834: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.031965: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032014: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032024: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032036: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032046: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032057: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032069: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032081: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032128: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032141: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032153: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032165: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032176: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032236: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032248: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032260: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032272: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032313: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032326: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032336: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032347: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032360: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank
2017-12-06 18:56:38.032399: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:181] Input too short to compute filterbank

and i am using below command to train
bazel run tensorflow/examples/speech_commands:train -- \ --data_dir=sound --wanted_words=yes,no --data_url=
"
866,20697,0,terminate called after throwing an instance of 'std::bad_alloc'
1312,25820,0,"[TF 2.0 API Docs] tf.math.add_n. **System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/add_n


**Describe the documentation issue**

* **Links**
https://github.com/tensorflow/tensorflow/blob/master/python/ops/math_ops.py
should be
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py

* **Clear Description**
The description is not clear enough.  In the description, it should probably link to [tf.math.accumulate_n](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/accumulate_n) and explain when to use .

* **Usage example**
No usage example is provided.

* **Visuals, if Applicable**
  No visuals are included.


**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes.
"
1045,13190,0,"TF_AddGradients gradients returns wrong result when multiple outputs specified. ### System information
Darwin Mac-Admin.local 15.6.0 Darwin Kernel Version 15.6.0: Thu Jun 23 18:25:34 PDT 2016; root:xnu-3248.60.10~1/RELEASE_X86_64 x86_64
Mac OS X 10.11.6

### Describe the problem
Hi. I've added a unit test for TF_AddGradients API (see code below) which is similar to
[this python test](https://github.com/tensorflow/tensorflow/blob/ca3bc0f1c2f917cf6e7c49d58f5ec604a9af9367/tensorflow/python/ops/gradients_test.py#L337)

In the test, I provide two outputs 
y[0]=x[0] ** 2 
y[1] = y[0] ** 8

where input x[0]=3.
According to the [documentation](https://github.com/tensorflow/tensorflow/blob/03619fab3f4dd6f28b67418455a953b0fccdd9bf/tensorflow/c/c_api.h#L1018)  result should be calculated by formula d(y_1 + y_2 + ...)/dx_1 and be equal to 17502, but the API prints 6.

What am I missing? Thanks.

### Source code / logs



"
1357,5984,0,"Bug report : tf.contrib.learn.train API has a problem. Hi, everyone

First, please understand that I cannot speak English well

I tried to use high-level API(tf.contrib) for code simplicity. 
When I use tf.contrib.learn.train API, I find a problem that execution time per batch increases

An example code is described as below 


when I run above code, execution time per batch increases after step 100.
So, I analyzed tf.contrib.learn.train API

In **graph_actions.py** file, there is train function and this function calls **_train_internal function** in the same file

In _train_internal function, _run_with_monitors function is called for execution of train_op and monitor

_run_with_monitors function is described as below

In this function, I examined two functions in class EveryN(monitors.py) : **monitor.step_begin**,  **monitor.step_end**

In monitor.step_begin function, monitor is executed when the following conditions are fulfilled

and monitor.step_end function is described as follows

In these codes, I find **self._last_active_step variable** is not updated.
So, I add the **monitor.post_step function** in _run_with_monitors function as follows

**monitor.post_step function** performs _last_active_step variable update as follows


When the code is revised above, tf.contrib.learn.train function is well operated.
"
1347,30556,0,"Which Bazel versions are okay for Tensorflow source install?. 

## URL(s) with the issue:

https://www.tensorflow.org/install/source_windows


## Description of issue (what needs changing):

It is purely a logical issue:
Install Bazel 0.24.1, the build tool used to compile TensorFlow. Set up Bazel to build C++.
[...] Ensure you install Bazel 0.23.0 or lower.

0.24.1 > 0.23.0 
"
417,25176,0,"Cannot build Tensorflow 1.12 On Windows 10 and CUDA 9.0. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): MSVC2015 
- CUDA/cuDNN version: 9.0.176 / 7.4.2
- GPU model and memory: Nvidia Geforce GTX 750 Ti/ 2GB DDR5
Cpu : AMD Athlon II x4 631


**Describe the problem**
So I've been using Anaconda to build Tensorflow 1.12, since my cpu is old and doesn't support AVX (i want tensorflow to run on gpu) i cant use the pre-built pip packages. 
I've installed bazel and msys64 correctly and added them to path directory. After configuring the configure.py and when trying to build, i get errors posted below.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


> WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
f:\tensorflow\tensorflow/.bazelrc
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: ce75c25f-4ec0-436f-91bb-da2ed1092a50
INFO: Build option --define has changed, discarding analysis cache.
WARNING: F:/tensorflow/tensorflow/tensorflow/python/BUILD:3099:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of  to .
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of  to .
WARNING: F:/tensorflow/tensorflow/tensorflow/python/BUILD:100:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of  to .
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of  to .
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of  to .
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 14335 targets configured).
INFO: Found 1 target...
ERROR: F:/tensorflow/tensorflow/tensorflow/core/BUILD:2497:1: ProtoCompile tensorflow/core/example/example_pb2.py failed (Exit -1073741795): protoc failed: error executing command
  cd C:/users/albis/_bazel_albis/a6so2axr/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET CUDNN_INSTALL_PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin
    SET PYTHON_BIN_PATH=F:/Anaconda3/envs/Tensorflow/python.exe
    SET PYTHON_LIB_PATH=F:/Anaconda3/envs/Tensorflow/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  bazel-out/x64_windows-opt/bin/external/protobuf_archive/protoc --python_out=bazel-out/x64_windows-opt/genfiles/ -I. -I. -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python tensorflow/core/example/example.proto tensorflow/core/example/feature.proto tensorflow/core/framework/allocation_description.proto tensorflow/core/framework/api_def.proto tensorflow/core/framework/attr_value.proto tensorflow/core/framework/cost_graph.proto tensorflow/core/framework/device_attributes.proto tensorflow/core/framework/function.proto tensorflow/core/framework/graph.proto tensorflow/core/framework/graph_transfer_info.proto tensorflow/core/framework/kernel_def.proto tensorflow/core/framework/log_memory.proto tensorflow/core/framework/node_def.proto tensorflow/core/framework/op_def.proto tensorflow/core/framework/reader_base.proto tensorflow/core/framework/remote_fused_graph_execute_info.proto tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/step_stats.proto tensorflow/core/framework/summary.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/tensor_description.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor_slice.proto tensorflow/core/framework/types.proto tensorflow/core/framework/variable.proto tensorflow/core/framework/versions.proto tensorflow/core/protobuf/config.proto tensorflow/core/protobuf/cluster.proto tensorflow/core/protobuf/debug.proto tensorflow/core/protobuf/device_properties.proto tensorflow/core/protobuf/queue_runner.proto tensorflow/core/protobuf/rewriter_config.proto tensorflow/core/protobuf/tensor_bundle.proto tensorflow/core/protobuf/saver.proto tensorflow/core/util/event.proto tensorflow/core/util/memmapped_file_system.proto tensorflow/core/util/saved_tensor_slice.proto tensorflow/core/example/example_parser_configuration.proto tensorflow/core/protobuf/checkpointable_object_graph.proto tensorflow/core/protobuf/control_flow.proto tensorflow/core/protobuf/meta_graph.proto tensorflow/core/protobuf/named_tensor.proto tensorflow/core/protobuf/saved_model.proto tensorflow/core/protobuf/tensorflow_server.proto tensorflow/core/protobuf/transport_options.proto tensorflow/core/util/test_log.proto
Execution platform: @bazel_tools//platforms:host_platform
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 18.245s, Critical Path: 4.43s
INFO: 2 processes: 2 local.
FAILED: Build did NOT complete successfully"
53,35439,1,"tf.metrics.Mean* metrics miscalculated. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip3
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
tf.metrics.MeanAbsoluteError and others compute the mean of means.

**Describe the expected behavior**
They should compute the mean of all the data to support iterating over evaluation datasets.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

import tensorflow as tf
m=tf.metrics.MeanAbsoluteError()
m.update_state(y_true=[0,0], y_pred=[1,1])
m.update_state(y_true=[0], y_pred=[2])
assert m.result() == 2


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The fix is simple, instead of the mean_squared_error function use a (new) sum_squared_error function in the metric and pass that to MeanMetricWrapper."
1140,17916,0," ImportError: libcublas.so.9.0: cannot. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
566,29274,0,"[TF 2.0 API Docs] tf.VariableSynchronization. ## URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/VariableSynchronization


### Usage example

No usage example is provided
"
793,27790,0,"google::protobuf::python::oneof_descriptor::GetHasOptions(google::protobuf::python::PyBaseDescriptor*, void*) in descriptor.o. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: -
- Python version:

- Installed using virtualenv? pip? conda?:

- GCC/Compiler version (if compiling from source):

- CUDA/cuDNN version: -
- GPU model and memory: -



on commit 



End of build log:

"
1471,23795,0,"Windows 10 Bazel Build failed . **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.10
- Python version: 3.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): not compiled from source(bazel version: 0.18.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7.0
- GPU model and memory: Zotac Gtx 1080ti mini



**Describe the problem**
Build fails with 1 error
initially i tried with cmake, it was giving few errors so started with bazel..no luck since many days! 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
i followed all the steps mentioned [here](https://www.tensorflow.org/install/source) 



The reason i'm trying to build from source is because ssd_mobilnet retraining is taking longer than expected (3seconds per step)
resizing of image is happening on cpu and training on gpu.
if both happens on gpu then it should take around half second.

i have attached full build output 
[Build output.txt](https://github.com/tensorflow/tensorflow/files/2588467/Build.output.txt)

i've notived too many  in build process

below is the gist of build 

> build --copt=-nvcc_options=disable-warnings --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
> WARNING: Processed legacy workspace file e:\git_projects\tensorflow-experimental\tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
> WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
> .
> .
> .
> .
> 
>    Creating library bazel-out/host/bin/tensorflow/cc/ops/image_ops_gen_cc.lib and object bazel-out/host/bin/tensorflow/cc/ops/image_ops_gen_cc.exp
> ERROR: E:/git_projects/tensorflow-experimental/tensorflow/tensorflow/core/kernels/BUILD:2123:1: C++ compilation of rule '//tensorflow/core/kernels:colorspace_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command
>   cd C:/users/neuroflares/_bazel_neuroflares/twu4fovw/execroot/org_tensorflow
> .
> .
> .
> .
> 
> 1 error detected in the compilation of ""C:/Users/user~1/AppData/Local/Temp/nvcc_inter_files_tmp_dir/colorspace_op_gpu.cu.cpp1.ii"".
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> INFO: Elapsed time: 1530.120s, Critical Path: 558.89s
> INFO: 878 processes: 878 local.
> FAILED: Build did NOT complete successfully


What could have gone wrong?
Thanks & Regards
"
161,27443,1,"Tensorflow lite model inference time increases when adding JNI on Android. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: HW P9, Mi8, Oneplus 5, VIVOX9, HuaWei Honor V9....
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  1.13.0.dev20190126
- Python version: 2.7/3.7(both tryed)


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I use tflite model (quantization-aware training and fully quantized with toco) and deploy on Android for segmentation task. I got correct output of the model, and the inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads). However, when i add some post-processing with C++(JNI), the inference time (only the function   run time)increases to 47ms(1 thread), 34ms(2 threads) , 30ms(3 threads), respectively, and I got the same inference time even though I didn't use the post-processing (just put the jni code in my project). 
When I use post-processing with java code, the inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads) again, so I guess JNI would influence the inference time.

**Describe the expected behavior**
Get same inference time with JNI on Android.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
just add some jni code to the official demo would got the same issue.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
407,28404,0,"Missing tensorflow.compiler.xla.service import hlo_pb2 . <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
I


I am trying to build a TensorRT file to run on my jetson nano.
I am running tensorflow 1.13. 
When I run this code:

trt_graph = trt.create_inference_graph(
    input_graph_def=['input_1'],
    outputs=['Logits/Softmax'],
    max_batch_size=1,
    max_workspace_size_bytes=1 << 25,
    precision_mode='FP16',
    minimum_segment_size=50
)
I get this error :
ImportError: cannot import name 'hlo_pb2'

It says its looking here for it:
 from tensorflow.compiler.xla.service import hlo_pb2 

It dosent exist?

"
864,7313,0,"AttributeError: module 'tensorflow.contrib.learn' has no attribute 'SKCompat'. Hi,

I cannon import SKCompat. Running 0.12.head on osx. Any ideas?"
244,10184,1,"the performance is Unexpectedly in iOS. today i test Tensorflow(TF) iOS example with my iPhone 6S , according to the introduction in TF Website and source code , i know it use Apple's Accelerate framework , i build the protobuf , and TF's source code in my Mac , then run iOS example , i record the time with the code

and the time is fast, only 90ms, i know TF's iOS example use the Google Inception V1 Model , and i test Apple's example which use Google Inception V3 Model , the time is 120ms, metal is more slow than Accelerate framework ? i can not understand . i do not think there is too much different feature that affect performance between inception V1 and V3... so how to explain it ?"
972,10279,0,"Windows GPU Nightly Build Failures. Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

* Windows 10
* With GPU


### Describe the problem

The last stable nightly build for TF on Windows with GPU is currently # 149 and 1+ month old.
Is there a specific blocking issue, why a stable nightly build for this platform is not available for such an extended period?

Thanks for all the hard work!!! As always.

### Source code / logs

n/a
"
669,22441,0,"[Request] Please make version-compatible table among them: ""python/tensorflow/tensorflow-gpu/tflearn/cuDNN/CUDA"". Request: Version-Compatible Table

There are a lot of people using Tensorflow, and I'm sure that those people at least have tried to use tflearn, tensorflow-gpu, cuDNN CUDA, etc. And again, I'm very sure we have had the problem of version incompatibility many many many... times. We've done a lot of research to resolve this incompatibility. We tried all kinds of version combinations. And something came to mind:
""Why there wasn't a Version-Compatible table?""

Like: https://www.tensorflow.org/install/source_windows (Tested build configurations)

Example:

Which version tensorflow-gpu (X version) want at least (also MAX.) Python version?
Which version tensorflow-gpu (X version) want at least (also MAX.) cuDNN version?
Which version tensorflow-gpu (X version) want at least (also MAX.) CUDA version?

Which tflearn versions are compatible with which tensorflow (X) versions?
Which tflearn versions are compatible with which tensorflow-gpu (X) versions?

Which CUDA versions are compatible (at least and MAX) with which tensorflow-gpu versions?

etc... It could be an Exel table that clears all of these questions from our minds. Wouldn't that be good? You need to enter individual wiki pages and investigate version compatible each time. (I mean that waste of time)

Thanks in advance...
"
1401,31991,0,"Iterate on Unknown Batch Size with Custom Layer. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **OS X 10.14.6**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **1.14.0**
- Python version: **3.6.6**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
I am attempting to build a custom TensorFlow Layer to perform K-Means clustering across channels of a given image. I am having difficulty creating this new layer to add to the model, as it seems that fundamentally, I don't have the ability to iterate over the batch size, which is unknown until runtime. I have tried a few alternatives such as the  function decorator and the  function, which have both been unsuccessful. 

**Describe the expected behavior**
I was expecting that since the batch size is unknown until runtime, that TensorFlow would be able to handle this error, similar to how TensorFlow can accept an unknown dimension and generate a matrix/tensor with the unknown shape.

**Code to reproduce the issue**





The error that I get from running the above code is as follows:




My main question is: can TensorFlow not handle iterating over an unknown batch size, or am I missing some functionality?

Thank you for the help!"
954,6431,0,"Does TF support multicore processing on Android?. As we know, iPhone play better performance than other Android mobiles on single CPU, presenting Android takes more time to run an inference. But Android usually have four or more CPU cores and iPone have only two. So I want to speed up Android's inference by using multicore processing.

**What solutions have you tried?**
I add -fopenmp build options according to Eigen multi-threading docs, but it doesn't work, the speed is still the same. 
Could anyone point me in the right direction here?
Thanks!"
987,24542,0,"Step to Build for Centos 7. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Latest
- Are you willing to contribute it (Yes/No): No



Can anyone give the steps for building this for Centos 7. "
133,9213,1,"System hangs when computing gradients on GPU. ### Problem description

My system hangs when computing gradients on the GPU.  I am able to compute gradients on the CPU without issue.  I can compute all the nodes in my graph, including my loss function, on the GPU without issue.  My system hangs when computing gradients on the GPU irrespective of which optimizer I use.  The code that produces this issue is:



### Further information

I tried setting  in my ConfigProto but that didn't help.  

Launching  before running the code shows Volatile GPU-Util goes to 100% just before hanging.

Computing gradients on the CPU and printing them out shows normal output (I'm not getting any NaNs or 0's or anything like that).

### System information

- System: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: v1.0.0-65-g4763edf-dirty 1.0.1
- CUDA version: 8.0, V8.0.61
- cuDNN version: 5.1
- GPU model and memory: NVidia GeForce GTX TITAN X (12GB)
- Video card driver: 375.39"
874,1558,0,"Cyclic dependency error on build. On my laptop, I get a cyclic dependency error
### Environment info

Operating System: Ubuntu 15.10

If installed from sources, provide the commit hash:
fd464caaa40cfa16c81712939e39bd14d88c6fd4 (most recent HEAD as of this submission)

google protobuf commit
fb714b3606bd663b823f6960a73d052f97283b74

bazel version 
Build label: 0.1.5
Build target: bazel-out/local_linux-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Feb 9 19:15:13 2016 (1455045313)
Build timestamp: 1455045313
Build timestamp as int: 1455045313
### Steps to reproduce
1.  bazel test tensorflow/...  or bazel build -c opt //tensorflow/cc:tutorials_example_trainer
### What have you tried?
1.  trying to go through the chain of protobuf dependencies, but pointers would be appreciated.
2.  tried HEAD on protobuf
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).
../../bin/bazel build //tensorflow/cc:tutorials_example_trainer
____Loading...
ERROR: /projects/tensorflow/google/protobuf/BUILD:272:1: in cc_binary rule //google/protobuf:protoc: cycle in dependency graph:
    //tensorflow/cc:tutorials_example_trainer
    //tensorflow/core:tensorflow
    //tensorflow/core:tensorflow_opensource
    //tensorflow/core:core
    //tensorflow/core:core_cpu
    //tensorflow/core:core_cpu_internal
    //tensorflow/core:framework_internal
    //tensorflow/core/kernels:bounds_check
    //tensorflow/core:lib
    //tensorflow/core:lib_internal
    //tensorflow/core:protos_all_cc
    //tensorflow/core:framework/function.pb.h
    //tensorflow/core:protos_all_cc_genproto
- //google/protobuf:protoc
  //google/protobuf:protoc_lib
  //google/protobuf:protobuf
  //google/protobuf:src/google/protobuf/map_lite_unittest.pb.h
  //google/protobuf:cc_test_protos_genproto
- //google/protobuf:protoc.
  ERROR: Loading of target '//tensorflow/cc:tutorials_example_trainer' failed; build aborted.
  ERROR: Loading failed; build aborted.
"
474,3886,0,"control_dependencies() maybe fails to mfence when an assign() with different shape occurs. ### Environment info

Operating System: OS X El Capitan

Installed CPU version (OSX pip package)
tensorflow version: 0.10.0rc0
### Steps to reproduce

Consider the following code snippet:



The expected output is , but instead I get 
### What have you tried?

If we don't assign an inconsistent shape (e.g. assign  instead of ), then things work the way one would expect (output is )

If we replace



with



then we get the expected output of  instead of .

I suspect this is because calling  may somehow trigger a memory fence which fixes the broken behavior.

Also, it is possible that this is the intended behavior and that I misunderstood control_dependencies(), in which case I apologize.
"
473,26218,0,"tf-gpu==1.13.1  : 35% less batch size before OOM vs tf-gpu==1.11.0. **System information**
- Windows 7
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.11.0 , 1.13.1
- Python version: 3.6.5
- CUDA/cuDNN version: 9/7.1.4 , 10/7.4.1
- GPU model and memory: GTX 1060 6GB

**Describe the current behavior**

I have standard AE network with pixel shuffler layer.

on  maximum batch size for my GTX 1060 6GB is 

but after upgrade to  tf cannot handle same batch size it produces OOM error
and maximum now  for my card.

**Describe the expected behavior**

expected not to downgrade performance when upgrading tensorflow

**Code to reproduce the issue**






**Other info / logs**



"
628,28053,0,"Failed to get device properties, error code: 30. **Describe the current behavior**
I commented under issue #26255 but the original poster closed the issue as his problem was solved by updating to tensorflow 2.

I am opening a new issue because updating to the pre-release is not an option and I have no way to even trap this error to try to handle it, plus it is an unknown error code so no hint as to how to proceed.

Unknown error and failure to initialize GPU. 

My configuration:
Windows 10 Home
Tensorflow 1.13.1
Python 3.5
GTX 1060 Mobile Max-Q

It doesn't happen every time I run my program. I have localized it to running load_model from keras, before reaching that point I have imported tensorflow and verified that gpu is available.

Is there a way to catch this error or check for it and recover/attempt to reinitialize?

Thanks
**Describe the expected behavior**
Report the actual error, provide mechanism to catch the exception and handle it.

**Code to reproduce the issue**
The failure is intermittent

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
783,30621,0,"RPI3 - g++-4.8 does'nt exist on debian buster (june 2019). no way I am gonna fill this
guys, update your doc please ;-)"
608,21782,0,"profiler trace_steps is not match global step in distribution training. ### System information

== cat /etc/issue ===============================================
Linux gpu0198 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux gpu0198 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.14.5
protobuf                           3.6.1
tensorflow-gpu                     1.10.0

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = v1.10.0-0-g656e7a2b34
tf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/nvidia/cpu_lib:/nodemanager/lib/native:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/java//jre/lib/amd64/server:/nodemanager/lib/native
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Aug 22 03:12:21 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.46                 Driver Version: 390.46                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000000:86:00.0 Off |                    0 |
| N/A   28C    P0    38W / 250W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a 


### Describe the problem

When I use tensorflow profiler to get the timeline for distribution training, I found the trace_steps didn't match the global step and it result in useless timeline.
I check the profile_context.py and find that the profile step is not related to the global step.

distribution config:
ps 2
worker 8
1 gpu per worker

### Source code / logs
sync code:
with tf.contrib.tfprof.ProfileContext(profile_dir=FLAGS.train_dir,
                                                trace_steps=range(100, 200,3),
                                                dump_steps=[200],
                                                enabled=FLAGS.enable_profile,
                                                debug=FLAGS.enable_profile_debug) as pctx:
...
...
                optimizer = tf.train.SyncReplicasOptimizer(
                    opt=optimizer,
                    replicas_to_aggregate=FLAGS.replicas_to_aggregate,
                    total_num_replicas=worker_replicas,
                    variable_averages=variable_averages,
                    variables_to_average=moving_average_variables)


sync train log :
worker 0:
debug: tracing step: 148
debug: tracing step: 151
INFO:tensorflow:global step 19: loss = 2.1377 (8.300 sec/step)
debug: tracing step: 154
debug: tracing step: 157

async code:
with tf.contrib.tfprof.ProfileContext(profile_dir=FLAGS.train_dir,
                                                trace_steps=range(800, 900),
                                                dump_steps=[200],
                                                enabled=FLAGS.enable_profile,
                                                debug=FLAGS.enable_profile_debug) as pctx:

async train log:
worker 0:
debug: tracing step: 838
INFO:tensorflow:global step 2198: loss = 2.0687 (6.361 sec/step)
debug: tracing step: 839

worker 7:
debug: tracing step: 825
INFO:tensorflow:global step 3137: loss = 2.5605 (7.343 sec/step)
debug: tracing step: 826
"
505,9056,0,"tensorboard doesnt display y-axis correctly.. ![image](https://cloud.githubusercontent.com/assets/11971499/24820310/61102b8a-1bb6-11e7-8128-36ac5ab99bf4.png)

This plot makes it look like AUC is greater than 1.0 when it is not.

c:\Python35\Scripts>pip show tensorflow-gpu
Name: tensorflow-gpu
Version: 1.1.0rc1
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: c:\python35\lib\site-packages
Requires: numpy, wheel, protobuf, six, werkzeug
"
1008,14265,0,"tf.layers generates an extra op when not specifying the name. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
pip install
- **TensorFlow version (use command below)**:
v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 
3.5.2
- **Bazel version (if compiling from source)**:
NA
- **GCC/Compiler version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
8.0.61
- **GPU model and memory**:
GeForce GTX 1080 Ti, 11GB
- **Exact command to reproduce**:

### Describe the problem
When using tf.layers, I notice that an extra op is generated when not specifying the name. 

For example, here is an example using tf.layers.dense. ""dense_1"" is an extra op. Similar problem has been observed for tf.layers.conv2d also. 

The graph in tensorboard is as follows. 
![image](https://user-images.githubusercontent.com/13603534/32421364-b057853e-c24c-11e7-8aa7-865a42fc22ee.png)

If I add the name, 

then the graph looks as expected,
![image](https://user-images.githubusercontent.com/13603534/32421378-daedd956-c24c-11e7-9487-ecf9ed0d8027.png)


"
977,18540,0,"Tensorflow leaks 1280 bytes with each session opened and closed? (Python API). - **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.6 (also tested on 1.7)
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
9.0/7.0
- **GPU model and memory**:
Titan XP, 12GB
- **Exact command to reproduce**:
To reproduce, save the following python script as :

Then run it from the command line using different number of iterations:
, , , and so on.

### Describe the problem
It seems that each Tensorflow session I open and close consumes 1280 bytes from the GPU memory, which are not released until the python kernel is terminated. 

Running the script given above, which simply opens and closes sessions without any further operation, yields these results:
 -  yields 
 -  yields .
 -  yields .
 -  yields .
 -  yields .

The math is easy - each session opened and closed leaks 1280 bytes. I tested this script on two different ubuntu 17.10 workstations with tensorflow-gpu 1.6 and 1.7 and different NVIDIA GPUs.

(here's a related [stackoverflow question](https://stackoverflow.com/q/49735217/1500585), at least one user was able to reproduce)."
1094,6606,0,"could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM Check failed: stream->parent()-GetConvolveAlgorithms(&algorithms) ```. 
I'm trying to use tensorflow for this project: https://github.com/ibab/tensorflow-wavenet

I've gotten to the point where when I import tensorflow, I get the messages that all the CUDA libraries are successfully opened locally.

I can run the following python code from https://www.tensorflow.org/get_started/os_setup#run_tensorflow_from_the_command_line and it works fine.

> import tensorflow as tf
> hello = tf.constant('Hello, TensorFlow!')
> sess = tf.Session()
> print(sess.run(hello))
Hello, TensorFlow!
> a = tf.constant(10)
> b = tf.constant(32)
> print(sess.run(a + b))
42
>

However when I run the wavenet project, I get the following error messages and then python crashes.

tf.global_variables_initializer




### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/4251


### Environment info
Operating System:
Windows

Installed version of CUDA and cuDNN: 
cuDNN v5.1 (August 10, 2016), for CUDA 8.0

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from .
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
NameError: name 'tensor' is not defined

If installed from source, provide 

1. The commit hash ()
2. The output of 


### What other attempted solutions have you tried?

I have tried reinstalling Cuda, different versions of cudnn. Looked at different issues with same error messages but nothing seemed to help.

"
615,23610,0,"Causal Convolutions in Tensorflow. **System information**
- TensorFlow version (you are using): 1.10
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
This is feature request to support **causal convolutions** in the Tensorflow layers API. Currently, causal convolutions require  with manual masking of weights. To improve speed, the data can be cropped/padded and then ordinary convolutions used.  

**Will this change the current api? How?**
Yes,  will have a  boolean flag.

**Who will benefit with this feature?**
Researchers and practitioners working on auto-regressive convolutional models.

**Any Other info.**
An analysis at OpenAI revealed that many open-source implementations of causal convolutions are slow, incorrect, and/or have uninterpretable code. This feature would help the ML community more easily develop architectures containing causal convolutions."
570,32793,0,"device_lib.list_local_devices() InvalidArgumentError: Invalid device ordinal value (1). Valid range is [0, 0].. 
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux
- TensorFlow installed from (source or binary): pip3 installed
- TensorFlow version (use command below): 2.0.0-rc1
- Python version: 3.6.2
- CUDA/cuDNN version: 10.0, 7.6.3
- GPU model and memory:

output of  from the terminal:


NOTE: in the above output it shows that it is using  but my  environment variable is pointing to CUDA 10.0.

Snippet of code that cause the problem:


error message:


Potential cause and current workaround:
In the terminal output I notice that because the Quadro P1000  in my workstation only has 5 multiprocessor and so by default tf will not use it (minimum 8), so I added the following line to my 



and run  and it works. Another potential solution if I don't want to set the min GPU Multiprocessor count I can remove the Quadro P1000 from my workstation. I suspect that there is an inconsistency within list_local_devices() that fetch all GPUs in the workstation but didn't update base on min gpu multiprocessor count rule. So I run another experiment to see if I can reproduce the error after setting  to 5

The below code will reproduce the same error:


This will produce the same error but if we call  before calling  and then we call  again, there is no error. I suspect that maybe setting device to visible may interact weirdly with list_local_devices().


"
373,7683,0,"Package not Reslove.. Hello,

I am using TensorFlow Android Camera Demo. In TensorFlowInferenceInterface class there are some package not reslove like:

import org.tensorflow.DataType;
import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import org.tensorflow.TensorFlow;

Can any one help to find out where some thing is missing."
1291,5413,0,"StudentT.cdf() bug. With the TF0.11 rc2 (also with rc0) I get the following error while trying to evaluate the CDF of the StudentT distribution:
self.dtype"
360,10729,0,"tf.nn.max_pool wrong docs?. ### System information
Not Applicable

### Describe the problem
[API](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) states that ksize has length >= 4, the size of window for each dimension of the input tensor. However, value is a 4-D Tensor so doesn't this mean that ksize should be length == 4? Same for strides.

Digging into maxpooling_op.cc shows that there's some check that does . Line 212:




"
1283,6862,0,"Error when tensorflow installed with conda virtual environment in windows 10. ### Environment info
Operating System:
    windows 10

I was trying to install tensorflow in  with the guidance of the .
Because the version of  in my pc is  which is not compatiable with the guidance as it say  . Therefore, I use  to create a virtual environmnt and install tensorflow.

However, an error occurs:
Traceback (most recent call last):
  File ""D:\Program Files\anaconda\envs\tensorflow\Scripts\pip-script.py"", line 5, in <module>
    sys.exit(pip.main())
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\__init__.py"", line 249, in main
    return command.main(cmd_args)
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\basecommand.py"", line 252, in main
    pip_version_check(session)
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\utils\outdated.py"", line 102, in pip_version_check
    installed_version = get_installed_version(""pip"")
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\utils\__init__.py"", line 838, in get_installed_version
    working_set = pkg_resources.WorkingSet()
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 644, in __init__
    self.add_entry(entry)
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 700, in add_entry
    for dist in find_distributions(entry, True):
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1949, in find_eggs_in_zip
    if metadata.has_metadata('PKG-INFO'):
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1463, in has_metadata
    return self.egg_info and self._has(self._fn(self.egg_info, name))
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1823, in _has
    return zip_path in self.zipinfo or zip_path in self._index()
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1703, in zipinfo
    return self._zip_manifests.load(self.loader.archive)
  File ""D:\Program Files\anaconda\envs\tensorflow\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1643, in load
    mtime = os.stat(path).st_mtime
FileNotFoundError: [WinError 2] 系统找不到指定的文件。: 'D:\\Program Files\\anaconda\\envs\\tensorflow\\lib\\site-packages\\setuptools-27.2.0-py3.5.egg'

Besides, the ananconda is installed in , while there is no file folder .
After searching in stackoverflow, I didn't find the same problem, since most guys just install tf in other systems. 

Is it that conda virtual environment of  not  compatiable with tf in windows?? And why tf in windows doesn't support python 2.7?? 

Thanks!"
227,26543,1,"loss becoming 'nan' and accuracy dropping to 5% using tf.keras. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab env
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): !pip3 install -U tensorflow==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: no GPU used


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

""loss becoming 'nan' and accuracy dropping to 5% using tf.keras, with fashion-mnist, on 4th epoch (sometimes it happens on 7th / 8th epoch)

""loss should not become 'nan' and accuracy should never drop unexpectedly
Print output : 
Epoch #1	 Loss: 0.679400	Accuracy: 0.729167
Epoch #2	 Loss: 0.558087	Accuracy: 0.770833
Epoch #3	 Loss: 0.487591	Accuracy: 0.812500
Epoch #4	 Loss: 0.429859	Accuracy: 0.833333
**Epoch #5	 Loss: nan	        Accuracy: 0.052083**
[Fashion_mnist_with_keras_eager_and_tf_data.zip](https://github.com/tensorflow/tensorflow/files/2949581/Fashion_mnist_with_keras_eager_and_tf_data.zip)


**Code to reproduce the issue**
Code attached in jupyter notebook format

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1021,7385,0,"[Tensorboard Request]Ignoring specific subdirectories. I think this will be simple but very useful feature for Tensorboard.

What I want is the ignoring specific subdirectories.

Tensorboard load all of subdirectries and display them, but it become slower as more subfolders are added. So, I just want to ignore some directories without moving them.

There's several options to do this.
1. If there is special character(like #) in folder name, just ignore it.
2. Like git, .ignore file manage folders to ignore.
3. If .nolog file in the folder to ignore.(Like .nomedia file)

I think option 3 is best, because folder structure can be changed."
788,33407,0,"Eager mode not being disabled tf 1.14.0. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Trying out example from Tensorflow Probability with the following code:


Getting following error:


Looks like eager mode is enabled. Have added a command to disable it still it's getting activated."
737,34762,0,"AttributeError: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 2.0.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.27.1
- **GCC/Compiler version (if compiling from source)**: 7.4.0
- **CUDA/cuDNN version**: 10.2
- **GPU model and memory**: NVIDIA 970
- **Exact command to reproduce**: python3 legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:



### Describe the problem
Trying tensorflow's trainning appears an error message about a non existed attribute **(register_op_list')** . I checked in the file that is supposed to have it and it really doesn't have it. 
I looked for that attribute in every single path I thought it could be but I didn't get anything at all.
I have no clue of how I can resolve this.


To resolve the contrib issue of tf2.0 i used [tf-slim](https://github.com/adrianc-a/tf-slim) but if you have a way to resolve it completely that doesn't trigger this error please let me know because I'm quite sure that everything comes because of the ""contrib"" problem.
Thanks




### Source code / logs

**### python3 legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config**

2019-12-02 13:59:36.908351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
Traceback (most recent call last):
  File ""legacy/train.py"", line 48, in <module>
    from tensorflow.contrib import framework as contrib_framework
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 31, in <module>
    from tensorflow.contrib import cloud
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/__init__.py"", line 24, in <module>
    from tensorflow.contrib.cloud.python.ops.bigquery_reader_ops import *
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/python/ops/bigquery_reader_ops.py"", line 21, in <module>
    from tensorflow.contrib.cloud.python.ops import gen_bigquery_reader_ops
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py"", line 369, in <module>
    _op_def_lib = _InitOpDefLibrary(b""\n\355\001\n\016BigQueryReader\032\024\n\rreader_handle\030\007\200\001\001\""\027\n\tcontainer\022\006string\032\002\022\000\""\031\n\013shared_name\022\006string\032\002\022\000\""\024\n\nproject_id\022\006string\""\024\n\ndataset_id\022\006string\""\022\n\010table_id\022\006string\""\027\n\007columns\022\014list(string)\""\027\n\020timestamp_millis\022\003int\""\034\n\016test_end_point\022\006string\032\002\022\000\210\001\001\n\331\001\n GenerateBigQueryReaderPartitions\032\016\n\npartitions\030\007\""\024\n\nproject_id\022\006string\""\024\n\ndataset_id\022\006string\""\022\n\010table_id\022\006string\""\027\n\007columns\022\014list(string)\""\027\n\020timestamp_millis\022\003int\""\025\n\016num_partitions\022\003int\""\034\n\016test_end_point\022\006string\032\002\022\000"")
  File ""/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py"", line 277, in _InitOpDefLibrary
    _op_def_registry.register_op_list(op_list)
**AttributeError: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'**"
463,27329,0,"Unable to convert frozen graph to usable model on iPhone. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 2.7.15rc1
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 1080


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**


I am fine-tuning the [pretrained SSD-MobileNetV1](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz ) model using the  [config file](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config) for detecting the bounding boxes of objects in an image. 






After training, I am freezing the model using 



After the   is being generated, I'd like to convert the model to a model format that is compatible with iPhones. I tried the following approaches: 

### Approach 1 - Convert to mlmodel



The conversion code looks like:



Tried the above with Faster RCNN ResNet101 and SSD MobileNet - both give the same error.

Not sure why the  has to be a  as its a detection problem.



### Approach 2 - Convert to tflite 



Also, tried the following:



But, I am not sure what the last layer is, as the model is fairly convoluted. Tried visualizing with Netron but 

**Describe the expected behavior**
Seamless conversion to tflite and mlmodel files.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
NA

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
NA
"
1130,19267,0,"Op type not registered 'NoOp' from toco. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux ubuntu 17.10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.6
- **Python version**: python 3.6.3
- **Bazel version (if compiling from source)**: bazel-0.11.1-installer-linux-x86_64.sh
- **GCC/Compiler version (if compiling from source)**: gcc/g++ 6.4.0
- **CUDA/cuDNN version**: CUDA:9.1 cuDNN 7.1
- **GPU model and memory**: gtx 1060 6GB
- **Exact command to reproduce**:
bazel run --config=opt --copt=-msse4.1 --copt=-msse4.2   //tensorflow/contrib/lite/toco:toco -- --input_file=/home/andy/data/models/Best_model/MFN.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=/home/andy/data/models/Best_model/MFN.tflite   --inference_type=FLOAT   --inference_input_type=FLOAT   --input_arrays=input   --output_arrays=embeddings   --input_shapes=1,112,112,3

### Describe the problem
I had trained and frozen a MobileNet_v2 pretrain model which is under slim architecture to tensorflow graphdef protocol, when I use  toco convert to tflite(command as flow item 1),  I got a error as flow item 2. hold for someone kindly help me solve it, thanks.

1. toco cmd


2. error logs

"
304,33302,0,"Tflite coverter error. tensorflow/lite/toco/tooling_util.cc:935. <em>I'm trying to convert a TensorFlow model to tflite model from a frozen graph.
I' getting an error saying I should make a bug report to tflite team.

**you can download the graph from [HERE.**](https://drive.google.com/file/d/1LqyZWJsZadOHCxY8kG8JAskMoRgOWzSH/view?usp=sharing) 

any help would be much appreciated. </em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7

**Describe the current behavior**
tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array ""sample_sequence/while/Exit_3"" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.
Aborted (core dumped)
**Describe the expected behavior**

**Code to reproduce the issue**
**import tensorflow as tf
import sys
from tensorflow.python.platform import gfile

from tensorflow.core.protobuf import saved_model_pb2
from tensorflow.python.util import compat

graph_def_file = ""frozen_355.pb""
input_arrays = [""sample_sequence/model/Shape""]
output_arrays = [""sample_sequence/while/Exit_3""]

converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""converted_model_0.tflite"", ""wb"").write(tflite_model)**


**Other info / logs**
ConverterError                            Traceback (most recent call last)
<ipython-input-4-e27c2f171b9b> in <module>()
      4 
      5 converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
----> 6 tflite_model = converter.convert()
      7 open(""content/drive/My Drive/converted_model_0.tflite"", ""wb"").write(tflite_model)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198       stdout = _try_convert_to_unicode(stdout)
    199       stderr = _try_convert_to_unicode(stderr)
--> 200       raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
    201   finally:
    202     # Must manually cleanup files.

ConverterError: See console for info.
2019-10-13 11:18:54.608948: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array ""sample_sequence/while/Exit_3"" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.
Aborted (core dumped)

"
370,24510,0,"pip install tensorflow - as described on tensorflow.org/install failed. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10 x64
- Python version: 3.7.1 (via Chocolatey)


**Describe the problem**

Trying to install tensorflow fails when following the instuctions on tensorflow.org/install

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Starting with a clean Windows 10 x64 machine
2. choco install python   (in Powershell, which installed 3.7.1)
3. python -m pip install --upgrade pip
4. pip install tensorflow    (as described at tensorflow.org/install)
5. failed with:
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow

**Any other info**

Google returned this stack overflow with lots of accepts:
https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip

It suggested the following, which worked:
python -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl

**PowerShell output**

PS C:\WINDOWS\system32> pip install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
You are using pip version 10.0.1, however version 18.1 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.
PS C:\WINDOWS\system32> python -m pip install --upgrade pip
Collecting pip
  Downloading https://files.pythonhosted.org/packages/c2/d7/90f34cb0d83a6c5631cf71dfe64cc1054598c843a92b400e55675cc2ac37/pip-18.1-py2.py3-none-any.whl (1.3MB)
    100% |████████████████████████████████| 1.3MB ...
Installing collected packages: pip
  Found existing installation: pip 10.0.1
    Uninstalling pip-10.0.1:
      Successfully uninstalled pip-10.0.1
Successfully installed pip-18.1
PS C:\WINDOWS\system32> pip install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow"
1055,35154,0,"Cannot train keras model with 2 outputs. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Family
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10 / 7.6.4
- GPU model and memory: GeForce 940M

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**




A tf.keras model is created with 2 output heads:




Then an arbitary sized training sample is created:




When the data is called for training,



 an error occurs saying that the module expected 2 arrays, but when I swap out the validation dataset with a pair of 2 arrays:



another error pops up saying that the x and y dimensions must be the same. To put simply, keras thinks that the pair of validation outputs is an array.

**Describe the expected behavior**
The model.fit method runs and the model is trained for 1 epoch with 50 samples.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
copy the above code in order, with the last model.fit method not usable.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1128,13473,0,"StochasticTensor strange behavior. I've noticed strange behavior of StochasticTensor. Please, look at this peace of code: 

    inputs = tf.placeholder(shape=(1, 10, ), name=""inputs"", dtype=tf.float32)

    outputs = tf.layers.dense(inputs, units=10, use_bias=False, activation=tf.nn.sigmoid)
    outputs = st.StochasticTensor(distributions.Bernoulli(probs=outputs, dtype=tf.int32))
    outputs = tf.reshape(outputs, shape=(-1, ))
    
    init_op = tf.group(
        tf.global_variables_initializer(),
        tf.local_variables_initializer()
    )
    
    with tf.Session() as sess:
        sess.run(init_op)
        x = np.random.rand(1, 10)
        
        tf.set_random_seed(2017)
        z1 = sess.run(outputs, feed_dict={inputs: x})
        
        tf.set_random_seed(2017)
        z2 = sess.run(outputs, feed_dict={inputs: x})
    
    print(z1)
    print(z2)

As the result I get:


But numpy sampling has another behavior:

        np.random.seed(2017)
        x = np.random.randint(0, 10, size=10)

        np.random.seed(2017)
        y = np.random.randint(0, 10, size=10)

        print(x)
        print(y)

And (again) the result:


Is it an issue?

P.S.
Tensorflow v1.3.0-rc2-20-g0787eee"
1141,30841,0,"cannot detect communication between clusters when CollectiveAllReduceStrategy . tensorflow 1.12
CUDA 10
Ubuntu 16.04

Here is the part of code in the main script:
  strategy=tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS.n_gpus)#devices=[""/gpu:0"", ""/gpu:1""]
  if FLAGS.strategy==""ps"":
    os.environ['TF_CONFIG'] = json.dumps({
        'cluster': {
            ""worker"": [""10.60.131.148:8081"", ""10.60.131.149:8082""],
            ""ps"": [""10.60.131.150:8083""]
        },
        'task': {'type': FLAGS.type, 'index': FLAGS.index}
    })
    strategy = tf.contrib.distribute.ParameterServerStrategy(num_gpus_per_worker=FLAGS.n_gpus)
  elif FLAGS.strategy==""collective"":
    os.environ['TF_CONFIG'] = json.dumps({
        'cluster': {
            ""worker"": [""10.60.131.148:8888"",""10.60.131.149:8888"", ""10.60.131.150:8888""],
        },
        'task': {'type': FLAGS.type, 'index': FLAGS.index}
    })
    strategy = tf.contrib.distribute.CollectiveAllReduceStrategy(num_gpus_per_worker=FLAGS.n_gpus)

  run_config = tf.estimator.RunConfig(
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      train_distribute=strategy
      )
  estimator = tf.estimator.Estimator(
      model_fn=model_fn,
      config=run_config,
      params={""batch_size"": FLAGS.train_batch_size}
  )

  if FLAGS.do_train:
    tf.logging.info(""***** Running training *****"")
    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)
    train_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=FLAGS.max_seq_length,
        max_predictions_per_seq=FLAGS.max_predictions_per_seq,
        is_training=True)
    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)

I have three machines each has 3 gpus.
When I run CollectiveAllReduceStrategy, set type as 'worker' and index, the num of gpus for each worker as 3.I run the script independently on three clusters,  and use iftop -n to monitor the network, I did notice any communication between the clusters.
No errors, and from the log I did't know if it now runs the correctly way. Or just single machine 3 gpus, running independently.

Any suggestions or reference links?
"
1308,9296,0,"Tensorflow 1.1 build breaks on CUDA code. ### System Information
- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*: Linux Ubuntu 17.04
- *TensorFlow installed from (source or binary)?*: source
- *TensorFlow version* (use command below): v1.1.0-rc2
- *Bazel version (if compiling from source)*: 0.4.5
- *CUDA/cuDNN version*: 8.0/5.1.5
- *GPU Model and Memory*:M2000M

### Describe the problem clearly

Fails to build after upgrade to Ubuntu 17.04 (from 16.10), using the same compiler (gcc 5.4.1) and compiler flags. 

Build command is 

I've tried compiling with the default -march=native and -march=core-avx-i as well as using command given [here](http://stackoverflow.com/a/41584791)

### Source Code / Logs


Same error is then repeated for different parameter types (ex. const long/int *)
"
263,23527,1,"Acc and loss evaluate to 0.00 and trains for only 1 global steps.. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):1.12
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:1xTesla K80  12GB GDDR5 VRAM


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
My model's accuracy and loss are evaluating to 0.

**Describe the expected behavior**
The global steps should be 1625 but it's 1.
The acc and loss shouldn't be equal to 0 as both of them are contradicting each other.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
my input function,keras estimator,train_and_evaluate are as follows:


def keras_estimator(model_dir,config):
  base_model = Xception(weights='imagenet', include_top=False,input_shape = (512,512,3),classes = 5)


  x = base_model.output
  x = GlobalAveragePooling2D()(x)
  
  x = Dense(1024, activation='relu')(x)
  x = Dropout(0.2)(x)
  x = Dense(256, activation='relu')(x)
  x = Dropout(0.2)(x)
  
  predictions = Dense(5, activation='softmax')(x)

  
  model = Model(inputs=base_model.input, outputs=predictions)

  
  for layer in base_model.layers:
      layer.trainable = False
  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])
  estimator = tf.keras.estimator.model_to_estimator(keras_model=model,model_dir=model_dir,
    config=config)
  return estimator


def train_and_evaluate(model_dir):
  t_batch_size = 512
  e_batch_size = 64
  num_epochs = 25
  import pandas as pd
  df = pd.read_csv('/content/trainLabels.csv')
  from random import shuffle
  addrs = ['/content/train/train/' + str(df.iloc[i]['image']) + '.jpeg' for i in range(len(df))]
  labels = df['level'].values.tolist()
  c = list(zip(addrs, labels))
  shuffle(c)
  addrs1, labels1 = zip(*c)
  train_addrs = addrs1[0 : int(0.9 * len(addrs))]
  train_labels = labels1[0 : int(0.9 * len(labels))]
  val_addrs = addrs1[ int(0.9 * len(addrs)) : ]
  val_labels = labels1[ int(0.9 * len(addrs)) : ]
  train_addrs = list(train_addrs)
  train_labels = list(train_labels)
  val_addrs = list(val_addrs)
  val_labels = list(val_labels)
  
  run_config = tf.estimator.RunConfig(save_checkpoints_secs=300)
  
  estimator = keras_estimator(model_dir,run_config)
  
  t_max_steps = (len(train_addrs) // t_batch_size) * num_epochs
  
  train_spec = tf.estimator.TrainSpec(input_fn = lambda : make_input_fn(train_addrs,train_labels,t_batch_size,mode=tf.estimator.ModeKeys.TRAIN),max_steps = t_max_steps)
  
  eval_spec = tf.estimator.EvalSpec(input_fn = lambda : make_input_fn(val_addrs,val_labels,e_batch_size,mode=tf.estimator.ModeKeys.EVAL),steps = None,start_delay_secs=10,
        throttle_secs=300)
  
  
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.
WARNING:tensorflow:From <ipython-input-7-80b5bdaf35df>:9: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use .
WARNING:tensorflow:From <ipython-input-7-80b5bdaf35df>:12: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use .
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/content/training/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})
INFO:tensorflow:Warm-starting from: ('/content/training/keras/keras_model.ckpt',)
INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/iterations; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/lr; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/beta_1; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/beta_2; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: Adam/decay; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_1; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_2; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_3; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_4; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_5; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_6; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_7; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_8; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_9; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_10; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_11; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_12; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_13; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_14; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_15; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_16; prev_var_name: Unchanged
INFO:tensorflow:Warm-starting variable: training/Adam/Variable_17; prev_var_name: Unchanged
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /content/training/model.ckpt.
INFO:tensorflow:Saving checkpoints for 1 into /content/training/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-11-05-13:21:17
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /content/training/model.ckpt-1
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-11-05-13:22:08
INFO:tensorflow:Saving dict for global step 1: acc = 0.0, global_step = 1, loss = 0.0
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1: /content/training/model.ckpt-1
INFO:tensorflow:Loss for final step: None.`

"
1418,31820,0,tflite output different result with pbfile when using only one convolutional layer ?. https://github.com/tensorflow/tensorflow/issues/31359
1328,2858,0,"[install problem] Tensorflow with Anaconda on Ubuntu. On my Ubuntu 14.04, I have installed tensorflow from source, as specified in the Tensorflow Installation instructions and it works well.

Then I install anaconda but it changes the $PATH environment variable, so I cannot import tensorflow.
Following the instruction in http://stackoverflow.com/questions/33646541/tensorflow-and-anaconda-on-ubuntu, I created an environment and installed tensorflow in it, as specified in https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#anaconda-installation. I use this command  to install tensorflow. But the tensorflow is not installed in this environment. It seems that it just updates the tensorflow I have installed previously. The terminal window is shown in the image:

I have solved this problem. Use  instead. Using sudo will install the package globally. 

![terminal](https://cloud.githubusercontent.com/assets/19935904/16053977/1d0deb32-3239-11e6-9875-f14eb6cc22b3.png)
"
1225,20997,0,"ValueError: Empty Range for RandRange() in Train Function for Custom Classifier. 
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
I used stock example script provided by TensorFlow to write my own custom code.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 - Colab Notebook

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
N/A

- **TensorFlow installed from (source or binary)**:
Not sure

- **TensorFlow version (use command below)**:
1.9

- **Python version**:
3.6 (I think)

- **Bazel version (if compiling from source)**:
Unsure

- **GCC/Compiler version (if compiling from source)**:
Unsure

- **CUDA/cuDNN version**:
N/A

- **GPU model and memory**:
N/A

- **Exact command to reproduce**:

        # Train the CNN model
        mnist_classifier.train(input_fn=train_input_fn,steps=train_steps)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I want to train multiple CNNs of the same structure (lenet) using randomly selected subsets of varying sizes from the MNIST training set. I was able to use the same line of code successfully in an earlier program. This work is contributing towards the development of a computational model for executive function deficits in ADHD as part of a PhD thesis in Pharmaceutical Sciences.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
[Quantitative WM Model.txt](https://github.com/tensorflow/tensorflow/files/2214655/Quantitative.WM.Model.txt)
"
379,29334,0,"[TF 2.0 API Docs] tf.image.central_crop. ## URL(s) with the issue:

https://www.tensorflow.org/versions/master/api_docs/python/tf/image/central_crop
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py

## Description of issue (what needs changing):

### Usage example

No usage example provided

### Submit a pull request?

Yes"
314,25069,0,"Floating point exception when trying to build and compile my keras model  with xla support. . SYSTEM INFO:

UBUNTU 16.04
TENSORFLOW 1.12.0 (compiled from source with XLA support)
KERAS 2.2.4
GPU Nvidia Geforce RTX 2080 Ti

I am trying to build and compile a simple CNN with XLA but I get a floating point exception.

# Here's the block of code




I am getting the following Output for this

2019-01-21 16:33:21.512092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-21 16:33:21.512662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:02:00.0
totalMemory: 10.73GiB freeMemory: 10.33GiB
2019-01-21 16:33:21.512678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-21 16:33:24.538253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-21 16:33:24.538298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-21 16:33:24.538312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-21 16:33:24.538695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9966 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 32)      320       
_________________________________________________________________
activation_1 (Activation)    (None, 128, 128, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 126, 126, 32)      9248      
_________________________________________________________________
activation_2 (Activation)    (None, 126, 126, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 63, 63, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 61, 61, 64)        18496     
_________________________________________________________________
activation_3 (Activation)    (None, 61, 61, 64)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 30, 30, 64)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 57600)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               14745856  
_________________________________________________________________
activation_4 (Activation)    (None, 256)               0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 34)                8738      
_________________________________________________________________
activation_5 (Activation)    (None, 34)                0         
=================================================================
Total params: 14,782,658
Trainable params: 14,782,658
Non-trainable params: 0
_________________________________________________________________
2019-01-21 16:33:25.223019: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f6f20001130 executing computations on platform CUDA. Devices:
**2019-01-21 16:33:25.223063: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
Floating point exception (core dumped)**


when I remove this part 


everything works fine.



 "
924,18914,0,"Dose Titan Xp work in Windows10?. Hi i bought new GPU - Titan Xp for learning, so i set the system like below:
CPU-AMD Ryzen 51600 Six-Core Processor
GPU-Titan Xp
Tensorflow-1.5.0 GPU
CUDA-9.0
cuDNN-7.0

But i found that when the calculation begins, the data looks not stacked on GPU, so that it occurs error message.

does Titan XP work in Windows 10 to use tensorflow gpu?"
1165,7980,0,"TensorFlow upgrade to 1.0.0 breaking import. On the Mac OS X El Capitan V 10.11.6 using python 2.7.11 with anaconda

when I run this code:

pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py2-none-any.whl

I get a working version of tensorflow 0.9.0. import tensorflow in python throws no errors.

I type in the command:

pip install --upgrade tensorflow

I get a successful install 



and I get a broken import statement in python (.. modify dir names)



when I import again in the same interface, I get a different error which repeats if I do it anymore

"
472,14504,0,"Cannot use keras estimator_from_model() in distributed cluster. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tf.VERSION = 1.4.0 tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0.61/6.0.21
- **GPU model and memory**: NVIDIA Tesla M60 8 GB
- **Exact command to reproduce**: See Below

### Describe the problem
When trying to use an estimator that is derived from  and training with , it will work as expected if in a standalone non-distributed session. However, when in a distributed training cluster and the TF_CONFIG has the cluster information set, there is a an explicit device assignment of an op to a device that is not valid in the current cluster spec.

Below is code to reproduce this issue. When  is set to True an error is throws as shown in the log below. When  is set to False the network is constructed and trained as intended. It should be noted that the error occurs when calling  and not when doing the training, the cluster config is required for the distributed training to take place.

The TF_CONFIG that is set below is derived from calling the code using the gcloud SDK as follows:


### Source code / logs
Minimal example:


InvalidArgumentError emitted when :


Full logs, tf_env, and more are here: https://gist.github.com/droidicus/2abd4ddad81a1e9169a1c7a100057b15
"
872,2867,0,"Support size 0 tensors with parse_example and parse_single_example. Currently, this fails :



(The same is true for .)

This sort of thing may seem useless, but can be handy for writing generic code that handles variable sized input (with fixed size for each graph).
"
240,26559,1,"tensorflow1.12 hangs at LocalMaster::RunStep with tf.train.MonitoredTrainingSession. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.12.0-18-gd60b574' 1.12.0

**Describe the current behavior**
In distributed tensorflow, some workers hang at the last batch of dataset when the dataset epoch is 1 and the step in StopAtStepHook() is very large. What's more, the number of samples in the last batch of dataset may less than the batch_size.
If we use tf.train.StopAtStepHook() to stop training, all workers can exit successfully.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
In local_master.cc, WaitForNotification() function is as follows:

Workers hang at 'n->WaitForNotification()'. We confuse that WaitForNotificationWithTimeout(n, timeout_in_us) has been called, why WaitForNotification() is called again ?

bt is as follows:



Using 'thread apply all bt' in gdb, we can see there are two threads wait at cond_var_.wait(l) in BackgroundWorker::WorkerLoop() method.
"
1484,33192,0,"Incorrect result when subtracting 1 from exponential of Variable . **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: Intel Iris Plus Graphics 640 1536 MB

**Describe the current behavior**
With the following setup

the following code

produces two different outputs, specifically:


**Describe the expected behavior**
That code should print identical arrays (the latter is correct, the former is not). 

**Code to reproduce the issue**

Also see Colab notebook here: https://colab.research.google.com/drive/1KT2gfuWeezhWOr3zfyHhk9HvCH7JcShm

**Other info / logs**
I have no idea what's happening, but some observations:
 - there needs to be a Variable involved. If I use a constant instead of a Variable, all works as expected.
 - the exponent value is important. If I change the 4's to 1's then all works as expected, for example.
 - the TF exponential is important. If instead I calculate the exponential with numpy and then set the Variable to that exponential, subtracting 1 works correctly.
 - specifically subtracting 1 seems important. If instead I subtract 2, or 0.1, or 1j, all works as expected."
816,21723,0,"I got a keras_applications ModuleNotFoundError when I compile my tensorflow 1.10. hi, I want compile my tensorflow 1.10 to update. But I got an error when I compile it:
ModuleNotFoundError: No module named 'keras_applications'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines failed build steps.
Anyone has the same error?"
489,9926,0,"Issues with RoCE support. When rendezvous_mgr->RecvLocalAsync fails, grpc responds with the Status, while grpc+verbs does not. Should we consider this situation ? @junshi15 "
218,10145,1,"local distributed tensorflow async btw graph multigpu example is extremely slow. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not really, this primarily a copy and paste of the distributed tensorflow example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.2 GPU
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: Titan X Pascal, 12G, 4 total
- **Exact command to reproduce**:



async_btwgraph_launcher.py

async_btwgraph_dist_trainer.py

### Describe the problem
I'm trying to use the distributed tensorflow [example](https://www.tensorflow.org/versions/r1.2/deploy/distributed) to do async between graph replication on a 4 Titan X machine, with 1 GPU per worker.  Without distributed TF and using a single GPU, the same code trains at ~150-200 steps/sec.  As shown at the end of the log below, this distributed trainer clocks at ~ 2 steps/sec.  The 4 GPUs are barely utilized,
![image](https://cloud.githubusercontent.com/assets/15891975/26372649/9d8f25cc-3fcc-11e7-87f0-0be4b1a80fb6.png)
with plenty of  CPU headroom,
![image](https://cloud.githubusercontent.com/assets/15891975/26372709/c4203ca8-3fcc-11e7-804f-5daa97bc5b70.png)

Also, if I simply remove the parameter server from this example, but keeping all 4 workers, they all grab GPU:0 maxing it out, and each worker process running at ~50steps/sec, and GPUs 1-3 are unused.
![image](https://cloud.githubusercontent.com/assets/15891975/26373341/28da56e0-3fcf-11e7-87a7-db64dfac13e4.png)
However, see that I'm setting os.environ[""CUDA_VISIBLE_DEVICES""] to enable only 1 unique GPU per worker.  

Is this expected behavior?
Thanks,
Luke 

### Source code / logs
"
448,6733,0,"Export meta graph option in image retraining. I was trying to tensorflow serve my retrained graph ( *.pb file ) but if I understand correctly it doesn't contain meta graph in it, so wouldn't it be nice to have export meta graph options in retrain.py ?"
1480,24388,0,"Way to use exported graph from speech recognition/commands example in TF serving. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): yes



**Describe the feature and the current behavior/state.**
I am using speech commands TF example and have trained and built the model with filename extension .pb and I would like to use somehow that model in tensorflow serving and thats where we have problem.
Tensorflow serving accepts other format of model that have trainedmodel.pb file and ""variables"" subdirectory.

TF serving accepted model view:
![image](https://user-images.githubusercontent.com/37185376/50054342-2e221b80-0141-11e9-9176-66a964c8a844.png)

exported model from speech recognition example:
![image](https://user-images.githubusercontent.com/37185376/50054362-5ad63300-0141-11e9-8953-870a8c9e9ea5.png)

If we are missing something here, excuse our incompetence and would likely help in any way here.

Also, if request is valid, there could be be used two approaches to tackle this request, one is allowing exporting of model from tf examples to format accepted by tf serving or to add feature to tf serving to accept more model variants.


**Will this change the current api? How?**

**Who will benefit with this feature?**
Any developer that is using tensorflow where model version variances would not be an obstacle to use tensorflow serving

**Any Other info.**
"
884,34561,0,"Import statement for Tensorflow returning error. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 3.6.2
- Installed using: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- UserBenchmarks: Game 11%, Desk 12%, Work 11%
- CPU: Intel Pentium D 3.40GHz - 1.4%
- GPU: Nvidia GeForce 8600 GT - 1%
- HDD: Seagate Barracuda 7200.10 320GB - 17.9%
- USB: JetFlash Transcend 8GB - 1.5%
- RAM: Unknown 3GB - 2.8%
- MBD: Gigabyte GA-945PL-S3


<em>Hi, I recently installed tensorflow using pip (after going through so much problems), and just when i tried to import tensorflow as tf in my project it returns this error:</em>

<p>Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Lase\Documents\Programming stuff\Python\demo 1\.vs\demo 1\main.py"", line 137, in <module>
    import tensorflow as tf
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
</p>

<em>Steps taken to solve problem</em>
1. Repeatedly uninstalling and reinstalling tensorflow to see any change, but none worked
2. I also noticed the same thing happens when i try to import keras . Keras tries to import tensorflow and returns the same error
3. Tried installing it on a virtual environment using virtualenv but still didnt work

   "
1164,34097,0,"python/keras/engine/network.py:layers took great deal of prediction time on deep networks when using methods like predict_on_batch. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary, docker
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0


**Describe the current behavior**

When I am using keras and profiling with cProfile, I found that seemly innocent s of  become performance bottleneck.



The functions are called fairly frequently without caching the results, and as a result I see them along with related functions ( for example) took inproportional amount of cpu cycles when gpu is hungry. It does not seem right to call the functions again and again when their values are fixed, and they are using 2/3 of the training time. (When will anyone train a network whose dynamicness and number of layers are constantly fluctuating?)

Anyone may try with standard deep resnet with predict_generator/fit_generator and find out whether they are most time consuming.
"
642,11051,0,"tensorflow inference in iOS produces EXC_BAD_ACCESS error when memory mapped graph is used. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX Sierra (10.12.5)
- **TensorFlow installed from (source or binary)**: source & binary (tested both)
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: 0.5.1-homebrew
- **CUDA/cuDNN version**: CPU version used
- **GPU model and memory**: CPU version used
- **Exact command to reproduce**: 

Apologies if I posted an issue not appropriate here.
I asked this issue on stackoverflow a week ago but seems that no one interested in this issue.

===========
I tested both iOS simple and camera example in tensorflow GitHub repo and they worked well.

I checked those projects and recognized that camera example can use memory mapped graph if I modify constant variable ""model_uses_memory_mapping"" to true, while simple example cannot.

So I modified simple example source to implement same function as camera example, and it seems that the mmapped graph loaded without problem, but when I run inference with session->Run method, the app gave me EXE_BAD_ACCESS error.

I think I've done everything what I can do but still same error.

No idea what else I can do for I'm not good at iOS nor tensorflow core functions.

Could someone guide me how can I resolve this?

FYI, run inference with optimized or quantized graph (with model_uses_memory_mapping set to false) works well.

a) here's message I get when i execute session->Run method:

tensorflow::Env::NewReadOnlyMemoryRegionFromFile:
    0x103157ad0 <+0>:  pushq  %rbp
    0x103157ad1 <+1>:  movq   %rsp, %rbp
    0x103157ad4 <+4>:  pushq  %r15
    0x103157ad6 <+6>:  pushq  %r14
    0x103157ad8 <+8>:  pushq  %rbx
    0x103157ad9 <+9>:  pushq  %rax
    0x103157ada <+10>: movq   %rcx, %r14
    0x103157add <+13>: movq   %rdx, %r15
    0x103157ae0 <+16>: movq   %rdi, %rbx
    0x103157ae3 <+19>: movq   (%rsi), %rax
    0x103157ae6 <+22>: leaq   -0x20(%rbp), %rcx
->  0x103157aea <+26>: callq  *0x10(%rax)  
    0x103157aed <+29>: cmpq   $0x0, (%rbx)
    0x103157af1 <+33>: jne    0x103157b06               ; <+54>
    0x103157af3 <+35>: movq   -0x20(%rbp), %rsi
    0x103157af7 <+39>: movq   (%rsi), %rax
    0x103157afa <+42>: movq   %rbx, %rdi
    0x103157afd <+45>: movq   %r15, %rdx
    0x103157b00 <+48>: movq   %r14, %rcx
    0x103157b03 <+51>: callq  *0x18(%rax)
    0x103157b06 <+54>: movq   %rbx, %rax
    0x103157b09 <+57>: addq   $0x8, %rsp
    0x103157b0d <+61>: popq   %rbx
    0x103157b0e <+62>: popq   %r14
    0x103157b10 <+64>: popq   %r15
    0x103157b12 <+66>: popq   %rbp
    0x103157b13 <+67>: retq   
    0x103157b14 <+68>: nopw   %cs:(%rax,%rax)

static NSString* model_file_name = @""mmapped_graph"";
static NSString* model_file_type = @""pb"";
const bool model_uses_memory_mapping = true;  //use memory mapped graph

static NSString* labels_file_name = @""retrained_labels"";
static NSString* labels_file_type = @""txt"";

const int wanted_width = 299;
const int wanted_height = 299;
const int wanted_channels = 3;
const float input_mean = 128.0f;
const float input_std = 128.0f;
const std::string input_layer = ""Mul"";
const std::string output_layer = ""final_result"";

tensorflow::Status LoadMemoryMappedModel(
    NSString* file_name, NSString* file_type,
    std::unique_ptr<tensorflow::Session>* session,
    std::unique_ptr<tensorflow::MemmappedEnv>* memmapped_env) {
    NSString* network_path = FilePathForResourceName(file_name, file_type);
    memmapped_env->reset(
        new tensorflow::MemmappedEnv(tensorflow::Env::Default())
    );
    tensorflow::Status mmap_status =
    (memmapped_env->get())->InitializeFromFile([network_path UTF8String]);
    if (!mmap_status.ok()) {
        LOG(ERROR) << ""MMap failed with "" << mmap_status.error_message();
        return mmap_status;
    }

    tensorflow::GraphDef tensorflow_graph;
    tensorflow::Status load_graph_status = ReadBinaryProto(
                                                           memmapped_env->get(),
                                                           tensorflow::MemmappedFileSystem::kMemmappedPackageDefaultGraphDef,
                                                           &tensorflow_graph);
    if (!load_graph_status.ok()) {
        LOG(ERROR) << ""MMap load graph failed with ""
        << load_graph_status.error_message();
        return load_graph_status;
    }

    tensorflow::SessionOptions options;
    // Disable optimizations on this graph so that constant folding doesn't
    // increase the memory footprint by creating new constant copies of the weight
    // parameters.
    options.config.mutable_graph_options()
    ->mutable_optimizer_options()
    ->set_opt_level(::tensorflow::OptimizerOptions::L0);
    options.env = memmapped_env->get();

    tensorflow::Session* session_pointer = nullptr;
    tensorflow::Status session_status =
    tensorflow::NewSession(options, &session_pointer);
    if (!session_status.ok()) {
        LOG(ERROR) << ""Could not create TensorFlow Session: "" << session_status;
        return session_status;
    }

    tensorflow::Status create_status = session_pointer->Create(tensorflow_graph);
    //tensorflow::Status create_status = session_pointer->Create(*(tensorflow::GraphDef *)tensorflow_graph);

    if (!create_status.ok()) {
        LOG(ERROR) << ""Could not create TensorFlow Graph: "" << create_status;
        return create_status;
    }

    session->reset(session_pointer);


    return tensorflow::Status::OK();
}

NSString* RunInferenceOnImage() {
  tensorflow::SessionOptions options;
    std::unique_ptr<tensorflow::Session> session;

  tensorflow::GraphDef tensorflow_graph;
  LOG(INFO) << ""Graph created."";

    tensorflow::Status load_status;

    if (model_uses_memory_mapping) {
        //use memmapped graph - gives me an error
        std::unique_ptr<tensorflow::MemmappedEnv>  tf_memmapped_env;
        load_status = LoadMemoryMappedModel(model_file_name, model_file_type, &session, &tf_memmapped_env);
    } else {
        // use optimized or quantized graph - this works well
        NSString* network_path = FilePathForResourceName(model_file_name, model_file_type);
        load_status = PortableReadFileToProto([network_path UTF8String],&session, &tensorflow_graph);
    }

    if (!load_status.ok()) {
        LOG(FATAL) << ""Couldn't load model: "" << load_status;
    }

  // Read the label list
  NSString* labels_path = FilePathForResourceName(@""retrained_labels"", @""txt"");
  std::vector<std::string> label_strings;
  std::ifstream t;
  t.open([labels_path UTF8String]);
  std::string line;
  while(t){
    std::getline(t, line);
    label_strings.push_back(line);
  }
  t.close();

  // Read the image.
  NSString* image_path = FilePathForResourceName(@""testimage"", @""jpg"");
  int image_width;
  int image_height;
  int image_channels;
  std::vector<tensorflow::uint8> image_data = LoadImageFromFile(
    [image_path UTF8String], &image_width, &image_height, &image_channels);
    LOG(INFO) << ""Graph created5."";

  // image_channel is set to 4 from LoadImageFromFile method (not modified)

  assert(image_channels >= wanted_channels);
  tensorflow::Tensor image_tensor(
      tensorflow::DT_FLOAT,
      tensorflow::TensorShape({
          1, wanted_height, wanted_width, wanted_channels}));
  auto image_tensor_mapped = image_tensor.tensor<float, 4>();

  tensorflow::uint8* in = image_data.data();
  tensorflow::uint8* in_end = (in + (image_height * image_width * image_channels));
  float* out = image_tensor_mapped.data();
  for (int y = 0; y < wanted_height; ++y) {
    const int in_y = (y * image_height) / wanted_height;
    tensorflow::uint8* in_row = in + (in_y * image_width * image_channels);
    float* out_row = out + (y * wanted_width * wanted_channels);
    for (int x = 0; x < wanted_width; ++x) {
      const int in_x = (x * image_width) / wanted_width;
      tensorflow::uint8* in_pixel = in_row + (in_x * image_channels);
      float* out_pixel = out_row + (x * wanted_channels);
      for (int c = 0; c < wanted_channels; ++c) {
        out_pixel[c] = (in_pixel[c] - input_mean) / input_std;
      }
    }
  }
    NSString* result = @"" Graph loaded!"";
  result = [NSString stringWithFormat: @""%@ - %d, %s - %dx%d"", result,
    label_strings.size(), label_strings[0].c_str(), image_width, image_height];

  std::vector<tensorflow::Tensor> outputs;
    if(session.get()) {
        LOG(INFO) << ""SESSION OK!!!!!!"";
    }


  tensorflow::Status run_status = session->Run({{input_layer, image_tensor}},
                               {output_layer}, {}, &outputs);
  // EXC_BAD_ACCESS error occur when session Run method called

  if (!run_status.ok()) {
  //  LOG(ERROR) << ""Running model failed: "" << run_status;
    tensorflow::LogAllRegisteredKernels();
    result = @""Error running model"";
    return result;
  }
  tensorflow::string status_string = run_status.ToString();
  result = [NSString stringWithFormat: @""%@ - %s"", result,
    status_string.c_str()];

  tensorflow::Tensor* output = &outputs[0];
  const int kNumResults = 5;
  const float kThreshold = 0.1f;
  std::vector<std::pair<float, int> > top_results;
  GetTopN(output->flat<float>(), kNumResults, kThreshold, &top_results);

  std::stringstream ss;
  ss.precision(3);
  for (const auto& result : top_results) {
    const float confidence = result.first;
    const int index = result.second;

    ss << index << "" "" << confidence << ""  "";

    // Write out the result as a string
    if (index < label_strings.size()) {
      // just for safety: theoretically, the output is under 1000 unless there
      // is some numerical issues leading to a wrong prediction.
      ss << label_strings[index];
    } else {
      ss << ""Prediction: "" << index;
    }

    ss << ""\n"";
  }

  LOG(INFO) << ""Predictions: "" << ss.str();

  tensorflow::string predictions = ss.str();
  result = [NSString stringWithFormat: @""%@ - %s"", result,
    predictions.c_str()];

  return result;
}
`

You can find original (unmodified) simple example project here: https://github.com/tensorflow/tensorflow/tree/v1.1.0/tensorflow/contrib/ios_examples/simple

https://github.com/tensorflow/tensorflow/blob/v1.1.0/tensorflow/contrib/ios_examples/simple/RunModelViewController.mm

also you can find LoadImageFromFile method here: https://github.com/tensorflow/tensorflow/blob/v1.1.0/tensorflow/contrib/ios_examples/simple/ios_image_load.mm"
80,18086,1,"distributed Tensorflow using grpc is slow. ### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6
- **Python version**:  python 3.4
- **CUDA/cuDNN version**: 8.0/6.1


### Describe the problem
Distributed Tensorflow using grpc is very slow , here is my timeline of a step 
![image](https://user-images.githubusercontent.com/33949779/38095081-2628836e-33a2-11e8-9f4c-883562bbf42f.png)
I want to know why there is a huge gap. GPU is idle for a long time, waiting for ps.

"
1406,29023,0,"Error occurred while compiling TensorFlow for Java . Error occurred while compiling TensorFlow for Java 
Hi guys. Can you help me to see why?
The following is the environment:
GPU: no
Gcc:4.8
TensorFlow: 1.7.1
Bazel: 0.10.0
![image](https://user-images.githubusercontent.com/10007145/58367415-9572a180-7f11-11e9-9c37-f1cb06670748.png)
"
1162,26577,0,"Accuracy Metric automatically selected, fails in certain cases. **System information**
- Uses a basic CNN MNIST Keras example
- CentOS7
- TensorFlow installed from: Anaconda
- TensorFlow version: Both 1.12-gpu and 2.0.0-alpha0-cpu
- Python version: 3.6

**Describe the current behavior**
* When using a ""built in"" loss function, the ""accuracy"" metric is **automatically** resolved to the ""correct one"".
* When using a custom loss function that ""touches""  and  in any way (even trivially as seen below), ""accuracy"" is no longer automatically resolved.

**Preamble:**


**""Working"" Example:**


**""Broken"" Example:**


**Describe the expected behavior**
I am not sure if this is intended, a bug, or a reasonable failure. 

**Broken Example Now Works:**
In order to get the ""Broken"" exmaple to ""work"" correctly, you have to specify the correct accuracy metric directly:



**Other info / logs**
* This was originally posted under #26490 but @timudk helped clarify my problem so I reposted as a separate issue."
661,7456,0,"Training using multiple GPUs returns Inf values for loss and Nan for grads. . I have two Tesla K80 cards (2 GPUs per card) and I spent few days testing a MNIST classification model using multiple GPUs. What I found is that the training process would always diverge (got Nan for grads and Inf for loss) when I use two GPUs which are in the same card, however when I allocated two GPUs to my training operation from different cards, it would lead to convergence. By the way, everything worked well on a single GPU. 

I am not sure about how GPUs compute those networks and it is really weird two GPUs from the same card make my model diverge and from different cards can make it converge.

The output for divergence is like the below:

I used cpu to preprocess the input data read from tfreords. My code for computing the average grads:


The training_op:

"
1262,1816,0,"`tf.reverse_sequence()` doesn't work with arguments of totally unknown shape.. From [Stack Overflow](http://stackoverflow.com/questions/36480456/dynamic-rnn-and-array-ops-reverse-sequence-problems):

> I am trying to reverse my inputs with array_ops.reverse_sequence() before sending it to dynamic_rnn(), the inference graph can be build with no problem, but when building the training graph, I got the following error:
> 
> 

The problem appears to arise when  is  (which is a valid possibility).
"
292,26060,0,"tf-lite: allocateTensors never returns. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I Have written custom code inspired from the tf-lite demo https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo. The code and model is here: https://github.com/BorisMansencal/tflite_test0
- OS Platform and Distribution: I build with up-to-date Android Studio 3.3.1 on macOS 10.13.6
- Mobile device: tested both on Nokia 7 Plus (TA-1046) with Android 9/API 28, and Motorola Moto G5S (XT1794) with Android 8.1.0/API 27.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): I use tensorflow-lite:0.0.0-nightly

**Describe the current behavior**

My code uses tf-lite for image segmentation.
It works correctly on Nokia 7 Plus (TA-1046) with Android 9/API 28.
It does not work on Motorola Moto G5S (XT1794) with Android 8.1.0/API 27. 

**Describe the expected behavior**

My code is doing image segmentation using a U-Net DNN architecture.  
It is inspired from the the tf-lite demo example. It returns an image, instead of a classification.

The code is available here: https://github.com/BorisMansencal/tflite_test0
When run on on mobile device, you should see an image and a ""run"" button. You click on the ""run"" button and when the inference is finished, you should see an image : with white where the object was detected, black elsewhere.

It works correctly on Nokia 7 Plus (TA-1046) with Android 9/API 28, but not on Motorola Moto G5S (XT1794) with Android 8.1.0/API 27.

It should work on any device with API 26 Android version.

**Code to reproduce the issue**
https://github.com/BorisMansencal/tflite_test0

**Other info / logs**

When I run the application in the debugger, from Android Studio, on the Motorola Moto G5S , I can see that the call to the Interpreter constructor (called at ImageClassifier.java:59) never returns.
If I step into the Interpreter constructor, the problem seems to come from void NativeInterpreterWrapper::init(long errorHandle, long modelHandle, Options options): the call to allocateTensors(this.interpreterHandle, errorHandle) never returns.


"
1033,31084,0,"Bug Issue: tf.case incompatible with list comprehension. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **custom**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **None**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below):
 **v1.14.0-rc1-22-gaf24dc91b5**
**1.14.0**
- Python version: **3.6**
- Bazel version (if compiling from source): **None**
- GCC/Compiler version (if compiling from source): **None**
- CUDA/cuDNN version: **None**
- GPU model and memory: **None**

**Describe the current behavior**

Bug 1: the function  below has the correct behaviour only when using the decorator . If the decorator is not used,  no longer consider the predicate and always outputs the same value.

Bug 2: even if the decorator  is used, the function  does not have the correct behaviour if I use list comprehension to create the list of pairs predicate / functions. Although, it has the correct behaviour when the list is populated iteratively with append (in a for loop). 

**Describe the expected behavior**

1. Decorating the function  with  should not affect , unless I am not aware of the intricacies of  and .

2. Choosing for loops or list comprehension to create the list of predicate / function pairs should not affect , unless (again) I am not aware of the intricacies of  and .

**Code to reproduce the issue**

@tf.functionrandom_choicetf.caseidx@tf.functiontf.caseidx

When removing , or when using  but using list comprehension to create  (this is the unexpected behaviour):


**Other info / logs**

I could reproduce this bug in tensorflow:
**2.0.0-beta1**
**v2.0.0-beta0-16-g1d91213fe7**
 
"
158,29494,1,"Slow model training in Tensorflow 1.11, 1.12, 1.13. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Docker
- TensorFlow version (use command below): 1.11, 1.12, 1.13.1,
- Python version: Version supplied in docker container
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory:

**Describe the current behavior**
I have a fairly large, complicated estimator-based model that I want to train using a Tesla P100. Previously, performance has been okay:



But starting from TF 1.11 and up (as of writing newest is 1.13.1), performance has been much worse:



Using , I have generated the following performance profile for Tensorflow 1.13.1: ![profile](https://user-images.githubusercontent.com/448023/59034033-71e11c80-886a-11e9-9fd6-d590a61b82eb.PNG). The last part of the profile is several applications of  and the operations they depend on (reshapes, sums, mul, etc). It seems that these Adam applications and intermediate operations are completely serial with no parallelism. Is this expected? I would imagine it to be as parallel as the model itself is (i.e. a model with many sequential dependencies would have many sequential dependencies when computing the gradients).

Unfortunately, as  is a fairly new addition I have no execution profiles for tensorflow versions older than 1.13.1.

There are a few differences in optimizer error/warnings that might explain why this difference in training speed happens:

- [ArithmeticOptimizer fails in 1.11 and up](https://github.com/tensorflow/tensorflow/issues/29052). The warning disappears if I use the nightly tensorflow docker images, but model training is still slow. A similar warning is logged in 1.10, but with a slightly different wording.
- Dependency optimizer fails in 1.11 and up, but not in 1.10:
    

[This issue](https://github.com/tensorflow/tensorflow/issues/20843) mentions a slowdown caused by additional graph optimization in 1.9. I do not think that this is the case here, as it is the steps of the model itself that is slow.

**Describe the expected behavior**
I would expect TF 1.11 to be as fast or faster as TF 1.10."
280,35242,0,"Flag --incompatible_disallow_empty_glob will break TensorFlow in Bazel 1.2.1. Incompatible flag --incompatible_disallow_empty_glob will break TensorFlow once Bazel 1.2.1 is released.

Please see the following CI builds for more information:

* [:darwin: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7"" target=""_blank"">:darwin: (OpenJDK 8)</a>)
* [:windows: (OpenJDK 8)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa"" target=""_blank"">:windows: (OpenJDK 8)</a>)
* [:ubuntu: 18.04 (OpenJDK 11)](<a href=""https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1"" target=""_blank"">:ubuntu: 18.04 (OpenJDK 11)</a>)

Questions? Please file an issue in https://github.com/bazelbuild/continuous-integration

**Important**: Please do NOT modify the issue title since that might break our tools.
"
85,28338,1,"Improper metric computation in fit method (tf 2.0). **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.13.6
- TensorFlow installed from (source or binary): from pip I think, I don't remember and I don't know the difference
- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.6.7

**Describe the current behavior**
When using the  method on a custom model (the model in the guide [Writing layers and models with TensorFlow Keras](https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example)), a simple  metric seems to be improperly computed. When fitting, the metric is always greater than the loss, whereas the loss is computed as the mean squared error plus a custom error defined as a KL divergence in the model.

**Describe the expected behavior**
The mean squared error metric should always be lesser than the loss, as it is when training the model with custom code instead of using the  method.

**Code to reproduce the issue**
 The following code is adapted from the guide [Writing layers and models with TensorFlow Keras](https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example). I simply customized the formatting and added a metric to track the mean squared error in addition to the total loss.

On a side note, the code in [the guide](https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example) seems to forget to use  on the metric at the beginning of each epoch.


The output of this code is something like this:

The mse loss is always smaller than the total loss which is expected.

Now if we run the following code:

The output looks like this:

Note that the loss seems be computed as in the previous code, but the mean squared error is definitely not.

Either there is a subtlety in  I am not aware of, either there is a bug.


"
740,16868,0,"Backpropagation/weight update issue with custom layer. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win 10
- **TensorFlow installed from (source or binary)**: From pip (binary)
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.5
- **Bazel version (if compiling from source)**:---
- **GCC/Compiler version (if compiling from source)**:----
- **CUDA/cuDNN version**:None
- **GPU model and memory**:None
- **Exact command to reproduce**:See source code

### Describe the problem
I am attempting to implement a custom layer. The layer uses the Image to Patch function and simple tensorflow operator. The layer is implemented in keras to simplify the model building and training but the backend is in tensorflow.

I am using a simple cnn as a benchmark, whenever I implement my custom layer ( even only as the first layer to 'encode' the data) backpropagation seems to break as no weights get updated in the entirety of the model.

From my understanding the all the operations used (mult, div, add, minus) are differentiable and things such as reshape, transpose and extract_image_patches should not prevent backpropagation and weight updates.

I tried using the basic layer building method and inheriting from the convolution class (_Conv) and both cases prevent the weight update for the whole model, but such a thing shouldn't be the case.

### Source code / logs

Prototype layer: https://github.com/roya0045/cvar2/blob/master/tfvar.py
Model builder: https://github.com/roya0045/cvar2/blob/master/test2.py"
155,3377,1,"Moving data from CPU to GPU is slow . I have noticed that when moving MNIST data from the CPU to the GPU, there is a significant time lag when using TensorFlow in comparison to Theano. Specifically, we have noticed this problem in the context of feed_dict, which moves information from the CPU to the GPU when running minibatches. I am using python 2.7. Our current solution to this problem is to move all of the data directly to the GPU at the beginning of the program, which is of course not sustainable unless one has a significant amount of space on their GPU. 

When we time 25000 minibatches of size 100 each, TensorFlow is approximately four times as slow as Theano. I have attached both files, and the time difference is evident in the final output. 
### Environment info

Operating System: Ubuntu 14.04

CPU Information:

Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                12
On-line CPU(s) list:   0-11
Thread(s) per core:    2
Core(s) per socket:    6
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Stepping:              2
CPU MHz:               1246.328
BogoMIPS:              6995.89
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              15360K
NUMA node0 CPU(s):     0-11

GPU: NVidia GeForce GTX TITAN X Graphics Card (12GB) 

Installed version of CUDA and cuDNN: 7.5.17
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).

Relevant files (I apologize for the poor naming conventions):

1.[mnist_softmax.txt](https://github.com/tensorflow/tensorflow/files/370296/mnist_softmax.txt)
This file contains the relevant program that uses TensorFlow. 
1. [mnist.pkl.gz](https://github.com/tensorflow/tensorflow/files/370306/mnist.pkl.gz)
   This file contains the dataset for the TensorFlow file. 
2.  [mnist_softmax_theano.txt(https://github.com/tensorflow/tensorflow/files/370314/mnist_softmax_theano.txt)
   This file contains the relevant program that uses Theano.
3. [tf_data.pkl.gz](https://github.com/tensorflow/tensorflow/files/370310/tf_data.pkl.gz)
   This file contains the dataset necessary for the _Theano_ file.
"
233,10163,1,"Custom Poets Models Run Slow on Android. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not really.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android/Windows
- **TensorFlow installed from (source or binary)**: Binary (From android nightly
- **TensorFlow version (use command below)**: 1.2

### Describe the problem
I've noticed that using a retrained inception model within the demo app, following the guidelines suggested, is awfully slow. Shouldn't the custom models generated in the style of Tensorflow for poets be pretty similar to the inception model that the demo comes with? I have noticed inference times to be around 5 times as slow on two devices. (Nexus 6P and Pixel C) compared to the original demo.
Even when the graphs are quantized I am getting no apparent performance increase (apart from model size). If anything it's actually slower.
Is this normal behaviour? I'm aware of the image size is different (224 vs 299) but is that enough to haemorrhage the performance?

### Source code / logs
Avg. ms for Conv2D is 1366ms 
Inference time ~1700ms (Pixel C) ~3500 (Nexus 6P)

Model building steps: Normal Model via Tensorflow for poets etc. --> strip nodes --> quantize --> replace in apk.
Same performance regardless of quantization.

@andrewharp this was what I referred to in the windows/android thread. Can move to s/o if preferred."
616,24043,0,"Pruning: Multi-GPU support. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): modified models/official/resnet/resnet_model.py to utilize Pruning's masked layers.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.10.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 16.1
- GCC/Compiler version (if compiling from source): gcc
- CUDA/cuDNN version: 8/6
- GPU model and memory: x2 nvidia titan Xp

**Describe the current behavior**
Multi-GPU training session of modified tensorflow/models for pruning, the session won't boot.
The same modified Resnet model _does_ execute iff num_gpus=1.

The error is:
ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context.
Raised from tensorflow/contrib/distribute/python/values.py"", line 336

[out_batch64.log](https://github.com/tensorflow/tensorflow/files/2629288/out_batch64.log)"
1068,11936,0,"does libtensorflow_inference.so contains both  ""code for training .pb file""  and ""code for analyzing .pb file""?. ### Describe the problem
 I'd like to import ""libtensorflow_inference.so"" to an android app to and make it possible to make some machine learning in the app. But the size of ""libtensorflow_inference.so""  file is too large which is 9.8M .

Because  I just want to import generated models(.pb file) to predict and don't need to train models in android app. Could you please tell me which code is used to import and analyze the .pb file and witch code is used to train a .pb file in tensorflow project?  

does libtensorflow_inference.so contains both  ""code for training .pb file""  and ""code for analyzing .pb file""? Is possible to remove the ""code for training .pb file"" to minimize its size  if it contains that?  


"
1079,5393,0,"Error in `/home/software/anaconda2/bin/python': invalid fastbin entry (free): 0x00007f2fa8023940. **Tensorflow version: 0.11.0rc1 (compile from source)
OS: CentOS Linux release 7.0.1406 (Core) 64bit
model: models/inception**

I am training inception model from scratch following [this](https://github.com/tensorflow/models/tree/master/inception), but after about 11500 steps got this error:

/home/software/anaconda2/bin/python': invalid fastbin entry (free): 0x00007f2fa8023940 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x7d19d)[0x7f315d7b919d]
/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x248ff48)[0x7f314baa2f48]
/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x244520f)[0x7f314ba5820f]
/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow19LocalRendezvousImpl4SendERKNS_10Rendezvous9ParsedKeyERKNS1_4ArgsERKNS_6TensorEb+0xf9)[0x7f314bb9e7f9]
/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow22IntraProcessRendezvous4SendERKNS_10Rendezvous9ParsedKeyERKNS1_4ArgsERKNS_6TensorEb+0xb4)[0x7f314ba57b74]
/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6SendOp7ComputeEPNS_15OpKernelContextE+0x346)[0x7f314baa3736]
/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x242ea59)[0x7f314ba41a59]
/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x2422e30)[0x7f314ba35e30]
/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x3c8)[0x7f314bc474a8]
/home/software/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x22)[0x7f314bc46c72]
/home/software/anaconda2/bin/../lib/libstdc++.so.6(+0xb4870)[0x7f3149153870]
/lib64/libpthread.so.0(+0x7df3)[0x7f315e20ddf3]
/lib64/libc.so.6(clone+0x6d)[0x7f315d8321ad]
======= Memory map: ========
00400000-00401000 r-xp 00000000 fd:02 34476856                           /home/software/anaconda2/bin/python2.7
00600000-00601000 rw-p 00000000 fd:02 34476856                           /home/software/anaconda2/bin/python2.7
0067e000-42ae4000 rw-p 00000000 00:00 0                                  [heap]
200000000-200100000 rw-s 1026d71000 00:05 221089                         /dev/nvidiactl
200100000-204100000 ---p 00000000 00:00 0 
204100000-204200000 rw-s f70ee2000 00:05 221089                          /dev/nvidiactl
204200000-204300000 ---p 00000000 00:00 0 
204300000-204400000 rw-s f75483000 00:05 221089                          /dev/nvidiactl
204400000-204500000 ---p 00000000 00:00 0 
204500000-204600000 rw-s 1014d38000 00:05 221089                         /dev/nvidiactl
204600000-208600000 ---p 00000000 00:00 0 
208600000-208700000 rw-s f7735a000 00:05 221089                          /dev/nvidiactl
208700000-208800000 ---p 00000000 00:00 0 
208800000-208900000 rw-s f7777d000 00:05 221089                          /dev/nvidiactl
208900000-208a00000 ---p 00000000 00:00 0 
208a00000-208b00000 rw-s f77eaa000 00:05 221089                          /dev/nvidiactl
208b00000-20cb00000 ---p 00000000 00:00 0 
...
...
`"
1380,34856,0,"Push up Switch ops in grappler. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
If we have a chain of ops  , both A and B will need to be evaluated regardless of the outcome of the Switch op. In this case, we can push up the Switch op so it becomes . That way, neither A or B needs to be evaluated if the wrong branch of the Switch is chosen. This would simplify the use of  by not requiring all conditional ops be created in the supplied true_fn or false_fn.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Users of tf.cond.

**Any Other info.**
"
1121,11612,0,pre-trained weights. can i use the tensorflow pre-trained weight model in keras and Vice versa?
1106,12874,0,"error in Building tensor flow. [sun.security.validator.ValidatorException:]. == cat /etc/issue ===============================================
Linux ravi 4.4.0-93-generic #116-Ubuntu SMP Fri Aug 11 21:17:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ravi 4.4.0-93-generic #116-Ubuntu SMP Fri Aug 11 21:17:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
Hello Below is compilation error i am getting.

$~/bin/bazel build -c opt --config=opt //tensorflow/tools/pip_package:build_pip_package
Extracting Bazel installation...
...........
ERROR: /home/ravi.spatil/LinuxShare/caffe_opecv_resources/mobilenet/tensorflow_ori/tensorflow/tools/pip_package/BUILD:101:1: no such package '@nsync//': Error downloading [https://github.com/google/nsync/archive/ad722c76c6e6653f66be2e1f69521b7f7517da55.tar.gz] to /home/ravi.spatil/.cache/bazel/_bazel_ravi.spatil/39b7c1709071717822a0ecd1200753ba/external/nsync/ad722c76c6e6653f66be2e1f69521b7f7517da55.tar.gz: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target and referenced by '//tensorflow/tools/pip_package:licenses'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted
INFO: Elapsed time: 28.497s
FAILED: Build did NOT complete successfully (106 packages loaded)

Below is system information.
--------------------------------------


== check pips ===================================================
numpy (1.13.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
../tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

"
666,4423,0,"set_shape cause gradient error in a condition case. ### Environment info

Operating System: Ubuntu 16.04

If installed from binary pip package, provide:
1. A link to the pip package you installed: [https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl)
2. The output from . 0.10.0rc0
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)



If the set_shape line is removed, everything works fine


"
169,30020,1,"tf.cond leads to memory leak?. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): Tf=1.13.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: TITAN RTX 24G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1247,27927,0,"Under MirroredStrategy and ParameterStrategy tf.trainable_variables() doesn't return the correct wrapped variables. Under MirroredStrategy and ParameterStrategy tf.trainable_variables() returns the normal unwrapped variables.

But according to the comments in mirrored_strategy.py and parameter_server_strategy.py, tf.trainable_variables() should return the wrapped variables.

Here tf 1.13.1 mirrored_strategy.py code segment,

The problem is that in kwargs past from up stream API, ""trainable"" is set but the value is None, so just ckeck

    if kwargs.get(""trainable"", True): 

is not enough, it should be

    if kwargs.get(""trainable"", True) or  kwargs.get(""trainable"", True) is None:

This bug exists in all version of tensorflow. For tf 1.13.1, here is the fix for mirred_strategy.py
"
174,19277,1,"SSD mobilenet inference is slower w/ MKL. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.8.0 (could repro the same problem from head too)
- **Python version**: 
N/A
- **Bazel version (if compiling from source)**:
0.13.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
OMP_NUM_THREADS=1 bazel run --config=mkl --config=opt --config=monolithic //tensorflow/tools/benchmark:benchmark_model -- --graph=ssd_mobilenet_v2_coco_2018_03_29_frozen.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=1,1920,1080,3 --output_layer=num_detections,detection_classes,detection_scores,detection_boxes --num_threads=1

### Describe the problem
w/ MKL, benchmark_model got 18.98B FLOPs/second, w/o MKL, it got 25.61B.
From the benchmark_model results, we could see that _MklConv2DWithBias is the culprit.
I am using MKL 2018.2.199 and mkldnn 0.14 on a i7-5557U CPU.

A unrelated question: @agramesh1 I filed the same bug on [mkldnn](https://github.com/intel/mkl-dnn/issues/234). They told me you have some plan to implement MKL version of DepthwiseConv2dNative. Do you have a timeline for it? I am eager to try it..

benchmark_model results:

 w/ MKL

             [Node type]  [count]  [avg ms]    [avg %]    [cdf %]  [mem KB][times called]
       _MklConv2DWithBias       12   143.936    42.710%    42.710% 56555.531       12
               _MklConv2D       43    48.872    14.502%    57.212% 43206.305       43
      DepthwiseConv2dNative       21    27.330     8.110%    65.321% 17180.992       21
                  _MklMul       43    12.751     3.784%    69.105% 32070.080       43
                  _MklAdd       53    12.137     3.601%    72.706% 33662.238       53
                     Cast      183    11.584     3.437%    76.143% 24883.217      183
        _MklInputConversion       96    10.990     3.261%    79.404% 32261.969       96
                    Const     1602    10.360     3.074%    82.479%     0.000     1602
                      Mul      127     7.063     2.096%    84.574%     0.004      127
                      Add      119     6.541     1.941%    86.515%    15.344      119
                    Relu6       47     3.715     1.102%    87.618%     0.000       47
                  Minimum      451     3.680     1.092%    88.709%     0.000      451
                   Gather      546     3.607     1.070%    89.780%     0.000      546
                    Slice       93     3.543     1.051%    90.831%  1380.240       93
       TensorArrayScatterV3        5     3.261     0.968%    91.799% 25604.012        5
              _MklReshape      107     3.183     0.944%    92.743%   810.972      107
                    Where      180     2.560     0.760%    93.503%     1.440      180
             _MklConcatV2       98     2.260     0.671%    94.173%   782.604       98
                  Greater      183     2.183     0.648%    94.821%   172.533      183

w/o MKL

              [Node type]  [count]  [avg ms]    [avg %]    [cdf %]  [mem KB][times called]
                   Conv2D       55   108.692    47.515%    47.515% 32798.539       55
    DepthwiseConv2dNative       21    28.022    12.250%    59.765% 17180.992       21
                      Mul      170    18.866     8.247%    68.012%     0.008      170
                      Add      172    18.460     8.070%    76.082%    15.344      172
                     Cast      183    12.116     5.297%    81.378% 24883.217      183
                   Gather      546     4.815     2.105%    83.483%     0.000      546
     TensorArrayScatterV3        5     3.971     1.736%    85.219% 25604.012        5
                    Relu6       47     3.619     1.582%    86.801%     0.000       47
                    Slice       93     2.915     1.274%    88.075%  1380.240       93
                  Minimum      451     2.877     1.258%    89.333%     0.000      451
                    Const      476     2.802     1.225%    90.558%     0.000      476
                  Maximum      360     2.761     1.207%    91.765%     0.000      360
                  Reshape      287     2.318     1.013%    92.778%     0.000      287
                    Where      180     1.793     0.784%    93.562%     1.440      180
                      Sub      190     1.749     0.765%    94.327%     0.008      190
                    Split      180     1.749     0.765%    95.091%     0.000      180
                  Greater      183     1.220     0.533%    95.625%   172.533      183
                 ConcatV2       99     1.057     0.462%    96.087%   730.868       99
                    Shape      112     1.001     0.438%    96.524%     0.952      112
                  Squeeze       92     0.978     0.428%    96.952%     0.000       92
           ResizeBilinear        1     0.885     0.387%    97.339%  1080.000        1
             StridedSlice      113     0.878     0.384%    97.722%     0.432      113
      NonMaxSuppressionV2       90     0.820     0.358%    98.081%     0.000       90
                Transpose        3     0.706     0.309%    98.390%    92.016        3
                    Enter       26     0.434     0.190%    98.579%     0.000       26
                     Pack       19     0.433     0.189%    98.769%    30.908       19
                ZerosLike       92     0.362     0.158%    98.927%     0.004       92
                  BiasAdd       12     0.326     0.143%    99.069%     0.000       12
            NextIteration        8     0.308     0.135%    99.204%     0.000        8


### Source code / logs
I run benchmark_model with MKLDDN_VERBOSE=1 and got this log.
https://drive.google.com/file/d/12ClzFKiOryge6So-trXrvEyEk6ycOheY/view?usp=sharing"
741,9467,0,"Link Error for the deprecated/__init__.py. It seems that the link ""look '[here](https://www.tensorflow.org/code/tensorflow/contrib/deprecated/__init__.py)'"" can not point to the right page in the [doc of histogram_summary](https://www.tensorflow.org/api_docs/python/tf/contrib/deprecated/histogram_summary), should we just change it to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/deprecated/__init__.py?

Please go to Stack Overflow for help and support:

http://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
425,3724,0,"Source-compiled .whl package is much slower in training. Recently, from my experiment, I found that running inception model using the .whl file generated by myself is much slower than using .whl file downloaded directly from the $TF_BINARY_URL.
### Environment info

Operating System: ubuntu14.04
CUDA: cuda7.5
CUDNN: cudnn 5

I followed the instructions
./configure
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
And I got tensorflow-0.9.0-py2-none-any.whl to install using:
pip install /home/dl/bxl/tensorflow/tensorflow_pkg/tensorflow-0.9.0-py2-none-any.whl
 (the name of this .whl file is automatically generated )

Then I downloaded the inception model related files to train following the instructions:
cd ~/models/inception
bazel build inception/imagenet_train
bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=64 --train_dir=/tmp/imagenet_train --data_dir=/data1/ImageNet

From the information printed out, I found that the speed is 8.5 samples/sec. (In the first several lines printed out, it says：
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally,
from which I guess CUDA/CuDNN are automatically loaded)

However, if I replace the .whl file by that directly downloaded from the $TF_BINARY_URL provided in www.tensorflow.org ( tensorflow-0.9.0-cp27-none-linux_x86_64.whl, which uses CuDNN4 and cuda7.5), and reinstall it using pip, and restart a new process to train the inception model using the same code. 

I got that the training speed was 20.2 samples/sec, which is nearly 2.5x faster than my first training using the .whl package generated by myself. 

So it is very difficult to explain. Because I use CUDNN5, I expect my version is faster, but in fact, it is 2.5x slower....

Does anyone know why??

Thanks in advance.
"
827,30703,0,"Compiling 1.14 with MPI support. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Centos 6.9
- 
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source):0.25.2
- GCC/Compiler version (if compiling from source): 4.9,3
- CUDA/cuDNN version:10.0.130/7
- GPU model and memory: cuda



Compiling with MPI support gives the following build errors:
INFO: From Compiling tensorflow/contrib/mpi_collectives/kernels/ring.cu.cc:
external/com_google_absl/absl/strings/string_view.h(495): warning: expression has no effect
tensorflow/contrib/mpi_collectives/kernels/ring.cu.cc(109): error: identifier ""CudaLaunchKernel"" is undefined
tensorflow/contrib/mpi_collectives/kernels/ring.cu.cc(110): error: identifier ""CudaLaunchKernel"" is undefined
tensorflow/contrib/mpi_collectives/kernels/ring.cu.cc(111): error: identifier ""CudaLaunchKernel"" is undefined
3 errors detected in the compilation of ""/tmp/tmpxft_00038d5b_00000000-6_ring.cu.cpp1.ii"".

Standard ./configure but answer yes to MPI support


Compiles fine without MPI. Have tried with both openmpi/3.1.3 and cuda enabled openmpi/3.1.3

"
347,28815,0,"ArrayIndexOutOfBoundsException: length=1; index=1. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Android Studio 3.4.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
OnePlus 6

**Describe the problem**

I am using the tensorflow example object_detection app, it's working fine. But when I try plugging in my own model ""tiny-yolo-obj.lite"" (created with Tiny YOLO) and ""labelmap2.txt"" (first line being ???, other 36 lines my objects) it throws the following error:



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

**My model meta:**



**My code in DetectorActivity**



Any help would be greatly appreciated! Thanks in advance."
458,7038,0,"multiple dequeue ops are optimized away in latest TF. Multiple ops Dequeue ops from the same queue started getting optimized away in latest TensorFlow:

The following executes dequeue once in Jan17 head, but 3 times in 12.1 


One gets expected behavior (3 dequeues) when graph optimization is turned off

 

Self-contained repro: https://github.com/yaroslavvb/stuff/blob/master/parallel_dequeue_test.py

Came up in http://stackoverflow.com/questions/41830206/how-to-share-a-queue-containing-variable-length-sequences-batches-between-multip"
361,18170,0,"How about supporting delegation of OEM operations in TFL ?. Currently, the interpreter delegates only built-in ops to NN API. I'm writing here to see if any plan to support to delegate OEM operations(ANEURALNETWORKS_OEM_OPERATION) defined in
https://android.googlesource.com/platform/frameworks/ml/+/master/nn/runtime/include/NeuralNetworksOEM.h

My understanding is it's just a minor change on TOCO and interpreter, right ?"
246,22988,1,"2x slower using post-training quantization from tensorflow-lite. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  
- **TensorFlow version (use command below)**: 
- **Python version**: 2.7.12

### Describe the problem
I read and run the post-training quantization code from official tensorflow Medium (https://medium.com/tensorflow/introducing-the-model-optimization-toolkit-for-tensorflow-254aca1ba0a3). 
The post said the technique can result in up to 3x faster execution for relevant machine learning models. But I found during inference is 2x slower. 

### Source code / logs
I run the Colab notebook (https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tutorials/post_training_quant.ipynb) and compare inference time before and after quantization from the *Evaluate the models* section. 

The log before quantization: 


The log after quantization: 


The accuracy is basically the same, but it's much slower. "
1454,33037,0,"radon and inverse radon transform, Projection and back_projection function in CT. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):  tf 1.14
- Are you willing to contribute it (Yes/No):  no
   When training a CT-reconstructed model, the projection and back_projection functions are usually used. However, to my knowledge, there is no api  to implement the above functions.


**Describe the feature and the current behavior/state.**
  Like in the scikit-image package, the projection function is the radon transform (skimage.transform.radon) while the back-projection is the dual operator of projection.

**Will this change the current api? How?**
Will add two maybe three apis.

**Who will benefit with this feature?**
anyone uses these functions to train CT-reconstruction moedl.

**Any Other info.**
"
668,18693,0,"supporting vector parameters for mixture . Hi,
In the example below:

# Create a mixture of two Gaussians:
tfd = tf.contrib.distributions
mix = 0.3
bimix_gauss = tfd.Mixture(
  cat=tfd.Categorical(probs=[mix, 1.-mix]),
  components=[
    tfd.Normal(loc=-1., scale=0.1),
    tfd.Normal(loc=+1., scale=0.5),
])

**_# Why cannot I apply it to vector parameters?_** 

    tfd.Normal(loc=[-1, 0]., scale=[0.1, 0.2),
    tfd.Normal(loc=[+1., 2.], scale=[0.5, 3.]),



Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
879,24209,0,"Tensorflow-GPU ImportError: DLL load failed. ### System information
- *OS:* Windows 10
- **TensorFlow version**: Tensorflow v 1.12
- **Python version**: 3.6.0
- **CUDA/cuDNN version**: CUDA 10.0, cuDNN 7.4.1.5
- **GPU model and memory**: 2080 ti

I'm trying to get Tensorflow set-up on my machine. I've followed about 10 different tutorials but always come back to this same issue. I see that a bunch of other people have the same issue and just downgraded their CUDA to 9.0 due to Tensorflow not supporting 10.0. My problem is that the 2080 ti is only supported with CUDA 10.0

Is anyone else using one of the RTX cards and have they had any of the same issues? My log is below.

### Source code / logs
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\cmacphail\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
"
649,18658,0,"TensorflowLite - Android. I am using TensorflowLite for Android. I tried setting up the java demo project as described in Tensorflow site (Android demo app) and succeeded.  
I am getting an error while running the app in Android studio.  ERROR (Error:(152, 15) error: cannot find symbol method setNumThreads(int))

In file  **ImageClassifier.java**
**public void setNumThreads(int num_threads) {
    if (tflite != null)
        tflite.setNumThreads(num_threads);

  }**

I understand that the method setNumThreads is not found in the Interpreter.  needed help to resolve this issue..


"
635,31361,0,"UniqueV2 reports incorrect output shape. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04.6 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.5.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0.130
- GPU model and memory: Tesla K40C, 11441MiB

**Describe the current behavior**
When not using eager execution, UniqueV2 always reports its first output to have rank 1.

**Describe the expected behavior**
UniqueV2 should report its first output to have the same rank as its input.

**Code to reproduce the issue**
The bug can be exposed by forcing non-eager execution through  or . The former is demonstrated below:

This outputs

but should output

"
822,19643,0,"keras.layers.BatchNormalization update_ops not added. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.8.0-1660-ga543d94
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.13.0
- **GCC/Compiler version (if compiling from source)**: 4.9.3
- **CUDA/cuDNN version**: 9.1 / 7.1
- **GPU model and memory**: GeForce GTX 1070 , 8119MiB
- **Exact command to reproduce**: See source code below

### Describe the problem
 does not add any operations to  under certain version constraints.

### Source code / logs

Output using v1.8.0-1660-ga543d94, cuda 9.1, cudnn 7.1, installed from source (other details as above):

Output using v1.8.0-0-g93bc2e2072, cuda 9.0, cudnn 7.0, pip installed
"
919,27931,0,"GPU build on ARM-64 failing. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04 running on Nvidia Jetson Nano board
Linux jetson-01 4.9.140-tegra #1 SMP PREEMPT Wed Mar 13 00:32:22 PDT 2019 aarch64 aarch64 aarch64 GNU/Linux

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A

- TensorFlow installed from (source or binary):
source

- TensorFlow version:
v1.13.1

- Python version:
Python 2.7.15rc1

- Installed using virtualenv? pip? conda?:
N/A

- Bazel version (if compiling from source):

- GCC/Compiler version (if compiling from source):
gcc (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04) 7.3.0

- CUDA/cuDNN version:
cuda 10, cuDNN 7

- GPU model and memory:
Tegra based GPU on Jetson Nano bord



**Describe the problem**
I am trying to build TensorFlow on ARM64 (Jetson Nano board) for use with GPU. Build is failing.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Enabled 6GB swap space per: https://www.jetsonhacks.com/2019/04/14/jetson-nano-use-more-memory/
- Built bazel 0.21 from source.
- Checked out v1.13.1 from TF repo
- Applied these changes to build files (diff attached below): https://github.com/tensorflow/tensorflow/issues/21852#issuecomment-477885516
- ran  saying no to most items except yes for CUDA support


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[nohup.txt](https://github.com/tensorflow/tensorflow/files/3091084/nohup.txt)


[tf_jetson_nano_build.diff.txt](https://github.com/tensorflow/tensorflow/files/3092069/tf_jetson_nano_build.diff.txt)
"
893,30316,0,"The docs are unscrollable with JavaScript disabled. ## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/stack

## Description of issue (what needs changing):



should be removed.

### Clear description
JS is considered harmful, so the docs should be usable without JS.

The same problem is present in Android and Fuchsia docs.
"
1265,32998,0,"Cannot compile TF 2.0.0 with MacOS 10.14.6 and XCode 11.0. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs 10.14.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 2.0.0 and 1.15.0-rc2
- **Python version**: 3.7.4
- **Bazel version (if compiling from source)**: either 2.6.1, 2.6.0, 2.5.3
- **GCC/Compiler version (if compiling from source)**: Apple clang version 11.0.0 (clang-1100.0.33.8)
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: within the tensorflow 2.0.0 source directory, after standard ./configure (CPU, no GPU) or particular optimization flags besides default:



### Describe the problem
Compilation stops with following error:



"
586,26806,0,"tflite runtime error with depthwise conv2D . **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows/Ubuntu
- TensorFlow installed from (source or binary): Tried source and binary
- Python version: 3.6
- CUDA/cuDNN version: 10/7.3
- GPU model and memory: RTX 2070

**Describe the current behavior**

I run into the following error when trying to run a tflite model.


The code runs fine normally, just not after exporting to tflite.
https://www.tensorflow.org/lite/guide/ops_compatibility -> States that the op is supported. 
I know it has been used for the MobileNet variants before.
I specified rate=[1,1] and the input kernel is constant (tf.constant).

This error has also been reported in the comments of https://github.com/tensorflow/tensorflow/issues/20798
I have tried with tf v1.12.0 and v2.0.

**Describe the expected behavior**
In my case, the depth multiplier should be 1 and the size of the input channel dimension should be 6. 
I'm not sure why it says it is 0?

**Code to reproduce the issue**
I will provide a small snippet of code to reproduce the error, if it is not well known."
1006,960,0,"bug in tf_session.i came from commit:8a59748. line 211:
    $result = PyString_FromStringAndSize(_$2, strlen(_$2));
To:
%#if PY_MAJOR_VERSION < 3
    $result = PyString_FromStringAndSize(
%#else
    $result = PyUnicode_FromStringAndSize(
%#endif
        _$2, strlen(_$2));
"
1127,8354,0,Test8 file missing in word2vec_basic file. Test File (test8.zip) is missing the location http://mattmahoney.net/dc/
833,16684,0,"The link for the  tutorial on Google's Tensorflow SyntaxNet  page  gives 404 error. Go tot the page 
https://www.tensorflow.org/versions/r0.12/tutorials/syntaxnet/

and click the ""tutorial"" link. It gets a 404 error.

The target of the link is
https://github.com/tensorflow/models/tree/master/syntaxnet#installation
"
115,30961,1,"Inconsistent behavior with and without distributed scope. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 16.7.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): installed from pypi
- TensorFlow version (use command below):  v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.7.3
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
Training on mac os, with CPU only, with and without distributed scope for mnist example. The loss and accuracy is incorrect with distributed scope turned off

With distributed scope on:
Train on 550.0 steps, validate on 50 steps
Epoch 1/5
550/550 [==============================] - 6s 10ms/step - loss: 4.5770 - accuracy: 0.7565 - val_loss: 1.0736 - val_accuracy: 0.7990
Epoch 2/5
550/550 [==============================] - 2s 3ms/step - loss: 0.7952 - accuracy: 0.8072 - val_loss: 0.6584 - val_accuracy: 0.8282
Epoch 3/5
550/550 [==============================] - 2s 3ms/step - loss: 0.5272 - accuracy: 0.8367 - val_loss: 0.5347 - val_accuracy: 0.8438
Epoch 4/5
550/550 [==============================] - 2s 3ms/step - loss: 0.4477 - accuracy: 0.8501 - val_loss: 0.4830 - val_accuracy: 0.8572
Epoch 5/5
550/550 [==============================] - 2s 3ms/step - loss: 0.4037 - accuracy: 0.8591 - val_loss: 0.4780 - val_accuracy: 0.8456

Without distributed scope:
Epoch 1/5
550/550 [==============================] - 3s 6ms/step - loss: 14.5062 - accuracy: 0.1000 - val_loss: 14.4869 - val_accuracy: 0.1012
Epoch 2/5
550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012
Epoch 3/5
550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012
Epoch 4/5
550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012
Epoch 5/5
550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012

**Describe the expected behavior**
The training is expected to work correctly even with distributed scope turned off. This was working correctly in the tf2.0 alpha release but is an issue in the tf2.0 beta1 release. 

**Code to reproduce the issue**


Please copy to a script and run as:
Without distributed scope: python3 script.py
With distributed scope : python3 script.py distributed

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
433,17350,0,"Feature request: Make div operator that sets 0/0=0 instead of 0/0=NaN. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Yes, code provided below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Pro, Version 1709
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.6.4
- **Bazel version (if compiling from source)**:
&mdash;
- **GCC/Compiler version (if compiling from source)**:
&mdash;
- **CUDA/cuDNN version**:
Using processor, not GPU
- **GPU model and memory**:
Using processor, not GPU
- **Exact command to reproduce**:
Run code provided below

### Describe the problem
When I divide 0/0 elements in TensorFlow, I get 0/0=NaN values. However, I want a division operator that returns 0/0=0 for these elements. Currently, I am using a workaround for this with a where operator, as can be seen in the d variable below. However, this decreases readability, is tedious when writing code with many div operators, and is probably less efficient than coding this functionality into source.

### Source code / logs
"
852,28457,0,"Tensor flow 2.0 and OpenCV usage. Hi,
       I have already opened this issue in [Stack Overflow ](https://stackoverflow.com/questions/55986982/what-is-the-way-to-use-tensor-flow-2-0-object-in-open-cv2-python-and-why-is-it-s)but without much notice, I decided to add it here too. My question is related to usage of OpenCv operations like for instance in a train_step lets say I want to use an opencv function of remap - say undistort the output from a training model, then how do I do this in tensorflow ? Is this impossible as tensor flow object resides on GPU and calling an open cv operation requires a CPU copy and then a GPU push ? Is there already exists a provision through py_function  (I tried it) or something fancier ? Please advice.  "
404,33241,0,"some Ops tensorflow could but tflite. hi,
I have some Ops which are valid in Tensorflow,but not valid in Tensorflow Lite,and if I want use the tflite file,how to solve the problem ?thx


I just want to use the tflite file in cellphone ,what should I do ?
Thanks a lot


"
1462,28538,0,"IDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.. 

**System information**
- OS Platform and Distribution Linux Ubuntu 16.04

- TensorFlow installed from pip:
- TensorFlow version 1.13:
- Python version 3.5:
- Installed using created virtualenvironment and then used ubuntu command to install :

- GPU model and memory   NVIDIA Corporation GT218 [GeForce 210] (rev a2)
 



# Install NVIDIA driver
apt-get install --no-install-recommends nvidia-410 .

to check this nvidia installation i used  
nvidia-smi 
command it ended up with above error message

https://www.tensorflow.org/install/gpu followedsame link

Is this issue is because the GPU version is not supported??
If so please to share the GPU compatible version and memory size"
756,11023,0,"Bug: placeholder input to tf.one_hot leads to hang. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.1.0
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8.0 / cuDNN 6.0
- **GPU model and memory**: GTX 1080 Ti 11GB
- **Exact command to reproduce**: see below

### Describe the problem

I'm not sure if you're not supposed to feed in a placeholder to , but if you do, it hangs and chews up 100% CPU.

### Source code / logs

Minimal example to reproduce bug:



The expected result should be either (1) it works and prints , or (2) it produces some kind of error."
755,33040,0,"Illegal Instruction on importing tensorflow. Tensorflow version = 1.14
OS : Kali Linux Rolling (2018.2)
Output on cat /proc/cpuinfo : 
processor : 0
vendor_id : GenuineIntel
cpu family : 6
model : 23
model name : Intel(R) Core(TM)2 Duo CPU E7500 @ 2.93GHz
stepping : 10
microcode : 0xa0b
cpu MHz : 2546.724
cache size : 3072 KB
physical id : 0
siblings : 2
core id : 0
cpu cores : 2
apicid : 0
initial apicid : 0
fpu : yes
fpu_exception : yes
cpuid level : 13
wp : yes
flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm pti tpr_shadow vnmi flexpriority dtherm
bugs : cpu_meltdown spectre_v1 spectre_v2
bogomips : 5850.33
clflush size : 64
cache_alignment : 64
address sizes : 36 bits physical, 48 bits virtual
power management:

processor : 1
vendor_id : GenuineIntel
cpu family : 6
model : 23
model name : Intel(R) Core(TM)2 Duo CPU E7500 @ 2.93GHz
stepping : 10
microcode : 0xa0b
cpu MHz : 2231.318
cache size : 3072 KB
physical id : 0
siblings : 2
core id : 1
cpu cores : 2
apicid : 1
initial apicid : 1
fpu : yes
fpu_exception : yes
cpuid level : 13
wp : yes
flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm pti tpr_shadow vnmi flexpriority dtherm
bugs : cpu_meltdown spectre_v1 spectre_v2
bogomips : 5850.33
clflush size : 64
cache_alignment : 64
address sizes : 36 bits physical, 48 bits virtual
power management:

"
658,29477,0,"Different Results on Different Machines. I am having a problem with the reproducibility of results. When I run the code with the same seed on two different machines I get different results. But on the same machine, it gives me the same results all the time. Are you guys aware of any such issue?"
1444,4175,0,"Contrib metric `metric_ops.streaming_auc` type error within function. ### Environment info

Version 0.10.0rc0
Ubuntu

I'm using a  Estimator and am feeding it  and it fails.
### What other attempted solutions have you tried?

Modifying source to convert both inputs to float using . It worked, but still an issue.
### Logs or other output that would be helpful


"
1447,11572,0,"How to install tensorflow c#. Hi,
I am trying to build email spam filtering .I have already done when email has text only but the problem when deal with images



Any help how to install tensorflow and deal with images. 

Thanks"
990,12253,0,"not installing . Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
"
220,28071,1,"train_and_evaluate hang forever without message. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux 3.10.0-693.11.6.el7.x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.6.0
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:CUDA Version 9.0.176
- GPU model and memory:P100 12.0G

**Describe the current behavior**
when LD_LIBARARY_PATH don't include the libhdfs.so,and we set estimator RunConfig-model_dir to a hdfs path, it will hang at tf.train_and_evaluate forever .I think it can't operate tf.gfile.MkDir() ,but it gives no info or any message.

**Describe the expected behavior**
I expect that it will catch the exception and give the info about loss xx.so.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

classifier = tf.estimator.Estimator(
        model_fn=text_cnn_fn,
        params={
            'feature_columns': feature_columns,
            'vocab_size': FLAGS.hash_size,
            'embedding_size': FLAGS.embedding_size,
            'filter_sizes': filter_sizes,
            'num_filters': FLAGS.num_filters,
            'sequence_length': FLAGS.sequence_length,
            'n_classes': FLAGS.num_classes,
            'learning_rate': FLAGS.learning_rate,
        },
        config=tf.estimator.RunConfig(model_dir=""hdfs://default/tmp"")
    )
...data process
 tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
237,14130,1,"Eager: CPU Performance/Operation Overheads. (This applies only when [eager execution](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/README.md) has been enabled via )

Eager execution re-uses most of the same Python code used for constructing TensorFlow graphs. Many of these paths have not been optimized for part of the critical path of computation. As a result, the CPU overheads of executing Python code for every operation are higher than we’d like.

Consequently, the performance of eager execution on models with many small computations, or models executed on CPU may be dominated by these overheads.

Overheads are measured using microbenchmarks such as in [](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/benchmarks_test.py) and model-level benchmarks such as those used for [ResNet50](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/resnet50) and the [PTB RNN](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/rnn_ptb)"
1056,35389,0,"ERROR: Next operations are not supported by GPU delegate.. INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Next operations are not supported by GPU delegate:
CONCATENATION: 
CONV_2D: 
CUSTOM TFLite_Detection_PostProcess: Operation is not supported.
DEPTHWISE_CONV_2D: 
LOGISTIC: 
RESHAPE: 
First 0 operations will run on the GPU, and the remaining 64 on the CPU.


I've converted model to tflite and i'm getting this error. Can anybody help me why i'm getting this and/or what could be the possible reason?

Here's the code:

bool ObjectDetector::init(const std::string &model_file, bool is_quantized,
                          const std::string &labels_file)
{
    // Load model.
    model_ = tflite::FlatBufferModel::BuildFromFile(model_file.c_str());
    if (!model_)
    {
        LOG(ERROR) << ""Failed to load model: "" << model_file;
        return false;
    }

    // Create interpreter.
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder(*model_, resolver)(&interpreter_);
    if (!interpreter_)
    {
        LOG(ERROR) << ""Failed to create interpreter!"";
        return false;
    }
    /*if (interpreter_->AllocateTensors() != kTfLiteOk)
    {
        LOG(ERROR) << ""Failed to allocate tensors!"";
        return false;
    }
    const TfLiteGpuDelegateOptions options = {
    .metadata = NULL,
    .compile_options = {
        .precision_loss_allowed = 1,  // FP16
        .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,
        .dynamic_batch_enabled = 0,   // Not fully functional yet
    },
    };*/
    auto* delegate = TfLiteGpuDelegateCreate(/*default options=*/nullptr);
    if (interpreter_->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;
    //LOG(ERROR) << ""DetectEmptyParkingSlots:: TfLiteGpuDelegateCreate Passed!!! Hurray!....."" << std::endl;
    
    //interpreter_->SetNumThreads(1);
    interpreter_->UseNNAPI(1);



    // Find input tensors.
    if (interpreter_->inputs().size() != 1)
    {
        LOG(ERROR) << ""Graph needs to have 1 and only 1 input!"";
        return false;
    }
    input_tensor_ = interpreter_->tensor(interpreter_->inputs()[0]);
    if (is_quantized)
    {
        if (input_tensor_->type != kTfLiteUInt8)
        {
            LOG(ERROR) << ""Quantized graph's input should be kTfLiteUInt8!"";
            return false;
        }
    }
    else
    {
        if (input_tensor_->type != kTfLiteFloat32)
            LOG(ERROR) << ""Quantized graph's input should be kTfLiteFloat32!"";
        {
            return false;
        }
    }

    // Find output tensors.
    if (interpreter_->outputs().size() != 4)
    {
        LOG(ERROR) << ""Graph needs to have 4 and only 4 outputs!"";
        return false;
    }
    output_locations_ = interpreter_->tensor(interpreter_->outputs()[0]);
    output_classes_ = interpreter_->tensor(interpreter_->outputs()[1]);
    output_scores_ = interpreter_->tensor(interpreter_->outputs()[2]);
    num_detections_ = interpreter_->tensor(interpreter_->outputs()[3]);

    std::vector<std::string> labels;
    ReadLines(labels_file, &labels);
    labels_ = labels;
    return true;
}
"
953,29067,0,"Unable to use tf.ragged_tensor with tf.from tensor_slices. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
1366,14807,0,"seg fault training tf.nn.conv3d with minibatch size >2. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code! You can find it here: https://github.com/NERSC/CosmoFlow/tree/master/SegFault
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: SUSE Linux 12.2
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: ('v1.3.0-rc1-3112-g65b6a75', '1.4.0-rc0') Note this is NOT compiled with the Intel MKL options. 
- **Python version**: 2.7.13 
- **Bazel version (if compiling from source)**: 0.6.0
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A, Running on x86_64 Intel Haswell node
- **Exact command to reproduce**: See README in https://github.com/NERSC/CosmoFlow/tree/master/SegFault


### Describe the problem

A seg fault when training a tf.nn.conv3d with minibatch size more than 2 on a single Intel Haswell. The seg fault occurs at [line 187](https://github.com/NERSC/CosmoFlow/blob/master/SegFault/CosmoNet.py#L187).  

### Source code / logs

GDB log: https://github.com/NERSC/CosmoFlow/blob/master/SegFault/gdbTrace.log
It looks like some kind of cyclic dependency in Eigen::TensorEvaluator. "
151,24362,1,"tf.cumsum low performance in 1.12.0. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 1803 Build 17134.407
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0-gpu
- Python version: 3.6.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: v9.0.176
- GPU model and memory: GTX-970M, NVIDIA-SMI 385.54

**Describe the current behavior**
In tensorflow-1.12.0-GPU, it takes around 0.8s for each iteration and the cumsum operation takes around 0.13s/run x 3class x 2run/class. So the cumsum operation nearly takes 0.78s for each iteration. After switching the code interpreter to python 3.6.0 with tensorflow-1.11.0-CPU, the iteration time decreases to around 0.14s and the highest time-consuming operation become the sorting operation.
I notice that this issue has been reported in [https://github.com/tensorflow/tensorflow/issues/19570](url). I wonder whether this issue has been fixed in version 1.12.0? Or is there something wrong in my code caused this issue? I'm still learning how to use tensorflow recently. The lovasz_loss imported can be found in [https://github.com/bermanmaxim/LovaszSoftmax/blob/master/tensorflow/lovasz_losses_tf.py](url)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
719,33796,0,"tf.data.Dataset.from_generator triggers a SIGBUS signal when operating with numpy arrays. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
OSX Mojave 10.14.6 

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:

- TensorFlow installed from (source or binary):
Binray
- TensorFlow version (use command below):
GIT v2.0.0-rc2-26-g64c3d382ca 
Tensorflow 2.0.0

- Python version: 
Python 3.6.8

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
On cpu
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

Clean virtualenv with tensorflow==2.0.0 installed 
tf.data.Dataset.from_generator raises a SIGBUS when operating on large enough Numpy arrays 

Using 
tf_dataset = tf.data.Dataset.from_generator(gen_callable,
                                                output_types=(tf.float32),
                                                output_shapes=(None, 3))
with a function that generates and operates on medium sized numpy arrays triggers a SIGBUS signal.

**Describe the expected behavior**

No SIGUS should be raised

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

### NOT A CONTRIBUTION

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

output of pip freeze 

absl-py==0.8.1
astor==0.8.0
gast==0.2.2
google-pasta==0.1.7
grpcio==1.24.1
h5py==2.10.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
Markdown==3.1.1
numpy==1.17.2
opt-einsum==3.1.0
protobuf==3.10.0
six==1.12.0
tensorboard==2.0.0
tensorflow==2.0.0
tensorflow-estimator==2.0.0
termcolor==1.1.0
Werkzeug==0.16.0
wrapt==1.11.2
"
19,24611,1,"performance issue in CollectiveAllReduceStrategy. Tried both  and  on a same 8 GPU machine. And  is constantly 30% slower than MirroredStrategy no matter how many cards I am using.

The tensorflow version I am running is 1.12.0.

I am using estimator API and tried many different models including resnet50, resnet34, vgg etc. 

Is this expected? Or I need some configuration to make CollectiveAllReduceStrategy faster? Thanks

"
614,30418,0,"Memory Saving Gradients for TF2. **System information**
- TensorFlow 2.0 beta

There are libraries for TF1 that are able to calculate more memory efficient gradients such as [gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing). They edit the graph with the [tf.contrib.graph_editor](https://www.tensorflow.org/api_docs/python/tf/contrib/graph_editor) to save memory. However, tf.contrib does not exist anymore which is why I cannot run large models such as GPT2 in TF2 (converted with ) as it overflows my GPU memory in Google Colab. Is there a TF2 feature that can help me? 
"
1277,17212,0,"got error when run eval.py in object detection api. Hi ,
i faced with this error when i run eval.py:

INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
Traceback (most recent call last):
File ""eval.py"", line 142, in
tf.app.run()
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 124, in run
_sys.exit(main(argv))
File ""eval.py"", line 138, in main
FLAGS.checkpoint_dir, FLAGS.eval_dir)
File ""/home/mm/models/research/object_detection/evaluator.py"", line 240, in evaluate
save_graph_dir=(eval_dir if eval_config.save_graph else ''))
File ""/home/mm/models/research/object_detection/eval_util.py"", line 448, in repeated_checkpoint_run
return metrics
UnboundLocalError: local variable 'metrics' referenced before assignment

########################################################################
i once saw that must be initial metrics : in the beginning of repeated_checkpoint_run i i added this :
metrics={}
pr_value={}
global_step={}
i this case , that error don't face but after for a while processing don't give me any value :
mm@mm:~/models/research/object_detection$ python3 eval.py \

        --logtostderr \
        --checkpoint_dir=training_ssd_mobile/model.ckpt-200000 \
        --eval_dir=eval_log\
        --pipeline_config_path=ssd_mobilenet_v1_coco_2017_11_17/ssd_mobilenet_v1_focal_loss_coco.config

INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0"
328,27581,0,"keras model.predict_on_batch/model.test_on_batch with tf.dataset: dataset.make_initializable_iterator is not supported when eager execution is enabled.. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): yes 
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

best_model.test_on_batch(testing_dataset)
and 
best_model.predict_on_batch(training_dataset)

crashing with the following error messages
""RuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled.""

but best_model.evaluate(testing_dataset,) is wokring.

**Describe the expected behavior**
Exact same code working with Tensorflow 1.12

**Code to reproduce the issue**
All the code and llogs are here:
https://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/TF_2.0/05-Mnist_model_validation_and_interpretation.ipynb

**Other info / logs**
Here the full logs:

---------------------------------------------------------------------------
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in make_initializable_iterator(dataset, shared_name)
   1852     # some datasets (e.g. for prefetching) override its behavior.
-> 1853     return dataset._make_initializable_iterator(shared_name)  # pylint: disable=protected-access
   1854   except AttributeError:

AttributeError: 'PrefetchDataset' object has no attribute '_make_initializable_iterator'

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-21-3d53d52be0f8> in <module>
      1 #!! Bug TF 2.0
----> 2 score = best_model.test_on_batch(testing_dataset)
      3 
      4 # print test accuracy
      5 print('Loss:')

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in test_on_batch(self, x, y, sample_weight, reset_metrics)
   1310     # Validate and standardize user data.
   1311     x, y, sample_weights = self._standardize_user_data(
-> 1312         x, y, sample_weight=sample_weight, extract_tensors_from_dataset=True)
   1313 
   1314     if self.run_eagerly:

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2439       if extract_tensors_from_dataset:
   2440         # We do this for /etc.
-> 2441         x, y, sample_weight = training_utils.extract_tensors_from_dataset(x)
   2442     elif isinstance(x, iterator_ops.Iterator):
   2443       # Graph mode iterator. We extract the symbolic tensors.

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in extract_tensors_from_dataset(dataset)
   1382     Tuple of tensors .  and  entry may be None.
   1383   """"""
-> 1384   iterator = get_iterator(dataset)
   1385   inputs, targets, sample_weight = unpack_iterator_input(iterator)
   1386   return inputs, targets, sample_weight

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in get_iterator(dataset)
   1362 def get_iterator(dataset):
   1363   """"""Create and initialize an iterator from a dataset.""""""
-> 1364   iterator = dataset_ops.make_initializable_iterator(dataset)
   1365   initialize_iterator(iterator)
   1366   return iterator

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in make_initializable_iterator(dataset, shared_name)
   1853     return dataset._make_initializable_iterator(shared_name)  # pylint: disable=protected-access
   1854   except AttributeError:
-> 1855     return DatasetV1Adapter(dataset)._make_initializable_iterator(shared_name)  # pylint: disable=protected-access
   1856 
   1857 

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in _make_initializable_iterator(self, shared_name)
   1495     if context.executing_eagerly():
   1496       raise RuntimeError(
-> 1497           ""dataset.make_initializable_iterator is not supported when eager ""
   1498           ""execution is enabled."")
   1499     _ensure_same_dataset_graph(self)

RuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled."
427,27474,0,"Shutdown crash while running this script (most times). <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MS WINDOWS 10 x64

- TensorFlow installed from (source or binary):
binary

- TensorFlow version (use command below):
2.0.0-a0

- Python version:
Python 3.7.1

- CUDA/cuDNN version:
cuda_10.1.105_418.96_win10.exe
cudnn-10.1-windows10-x64-v7.5.0.56.zip

- GPU model and memory:
Gforce GTX 1050 Ti (DELL laptop)

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

EXECUTION OF LINE:
C:\Users\steph>python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'


**Describe the current behavior**
Durring running of this script, the computer shutdown crashes and reboots.

**Describe the expected behavior**
Finish the 100 Epochs of training and save the model *.h5

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



[Python Scripts.zip](https://github.com/tensorflow/tensorflow/files/3040051/Python.Scripts.zip)

The data is from Kaggle dogs-vs-cats.zip (file too big to attach: extract all to same subdirectory)

The crash seems to occur most often during the 100 Epochs (2nd) fit.

"
710,1811,0,"Python 3 not installing in virtual environment. Using Tensorflow 0.7.1

I have tried to install the Python 3 version of Tensorflow in a Virtual environment but I think it didn't work since the command

$ python3 -c 'import os; import inspect; import tensorflow; print(os.path.dirname(inspect.getfile(tensorflow)))' 

returned

$ /usr/local/lib/python3.4/dist-packages/tensorflow

instead of

/home/dlm/tensorflow/local/lib/python3.4/site-packages/tensorflow

I was sure I had activated the tensorflow virtual env before insstaling tensorflow with suport to python 3.
"
1335,23216,0,"Failing isinstance check in tensorflow.python.keras.models._clone_functional_model. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
CentOS, specifically
Linux 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

- TensorFlow installed from (source or binary): Through conda
- TensorFlow version (use command below): 1.11.0
- Python version:
Python 2.7.15 |Anaconda, Inc.| (default, Oct 10 2018, 21:32:13)
[GCC 7.3.0] on linux2
-Bazel version: NA
-CUDA/cuDNN version: NA
-GPU model and memory: NA
-Exact command to reproduce: NA
-Mobile device: NA

**Describe the current behavior**
raises a because the model passed to it is supposedly not an instance of , even though it is.

**Code to reproduce the issue**

This returns False. Even though  should just be an alias for .

**Other info / logs**
I run into this issue when calling.
I can work around it by importing all my keras imports using the full but I'm not sure why that should be necessary.
"
1368,20452,0,"Keras TimeDistributed with Input and no batch_size fails. OS: Windows 10 & Ubuntu 16.04 tested
Tensorflow 1.8
Python 3.6.5

The following code shows the problem.
When using  with a  everything works fine, but without it it fails.
I assumed that when setting the  in  to  or using  instead that the  would then be dynamic.

"
127,26375,1,"Train Keras model with GPU Google Colab very slow when compared with my personal cpu. **Information environment when I run [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)**

== cat /etc/issue ===============================================
Linux dc5aaefaf4e1 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux dc5aaefaf4e1 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
mesh-tensorflow          0.0.5                
msgpack-numpy            0.4.3.2              
numpy                    1.14.6               
protobuf                 3.6.1                
tensorflow               1.13.1               
tensorflow-estimator     1.13.0               
tensorflow-hub           0.3.0                
tensorflow-metadata      0.12.1               
tensorflow-probability   0.6.0                

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.13.1
tf.GIT_VERSION = b'v1.13.1-2-g09e3b09e69'
tf.COMPILER_VERSION = b'v1.13.1-2-g09e3b09e69'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/lib64-nvidia
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Mar  6 04:31:12 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   36C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/lib/python3.6/dist-packages/torch/lib/libcudart-1581fefa.so.10.0
/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart-1581fefa.so.10.0
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.7


**Describe the current behavior**
Train time of each epoch on google colab gpu: ~60s
Train time of each epoch on my personal pc cpu: ~10s

**Code to reproduce the issue**
I train crf model use keras, keras_contrib library
Code:


**Logs**
2019-03-06 04:13:53.402787: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-03-06 04:13:53.403069: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x23c8dc0 executing computations on platform Host. Devices:
2019-03-06 04:13:53.403105: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-06 04:13:53.491058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-06 04:13:53.491582: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55374a0 executing computations on platform CUDA. Devices:
2019-03-06 04:13:53.491616: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2019-03-06 04:13:53.492038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2019-03-06 04:13:53.492071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-06 04:13:53.863400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-06 04:13:53.863478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-06 04:13:53.863513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-06 04:13:53.863768: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2019-03-06 04:13:53.863821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2019-03-06 04:13:55.629273: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
"
516,31231,0,"DistributionStrategy is not supported by tf.keras.models.Model.fit_generator . Hi! Recently I've encountered a  while trying to apply a fit_generator method of a  with a . It is almost a year since this handlers were added to the code ( https://github.com/tensorflow/tensorflow/commit/9541ce3475ea70fd8eb9552f60de462127f15440#diff-de9b96ac2d81503324cbbbe21732031f ) and I'm wondering whether to expect an implementation to be added any time soon? (with the release of TF2.0 for example)

Making efforts to find a workaround I've tried to transform a generator to TF Dataset by  to replace the  by  but encountered similar problem. The obtained object has type which is also incompatible with distribution strategies

I dare to assume that for a wide society of TF users and for me in particular this functionality would be of a great interest. Dealing with large, domain specific data sets that doesn't fit into memory, one  often has no choice other than writing a custom data generator. When big data is involved the distributed training might be crucial. 

I would highly appreciate any information on the current state of the problem or possible workarounds from the TensorFlow developers team. Thanks in advance!


**System information**
- TensorFlow version (you are using): 2.0.0.dev20190729
- Are you willing to contribute it (Yes/No): No

"
1069,7445,0,"memory overflow when processing Variable.eval(). I'm using Tensorflow to process a simple matrix factorization algorithm. Every step went correct but at the last step, where I want to  a Tensor to store it, the program didn't work and only occupied more and more memory. I tried to  the initial parameters before the algorithm, it correctly processed. Then I'm confused where the bug is.
Core code is as follows.


It's fairly strange because eval() just work well before the optimization step, but crashed after it. Is this a bug in eval()?"
542,33987,0,"Update `nogpu` tag references to be `no_gpu` instead?. Currently the scripts used to run TF unit tests on the ROCm platform filter out tests that are tagged with the  tag

* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/rocm/run_cc_core.sh
* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/rocm/run_csb_tests.sh
* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/rocm/run_py3_core.sh
* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/xla/linux/rocm/run_py3.sh

The same applies for scripts for the CUDA platform as well.

There seem to be some tests that are tagged with the  tag instead of the  tag.
One example is 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tpu/tpu.bzl#L44

Such tests do not get filtered out by the test scripts above, and as a consequence they get reported as regression failures when those test scripts are run.

So the question here is 
* should we update all references to the  tag to instead be 
OR
* should we update all the scripts instead to also filter out the  tagged tests?

-----------------------

@chsigg 
"
885,20068,0,"streaming data from google cloud storage to tensorflow input pipeline is slow. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04 and MacOS High Sierra 10.13.4
- **TensorFlow installed from (source or binary)**: docker and virtualenv
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: N/A

Describe the problem:
My data is stored on google cloud storage (millions of small audios); my tensorflow input pipeline sample code is below.
 
tf.data.Dataset.from_generator(generator_filename_func).\
map(signal_processing_func, num_parallel_calls=CPU_NUM).\
batch(100).\
make_one_shot_iterator()

The pipeline starts with a generator (a generator that generates audio filenames from google cloud storage); then run signal processing with map function and multithread. 

The input pipeline is fast when reading data from local disk (download audios to cloud compute engine VM), however, reading data from mounted folder (mount google cloud storage to the folder with gcsfuse command) is slow (about 5-10 times slower). I know google cloud supports using python multithread to read data, but it does not seem to be compatible with tensorflow input pipeline. I also tried google.cloud.storage to stream data, it's also very slow for small files. How should I stream data from google cloud storage to tensorflow input pipeline with low latency? What tools/library is recommended and compatible with tensorflow input pipeline?

The other weird thing is that the pipeline speed does not change much as I add more CPU and SSD. I believe the bottleneck is reading files, how can I optimize reading many small files in the tensorflow input pipeline?

"
1193,8486,0,"Invalid argument: No OpKernel was registered to support Op 'Add' with these attrs.. ### Environment info
Operating System:

Mac 

 I hava look Mobile and Embedded TensorFlow (TensorFlow Dev Summit 2017) video on yotube.

In the video , I hava learn some function to reduce tensorflow so file size on Android.

I do here


the *.pb file is myself , then I get the ops_to_register.h file here



I put ops_to_register.h in tensorflow/tensorflow/core/framework dir.

then I do:   




In my android studio project , initializeTensorFlow() My slef .pb file ,but I got the error:







"
1029,30722,0,"TensorFlow Lite build fatal error: gtest/gtest.h: No such file or directory. **System information**
- Raspberry Pi 3
- TensorFlow installed from (source or binary): source
- TensorFlow version:  1.14.0

**Describe the problem**

When natively compiling TensorFlow Lite 1.14.0 in Raspberry Pi 3, I am getting the following compile error:
fatal error: gtest/gtest.h: No such file or directory

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is the error log:
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/test_main.cc -o /home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/test_main.o
tensorflow/lite/kernels/test_main.cc:17:25: fatal error: gtest/gtest.h: No such file or directory
 #include <gtest/gtest.h>
                         ^
compilation terminated.
tensorflow/lite/tools/make/Makefile:241: recipe for target '/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/test_main.o' failed
make: *** [/home/pi/tensorflow-1.14.0/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/test_main.o] Error 1
make: *** Waiting for unfinished jobs....
In file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:22:0,
                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:42,
                 from tensorflow/lite/kernels/sub.cc:18:
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function ‘static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)’:
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:484:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:487:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]
             [[clang::fallthrough]];
             ^
pi@raspberrypi:~/tensorflow $ 
"
1421,33807,0,"TensorFlow pre-built package not available for download. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): NA
- TensorFlow version: 2.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip using wheel
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 
- GPU model and memory:



**Describe the problem**
We are not able to download the wheels given on page -> https://www.tensorflow.org/install/pip. We are getting the following error when we try to get the *.whl in a browser:

<Error>
<Code>NoSuchKey</Code>
<Message>The specified key does not exist.</Message>
<Details>
No such object: tensorflow/linux/gpu/tensorflow_gpu-2.0.0-cp36-cp36m-linux_x86_64.whl
</Details>
</Error>

The issue occurs with all the links. We tried wget command on linux and get 404 error.
 
**Provide the exact sequence of commands / steps that you executed before running into the problem**

wget https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.0.0-cp36-cp36m-linux_x86_64.whl

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1126,3317,0,"handling multiple graphs. In numerous locations in the documentation, it has been mentioned that it is possible to handle multiple s, but I couldn't find any example. 

Since  only support a single , it seems the only way of doing it is to create multiple  instances. 

Is it **safe** to have multiple instances  at the same time? 
if not, what is the proper way of handling multiple s? 

Thanks
"
1374,32606,0,"[model_to_estimator] estimator not evaluate all outputs defined in keras model but only one. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: 

**Describe the current behavior**
When using keras model with **multiple outputs**, and convert it to estimator by , it only evaluate one output by  or . 
And don't know which output it evaluate (it doesn't display output name but only metrics name).

**Describe the expected behavior**
It should only evaluate all outputs defined in keras model, not only pick one.

**Code to reproduce the issue**
A example here:


**Other info / logs**
tf log here, except two outputs log, but only get one:


TensorBoard except two outputs scalar, but only get one:
![2019-09-18 11 27 54](https://user-images.githubusercontent.com/7609173/65105491-aaff6000-da07-11e9-916f-473ba7847ba6.png)
"
1091,6330,0,"Error of using a custom classifier for android demo. I want to build android demo of my classifier.
Following the steps at [https://www.oreilly.com/learning/tensorflow-on-android](url),
the error appeared for 
 .
My bazel version is 0.4.2.
Error :
> ERROR: /home/dlm/tensorflow_new/tensorflow/python/BUILD:1907:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored /usr/bin/gcc -shared -o bazel-out/local-fastbuild/bin/tensorflow/python/_pywrap_tensorflow.so -Wl,--version-script ... (remaining 14 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
bazel-out/local-fastbuild/bin/tensorflow/core/libversion_lib.a(version_info.pic.o): In function tf_git_version()'
bazel-out/local-fastbuild/bin/tensorflow/core/libframework_internal.lo(version_info.pic.o):version_info.cc:(.text+0x0): first defined here
bazel-out/local-fastbuild/bin/tensorflow/core/libversion_lib.a(version_info.pic.o): In function tf_compiler_version()'
bazel-out/local-fastbuild/bin/tensorflow/core/libframework_internal.lo(version_info.pic.o):version_info.cc:(.text+0xd): first defined here
collect2: error: ld returned 1 exit status
Target //tensorflow/python/tools:optimize_for_inference failed to build
Use --verbose_failures to see the command lines of failed build steps.

How do I fix this error ?
"
1420,26651,0,"resize_image_with_pad is not available as described in documentation. 
**System information**
- TensorFlow version: 2.0.0-alpha
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/resize_image_with_pad


**Describe the documentation issue**
This is not available in 2.0.0 it is renamed as tf.image.resize_with_pad
"
1061,9906,0,"tf.import_graph_def() restricts the order of nodes in graph proto.. -----------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
commit 3bee923c9
- **Bazel version (if compiling from source)**:
0.4.5
- **CUDA/cuDNN version**:
cuda 8.0/cudnn 5.1.5
- **GPU model and memory**:
Tesla P40 
- **Exact command to reproduce**:

### Describe the problem
I used auto parallel optimizer in grappler for cifar10 example, but grappler changes the order of nodes in graph proto. It causes the failure of import_graph_def, because ops.set_shapes_for_outputs(op), which is line 407 of tensorflow/tensorflow/python/framework/importer.py is failed. This is because all of the input nodes must be defined before to define a node. importer restricts the order of nodes for graph definition proto, but I think importer shoud be flexible to the order of nodes. 

### Source code / logs
  

This is the log
"
883,33356,0,"'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction'. I am using Huber loss implementation in tf.keras in tensorflow 1.14.0 as follows:



I am getting the error AttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction'

I have tried using tf.losses.Reduction, tf.compat.v2.losses.Reduction nothing seems to work.

Did tensorflow remove Reduction from tf.keras.losses, it is strange if they did so because their documentation still shows: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/Huber#args"
18,543,1,"Zero volatile GPU-Util but high GPU Memory Usage. Hi I am running a model implemented by tensorflow with only one GPU, the GPU usage is 95% while the volatile GPU-Util is 0.

Specifically I have Tesla k40m with cuda 7.0 and cudnn 6.5v2 installed on Centos 7.0. There are three files: data_loader.py, model.py and train.py in my project. In the train.py I firstly declared 
"" with tf.device('/gpu:0'):"" and then sess.run([train_op]). When I run the code, errors raised:

""tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 0 to device ordinal 2""

On the other hand, I installed tensorflow with Pip.

Any help are more than welcome.
"
1381,34749,0,"how to convert tensorflow1.13 trained model ckpt to tflite use tensorflow2.0?. i have a model trained use tensorflow1.13 and i can export model to pb from checkpoints and then convert to tflite file by tf.lite.TFLiteConverter.from_frozen_graph. but in tensorflow 2.0 there is no this method. my code trained use slim, so is very hard to change to tf2.0 retrain the model, are there any method to read from the original ckpt files to convert tf2.0 tflite file? "
838,12749,0,"Android- Tensorflow model loading issue. I have saved model using tf.train.Saver.save() and then freeze the graph bu using below method:
def freezeModel(modelName):
    input_graph_path = modelName + '.pb'
    checkpoint_path = './' + modelName + '.ckpt'
    input_saver_def_path = """"
    input_binary = True
    output_node_names = ""x_indices,x_values,x_dense_shape,unary_scores,transition_params,train_op""
    restore_op_name = ""save/restore_all""
    filename_tensor_name = ""save/Const:0""
    output_frozen_graph_name = 'frozen_' + modelName + '.pb'
   
    clear_devices = True

    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,input_binary, checkpoint_path, output_node_names,restore_op_name, filename_tensor_name,output_frozen_graph_name, clear_devices, """")
    #freeze_graph.freeze_graph(input_saved_model_dir='Model/', saved_model_tags='serve')
    return


Then loading the modelName.pb file in Android using inference api as below:
TensorFlowInferenceInterface inferenceInterface = new **TensorFlowInferenceInterface**(context.getAssets(), MODEL_FILE);

I am getting **below error**:
Caused by: **java.io.IOException: Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32**
; NodeDef: Less_1 = Less[T=DT_INT64](ToInt64_2, ToInt64_3); Op<name=Less; signature=x:T, y:T -> z:bool; attr=T:type,allowed=[DT_FLOAT, DT_INT32]>
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:439)
at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:98)


 **Here is the code using to create the graph:**
x_indices_t= tf.placeholder(tf.int64, shape=(None, 3), name=""x_indices"")
x_values_t= tf.placeholder(tf.float32, shape=(None), name=""x_values"")
x_dense_shape_t= tf.placeholder(tf.int64, shape=(3), name=""x_dense_shape"")#[len(X_train), max_word_count, feature_count]
shape_unary_score_t = tf.placeholder(dtype=tf.int64, shape=(None, None, None), name=""shape_unary_score"")#[num_examples_t, max_word_count, num_tags]
shape_t = tf.shape(shape_unary_score_t)

x_t = tf.SparseTensor(indices=x_indices, values=x_values, dense_shape=x_dense_shape)


To resolve above error i changed the dtype of indices and shapes to int32 then tf.SparseTensor() is givving below error:
File ""CRF_POS_Trainer_Tensor - Training.py"", line 323, in <module>
    x_t = tf.SparseTensor(indices=x_indices_t, values=x_values_t, dense_shape=x_dense_shape_t)
  File ""/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py"", line 119, in _init_
    indices, name=""indices"", dtype=dtypes.int64)
  File ""/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 676, in convert_to_tensor
    as_ref=False)
  File ""/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 741, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 614, in _TensorTensorConversionFunction
    % (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(""x_indices:0"", shape=(?, 3), dtype=int32)'
        

So how to i resolve issue to create SparseTensor with int32 data.

**Tensorflow Version: 1.3.0**"
1248,30493,0,"[Feature] To support casting float32 to int32 in HEX format. **System information**
- TensorFlow version (you are using):
    1.13.x
- Are you willing to contribute it (Yes/No):
    Yes


**Describe the feature and the current behavior/state.**
I want something like  which allows me to get the hex value representation and can be used to further compute with bit-wise operators.

**Will this change the current api? How?**
No, just extend a new op to handle it.

**Who will benefit with this feature?**
Any requirement that is needed to edit the bit value for a float32-type tensor.

**Any Other info.**
No."
904,14347,0,"Is my code right to use batch normalization layers in tensorflow?. I have two inputs:  with the same shape. They should be processed by the two mlp layers, and finally get two results which acts as score. Here is my codes:


I use BN layer to normalize the nodes in hidden layers:



Below is the loss definition.It seems all right.

with tf.name_scope('predictions'):
  sim_diff = pos_pair_sim - neg_pair_sim
  predictions = tf.sigmoid(sim_diff)
  self.infers = pos_pair_sim
## loss and optim
with tf.name_scope('loss'):
  self.loss = nn_layers.cross_entropy_loss_with_reg(self.labels, self.preds)
  tf.summary.scalar('loss', self.loss)

I am not sure whether I have use the BN layers in right way. I mean that the BN parameters are derived from the hidden units from the two separate parts, which are based on qi_pos and qi_neg tensors as inputs. Anyway, anyone could help check it ?"
455,17081,0,"keras multi_gpu_model broken going from 1.6.0-rc0 to 1.6.0-rc1. Have I written custom code: Yes
OS Platform and Distribution: Linux Ubuntu 17.04
TensorFlow installed from: source
TensorFlow version: 1.6.0-rc1
Python version: 3.6 
Bazel version: 0.10
GCC/Compiler version: 6.0
CUDA/cuDNN version: CUDA 9.1 cuDNN 7.0.5
GPU model and memory: NVIDIA Titan Z 12GB
 command to reproduce: multi_gpu_model(model, gpus=2)

Just upgraded from rc0 to rc1 of release 1.6.0 and I'm now getting the following crash when running the multi_gpu_model function (was working fine with rc0):

    parallel_model = multi_gpu_model(model, gpus=2)

File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/utils/training_utils.py"", line 207, in multi_gpu_model

    return Model(model.inputs, merged)

File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 694, in __init__

    self._init_graph_network(*args, **kwargs)

File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 733, in _init_graph_network

    if layer.is_placeholder:

AttributeError: 'Lambda' object has no attribute 'is_placeholder'

I rolled back to the 1.5 branch and I'm not having any issues running multi_gpu_model there.


"
698,11123,0,"Correction to Getting Started. Hello,

I would like to submit that there is a programming bug on line 29 of the ""custom model tutorial"" within Getting Started with Tensorflow. 

The line should read:

input_fn = tf.contrib.learn.io.numpy_input_fn({""x"":x_train}, y_train, batch_size=4, num_epochs=1000)


​Best regards.
Jeff​"
48,8795,1,"[CPU Performance Issue] Why don't hugepages, prefetch.. 
Hugepages and Prefetch are considered as two main techniques for performance improvement.

https://wiki.debian.org/Hugepages
Intel® 64 and IA-32 Architectures Optimization Reference Manual (http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html)

Just don't understand why TF does not use hugepages and prefetch techniques for CPU version.


Thanks for great TF.

"
744,32547,0,"tensorflow.contrib.integrate.odeint does not work with the default __call__ method of subclass of tf.Keras.layers.Layer. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): both 1.14 and 1.15-rc0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.0
- GPU model and memory: Titan RTX, 24 GB

**Describe the current behavior**
__For Tensorflow 1.14:__
When defining ODE function using the default  method of any subclass of , an  is raised, stating  has been marked as not fetchable. To avoid this issue, the ODE function has to be defined using either pure TensorFlow functions, or the  method of the subclass instead of . 
__For Tensorflow 1.15-rc0:__
The same code will run forever without error message. The above solution (using  instead of ) also applies here. 

**Describe the expected behavior**
 should work with the default  method of any subclass of  under both TensorFlow 1.14 and 1.15-rc0.

**Code to reproduce the issue**


**Other info / logs**
<details><summary>For TensorFlow 1.14: ValueError: '.../VarIsInitializedOp' not fetchable; click here for detailed logs.</summary>
<p>



</p>
</details>



"
1205,5460,0,"reduce_sum extremely slow on gpu for complex dtypes. ## Description
 takes an order of magnitude longer to compute when the tensor is a complex dtype and the computation takes place on a gpu. I've tried to investigate the source of the problem and it's not clear to me whether this is a Tensorflow issue or an Eigen issue, but I thought I'd raise it here first.

## Environment
Ubuntu 16.04 LTS
Tensorflow: 1fcd6d1294564066c6f92b121a3aaf4ed186dc1a
GPU: Titan X
Python 2.7.11

## Diagnostics/Reproducibility:
I produced a tensorflow timeline trace. The trace shows that reductions for a real and complex tensor on the cpu, and for a real tensor on the gpu, takes no longer than 1.3 ms. The reduction of a complex tensor of the same size takes about 196 ms on the gpu.  In particular this time is spent in a dedicated GPU stream, where the input arguments are marked as undefined in the trace.

More specifically, here's a table summarizing the relevant parts of the trace:

### CPU, Complex Reduction
Start | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op
-----|--------------------|---------------|------------|------------|------------|--------
0.076 ms | 1.174 ms | 1.174 ms | ""cplx_var/read"" | ""cplx_reduction/range"" | ""cplx_reduction/Sum"" | ""Sum""

### GPU, Real Reduction
Start | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op
-----|--------------------|---------------|------------|------------|------------|--------
0.087 ms | 1.267 ms | 1.267 ms | ""real_var/read"" | ""real_reduction_1/range"" | ""real_reduction_1/Sum"" | ""Sum""
1.355 ms | 0.043 ms | 0.043 ms | undefined | undefined | ""real_reduction_1/Sum"" | ""Sum""

### CPU, Real Reduction
Start | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op
-----|--------------------|---------------|------------|------------|------------|--------
4.010 ms | 0.465 ms | 0.465 ms | ""real_var/read/_7"" | ""real_reduction/range"" | ""real_reduction/Sum"" | ""Sum""

### GPU, Complex Reduction (Slow)
Start | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op
-----|--------------------|---------------|------------|------------|------------|--------
4.019 ms | 0.070 ms | 0.070 ms | ""cplx_var/read/_5"" | ""cplx_reduction_1/range"" | ""cplx_reduction_1/Sum"" | ""Sum""
4.078 ms | 195.994 ms | 195.994 ms | undefined | undefined | ""cplx_reduction_1/Sum"" | ""Sum""

The trace and a screenshot are attached as a text file and pdf file, respectively. The code that generated the trace is provided below:



## Partial Workaround
It's easy to avoid placing explicit calls to  by flattening the tensor into a row and performing a matrix multiply with a tensor column of 1s. The performance on complex tensors using the gpu is reasonable using this approach. However Tensorflow uses reductions internally for many gradient computations and these are harder to avoid.

[timeline.trace.txt](https://github.com/tensorflow/tensorflow/files/576345/timeline.trace.txt)
[chrome_tracing.pdf](https://github.com/tensorflow/tensorflow/files/576234/chrome_tracing.pdf)
"
511,24032,0,"TFTRT and `tf.keras.application.ResNet50` INT8 and FP32 not working. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Ubuntu 18.04**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **tf-nightly (1.13.0-dev20181127).**
- Python version: **3.5**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source):  **N/A**
 CUDA/cuDNN version: **9.0 / 7.3**
- GPU model and memory: **GTX 1080 Ti (11GB)**

**Describe the current behavior**

Conversion to FP32 using TensorRT fails on ,  with:



With , conversion to FP32 fails with:

d.nbDims >= 3' failed.
tf.keras.applications.ResNet50VGG19
python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion tf.keras.applications.*python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import tensorflow as tf
import tensorflow.contrib.tensorrt as trt
import utils
import time

from imutils import paths
from tensorflow.python.client import session as csess
from tensorflow.python.framework import importer as importer
from tensorflow.python.framework import ops as ops
from tensorflow.core.protobuf import config_pb2 as cpb2
from tensorflow.python.platform import gfile


print(tf.__version__)

gpu_memory = 10000000000
trt_frac = 0.5

# Create a session first...
sess = tf.Session(config=tf.ConfigProto(
    gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=trt_frac)))

tf.keras.backend.set_session(sess)

input_tensor = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name='image_tensor')

tf.keras.backend.set_learning_phase(0)

# NOTE: ResNet50 doesn't work with INT8
# python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion d.nbDims >= 3' failed.
# model = tf.keras.applications.MobileNetV2(weights='imagenet')

# NOTE: InceptionResNetV2 and InceptionV3 doesn't work:
# Engine my_trt_op_0 creation for segment 0, composed of 2049 nodes failed:
# Invalid argument: Validation failed for TensorRTInputPH_0 and input slot 0:
# Input tensor with shape [?,?,?,3] has an unknown non-batch dimemension at dim 1.
# model = tf.keras.applications.InceptionResNetV2(weights='imagenet')
# model = tf.keras.applications.InceptionV3(weights='imagenet')

# NOTE: VGG19 doesn't work
# python: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I have also tried messing around with every combination of , ,  but no joy.

"
736,6629,0,"Feature request: configuration files for tensorflow. Matplotlib and theano provide [](http://matplotlib.org/users/customizing.html) and [](http://deeplearning.net/software/theano/library/config.html) files to configure the libraries, respectively. It would be great if we could add such functionality to tensorflow. 

In particular, it would be great to 

* expose a  variable such that it is straightforward to switch between 16, 32, and 64 bit representations without having to modify code
* expose the [](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto)

A JSON representation of the configuration may be more suitable than the formats used by matplotlib and theano because it can more easily capture the hierarchical structure of the configuration."
506,2292,0,"Bug on specifying GPU to tutorial example minist. I tried to specify GPU ID to run the tutorial example [mnist](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py). I change the code to:



Then it reports error when running:

tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'global_step': Could not satisfy explicit device specification '/device:GPU:3' because no supported kernel for GPU devices is available
     [[Node: global_step = Variable[container="""", dtype=DT_INT32, shape=[], shared_name="""", _device=""/device:GPU:3""]()]]
Caused by op u'global_step', defined at:
  File ""fully_connected_feed.py"", line 232, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""fully_connected_feed.py"", line 228, in main
    run_training()
  File ""fully_connected_feed.py"", line 150, in run_training
    train_op = mnist.training(loss, FLAGS.learning_rate)
  File ""/search/guangliang/package/tensorflow/tensorflow/examples/tutorials/mnist/mnist.py"", line 125, in training
    global_step = tf.Variable(0, name='global_step', trainable=False)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 209, in **init**
    dtype=dtype)
...

Then I fix the line 125 in ""mnist.py"" with the following code:

  with tf.device('/cpu:0'):
    global_step = tf.Variable(0, name='global_step', trainable=False)

Then it reports the following error on rerunning:

tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'gradients/xentropy_mean_grad/Prod': Could not satisfy explicit device specification '/device:GPU:3' because no supported kernel for GPU devices is available
     [[Node: gradients/xentropy_mean_grad/Prod = Prod[T=DT_INT32, keep_dims=false, _device=""/device:GPU:3""](gradients/xentropy_mean_grad/Shape_2, gradients/xentropy_mean_grad/range_1)]]
Caused by op u'gradients/xentropy_mean_grad/Prod', defined at:
  File ""fully_connected_feed.py"", line 232, in <module>
    tf.app.run()
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""fully_connected_feed.py"", line 228, in main
    run_training()
  File ""fully_connected_feed.py"", line 150, in run_training
    train_op = mnist.training(loss, FLAGS.learning_rate)
  File ""/search/guangliang/package/tensorflow/tensorflow/examples/tutorials/mnist/mnist.py"", line 129, in training
    train_op = optimizer.minimize(loss, global_step=global_step)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 190, in minimize
    colocate_gradients_with_ops=colocate_gradients_with_ops)
...

Would you please help on this?
Thanks a lot in advance!
"
1488,34140,0,"[tf2.0] Can not load keras saved model with customized loss function. I run tensorflow.keras on colab.research.google.com. 

In tf2.0, I trained a model with a customized loss function named Loss, then saved it by keras.Model.save().
When I try loading it by 

It raises:
> /tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_utils.py in get_loss_function(loss)
>    1092   return losses.LossFunctionWrapper(
>    1093       loss_fn,
> -> 1094       name=loss_fn.__name__,
>    1095       reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE)
>    1096 
> 
> AttributeError: 'Loss' object has no attribute '__name__'

Even to set Loss.__name__ = 'xxx' before calling load_model does not help.

If set the 'compile' param to True for load_model, it may load successfully. But the loaded model can not work on evaluating or predicting.

To load the files saved by keras of tf1.5 is all OK. The model is trained by the same customized loss function. The model is saved to a file by tf1.5, while to a path by tf2.0.

How can I solve it?"
797,32806,0,"keras.Model.fit does not work with custom layers in Tensorflow 2.0. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): 2.0.0-rc1
- Python version: 3.6.8 (default, Jan 14 2019, 11:02:34) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]

**Describe the current behavior**
https://colab.research.google.com/drive/1dUGcOfRIXkP6ZaI-9xKRXAntf8ZzS6bp



This code dies at model.compile with .According to the stack trace, it seems like it falls into an infinite loop with  and .

**Describe the expected behavior**
Tensorflow 2.x should work with a custom nested layer as Tensorflow 1.x can.

**Code to reproduce the issue**

2.0.0-rc1:https://colab.research.google.com/drive/1dUGcOfRIXkP6ZaI-9xKRXAntf8ZzS6bp
1.14.0:
https://colab.research.google.com/drive/1DeOxKVDM8xEJmU_8GZo-h0iJkWGA4De8
"
341,25841,0,"[tflite][java] how to deal with result of runForMultipleInputsOutputs(). I am running posenet on android with tflite.
The model has multiple output arrays with the following dimensions:


Therefore running the java tfliter interpreter with


i can access the four output tensors with  or with  (with i el. [0,3]) as  is a  filled with  objects.

How can I convert these outputs or tflite tensors to java multi-dimensional arrays (something like ) to be able to perform mathematical computations on them?"
1186,34963,0,"R2.1 pip install failed to find a correct tensorflow-estimator. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source code build and then pip install
- TensorFlow version: R2.1 (b2474d99ae962ce66d6db896606488797152dc4d)
- Python version: 2.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CPU(MKL)


**ERROR: No matching distribution found for tensorflow-estimator<2.2.0,>=2.1.0 (from tensorflow==2.1.0rc1)**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**

"
1010,12128,0,"Where is device_attributes.pb.h . I have to use the  device_attributes.pb.h  and I don't know where it is.

Thank you !"
1107,9341,0,"Tensorflow looks for wrong libcupti.so version. - OS: Ubuntu 16.04
- *TensorFlow installed from: source
- *TensorFlow version*: 1.1.0-rc2
- *Bazel version (if compiling from source)*: 0.4.5
- *CUDA/cuDNN version*: 8.0/6.0
- *GPU Model and Memory*: NVIDIA GTX 1060, 6 GB RAM
- *Exact command to reproduce*:
------------------------
MNIST examples works fine, bute MNIST example with summaries crashes with:



source code:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py

Problem is, that tensorflow looks for libcupti.so.8.0, but only libcupti.so.7.5 is installed:



I installed libcupti like described in the Install section on the Tensorflow website:



libcupti-dev 8.0 is not available for Ubuntu 16.04."
258,25057,1,"tensorflow mirroredstrategy takes forever to start training. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- CUDA/cuDNN version: 9.0/7.3
- GPU model and memory: 1080ti

**Describe the current behavior** 
tf.Estimator using mirrored strategy takes forever to start training. Details are provided here https://stackoverflow.com/questions/54125722/tensorflow-mirroredstrategy-takes-forever-to-initialize
After asking the question I additionaly tried different versions of tf/cuda, no changes. Also I was finally able to see the start of the training, It took about 40 minutes! 


**Describe the expected behavior**
The same model starts training in a couple of minutes without distribute strategy

When using much simpler model, like ResNet50 mirrored strategy also lags at startup compared to single GPU, but nevertheless training starts in a couple of minutes. What may be the problem with mirrored strategy? "
356,4911,0,"Greenish image when using convertYUV420ToARGB8888. I'm experimenting the [TensorFlow Android Camera Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android). but it seems that the [ImageUtils.convertYUV420ToARGB8888](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageListener.java#L164) used to convert images from the camera input to RGB  bytes array is not handling very well the [YUV_420_888 format](https://developer.android.com/reference/android/graphics/ImageFormat.html).

Here is the output of 

![](http://i.imgur.com/bsL1czy.png)
"
830,33383,0,"Desynchronized zipped datasets when using tf.data.experimental.ignore_errors. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
na
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version:
Python 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
cuda 10.0
- GPU model and memory:
P100 / 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

When an error is caught using the  on a zipped dataset, only the faulty dataset drops an element. The datasets are therefore desynchronized.

**Describe the expected behavior**

Datasets should stay synchronized by dropping an element from both datasets.

**Code to reproduce the issue**




**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
861,19606,0,"Android tensorflow lite kernel_util.cc:34 input_product_scale < output_scale. ------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
1383,552,0,"TF 0.6.0 slower than 0.5.0 with the cifar10 example. With the tensorflow/model/image/cifar10 example. While I run cifar10_train.py, 
It used to be able to achieve around 400+ examples/sec with my single K40 GPU. Today I pulled the code and installed the latest version (0.6.0x), but it can only achieve 300 examples/sec or less sometimes. 

I see Alexnet benchmark performance gets improved, but this actually use case gets worse performance. 
"
1469,1872,0,"How to get top N results for seq2seq?. In the example translate seq2seq model, mentioned in the tutorial and in the code here (reproduced below):



It uses only a greedy decoder, and uses argmax to find the best match. I'm wondering if there's a way to get the top N results instead of just doing it greedily. I've tried argsort, but everything apart from the 0th index is just garbage results. I've also looked into tf.nn.top_k(), but because this is batched, I get the error ""List of Tensors when single Tensor expected"" and I'm not sure how to unroll the list within TF.
"
1084,31725,0,"Error loading tensorflow. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.14.0
- **Python version**: 3.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 10.1 - 10.1
- **GPU model and memory**: Nvidia Quadro K5000 - 4Gb
- **Exact command to reproduce**: import tensorflow as tf

### Describe the problem
Error loading Tensorflow. Does not provide a clue about which DLL is failing to initialize.

### Source code / logs
> import tensorflow as tf
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).
Failed to load the native TensorFlow runtime.

Calling from a program:
>python style.py
Traceback (most recent call last):
  File ""C:\Users\Javier\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Javier\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Javier\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""style.py"", line 9, in <module>
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL).
Failed to load the native TensorFlow runtime."
691,16798,0,"tf.nn.conv2d on GPU with data_format='NHWC' gives corrupted result for specific shapes. Basically what the title says.

For an image of size (1096, 2449) EXACTLY, not 1097 or 1095 or 2448 or 2450 (but 2451 for some reason produces the same effect). The bottom part of the convolution result gets corrupted. It only affects convolution done on the GPU with the 'NHWC' data format.

Honestly, it feels more of a cuDNN bug than anything, but not sure where to post it otherwise.

**Important note : I am on Ubuntu 14.04 hence I can not try tensorflow 1.5 which needs CUDA 9 which needs 16.04. So my test is done on 1.4 with cuda8 and cuDNNv6.**

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: pip install 1.4 tensorflow-gpu
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.5
- **CUDA/cuDNN version**: cuda 8 cudnn 6
- **GPU model and memory**: TITAN X Pascal
- **Exact command to reproduce**:


Plotting the difference between the CPU conv and the GPU conv :

<img width=""359"" alt=""capture d ecran 2018-02-06 a 12 31 03"" src=""https://user-images.githubusercontent.com/7132817/35857501-a74bb66e-0b39-11e8-9edd-98e69eba5c48.png"">

"
355,1513,0,"seems issues with softmax_cross_entropy_with_logits. matrix1 = tf.constant([[ -2.88181912e+11,-4.40300175e+11], [ -2.29688918e+11, -3.53027424e+11] ,[ -2.14650274e+11, -3.81851140e+11]])
matrix2 = tf.constant([[0, 1.0], [0, 1], [0, 1]])
r = tf.nn.softmax_cross_entropy_with_logits(matrix1, matrix2)

with tf.Session() as sess:
    result = sess.run(r)
    print result

when i run this, i get:

[  1.52118264e+11   1.23338506e+11   1.67200866e+11]

is this wrong ?
"
496,383,0,"tf.train.saver.restore failed error. Cause training a model is time consuming, So Save a Checkpoint on training, but error occurred when to restore.
The  says as follow:

save_pathsave()latest_checkpoint()
"
795,8687,0,"[HDFS]OutOfRangeError cause java runtime error. ### Environment info
TensorFlow: v1.0.0 and v1.0.1
JDK: 1.8.0_111
Hadoop: 2.6.0
GCC: 4.8.2

### OutOfRangeError
I think the  is a bug, at least, if you want to read data(tfrecords, csv, etc) from hdfs with readers and queues. 

### How to reproduce 
I followed the guide of [reading data](https://www.tensorflow.org/programmers_guide/reading_data).
I used the recommended mnist example [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data).  

Firstly, convert MNIST dataset by this script: [convert_to_records.py](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py)

Secondly, put the tfrecords of MNIST dataset into your hdfs. For example, assume the url is .

Then, train the MNIST network by this script: [fully_connected_reader.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py). And feed the  argument with .

Finally, you can get the train log and error log as below:
### Train log


### Error log


### Ask for a better way
I know it's hard to avoid the  because the multithreads of readers. But can we have a better way to read? Especially when you want to read data from hdfs, it's inconvenient and difficult to catch the exception. "
1252,31724,0,"Tensorflow distribute strategy with custom layers gives an error when using Adam optimizer.  make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below):1.14.0
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
I am getting an error when I use a model with custom layers with trainable weights. It works fine without the distribute strategy, and also works fine if the custom layer is replaced by  an existing tf.Keras layer. The error appears to be related to Adam optimizer finding an empty var list when it tries to colocate variables. 

**Describe the expected behavior**

**Code to reproduce the issue**
`
      
     

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1168,17907,0,"Error with GDR. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Red hat
- **TensorFlow installed from (source or binary)**:
source (1.6)
- **TensorFlow version (use command below)**:
1.6
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.11
- **GCC/Compiler version (if compiling from source)**:
4.85
- **CUDA/cuDNN version**:
9/7
- **GPU model and memory**:
P100
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

While building from source, I got the following error:
                                                                             ^
ERROR: /gpfshome01/u/amalik/Tensorflow/tensorflow/tensorflow/contrib/gdr/BUILD:52:1: C++ compilation of rule '//tensorflow/contrib/gdr:gdr_memory_manager' failed (Exit 1)
tensorflow/contrib/gdr/gdr_memory_manager.cc:28:27: fatal error: rdma/rdma_cma.h: No such file or directory
 #include <rdma/rdma_cma.h>
                           ^
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 556.299s, Critical Path: 183.28s
FAILED: Build did NOT complete successfully

-----
I check on google but could not find any information. 


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
397,27311,0,"[TF 2.0] tf.summary.histogram error. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Testing
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): '2.0.0-dev20190327'
- Python version: 3.7


**Describe the current behavior**

I want to log gradients and weights of the model as it is training, using .
It seems that the function is not properly implemented, however, as using it results in an error:


**Describe the expected behavior**

 should properly log histogram data.

**Code to reproduce the issue**


**Other info / logs**


"
330,22739,0,"AttributeError: module 'tensorflow.python.keras.engine.base_layer' has no attribute 'Layer'. # I just update my keras so my tensorflow update from 1.7 update 1.9
but I found that tensorflow can't work,I just run ,the error show as title.My system just following with:
- ubuntu16.0.4 64bit
- gtx980m 8G
- intel i7 6820hk
- cuDNN7
- CUDA 9.0
I just tried that conda create a new environment and reinstall tensorflow with conda or pip,but the result is same.Use conda install from tsinghua's mirror,tensorflow's version is tensorflow1.10,pip install's tensorflow is tensorflow1.11,Had sameone happened the same question? 
Network"
288,26727,0,"[Feature] Support RISCV with tfcompile --target_tuple. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
third_party/llvm/llvm.autogenerated.BUILD supports AArch64, ARM, PowerPC, and X86 in llvm_target_list, so I can use 'tfcompile --target_triple=aarch64-none-android' to generate  AArch64 ELF object.
It seems the tool that generates llvm.autogenerated.BUILD is not released yet, I suggest adding a RISCV target in llvm.autogenerated.BUILD.

**Will this change the current api? How?**
No.
**Who will benefit with this feature?**
People who use RISCV cpus.
**Any Other info.**
"
235,2996,1,"tf.reduce_sum(a) is slow. For large vectors, cost of  is dominated by cost of  that's called to construct list of reduction indices.

Summing up 10k elements 1500 times, I get about 0.8 seconds spent in  while 2 seconds is spent in . Also, transferring all those indices to sum probably slows things down. This only happens when static graph optimizations don't apply. Will update with reproducible benchmark in a bit
"
1057,21411,0,"Tensorflow does not recognize GPU . 
OS: Ubuntu 18.04.1 LTS
Tensorflow: 1.9.0 
Tensorflow- GPU: 1.9.0
Both installed from pip/pip3

Graphics Card: GeForce GTX 1080 TI 
Nvidia SMI Driver Version: 390.77
Python: 2.7.15, 3.6.5 
CUDA Toolkit:  9.0, V9.0.176 ( I also have 8 installed) 
CuDNN: 
![asfd](https://user-images.githubusercontent.com/22109013/43723326-5e2d6d0a-9965-11e8-9362-e5ff24c99e4f.png)

Running this: 
![screenshot from 2018-08-06 10-34-04](https://user-images.githubusercontent.com/22109013/43722870-4de50daa-9964-11e8-9648-050d643e0d16.png)

I get the following errors/CPU only: 
![screenshot from 2018-08-06 10-33-35](https://user-images.githubusercontent.com/22109013/43722894-5ad5ef02-9964-11e8-9b17-721294c7e15d.png)


I need to be able to access/run models off the GPU and am struggling to debug this issue alone. This seems to be a common problem, any help would be appreciated! "
429,3238,0,"Android example fails to build with AndroidResourceProcessingAction: No such file or directory. ### Environment info

Operating System:
Ubuntu 16.04 LTS

Installed version of CUDA and cuDNN: 
(please attach the output of ):



If installed from binary pip package, provide:
1. Which pip package you installed.:
   Compiled python wheel from source and installed it. But I don't think that is relevant for building the Android demo. I maybe wrong.
2. The output from .
   0.8.0

If installed from sources, provide the commit hash:
Don't have it
### Steps to reproduce


### What have you tried?

Nothing yet.
### Logs or other output that would be helpful



I notice that although the file bazel is looking for does not exist, this file seems to be there



It looks to me that some component is not attaching ""org_tensorflow"" to some path

I did a fresh clone and tried compiling the Android example without compiling the rest of tensorflow. My assumption is that bazel would build whatever was necessary. I will attach the full log if it's required. 
"
1138,13294,0,"Input Pipeline for High-performance Model. ### Describe the problem
When using data_flow_op.RecordInput in input pipeline on distributed TensorFlow, every input thread seems load all files in the data_dir to the local buffer after shuffling and left-shifting all matched file names in data_dir (https://github.com/tensorflow/tensorflow/blob/40eef4473bda90442bb55fcc67842f097c024580/tensorflow/core/kernels/record_yielder.cc#L139). 
Based on the code, it seems like data input for each epoch ends with loading all files instead of part of the files on every worker.

 If I understand correctly, each input thread from each worker task should read a portion of the files, which should be the shift_ratio * file_num. It will be very helpful, if anyone can explain this. 
"
937,9801,0,"ValueError: Cannot feed value of shape (1, 40, 3) for Tensor u'cls1_fc_pose_xyz_target:0', which has shape '(?, ?). I have installed keras using conda with tensorflow backend. I am experimenting with the keras version of posenet architecture. I have introduced a few LSTM layers into the original architecture and now my input should be of 5 dim(n_batch,n_frame,row,col,channel) and my model compiles correctly but when I call the fit function, it throws following **error:**

    Train on 129 samples, validate on 129 samples

    Epoch 1/800

    Traceback (most recent call last):

    File ""train.py"", line 72, in <module>

    callbacks=[checkpointer])

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/keras/engine/training.py"", line 1485, in fit

    initial_epoch=initial_epoch)

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/keras/engine/training.py"", line 1140, in _fit_loop

    outs = f(ins_batch)

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 2073, in __call__

    feed_dict=feed_dict)

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run

    run_metadata_ptr)

    File ""/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 944, in _run

    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))

    ValueError: Cannot feed value of shape (1, 40, 3) for Tensor u'cls1_fc_pose_xyz_target:0', which has shape '(?, ?)'
`

and here the piece of code that I am using in **training**:

    X_train=np.squeeze(np.array(dataset_train.images,dtype=float))

    X_test=np.squeeze(np.array(dataset_test.images,dtype=float))

    y_train=np.squeeze(np.array(dataset_train.poses,dtype=float))

    print(""X_train shape:""+str(X_train.shape))#X_train shape:(129, 40, 224, 224, 3)

    print(""y_train shape:""+str(y_train.shape))#y_train shape:(129, 40, 7)

    y_train_x = y_train[:,:,0:3]

    y_train_q = y_train[:,:,3:7]

    y_test = np.squeeze(np.array(dataset_test.poses))

    print(""X_test shape:""+str(X_test.shape))#X_test shape:(129, 40, 224, 224, 3)

    print(""y_test shape:""+str(y_test.shape))#y_test shape:(129, 40, 7)

    y_test_x = y_test[:,:,0:3]

    y_test_q = y_test[:,:,3:7]

    #Setup checkpointing
    checkpointer = ModelCheckpoint(filepath=""checkpoint_weights.h5"", verbose=1, save_best_only=True, save_weights_only=True)

    model.fit(X_train, [y_train_x, y_train_q, y_train_x, y_train_q, y_train_x, y_train_q],
      batch_size=batch_size,
      nb_epoch=800,
      verbose=1,
      validation_data=(X_test, [y_test_x, y_test_q, y_test_x, y_test_q, y_test_x, y_test_q]),
      callbacks=[checkpointer])
and here is the **input** of my model:

    input = Input(shape=(helper.frames_per_sequence,224, 224, 3))

and here is the **lines causing error**:

    cls1_fc1_flat = Flatten()(cls1_reduction_pose)

    cls1_fc1_pose = Dense(1024,activation='relu',name='cls1_fc1_pose')(cls1_fc1_flat)

    cls1_fc_pose_xyz = Dense(3,name='cls1_fc_pose_xyz')(cls1_fc1_pose)

    cls1_fc_pose_wpqr = Dense(4,name='cls1_fc_pose_wpqr')(cls1_fc1_pose)"
1040,28432,0,"Build failure while installing TensorFlow from source following TF documentation. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 
- TensorFlow installed from (source or binary): Source 
- TensorFlow version: Tensorflow 2.0 
- Python version: Python 3.7
- Installed using virtualenv? pip? conda?: Created a Conda virtual environment 
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc version 6.5.0 
- CUDA/cuDNN version: CUDA 9.0/ cuDNN 7.4.2
- GPU model and memory: Geoforce GTX 1050ti



**Describe the problem**

**Provide the exact sequence of commands/steps that you executed before running into the problem**

I followed the steps listed here for building from source (https://www.tensorflow.org/install/source). I keep getting stuck at the step wherein I invoke ""bazel build"" which is the penultimate step in the installation process. Now, these errors aren't consistent. I have run the command ""bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures"" multiple times and each time the build fails with a different error. However this is the error I got for the last two times I tried the installation process


ERROR: /home/wannabe/tensorflow/tensorflow/contrib/resampler/BUILD:65:1: output 'tensorflow/contrib/resampler/_objs/python/ops/_resampler_ops_gpu/resampler_ops_gpu.cu.pic.o' was not created
ERROR: /home/wannabe/tensorflow/tensorflow/contrib/resampler/BUILD:65:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 234.229s, Critical Path: 40.57s
INFO: 648 processes: 648 local.
FAILED: Build did NOT complete successfully

I copy pasted the entire stack of messages printed out to the console here https://pastebin.com/kJFxdHgA"
539,34393,0,"TFLite SIGILL on Invoke(). <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I am using the C++ API on Android
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 10, Android 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 3, Samsung Galaxy S8
- TensorFlow installed from (source or binary): Built from tag (build included in repro repo)
- TensorFlow version (use command below): v2.0.0
- Python version:
- Bazel version (if compiling from source): 0.28.0
- GCC/Compiler version (if compiling from source): NDK 18
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behaviour**
While debugging C++ on Android, SIGILL is triggered on the interpreter->Invoke() call.  If you step past inference, TFLite still successfully infers and produces the correct result, but SIGILL shouldn't happen.

**Describe the expected behavior**
SIGILL shouldn't happen

**Code to reproduce the issue**
This repro reproduces this issue:
git@github.com:infil00p/TFLiteSigILL.git

**Other info / logs**
Debug frame for SIGILL
<img width=""1286"" alt=""Screen Shot 2019-11-18 at 12 56 57 PM"" src=""https://user-images.githubusercontent.com/2313/69093285-026b7d80-0a03-11ea-809b-1e73d1537bf4.png"">
"
1133,34507,0,"How to get symbolic learning_phase? (TF2 Eager).  fetches the value, not the tensor itself - following _backend.py_, I found somewhat of a workaround, but it isn't user/API-friendly. I need the learning phase tensor to feed to  to get layer gradients, outputs, etc. Works fine w/ , but fails for .

Passing the symbolic learning_phase into  yields:



<hr>

**Minimal applied example**:



**Full error trace**:



<hr>

**Partial workaround**:


"
195,20346,1,"Tensorflow Program Hangs on pthread_cond_wait. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS, xenial
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  tensorflow-gpu (1.8.0)
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0.176/7.1.3.16
- **GPU model and memory**: GeForce GTX 850M with 2GB of memory
- **Exact command to reproduce**:

### Describe the problem
I have a program using Tensorflow and it consistently hangs after a week or so, arbitrarily.  The GPU utilization goes to 0%, and the program stops training.  A stack trace shows that the program is stuck waiting on pthread_cond_wait.

### Source code / logs
I don't know that the source code will be of much use since it's not a concise example that reproduces the problem, but the code is available here: https://github.com/benbotto/bsy-dqn-atari/tree/breakout-best  My code is single threaded.

When last the program hung I took a stack trace.  That's available here: https://pastebin.com/LiPhz2CE

This looks similar to this report: https://github.com/tensorflow/tensorflow/issues/1947
 
Let me know if more information is needed."
903,29132,0,"Tensorflow2.0 keras subclassed model, problems with getting correct output shape, shows 'multiple'. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TensorFlow 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0, 7.5
- GPU model and memory: 1080ti 11gb

**Describe the current behavior**
model.summary() prints 'multiple' as layer output shapes

**Describe the expected behavior**

**Code to reproduce the issue**

output log
"
144,1821,1,"Basic Lstm cell giving NAN loss and 0 acuraccy.. ### Environment info

Operating System: Ubuntu 14.04 

Installed version of CUDA and cuDNN:  7.5 cudnnv4.0
(please attach the output of ):



If installed from binary pip package, provide:
1. Which pip package you installed. 
2. The output from python -c ""import tensorflow; print(tensorflow.**version**)"".



Hi,

I had modified [this](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/recurrent_network.py) code to accept an input 4096 vector with 16 time steps.
and changed very little.
The loss is always nan and acuraccy 0.
I have tried several combinations of Learningrate/batchsize/optimizers with no change.

[Here Is my code.](http://pastebin.com/S1WcVNmk)
# Output



Any ideas on y this is happening? 
"
886,29997,0,"libtensorflow_jni.so: libcudnn.so.7: cannot open shared object file: No such file or directory. tensorflow version=1.8.0
java=1.8.0
cuda=9.1
cudnn=7

**I used java to call the GPU when I reported the following error:**

java.lang.UnsatisfiedLinkError: /tomcat/temp/tensorflow_native_libraries-1561013286760-0/libtensorflow_jni.so: libcudnn.so.7: cannot open shared object file: No such file or directory



when delete as follows. This program prompts that the GPU cannot be found, forcing the CPU

Can you help me? Thanks a lot.

"
59,34424,1,"Extremely slow pre-processing of images using InceptionV3. **System information**
- OS Platform and Distribution: Google AI Platform
- TensorFlow version: v2.0
- Python version: v3.*
- Machine type: 4 vCPUs, 15 GB RAM
- GPU: 1x NVIDIA Tesla K80

I am training a model to caption images and have mostly used this tutorial: https://www.tensorflow.org/tutorials/text/image_captioning#download_and_prepare_the_ms-coco_dataset

The issue is, I am trying to pre-process each image with InceptionV3 and cache the output to disk, however, each iteration (code snippet below) takes about 22s without GPUs and 18s with NVIDIA Tesla K80. This is way too slow for 30,000+ images for example. 

My image file size ranges between 60-150kb each.



The tensorflow tutorial claims that caching will take about 10 minutes to run in Colab with a GPU (for 30,000 images).

I wonder if I am doing something wrong, or if there is a more efficient way to achieve this?

Any help is appreciated!
"
1100,5729,0,"Feature request: Please release an official binary for Raspberry Pi. The Raspberry Pi is an excellent platform for robotics and automation. The native camera also makes a great combo for portable / embedded / robotics computer vision. Having TensorFlow available on this platform would really open up a lot of options in this context.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

none

### Environment info
Operating System:

Raspbian Jessie on Raspberry Pi 3

### What other attempted solutions have you tried?

https://github.com/samjabrahams/tensorflow-on-raspberry-pi

It only provides a binary for TensorFlow 0.10. I've had lots of issues with it running DNNs trained on 0.11. I need to train on 0.11 because the only fast GPU I have is an Nvidia Pascal chip, and 0.10 doesn't work well on Pascal (due to CUDA version issues).

I've tried to compile 0.11 following the guide there, but there were a lot of errors."
1048,14759,0,"Version 1.4.0 Can't enable peer access between some devices. 
If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7-3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**:  3.4.5
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.3.0
- **CUDA/cuDNN version**: CUDA 9.0.175 / cuDNN 7.0
- **GPU model and memory**:  10 x GeForce GTX 1080 Ti  12 GB
- **Exact command to reproduce**: 

### Describe the problem
System has 10 GPUs on one pci root hub but Tensorflow can not enable peer access to all devices. Nvidia CUDA-Example 1_Utilities/p2pBandwidthLatencyTest is able to enable these

### Source code / logs
Tensorflow output:
> 2017-11-21 13:58:12.914211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:04:00.0
totalMemory: 10.91GiB freeMemory: 10.72GiB
2017-11-21 13:58:13.249428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:05:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2017-11-21 13:58:13.574464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 2 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:06:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2017-11-21 13:58:13.899631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 3 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:07:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2017-11-21 13:58:14.219023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 4 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:08:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2017-11-21 13:58:14.553864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 5 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:0b:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2017-11-21 13:58:14.888727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 6 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:0c:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2017-11-21 13:58:15.208341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 7 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:0d:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2017-11-21 13:58:15.524748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 8 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:0e:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2017-11-21 13:58:15.831437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 9 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:0f:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2017-11-21 13:58:15.837982: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 0 and 9
2017-11-21 13:58:15.843596: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 1 and 9
2017-11-21 13:58:15.848661: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 2 and 9
2017-11-21 13:58:15.852889: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 3 and 9
2017-11-21 13:58:15.856213: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 4 and 9
2017-11-21 13:58:15.858748: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 5 and 9
2017-11-21 13:58:15.860537: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 6 and 9
2017-11-21 13:58:15.861548: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 7 and 9
2017-11-21 13:58:15.861791: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 8 and 9
2017-11-21 13:58:15.861915: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 0
2017-11-21 13:58:15.862038: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 1
2017-11-21 13:58:15.862161: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 2
2017-11-21 13:58:15.862283: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 3
2017-11-21 13:58:15.862405: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 4
2017-11-21 13:58:15.862523: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 5
2017-11-21 13:58:15.862642: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 6
2017-11-21 13:58:15.862759: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 7
2017-11-21 13:58:15.862878: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 8
2017-11-21 13:54:16.736201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix
2017-11-21 13:54:16.736590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1 2 3 4 5 6 7 8 9 
2017-11-21 13:54:16.736598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y Y Y Y Y Y Y Y Y Y 
2017-11-21 13:54:16.736602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   Y Y Y Y Y Y Y Y Y Y 
2017-11-21 13:54:16.736606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 2:   Y Y Y Y Y Y Y Y Y Y 
2017-11-21 13:54:16.736610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 3:   Y Y Y Y Y Y Y Y Y Y 
2017-11-21 13:54:16.736613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 4:   Y Y Y Y Y Y Y Y Y Y 
2017-11-21 13:54:16.736617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 5:   Y Y Y Y Y Y Y Y Y Y 
2017-11-21 13:54:16.736621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 6:   Y Y Y Y Y Y Y Y Y Y 
2017-11-21 13:54:16.736625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 7:   Y Y Y Y Y Y Y Y Y Y 
2017-11-21 13:54:16.736629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 8:   Y Y Y Y Y Y Y Y Y Y 
2017-11-21 13:54:16.736633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 9:   Y Y Y Y Y Y Y Y Y Y 


Nvidia CUDA P2P Output:

> [P2P (Peer-to-Peer) GPU Bandwidth Latency Test]
Device: 0, GeForce GTX 1080 Ti, pciBusID: 4, pciDeviceID: 0, pciDomainID:0
Device: 1, GeForce GTX 1080 Ti, pciBusID: 5, pciDeviceID: 0, pciDomainID:0
Device: 2, GeForce GTX 1080 Ti, pciBusID: 6, pciDeviceID: 0, pciDomainID:0
Device: 3, GeForce GTX 1080 Ti, pciBusID: 7, pciDeviceID: 0, pciDomainID:0
Device: 4, GeForce GTX 1080 Ti, pciBusID: 8, pciDeviceID: 0, pciDomainID:0
Device: 5, GeForce GTX 1080 Ti, pciBusID: b, pciDeviceID: 0, pciDomainID:0
Device: 6, GeForce GTX 1080 Ti, pciBusID: c, pciDeviceID: 0, pciDomainID:0
Device: 7, GeForce GTX 1080 Ti, pciBusID: d, pciDeviceID: 0, pciDomainID:0
Device: 8, GeForce GTX 1080 Ti, pciBusID: e, pciDeviceID: 0, pciDomainID:0
Device: 9, GeForce GTX 1080 Ti, pciBusID: f, pciDeviceID: 0, pciDomainID:0
Device=0 CAN Access Peer Device=1
Device=0 CAN Access Peer Device=2
Device=0 CAN Access Peer Device=3
Device=0 CAN Access Peer Device=4
Device=0 CAN Access Peer Device=5
Device=0 CAN Access Peer Device=6
Device=0 CAN Access Peer Device=7
Device=0 CAN Access Peer Device=8
Device=0 CAN Access Peer Device=9
Device=1 CAN Access Peer Device=0
Device=1 CAN Access Peer Device=2
Device=1 CAN Access Peer Device=3
Device=1 CAN Access Peer Device=4
Device=1 CAN Access Peer Device=5
Device=1 CAN Access Peer Device=6
Device=1 CAN Access Peer Device=7
Device=1 CAN Access Peer Device=8
Device=1 CAN Access Peer Device=9
Device=2 CAN Access Peer Device=0
Device=2 CAN Access Peer Device=1
Device=2 CAN Access Peer Device=3
Device=2 CAN Access Peer Device=4
Device=2 CAN Access Peer Device=5
Device=2 CAN Access Peer Device=6
Device=2 CAN Access Peer Device=7
Device=2 CAN Access Peer Device=8
Device=2 CAN Access Peer Device=9
Device=3 CAN Access Peer Device=0
Device=3 CAN Access Peer Device=1
Device=3 CAN Access Peer Device=2
Device=3 CAN Access Peer Device=4
Device=3 CAN Access Peer Device=5
Device=3 CAN Access Peer Device=6
Device=3 CAN Access Peer Device=7
Device=3 CAN Access Peer Device=8
Device=3 CAN Access Peer Device=9
Device=4 CAN Access Peer Device=0
Device=4 CAN Access Peer Device=1
Device=4 CAN Access Peer Device=2
Device=4 CAN Access Peer Device=3
Device=4 CAN Access Peer Device=5
Device=4 CAN Access Peer Device=6
Device=4 CAN Access Peer Device=7
Device=4 CAN Access Peer Device=8
Device=4 CAN Access Peer Device=9
Device=5 CAN Access Peer Device=0
Device=5 CAN Access Peer Device=1
Device=5 CAN Access Peer Device=2
Device=5 CAN Access Peer Device=3
Device=5 CAN Access Peer Device=4
Device=5 CAN Access Peer Device=6
Device=5 CAN Access Peer Device=7
Device=5 CAN Access Peer Device=8
Device=5 CAN Access Peer Device=9
Device=6 CAN Access Peer Device=0
Device=6 CAN Access Peer Device=1
Device=6 CAN Access Peer Device=2
Device=6 CAN Access Peer Device=3
Device=6 CAN Access Peer Device=4
Device=6 CAN Access Peer Device=5
Device=6 CAN Access Peer Device=7
Device=6 CAN Access Peer Device=8
Device=6 CAN Access Peer Device=9
Device=7 CAN Access Peer Device=0
Device=7 CAN Access Peer Device=1
Device=7 CAN Access Peer Device=2
Device=7 CAN Access Peer Device=3
Device=7 CAN Access Peer Device=4
Device=7 CAN Access Peer Device=5
Device=7 CAN Access Peer Device=6
Device=7 CAN Access Peer Device=8
Device=7 CAN Access Peer Device=9
Device=8 CAN Access Peer Device=0
Device=8 CAN Access Peer Device=1
Device=8 CAN Access Peer Device=2
Device=8 CAN Access Peer Device=3
Device=8 CAN Access Peer Device=4
Device=8 CAN Access Peer Device=5
Device=8 CAN Access Peer Device=6
Device=8 CAN Access Peer Device=7
Device=8 CAN Access Peer Device=9
Device=9 CAN Access Peer Device=0
Device=9 CAN Access Peer Device=1
Device=9 CAN Access Peer Device=2
Device=9 CAN Access Peer Device=3
Device=9 CAN Access Peer Device=4
Device=9 CAN Access Peer Device=5
Device=9 CAN Access Peer Device=6
Device=9 CAN Access Peer Device=7
Device=9 CAN Access Peer Device=8

Any idea how i can fix it ?
"
210,29075,1,"tf.function with input_signature slower on unseen sequence length. **System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04:
- TensorFlow installed from: binary
- TensorFlow version: 
- Python version: 3.6.6
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1060

**Describe the current behavior**

When running a  on 3D inputs (a batch of sequence), I found that the execution is slower on unseen sequence length even if a compatible  is set. On a large graph, this results in a low GPU usage and a growing CPU memory usage for several iterations until most sequence lengths are seen. It is as if new graphs were compiled internally even though the function does not seem to be retraced.

This effect does not affect eager mode or V1 graph mode where the execution directly runs at its target speed and memory usage.

**Describe the expected behavior**

 with an input signature should behave like graph mode with constant memory usage and no ""warmup"" phase.

**Code to reproduce the issue**

While this issue is very visible on large graphs, I tried to compile a small example to consistently show the effect:



**Other info / logs**

The above code produced the following logs when run on CPU:



When the length is unseen, it takes about 0.113s but 0.092s after that.

On this example the effect is small but I'm trying to train a Transformer model with  and it takes very long for the training to reach full speed. The CPU memory usage also keeps growing during this ""warmup"" phase. The same model works well when integrated with  as I'm trying to move from Estimator to V2 custom loops."
1318,22398,0,"CUDA implementation of BiasAddGrad op is non-determinstic. I'm running TensorFlow 1.5.0 on a K80 GPU on Python 2.7

Failing test case:


Outputs


This bug still exists in master, because the code in master uses the unsafe CUDA atomic floating point add in several places. See https://github.com/tensorflow/tensorflow/blob/abc55107eb7a03fe3d83f95fd5e1b8e4def90826/tensorflow/core/kernels/bias_op_gpu.cu.cc 
If TensorFlow will be ever be fully determinstic, atomic floating point add should never be used (it is inherently non-determinstic due to non-associativity of floating point).

Notably, Keras's  layer uses , so all networks that use this layer are non-reproducible. This is relevant to https://github.com/keras-team/keras/issues/2280 .

 This can currently be avoided by using  instead of  at a slightly performance hit. The correct fix would be refactor the  op to use a (deterministic) reduction tree.

edit with issue template fields:
Have I written custom code: Python test case, see above
OS Platform and Distribution: RHEL 7.5 (Linux)
TensorFlow installed from: source
TensorFlow version: 1.5.0
Bazel version: unknown
CUDA/cuDNN version: CUDA 8.0.61, cuDNN v6
GPU model and memory: Nvidia K80, 12GB memory
Exact command to reproduce: run the above script
Mobile device: NA"
582,12761,0,"Allow to build tensorflow as a bazel external dependency.. It would be nice to be able to just add a @org_tensorflow in bazel and be able to build it as a dependency.  

At the best of my knowledge the syntaxnet dockerfile is the best example on having tensorflow as a submodule.
https://github.com/tensorflow/models/blob/c259259299db3b486ccdfd6cdec44b884623053a/syntaxnet/Dockerfile#L63

But still building it is not straightforward and probably does not work if you activate CUDA/XLA/MLK because those depends on other bazel subprojects.

Are there any plans to support this use case or I am missing something in the best practices to build tensorflow as a subpackage ? 
"
1125,11253,0,"pip or package issues. Over the past few hours I have tried to run pip install --upgrade tensorflow_gpu  roughly 20 times, and keep getting read time out from pypi.python.org.  I finally added the --verbose and -- timeout 10000 to troubleshoot. Now I get this:

  Using version 1.2.1 (newest of versions: 1.2.0, 1.2.1)
  Looking up ""https://pypi.python.org/packages/47/81/2b8020393615b06af06e0d7c32d74b9a844ebebf4385f9eb00cfdfdbdd92/tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64.whl"" in the cache
  No cache entry available
  Starting new HTTPS connection (1): pypi.python.org
  ""GET /packages/47/81/2b8020393615b06af06e0d7c32d74b9a844ebebf4385f9eb00cfdfdbdd92/tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64.whl HTTP/1.1"" 200 51299687
  Downloading tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64.whl (51.3MB)
  Downloading from URL https://pypi.python.org/packages/47/81/2b8020393615b06af06e0d7c32d74b9a844ebebf4385f9eb00cfdfdbdd92/tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64.whl#md5=46bb283df033c7fb7c233346eb26d40f (from https://pypi.python.org/simple/tensorflow-gpu/)
    12% |████                            | 6.3MB 6.4kB/s eta 1:57:37
Cleaning up...
**THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
    tensorflow_gpu from https://pypi.python.org/packages/47/81/2b8020393615b06af06e0d7c32d74b9a844ebebf4385f9eb00cfdfdbdd92/tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64.whl#md5=46bb283df033c7fb7c233346eb26d40f:
        Expected md5 46bb283df033c7fb7c233346eb26d40f
             Got        7ba60530da51fdc733b89f9dd3b660fc**

Exception information:
Traceback (most recent call last):
  File ""c:\users\roger\envs\tensorflow_attention_ocr\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""c:\users\roger\envs\tensorflow_attention_ocr\lib\site-packages\pip\commands\install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""c:\users\roger\envs\tensorflow_attention_ocr\lib\site-packages\pip\wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""c:\users\roger\envs\tensorflow_attention_ocr\lib\site-packages\pip\req\req_set.py"", line 386, in prepare_files
    raise hash_errors
pip.exceptions.HashErrors: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
    tensorflow_gpu from https://pypi.python.org/packages/47/81/2b8020393615b06af06e0d7c32d74b9a844ebebf4385f9eb00cfdfdbdd92/tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64.whl#md5=46bb283df033c7fb7c233346eb26d40f:
        Expected md5 46bb283df033c7fb7c233346eb26d40f
             Got        7ba60530da51fdc733b89f9dd3b660fc"
920,4981,0,"FastGFile ResourceExhaustedError. In https://github.com/tensorflow/models/commit/2390974a03a62b1388a004173477418db267074a @cshallue changed some  calls to  because two RedHat users reported in #4685 that it causes ResourceExhaustedError even though resources don't appear to be exhausted.

We've also had the same issue reported in tensorflow/models#531, tensorflow/models#489, and tensorflow/models#480 but we're still waiting to learn if they're using RedHat.

Assigning to @rohan100jain who has done work on this code in the past.
"
122,24575,1,"[Performance] Unnecessary 'memcpy' in Gather Op. **Describe the current behavior**
In distributed mode, user define a Graph, 'distribute_runtime' partition graph, register graph, and then run graph, For example as below.
![image](https://user-images.githubusercontent.com/7778833/50445822-3140b700-094c-11e9-8d5e-78418403836f.png)
ps: Gather Op generates a new Tensor (allocation and **memcpy**), Send Op put the Tensor to local Rendezvous's table.
worker: Recv Op triggers GRPC RecvTensorResponse method via remote Rendezvous, calls EncodeTensorToByteBuffer(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_tensor_coding.cc), if DataTypeCanUseMemcpy==true, encodes tensor with wire format instead of TensorProto serialization. in this case, TensorEncoding doesn't depend on a continuous memory(Gather Op output Tensor)

**Describe the expected behavior**
When Gather Op -> Send Op and Tensor's dtype is simple type, Gather Op's output is a 'IndexedTensor' without memcpy. The 'IndexedTensor' contains some offsets of IDs and a RefCounted TensorBuffer. In GRPC ZeroCopyStream, send the list<pair<pointer, length>> by grpc::Slice.
Performance: reduce a memcpy
Same optimization can apply to other Op, such as: Concat Op, Slice Op ...

**Other info**

"
1255,15254,0,"CMake build on Windows (tensorflow.dll) does not include many GPU ops. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
from source
- **TensorFlow version (use command below)**:
1.4

- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**:
using CMake (3.8.2)
- **GCC/Compiler version (if compiling from source)**:
MSVC 14 (C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64\CL.exe)
- **CUDA/cuDNN version**:
CUDA 8.0, cuDNN: 6 (6.14)
- **GPU model and memory**:
NVidia GTX 1070 (8GB)
- **Exact command to reproduce**:

### Describe the problem
I build tensorflow from source following these instructions:
 https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake
and I use this CMake command:

and 

(I also tried 'Release' as Configuration - same outcome).
The result of the build process is  . I use a separate C++ program (using Qt) to link against the built DLL. In general, everything works fine: I can load a saved tensorflow graph and run it (=inference).
The problem is now, that many GPU-ops are not linked into tensorflow.dll (for example, ) - in my example most ops run on GPU but Softmax on CPU - with a huge performance impact (GPU use <10%). Why I think this is the case:
*  lists Softmax, but with CPU only
* looking at tensorflow.dll with DependecyWalker has the same result (Softmax CPU only)
* when I check , then GPU code is there (e.g., ).

### Workaround
After quite some time trying to figure this out, I found a hackish workaround:
Looking at the output of MSBuild (increased verbosity level), it looks as if the python script create_def_file.py is executed without using the tf_core_kernels.lib (). 
What I did is the following:
* create a def-file including the GPU kernels:


* Link tensorflow.dll with the created def file (tensorflow_wr.def). This did not work, since I got a couple of missing-unresolved-externals errors. As they were all related to LSTM/RNN, I ended up re-creating  (issuing the Lib.exe omitting tf_core_gpu_kernels_generated_gru_ops_gpu.cu.cc.obj and tf_core_gpu_kernels_generated_lstm_ops_gpu.cu.cc.obj)

* Finally linking tensorflow.dll worked after I manually dropped a couple of symbols from 'tensorflow_wr.def) (unresolved external symbols).

With this workaround it works fine - all ops (including Softmax) run on GPU, performance increased by a factor of 10. I submit as an issue, since I believe it should work out of the box!

Some more details on [StackOverflow](https://stackoverflow.com/questions/47636903/tensorflow-places-softmax-op-on-cpu-instead-of-gpu)
"
593,20781,0,"Feature request: preserve cycle order of open iterators in tf.data.Dataset.interleave. I'm trying to train RNNs with truncated BPTT with  (a great API by the way!) but got tripped up by [these lines](https://github.com/tensorflow/tensorflow/blob/744cf3d3e06fb63ffa40086766137daedc01a5ba/tensorflow/core/kernels/data/interleave_dataset_op.cc#L190-L195) as I've assumed an exhausted iterator would result in a new element being opened directly at the same position in the cycle (in order to pass around RNN states reliably).

Instead what seems to be happening is that my sequences are accidentally shifted in in the subsequent  call whenever a sequence is done. Could the default be changed so that a new element is consumed directly as long as there are any left, such that consecutive dataset elements can be batched in a more straightforward way for RNN training.

Or could we have a  or similar?"
479,6716,0,"Feature request: easier access to tensor de-allocation information. TLDR; to debug TensorFlow out-of-memory situations one needs to see tensor de-allocation info.

You can see allocation stats in timeline, but without de-allocation info you can't calculate peak memory. Currently getting peak memory is possible by:

1. Hacking TensorFlow to print deallocation messages with timestamps as [here](https://github.com/yaroslavvb/tensorflow/commit/5d4cd97c0a73e91ee37c025cd7a62fb46aae76a0)

2. A bunch of regular expression to parse  messages as in [here](https://github.com/yaroslavvb/notebooks/blob/master/saving%20memory%20by%20using%20functions.ipynb)

Since this needs building your own version of tensorflow, this is not accessible to most people. Perhaps this can be remedied by adding deallocation events to session run timeline? @michaelisard 

Some recent places this issue came up:
http://stackoverflow.com/questions/41517145/outer-product-based-conv-filters-consume-disproportionately-high-memory
http://stackoverflow.com/questions/41496251/fitting-large-matrix-calculations-into-memory-when-using-tensorflow
http://stackoverflow.com/questions/41451273/tensorflow-specifying-storage-of-layer-activations
http://stackoverflow.com/questions/40190510/tensorflow-how-to-log-gpu-memory-vram-utilization/40197094#comment70145571_40197094
https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-268037463"
826,13186,0,"Renaming checkpoint directory. If you want to rename a checkpoint directory, you currently need to also do a find-replace in the 'checkpoint' file. It might be nice to have that handled automatically.
"
1407,1165,0,"Can tensor take advantage of  multiple machines with multiple gpus?. As title
"
812,32490,0,"//tensorflow/contrib/distributions/python/kernel_tests/independent_test.py test fails with Assertion error. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04 s390x
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**



"
369,27853,0,"Tensorflow variables not casting to ref type [BUG][TF 2.0]. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0.0-alpha
- Python version: 3.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
**Describe the current behavior**
When passed to different Function, tf.Variable dtype doesn't cast to _ref type.
For example:

Output: 
**Describe the expected behavior** (according to tf r1.13)

Output: 

**Code to reproduce the issue**
Given Above

**Other info / logs**
For this reason, a lot Function Calls like  are failing. This Leads to Failing of Keras Backend Calls, like , even in Graph Mode.
Traceback created after Appending  to the issue https://github.com/tensorflow/tensorflow/issues/27739, shows that assign_sub is failing as Tensor Object have no assign_sub. Upon Further Investigation, this error was found here:
https://github.com/tensorflow/tensorflow/blob/0c464c70cef2369b6ef5c5e17dbd2cda2a6107fb/tensorflow/python/ops/state_ops.py#L159-L162"
1408,5862,0,"Feature request: easier access to all variables inside a scope. Right now, in order to access the variables inside a scope, AFAIK we have to do the following:

    variables = tf.get_collection(tf.GraphKeys.VARIABLES, scope)

It would be far easier if we could just write

    variables = scope.get_all_variables()

or maybe

    variables = tf.get_scope_variables(scope)

Is it a reasonable request?"
1053,17127,0,"Write to tensor array fails within while loop. 



### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
 Yes I am writing a while loop that reads and also updates the tensor array that's being passed to it.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.3
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
When I attempt to write to a tensor array within a while loop, it leads me to an error 
""Could not write to TensorArray index 0 because it has already been read.""

### Source code 




"
1013,206,0,"Generalize slicing and slice assignment ops (including gather and scatter). We should make our slicing and assignment ops more general to capture more of the functionality of numpy slicing, and add  sugar for all of it.  Specifically,
1. We should have a 2.5 dimensional set of ops, with dimensions (1) get vs. set, (2) slice type, and for the assignment ops (3) the update op.  Currently we have , , , , , , , .  We should also have , , .
2. Both slicing and slice assignment should support strides, with no performance cost if strides aren't used.
3. Ideally, the slice ops should support negative indexing a la Python.  Since the slice parameters are already CPU, this is implementable with near zero cost.   The unfortunate bit is that since we picked the wrong format for specifying ranges (start + length instead of start : end), negative indexing might be awkward.  Thus, it might be best left to a separate bug.
4. Support numpy-like boolean indexing.
5. Generalize  and  to take an array of input index tensors, efficiently broadcast them, and do multidimensional indexing similar to numpy.
6. Make  provide sugar for all of the above.  Ideally we'd have something idiomatically similar at least to , but this is problematic since the returned assignment op is important to have,  does not return a value, and the nice range sugar is available only inside indexing / assignment calls.

@ebrevdo: I'm assigning this to you for now since you might get to it first, but feel free to grab only the piece of it that you need for now.
"
368,23346,0,"Installation error in MacOs,Throw error about numpy. I ran ""import tensorflow as tf"" in pyCharm. and Throw the errors.

> Using TensorFlow backend.
RuntimeError: module compiled against API version 0xc but this version of numpy is 0xb
ImportError: numpy.core.multiarray failed to import
ImportError: numpy.core.umath failed to import
ImportError: numpy.core.umath failed to import
2018-10-29 17:42:25.858222: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr 

I using anaconda, my python version is 3.6, numpy version is 1.13.3. My System is MacOS"
847,31705,0," The arguments and returns of tf.keras.layers.GRUCell.call() make no sense at all. ## URL(s) with the issue: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1615 https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1718

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell

## Description of issue (what needs changing):
The  argument to tf.keras.layers.GRUCell.call() is indexed with , and the function returns  and , which is the same value?

### Clear description
Why does this occur? Its inconsistent with the pytorch torch.nn.GRUCell implementation (https://pytorch.org/docs/stable/nn.html#grucell). 

I noticed the  issue when I was converting a project from pytorch to tf.keras and the same code, with just the GRUCell swapped from pytorch to tf.keras, did not work. The error message was , and the solution was to replace my  with  for the states parameter. 

Furthermore, when I got my return types, they were a tuple rather than the output. Upon further inspection, the tuple CONTAINED THE SAME VALUE TWICE.

Is there any reason it does this? In the doc this is not explained AT ALL.

### Correct links
https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1615
https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L1718

### Parameters defined
 parameter is the confusing one in question.

### Returns defined
 makes no sense

### Raises listed and defined
Irrelevant

### Usage example
self.rnn = tf.keras.layers.GRUCell(num_units)
self.rnn(input, hidden)

where input, hidden = shape(N, num_units) doesn't work. Needs to be changed to:
self.rnn(input, [hidden]) to execute.

Furthermore, on the LHS of self.rnn, rather than just an x = self.rnn(...), I need to do x, _ = self.rnn(...)
Why?

### Submit a pull request?
I would gladly change this if someone would confirm this is an issue."
255,31709,1,"Tensorflow 2.0 is much slower than pytorch for large matrix assignment. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow 2.0 beta
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0/cuDNN 7
- GPU model and memory: GeForce RTX 2080 Ti

**Describe the current behavior**
In one of my research, I need to assign values to large sparse matrices. Since tensorflow does not support value assignment/update to large matrices , I have to use a lot of tf.stack()/tf.concat() functions. 

I compared the same function implemented in tensorflow 2.0 and pytorch 1.1.0, and the execution time for tensorflow was much slower than pytorch. The execution times are listed below:
pytorch: 0.0036 secs
tf 2.0    : 0.1734 secs

**Describe the expected behavior**
Is there a way to optimize the tensorflow codes to have comparable performance?

**Code to reproduce the issue**


"
295,6141,0,"Feature request: directional distributions (like von Mises-Fisher). Are there plans to implement directional statistical distributions? In particular, I'm looking to get a von Mises-Fisher distribution into .

I'm beginning work on a VMF distribution myself, but as a new TF user my progress is likely to be slow.

(Note that  implements a basic von Mises distribution.)

References:
[Wikipedia: Von Mises–Fisher distribution](https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution)
[Arxiv: Directional Statistics in Machine Learning: a Brief Review]( https://arxiv.org/pdf/1605.00316.pdf
)"
1321,19219,0,"unorderable types: str() < tuple() in /tensorflow/python/feature_column/feature_column.py. Google Tensorflow forum guy asked me to open this issue:

### System information
- **Have I written custom code **:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04 LTS
- **TensorFlow installed from (source or binary)**:
pip3 install tensorflow --upgrade
- **TensorFlow version (use command below)**:
TF 1.7.0.
- **Python version**: 
Python 3.4.3 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

run time error in python: unorderable types: str() < tuple() in /tensorflow/python/feature_column/feature_column.py

### Source code / logs
https://github.com/werowe/tripAdvisorNeuralNetworkTensorFlow/blob/master/tripadvisorNN.py
data:   https://raw.githubusercontent.com/werowe/tripAdvisorNeuralNetworkTensorFlow/master/tripAdvisorFL.csv


dictionary {'Nrhotelreviews': <tf.Tensor 'DecodeCSV:2' shape=() dtype=int32>, 'Casino': <tf.Tensor 'DecodeCSV:11' shape=() dtype=int32>, 'Travelertype': <tf.Tensor 'DecodeCSV:6' shape=() dtype=int32>, 'Helpfulvotes': <tf.Tensor 'DecodeCSV:3' shape=() dtype=int32>, 'Usercontinent': <tf.Tensor 'DecodeCSV:16' shape=() dtype=int32>, 'Tenniscourt': <tf.Tensor 'DecodeCSV:9' shape=() dtype=int32>, 'Usercountry': <tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, 'Reviewmonth': <tf.Tensor 'DecodeCSV:18' shape=() dtype=int32>, 'Hotelname': <tf.Tensor 'DecodeCSV:13' shape=() dtype=int32>, 'Nrrooms': <tf.Tensor 'DecodeCSV:15' shape=() dtype=int32>, 'Freeinternet': <tf.Tensor 'DecodeCSV:12' shape=() dtype=int32>, 'Nrreviews': <tf.Tensor 'DecodeCSV:1' shape=() dtype=int32>, 'Gym': <tf.Tensor 'DecodeCSV:8' shape=() dtype=int32>, 'Memberyears': <tf.Tensor 'DecodeCSV:17' shape=() dtype=int32>, 'Periodofstay': <tf.Tensor 'DecodeCSV:5' shape=() dtype=int32>, 'Pool': <tf.Tensor 'DecodeCSV:7' shape=() dtype=int32>, 'Reviewweekday': <tf.Tensor 'DecodeCSV:19' shape=() dtype=int32>, 'Spa': <tf.Tensor 'DecodeCSV:10' shape=() dtype=int32>, 'Hotelstars': <tf.Tensor 'DecodeCSV:14' shape=() dtype=int32>}  label =  Tensor(""DecodeCSV:4"", shape=(), dtype=int32)
INFO:tensorflow:Calling model_fn.
Traceback (most recent call last):
  File ""<stdin>"", line 3, in <module>
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 355, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 824, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 805, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 354, in _model_fn
    config=config)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 184, in _dnn_model_fn
    logits = logit_fn(features=features, mode=mode)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 92, in dnn_logit_fn
    features=features, feature_columns=feature_columns)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/feature_column/feature_column.py"", line 274, in input_layer
    trainable, cols_to_vars)
  File ""/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/feature_column/feature_column.py"", line 192, in _internal_input_layer
    for column in sorted(feature_columns, key=lambda x: x.name):
TypeError: unorderable types: str() < tuple()
"
261,10723,1,"Poor Scalability of TensorFlow MultiGPU Training on a Single Machine [Performance Bug]. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 3.16.7
- **TensorFlow installed from (source or binary)**: source, hash: b1e174e
- **TensorFlow version (use command below)**: v1.1.0-rc2-119-gb1e174e 1.1.0-rc2
- **Bazel version (if compiling from source)**:  0.4.5-jdk7
- **CUDA/cuDNN version**: CUDA 8.0, cudnn-v5.1
- **GPU model and memory**: Titan X with 12 GiB
- **Exact command to reproduce**: ./tf_multiGPU.py

- **tensorflow/benchmark version**: source, hash: 9165a70

### Describe the problem
Recently, we are testing the TensorFlow scalability on multiGPU machines. We use the official scripts provided in the [benchmarks](https://www.tensorflow.org/performance/benchmarks) using codes from GitHub repository tensorflow/benchmark. We execute the script according to the official website to test the scalability of TensorFlow on the machine equipped with 8 Titan X GPUs. We test the model VGG16 with batch size equaling to 64.

The results are shown in the following table:

| Num of GPUs | Throughput (images/sec) |
|-------------|-------------------------|
| 1           | 83.13                   |
| 2           | 155.06                  |
| 3           | 211.8                   |
| 4           | 278.51                  |
| 5           | 265.53                  |
| 6           | 268.19                  |
| 7           | 272.8                   |
| 8           | 302.27                  |

We are surprised to find that the total throughput of 5 GPUs is smaller than 4 GPUs, which means TensorFlow incurs significant overheads when the number of GPU is larger than 4. Because I don't know whether this performance issue belongs to the tensorflow/tensorflow repository or tensorflow/benchmark repository, I submit this issue here looking forward to the official response. The script for reproducing this issue can be found in Source code/logs section with the results.

The scalability is strongly related to the topology of GPU interconnection. In our machine, we have a tree topology for GPUs. The topology details can be found at the [nv-topo-matrix.txt](https://gist.github.com/583d689fd16ee24f1924b83ca3dea5b9.git)

We believe this is a performance bug since if more GPUs can not achieve higher throughput, at least they should obtain similar throughput. 

### Source code / logs
Source code: [tf_multiGPU.py](https://gist.github.com/b4024e1d8bfbbdf0cf917798d677aaae.git)
The log after executing the script above: https://drive.google.com/file/d/0Bw3_V-EwBVToYXVqZDkzQkN6c3M/view?usp=sharing"
630,27955,0,"TF Nightly 20190418 Crashes on Estimator Import. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **tf-nightly-20190418**
- Python version: **3.6**

**Describe the current behavior**
Nightly tensorflow crashes on importing estimator-nightly

**Describe the expected behavior**
Able to import

**Code to reproduce the issue**

**Other info / logs**
https://colab.research.google.com/drive/1mvVTcAWHwJY1Kko49Ae93iQum6lT9W8D

Crash:
"
1427,8749,0,s ufw. Sorry about this. No clue how this got posted. :(
876,25594,0,"Test sharding appears to be broken. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.8.0-17025-g3e713f9 1.12.0
- Python version: 3.5
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

I'm currently testing the AVX512 builds.  There are some tests that are known to fail on these builds.  Patches are pending for these issues (#21676) but they are not yet merged.  The weird thing is that the tests have been passing on my machine for the last two weeks even though they should fail.  I did some digging and it turned out that the tests only pass if sharding is enabled.  They fail as expected if sharding is disabled.  So if, on an AVX512 build,  I do 

bazel test --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test

the test passes

and if I do

 bazel test --test_sharding_strategy=disabled --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test

the test fails, as it should.  It turns out that the test is passing when sharding is enabled as not all of the sub-tests are being run.  This issue seems to have been caused by commit #87cc788 which changed the unit test framework.  As far as I can tell, filtering for sharding is now happening twice, once in googletest.py and once in absltest.

The issue can be reproduced on my machine, a 10 core SKX core i9 as follows

Here's how to reproduce the issue



Then check shard 18.  I see


Note 0 tests run.

**Describe the expected behavior**
And what I'm expecting to see is 



Here I've chosen a shard with a test that passes on my machine but you can hopefully see the difference.

**Code to reproduce the issue**

On a 10 core machine

$ bazel test --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test
$ cat /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/embedding_ops_test/shard_18_of_20/test.log

**Other info / logs**

I have a patch that I will post shortly.  It may not be the correct fix, but it should point to the part of the code where I believe the problem to be."
159,9527,1,"tf.while_loop much slower than static graph?. I'm running on TF 1.1, and I've used  +  to implement dynamic unrolling of a type of recurrence that I previously unrolled statically through python code. The difference in speed is very dramatic, with forward inference being about 200x slower when dynamically unrolled, and backprop about 2x slower. Is this expected? Are there any tricks for optimization that I'm missing? This is on CPU. Performance gap on GPU is even larger."
266,27111,1,"CPU version of Tensorflow training is very slow with PlotLossesCallback. I have the CPU version of tensorflow and have been training a 3 layered Simple Recurrent keras model. However, the model takes takes too long to complete the an epoch (in excess of 30 minutes). 
This started when I added the  so that I can get live progress plots of the progress on the losses and accuracies for each epoch. I'm not sure how to remediate the issue. Would installing cuDNN be helpful to fixing the issue or does it not work with the CPU version of tensorflow?
Or is there another when this can be fixed?"
1063,3835,0,"Some types of fetches in tf.Session.run are not supported and it behaves strange in container. There're two issues I have found when giving different type of fetches in .

The [doc](https://www.tensorflow.org/versions/master/api_docs/python/client.html) says that we can pass the nested list or tuple to  but it doesn't work.



It's easy to test with this script.



And I found it works if I pass the  or . But what's strange is that the  doesn't work in docker container which is also easy to reproduce by creating the tensorflow container to run the same script. You can find my test code and error log below.

![screen shot 2016-08-16 at 11 52 31](https://cloud.githubusercontent.com/assets/2715000/17687714/3e4a83a8-63aa-11e6-98be-e20a7099f77b.png)
### Environment info

Operating System: Ubuntu 16.04

If installed from binary pip package, provide:
TensorFlow 0.8
"
116,24304,1,"Tensorflow Lite: ResNet example model gave VERY poor result during validation with ImageNet. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): The version I download from master branch
- Python version: 3.6
- Bazel version (if compiling from source): 0.15
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I am studying tensorflow lite. I downloaded the ResNet frozen graph ResNet_V2_101 from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models.md#image-classification-float-models .

And then I followed https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb to convert this frozen graph to both Lite model and quantized lite model.

Then I followed https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/accuracy/ilsvrc to evalute its accuracy using ImageNet Validation Dataset (50000 images) on my desktop.

However, when I run 

and checked the output accuracy_output.txt. The accuracy is very poor. I can capture some results among the 50000 images. 


After running 50000 validation images, the top-1 to top-10 results are

However, according to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb, the top-1 accuracy can reach 76.8 but my attempt even cannot reach 1% in the end. Why this happens? Where I did wrong? Thanks!"
865,18894,0,"TfLite Image classification score is not consistent it keeps increasing for same image untill it reaches to some saturation(actual score). With same instance of 'interpreter' score is getting increased for same image until it reaches at some saturation.


Create Instance for ImageClassifier and use the same instance to classify Frame and run inference for the same image.


Classifies a frame for the same image. Same image can be picked up from the Sd card.



At the first time when application gets launched score the image classification is 0.06 and then again if we called classifyImage() on some event click score gets increased to 0.13 and with same process it keeps increasing until it reached to 0.86(saturation).

I am not sure why its happening but it happened for both type of TfLite models inceptionV3 and MobileNet."
457,11327,0,"Backprop through conv2d with large tensors fails. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.1.0
- **Python version**: 
3.5.2
- **Bazel version (if compiling from source)**:
0.4.5
- **CUDA/cuDNN version**:
CUDA Version 8.0.61
- **GPU model and memory**:
GeForce GTX 1080, 8GB
- **Exact command to reproduce**:


### Describe the problem
Backpropagation through  for large tensors produces an error for me (see log below). This is true for the code above. Lowering the batch size in  from 720 to 100 eliminates the error. The issue has been confirmed by another user at [stackoverflow](https://stackoverflow.com/questions/44901742/tensorflow-error-while-backpropagating-through-conv2d).

### Source code / logs


Please find the full log attached.

"
201,32128,1,"Sessions that are closed and reset and all inputs and outputs are out of scope, do not release GPU memory.. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS 10.13
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source 1.12.0
- TensorFlow version (use command below):
- Python version:NA
- Bazel version (if compiling from source):0.19.2
- GCC/Compiler version (if compiling from source):Apple LLVM version 9.1.0 (clang-902.0.39.2)
- CUDA/cuDNN version:10.0.130
- GPU model and memory:GTX 1060

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
Memory from tensorflow sessions is not freed when a session is closed and no variables are in scope, memory is only freed on program exit
**Describe the expected behavior**
Memory should be freed from the GPU on calling close and reset in C++, or some other way to release GPU resources without forking a process.
**Code to reproduce the issue**



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is the output from the program above:


Note the GPU memory available keeps going down even though the session has been closed and reset and all variables are out of scope."
842,18218,0,"Feature Request: Named global_step. I have been developing GAN models for some time now and I find it useful to track the individual stages (pretraining of generator/discriminator, adversarial training) in different global_steps so I can save and restore my actual progress without doing a lot of calculations based on configured training steps and so on.

Therefore, I have basically rebuild the functionality that [tf.train.get_or_create_global_step](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/get_or_create_global_step) provides but with support for named global_steps.

Looks something like .

Am I the only one finding this particularly helpful?"
688,22251,0,error when adding library 
594,23017,0,"tf.estimator.train's incompatibility with distributed training on Cloud ML Engine is not well-documented. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 28
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.5.0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

The page https://cloud.google.com/ml-engine/docs/tensorflow/distributed-training-details#tensorflow-config notes that ""The tf.estimator.train method doesn't work with distributed training on Cloud ML Engine. Please use train_and_evaluate instead."". This is not documented on the Estimator page (https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) or anywhere else I've seen. I believe it would be helpful to document it more prominently, as I can't be the only one who didn't read the ""Using TF_CONFIG for Distributed Training Details"" page and wasted time debugging why a model wouldn't work when distributed.

If possible, it would be even more helpful to make tf.estimator.train raise an exception when run in a distributed ML Engine context, or log a warning. It's unreasonable to expect the user to figure this out themselves as from an API perspective there's no reason to expect  would work where  fails (one might reasonably assume  calls )."
102,20699,1,"Increasing memory use of Estimator with Dataset. ------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Binary (pip install)
- **TensorFlow version (use command below)**: Tested on 1.8.0 and 1.10.0-dev20180620
- **Python version**: 3.6
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX1050
- **Exact command to reproduce**: See below for minimal test case

### Describe the problem
I'm experiencing a memory leak that leaves some allocated memory behind after each Estimator training/evaluation run. I noticed this when my ML Engine jobs started failing after 1.5 hours of training with an OOM error on big CSV files, using the standard Dataset input pipeline with some preprocessing, batching, prefetching etc. The memory graph shows slow but steady increase of memory use until total RAM is filled. I observe the same behavior on my local machine.

I've distilled it down to a minimal test case with use of Estimator and Dataset input and tested on my local machine, where I also get slowly increasing memory use over time. It seems most pronounced when using a big shuffle buffer, but after all this time staring at it I'm not so sure of anything any more.

Test script (first ):


Output of  (so it runs on CPU, for GPU use  option) and plotting with  shows memory usage slowly but surely creeping upward:
![tf-memoryuse](https://user-images.githubusercontent.com/5527529/42563920-b7986b62-84ff-11e8-9fa4-304f319927a0.png)

I would expect memory usage after every train/evaluate loop to return to the initial level.

Am I using the Dataset in the wrong way? Should I convert to an Iterator first? Is slightly increasing memory use normal and should I restart my train script every once in a while?
"
922,23021,0,"I am getting the same prediction value for all of my test data. I tried with many combinations of hyper parameters, optimizers and cost functions but I'm getting R2 = 0.33 at max. Please help me finding a solution.
[Python code.txt](https://github.com/tensorflow/tensorflow/files/2482551/Python.code.txt)
[Gholamnejad_dataset1.xlsx](https://github.com/tensorflow/tensorflow/files/2482564/Gholamnejad_dataset1.xlsx)

"
1201,23625,0,"question about quantization. Using quantization, with the same parameters, the difference of the training accuracy is about 5-10% each time？

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:pip
- **TensorFlow version (use command below)**:1.4
- **Python version**:3.4.3
- **Bazel version (if compiling from source)**:1.9
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
114,27888,1,"tensorflow.multinomial performance . <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
tensorflow.multinomial and pytorch.multinomial,the performance gap is huge.In particular I think tensorflow.multinomial is very problematic

I wrote the following code to compare the two functions,I set the first number to 10 and the rest to 0.0001, followed by softmax as the probability of selection.

**Describe the expected behavior**
I think tensorflow's performance is very unusual


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
[https://stackoverflow.com/questions/55705625/how-to-get-the-same-effect-and-what-makes-a-differencei-between-tensorflow-multi](url)


import torch

import numpy as np

import  tensorflow as tf

single_data = np.array([10,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001])


single_data = np.expand_dims(single_data, axis=0)

data =np.repeat(single_data, 50, axis=0)

data = tf.nn.softmax(data,axis=-1)

sampled_word = tf.multinomial(data,1)

sample = tf.reshape(sampled_word,[-1])

with tf.Session() as sess:

    a_data=sess.run(data)

    print(""prob:"",a_data)

    print(""tensorflow.multinomial"",sess.run(sample))

    a_data = torch.from_numpy(a_data)

    idx = torch.multinomial(a_data, num_samples=1)

    print(""tensorflow.multinomial"",np.reshape(idx.data.numpy(),[-1]))
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
but I get the result 
prob: [[9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 ...
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]
 [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05
  4.51278541e-05 4.51278541e-05]]
tensorflow.multinomial [121 129  35 104   4 133  60  92 104 129   4  49  35  99 109 111  62  87
  23   5 109  63 103  61  78  43 101  85   2 127   0  36  53   0  74  44
  64  55  51  59 108   0 112  32  36  24  68 135  72  22]
tensorflow.multinomial [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0]"
927,11162,0,"Rank mismatch error when using contrib.seq2seq.AttentionWrapper in a dynamic_rnn with sequence_length. ### System information
Linux Ubuntu 16.04
TensorFlow installed from pip binary
TensorFlow version 1.2.0 (v1.2.0-rc2-21-g12f033d)
Python 3.6.1 Anaconda 4.4.0
CUDA 8.0 / cuDNN 5.1
TITAN X (Pascal) 11.9GB

### Describe the problem

Error:  when using  in a  with  provided.

Code to reproduce:



### Traceback

"
73,6298,1,"Long sentence take too much memory. I use one layer LSTM to train my data.
I use Dynamic_rnn, rnn_size=128, num_layers=1, seq_max_length=2500, batch_size=10, embedding_size=128, softmax_size=1600.


My code like this:

> x_vec = tf.nn.embedding_lookup(embedding_matrix_variable, self.x)
> lstm_fw_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = hidden_unit)
> lstm_fw_cell = rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob, input_keep_prob=self.dropout_keep_prob)
> outputs, _ = rnn.dynamic_rnn(lstm_fw_cell, x, dtype=tf.float32, sequence_length=real_length, swap_memory=False)
> 


I specify the GPU like this:
<img width=""281"" alt=""screenshot"" src=""https://cloud.githubusercontent.com/assets/4569055/21170698/3bbf5de8-c20a-11e6-9b66-647830bdd371.png"">



Command ""nvidia-smi"" shows as:
<img width=""511"" alt=""screenshot"" src=""https://cloud.githubusercontent.com/assets/4569055/21170316/7738d3b6-c207-11e6-860a-cdc5ea166f40.png"">



After lunching the program, it always shows :

> ""I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator:..............."":

details show as below:

> Step: 0       (epoch: 0.00000)       time:28.85937s
> Minibatch loss:          7.43977     Minibatch accuracy:  0.00000
> Test loss:         nan     Test accuracy: 0.00000
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=9012 evicted_count=9000 eviction_rate=0.998668 and unsatisfied allocation rate=0
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 9559 get requests, put_count=15572 evicted_count=6000 eviction_rate=0.385307 and unsatisfied allocation rate=0
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 25809 get requests, put_count=41821 evicted_count=16000 eviction_rate=0.382583 and unsatisfied allocation rate=3.87462e-05
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 42059 get requests, put_count=68071 evicted_count=26000 eviction_rate=0.381954 and unsatisfied allocation rate=2.37761e-05
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 58309 get requests, put_count=94321 evicted_count=36000 eviction_rate=0.381675 and unsatisfied allocation rate=1.715e-05
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=8014 evicted_count=8000 eviction_rate=0.998253 and unsatisfied allocation rate=0
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7817 get requests, put_count=25828 evicted_count=18000 eviction_rate=0.696918 and unsatisfied allocation rate=0.000383779
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 24067 get requests, put_count=52078 evicted_count=28000 eviction_rate=0.537655 and unsatisfied allocation rate=0.000124652
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 40317 get requests, put_count=78328 evicted_count=38000 eviction_rate=0.485139 and unsatisfied allocation rate=7.44103e-05
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 56567 get requests, put_count=104578 evicted_count=48000 eviction_rate=0.458988 and unsatisfied allocation rate=5.30345e-05
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=7016 evicted_count=7000 eviction_rate=0.997719 and unsatisfied allocation rate=0
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 6070 get requests, put_count=23084 evicted_count=17000 eviction_rate=0.736441 and unsatisfied allocation rate=0.000329489
> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 22320 get requests, put_count=49333 evicted_count=27000 eviction_rate=0.547301 and unsatisfied allocation rate=0.000134409
> 


"
988,17411,0,"Illegal instruction (core dumped) after running import tensorflow. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 
1.6.0-cp27-cp27mu-manylinux1_x86_64 (can only guess since  gives me an error already)
- **Python version**: Python 2.7.12
- **Exact command to reproduce**: 

I created a fresh virtual environment: 
And installed tensorflow: 
 gives me 

Please help me understand what's going on and how I can fix it. Thank you.

CPU information:

*EDIT*
Stacktrace obtained with gdb:



*EDIT 2*
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

After downgrading to an older version of tensorflow the error goes away. I've been advised that my CPU (see information above) might not work with some improvements in the new API. If this is the case, I suppose there's no solution for my problem. Therefore, I will close this thread. Feel free to correct me though. Thank you for your support"
976,26242,0,"Build failure on s390x on hwloc . **System information**

    * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04

    * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA

    * TensorFlow installed from (source or binary): source

    * TensorFlow version: master as on today

    * Python version:  2.7.x

    * Bazel version (if compiling from source): 0.19.0 

    * GCC/Compiler version (if compiling from source): gcc 7.3.0, glibc 2.28

    * CUDA/cuDNN version: NA

    * GPU model and memory: NA


**Describe the problem**
Tensorflow build fails with an error:

"
946,17842,0,"Fix deprecated api call in _NonAtrousConvolution. ### System information
I am using tf version  on OSX in standard non-eager mode.

### Describe the problem
Lines  in class  in  set  as data format when  is 1.  



But this causes a warning later on at line  when  is called from inside the  method at line 

NHWCNWC"
1037,25527,0,"ROCm build doesn't detect GPU. 
**System information**
- OS Platform and Distribution  Ubuntu 18.10
- TensorFlow installed from source
- TensorFlow version tensorflow-1.13.0rc0-cp36-cp36m-linux_x86_64.whl
- Python version: 3.6
- Installed using pip with wheel built from source:
- Bazel version 0.21:
- GCC/Compiler version 8.2:
- CUDA/cuDNN version N/A:
- GPU model and memory: AMD Vega RX 64 



**Describe the problem**

Selecting ROCm in ""./configure"" doesn't find GPU in ""tf.session()""
"
948,6027,0,"tracing batch normalization: apparently unnecessary gpu-cpu-gpu transfer for gather op.. I am running:
CUDA 8.0. CUDNN 5.1. tensorflow 0.11. Ubuntu 12.04.

I was experiencing low GPU utilization. As per #1824 I did a trace and found many instances of the following interaction, as shown by the screenshot in chrome:

The op I have highlighted (with arrows going in and out in the screenshot) is a  op. I provide a link to the context in which it is called.
https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/python/ops/nn_impl.py#L472

The call stack (at graph construction) time is:
, , 

If I understand correctly, I am seeing that the GPU gets blocked as a result of shuffling a trivial  op unnecessarily off to the CPU only to wait forever for it to come back. 

![screenshot from 2016-12-01 11 45 17](https://cloud.githubusercontent.com/assets/4404828/20810886/67cbd92e-b7c0-11e6-991d-75ab54a1c699.png)

"
802,189,0,"No model directory. I just installed tensorflow in a Virtualenv on Mac OS. There is no models directory. How do I get the models installed?
"
1039,8196,0,"tf.py_func treating result different on Windows and Ubuntu in Tensorflow 1.0. ### Description

I'm using tf.py_func in my data fetching pipeline. The applied function basically calculates an int from an int. When I run the tensorflow code, it works on my Windows 10 development laptop, but fails on the Ubuntu server with an error that the python function would return an int64 instead of the expected int32. It is the same behavior for both, the CPU and the GPU backend. 

I would expect tensorflow to behave the same across different platforms. Am I doing something wrong or is this a bug?

### Environment info
Operating Systems:

* Windows 10: Python 3.5.2; Tensorflow 1.0.1 (I tried both, CPU and GPU) installed via pip; CUDA 8.0, cudnn 5.1

   https://gist.github.com/andreas-eberle/76dfaeb8467dd3b520aa8390bd2b5d33

* Ubuntu 14.04: Python 3.4.3 (I cannot update it because the server is managed); Tensorflow 1.0.0 (there seems to be no 1.0.1 for Ubuntu) (I tried both, CPU and GPU) installed via pip (https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp34-cp34m-linux_x86_64.whl); CUDA 8.0, cudnn 5.1

   https://gist.github.com/andreas-eberle/fbba8fdb73bff433d89ece9a1946f269

### Minimal reproducible example


#### Output on Windows (using tensorflow-cpu)


#### Output on Ubuntu server (using tensorflow-cpu)

"
853,26778,0,"[TF2.0] Possibly wrong path to decode_predictions in keras. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version: 3.6.5_1/2.7.15
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

Currently, Mobilenet_V2 Keras in  references the function  from  [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v2.py)
 
**Describe the expected behavior**

 seems to be from  [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py) instead. 

**Code to reproduce the issue**

Provide a reproducible test case that is the bare minimum necessary to generate the problem.
NA

**Other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
NA"
60,20602,1,"Different accuracy for quantized model when inferring from one image vs batch. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: gtx 1080ti
- **Exact command to reproduce**:



TF Prediction acc:0.79
Inference time(ms): 56.0006372856




 TF Prediction acc:0.76
Inference time(ms): 406.530380249



You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Different accuracy for quantized model when inferring from one image vs batch

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
93,6346,1,"Windows 10 -> tensorflow-gpu -> TCC mode = Hangs forever. Anybody getting this to work in windows while in TCC CUDA mode? Seems to work fine in WDDM driver mode but WDDM cripples performance. To replicate issue, run on machine with Tesla class GPU in TCC mode, then try to create a session (sess = tf.Session()). The python process consumes a CPU core indefinitely and the session is never created."
421,10061,0,"tensorflow is not a supported wheel on this p latform. I have python 2.7.13 (Anaconda 4). When i try install **tensorflow** its raise error.


I try use gpu version - same result."
966,27886,0,"[TF2.0] TypeError: Cannot convert provided value to EagerTensor when applying constraint on variable. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0 alpha
- Python version: 3.7.3
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: V100, 16GB

**Describe the current behavior**
Cannot convert provided value to EagerTensor when applying keras constraint on variable in TF2.0 eager mode.

**Describe the expected behavior**
Variable should be converted to EagerTensor, operation should return constrained variable.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


**Other info / logs**
> TypeError                                 Traceback (most recent call last)
> <ipython-input-35-760885333ab7> in <module>
> ----> 1 tf.keras.constraints.UnitNorm(axis=1)(v)
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/constraints.py in __call__(self, w)
>     110         K.epsilon() + K.sqrt(
>     111             math_ops.reduce_sum(
> --> 112                 math_ops.square(w), axis=self.axis, keepdims=True)))
>     113 
>     114   def get_config(self):
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in sqrt(x)
>    1878       A tensor.
>    1879   """"""
> -> 1880   zero = _to_tensor(0., x.dtype.base_dtype)
>    1881   inf = _to_tensor(np.inf, x.dtype.base_dtype)
>    1882   x = clip_ops.clip_by_value(x, zero, inf)
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in _to_tensor(x, dtype)
>     612       A tensor.
>     613   """"""
> --> 614   return ops.convert_to_tensor(x, dtype=dtype)
>     615 
>     616 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
>    1048   preferred_dtype = deprecation.deprecated_argument_lookup(
>    1049       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
> -> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
>    1051 
>    1052 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
>    1106       name=name,
>    1107       preferred_dtype=dtype_hint,
> -> 1108       as_ref=False)
>    1109 
>    1110 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
>    1184 
>    1185     if ret is None:
> -> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
>    1187 
>    1188     if ret is NotImplemented:
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
>     302                                          as_ref=False):
>     303   _ = as_ref
> --> 304   return constant(v, dtype=dtype, name=name)
>     305 
>     306 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
>     243   """"""
>     244   return _constant_impl(value, dtype, shape, name, verify_shape=False,
> --> 245                         allow_broadcast=True)
>     246 
>     247 
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
>     251   ctx = context.context()
>     252   if ctx.executing_eagerly():
> --> 253     t = convert_to_eager_tensor(value, ctx, dtype)
>     254     if shape is None:
>     255       return t
> 
> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
>     108       return ops.EagerTensor(
>     109           value, handle, device, dtype, tensor)
> --> 110     t = ops.EagerTensor(value, handle, device, dtype)
>     111     scalar_cache[cache_key] = t
>     112     return t
> 
> TypeError: Cannot convert provided value to EagerTensor. Provided value: 0.0 Requested dtype: int32"
1082,24505,0,"Is there any plan to integrate NCCL 2.0 or horovod into CollectiveAllReduceStrategy?. **System information**
- TensorFlow version (you are using): master
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
 brings significant performance to distribution execution especially when using  to do . Is there any plan to do integration  or simply support NCCL 2.0 in ? I am willing to contribute for it!

**Will this change the current api? How?**
No，it will be another  for candidate.

**Who will benefit with this feature?**
Anyone who wants to use  to improve performance.


**Any Other info.**
Currently I am working for distribution optimization and I hope to have a discussion. Thanks.

@yuefengz 
"
1280,25257,0,"TF 2.0 Conversion Script does not have a replacement for tf.contrib.eager.defun(train_step).. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Submitting on behalf of @victordibia. Thanks, Victor! 😊

**Describe the expected behavior**

I used the TF 2.0 conversion script to convert my DCGAN sample from tf1.x to tf2.0.

It suggested use of tf.compat.v1.losses.sigmoid_cross_entropy  (previously in contrib), but am unclear if this has same behaviour as tf.nn.softmax_cross_entropy_with_logits. The script did not recommend a replacement for tf.contrib.eager.defun(train_step).

**Code to reproduce the issue**
https://github.com/victordibia/tf2/blob/master/dcgan/dcgan.py

**Other info / logs**
Friction log: https://docs.google.com/document/d/1Sv6INZzqLUUChy46jeawX1FdO1rG5aAjjGde7wOSBb8/edit?usp=sharing"
619,25830,0,"CUDNN_STATUS_INTERNAL_ERROR while using tensorflow.keras.applications.VGG16. I am trying to use a pretrained network from tensorflow.keras.applications with 1.13rc2 and it crashes with a cudnn error.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from source: 1.13.0-rc2
- TensorFlow version (use command below):
- Python version: 3.6.5
- Bazel version: 0.22
- GCC/Compiler version: 7.3
- CUDA/cuDNN version: 10.0 / 7.4.2
- GPU model and memory: GeForce RTX 2070 8GB

**Code to reproduce the issue**

Minimal working example:


**Other info / logs**

Using TensorFlow backend.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-02-18 09:11:29.560595: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399705000 Hz
2019-02-18 09:11:29.561392: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2116980 executing computations on platform Host. Devices:
2019-02-18 09:11:29.561431: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-02-18 09:11:30.251516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-18 09:11:30.252153: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2ddff10 executing computations on platform CUDA. Devices:
2019-02-18 09:11:30.252175: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-02-18 09:11:30.252593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.725
pciBusID: 0000:26:00.0
totalMemory: 7.76GiB freeMemory: 7.64GiB
2019-02-18 09:11:30.252614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-02-18 09:11:30.253693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-18 09:11:30.253709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-02-18 09:11:30.253716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-02-18 09:11:30.254007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7436 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:26:00.0, compute capability: 7.5)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Epoch 1/50
2019-02-18 09:11:34.331098: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-02-18 09:11:35.167778: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-02-18 09:11:35.167818: W ./tensorflow/stream_executor/stream.h:2099] attempting to perform DNN operation using StreamExecutor without DNN support
Traceback (most recent call last):
  File ""test.py"", line 9, in <module>
    model.fit(x=X_train, y=y_train, epochs=50, batch_size=16)
  File ""/home/mate/dev/backend/env/backend/lib/python3.6/site-packages/keras/engine/training.py"", line 1039, in fit
    validation_steps=validation_steps)
  File ""/home/mate/dev/backend/env/backend/lib/python3.6/site-packages/keras/engine/training_arrays.py"", line 199, in fit_loop
    outs = f(ins_batch)
  File ""/home/mate/dev/backend/env/backend/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2715, in __call__
    return self._call(inputs)
  File ""/home/mate/dev/backend/env/backend/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2675, in _call
    fetched = self._callable_fn(*array_vals)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape([16,3,224,224]) filter shape([3,3,3,64])
	 [[{{node block1_conv1/convolution}}]]
	 [[{{node loss/mul}}]]
"
1172,29852,0,"[TF 2.0 API Docs] Documentation describes non existing symbol. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/debugging/check_numerics

This link gives a description of a symbol in a module which is supposed to be at https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/python/ops

But the module doesn't seem to exist"
1095,15049,0,"No OpKernel was registered to support Op 'OneHot' with these attrs. Tensorflow1.4. I have trained and saved a CNN model in python Tensorflow 1.3. 
I can successfully load and run the graph previously saved from my python model in Tensorflow 1.4 using CPU and c++ with no problem; but when I tried to load the same graph using Tensorflow 1.4 using GPU c++ and I get the following error:



My system:
Windows 10
Cuda 8.0
Cudnn 6
cmake cmake-3.9.4-win64-x64
Python 3.5.2
VS2015
@cuevas1208"
24,32810,1,"Send/Recv of collective_ops hangs in a distributed environment . **System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux CentOS 7.6.1810
- Mobile device if the issue happens on mobile device: No
- TensorFlow installed from: Binary
- TensorFlow version: v1.13.1-0-g6612da8951
- Python version: 3.6.8
- Bazel version: None
- GCC/Compiler version: None
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

The monitored session hangs in there fetching the send/recv tensors of . The same code works well for fetching  tensors.

**Describe the expected behavior**

The send/recv tensors  gives proper answer on all workers.

**Code to reproduce the issue**


**Other info / logs**

There is a related issue #31913, which can be solved by specifying the  in [tf.ConfigProto](https://www.tensorflow.org/api_docs/python/tf/ConfigProto/Experimental#collective_group_leader).
An info about  and a warning about are raised from the . Nevertheless, the script works fine for the commented . "
810,17211,0,"Getting Redeclaration Error of ""error: redeclaration of 'P_ALL'    P_ALL,  /* Wait for any child.  */"" while cross compiling for raspberry pi 3. # I'm trying to cross compile TensorFlow for Raspberry pi 3 and every time m getting the error like ""error: redeclaration of 'P_ALL'
   P_ALL,  /* Wait for any child.  */""

# I Followed these steps,
1.Installed Bazel
2.Cloned Tensorflow and checkout r1.5
3.Running the ""./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh""

Error:-/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/grpc/BUILD:431:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command 
  (cd /home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/baladev/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python2.7 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/external/grpc/_objs/gpr_base/external/grpc/src/core/lib/support/subprocess_posix.pic.d '-frandom-seed=bazel-out/armeabi-opt/bin/external/grpc/_objs/gpr_base/external/grpc/src/core/lib/support/subprocess_posix.pic.o' -fPIC '-DGRPC_ARES=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote external/grpc -iquote bazel-out/armeabi-opt/genfiles/external/grpc -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote external/com_google_absl -iquote bazel-out/armeabi-opt/genfiles/external/com_google_absl -isystem external/grpc/include -isystem bazel-out/armeabi-opt/genfiles/external/grpc/include -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/grpc/src/core/lib/support/subprocess_posix.cc -o bazel-out/armeabi-opt/bin/external/grpc/_objs/gpr_base/external/grpc/src/core/lib/support/subprocess_posix.pic.o)
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
In file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:
/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:101:3: error: redeclaration of 'P_ALL'
   P_ALL,  /* Wait for any child.  */
   ^
In file included from /usr/include/stdlib.h:41:0,
                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:
/usr/include/bits/waitflags.h:52:3: note: previous declaration 'idtype_t P_ALL'
   P_ALL,  /* Wait for any child.  */
   ^
In file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:
/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:102:3: error: redeclaration of 'P_PID'
   P_PID,  /* Wait for specified process.  */
   ^
In file included from /usr/include/stdlib.h:41:0,
                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:
/usr/include/bits/waitflags.h:53:3: note: previous declaration 'idtype_t P_PID'
   P_PID,  /* Wait for specified process.  */
   ^
In file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:
/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:103:3: error: redeclaration of 'P_PGID'
   P_PGID  /* Wait for members of process group.  */
   ^
In file included from /usr/include/stdlib.h:41:0,
                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:
/usr/include/bits/waitflags.h:54:3: note: previous declaration 'idtype_t P_PGID'
   P_PGID  /* Wait for members of process group.  */
   ^
In file included from external/grpc/src/core/lib/support/subprocess_posix.cc:33:0:
/home/baladev/.cache/bazel/_bazel_baladev/da0e175f87e998fe3d550279550cec2c/external/arm_compiler/bin/../arm-linux-gnueabihf/sysroot/usr/include/sys/wait.h:104:3: error: conflicting declaration 'typedef enum idtype_t idtype_t'
 } idtype_t;
   ^
In file included from /usr/include/stdlib.h:41:0,
                 from external/grpc/src/core/lib/support/subprocess_posix.cc:30:
/usr/include/bits/waitflags.h:55:3: note: previous declaration as 'typedef enum idtype_t idtype_t'
 } idtype_t;
   ^
INFO: Elapsed time: 984.156s, Critical Path: 40.58s
FAILED: Build did NOT complete successfully

_NOTE:-I'm Cross Compiling for Raspberry PI 3 In ubuntu 16.04LTS_

M I missing something??If yes kindly let me know.
Thanks in advance!!"
577,26839,0,"Numpy like slicing on Tensors. I was wondering if it is possible to implement Numpy like slicing annd updating a[1:10,2:20....]  in Tensorflow. It would make life much easier. Right now it code just gets bigger and uglier and bug prone.

"
1198,14392,0,"The weights argument in Keras's Embedding does not work. I am using Tensorflow 1.4.0.

According to this [blog post](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html), we can use the weights argument in the call to Embedding to specify some matrix that represents a pre-trained word embeddings (see the section titled Preparing the Embedding Layer).

However, this code does not work:



 should print [[3 4]] but instead random numbers are printed.

The solution is to define  as follows:

"
900,14558,0,"Bug: Inclusion missing in TF 1.4 BUILD file for mpi component. 
### System information
- **I'm compiling TF from sources to have AVX512F instruction support**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: **16.04**
- **TensorFlow installed from (source or binary)**: **source (11/14/2017)**
- **TensorFlow version (use command below)**: **1.4.0**
- **Python version**: **3.6.1**
- **Bazel version (if compiling from source)**: **0.7.0**
- **GCC/Compiler version (if compiling from source)**: **5.4.0**
- **CUDA/cuDNN version**: **9.0 /7.0.3**
- **GPU model and memory**: **GTX 1080Ti**
- CPU i9 7900x
- **Exact command to reproduce**:


bazel build --config=mkl  --config=cuda  --copt=""-O3"" --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=""-DOMPI_SKIP_MPICXX""  //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
error in compile/build
....
**ERROR**: /home/tadeusz/temp/TF14a/tensorflow-master/tensorflow/contrib/mpi/BUILD:60:1: undeclared inclusion(s) in rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc':
  '/home/tadeusz/temp/TF14a/tensorflow-master/tensorflow/core/distributed_runtime/tensor_coding.h'
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/gtl/array_slice_internal.h:32,
                 from ./tensorflow/core/lib/gtl/array_slice.h:101,
                 from ./tensorflow/core/lib/strings/str_util.h:23,
                 from ./tensorflow/contrib/mpi/mpi_utils.h:25,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:33,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':
./tensorflow/core/util/tensor_format.h:372:47:   required from here
./tensorflow/core/util/tensor_format.h:340:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]

### Source code / logs
the following helps
in file //tensorflow/contrib/mpi/BUILD

"
380,8560,0,"GPU PoolAllocator never satisfied with the eviction rate. Can we limit its allocated size?. I have a tensorflow network where I call  with **tensors of widly changing sizes**. _(to be more precise it takes two tensors as input a [None, E, None, None] and a [None, None, M])_. Additionally, I use some  calls which generate variable sized tensors to be transferred to the GPU.

Everything works fine and training goes nicely and quickly, but eventually it seems that the ** is never satisfied with the eviction rate it gets and constantly try to increase its size**. 



Eventually, it **fills the whole machine RAM** (256GB....) and **kills itself by running out-of-memory** (in less than 1h...).

This behavior only happens when running on GPU. I checked that no ops are added during training through calling .

I also tried using  as proposed [here](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201703201715326785429), and ran the profiler which was saying it was staying at a happy 500MB while the memory usage in  was multiple GB. As far as I understand, the  uses his own  system so it would not show in  profiler right?

[tcmalloc profiler output](https://github.com/tensorflow/tensorflow/files/855907/output_1.pdf)

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

- [How to interpret PoolAllocator messages](http://stackoverflow.com/questions/35151207/how-to-interpret-poolallocator-messages-in-tensorflow)
- [How to debug a memory leak in TF](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201703201715326785429)
- The question I asked [here](http://stackoverflow.com/questions/42861956/gpu-poolallocator-explodes-the-cpu-memory) on SO, without much success.

### Environment info
Operating System:
Ubuntu 14.04
CUDA 8.0 + cuDNN 5.1

python 3.5 nighty build

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I honestly tried, but it seems that if I simplify the system the problem goes away."
403,3308,0,"contrib/makefile: No session factory registered for the given session options. ### Environment info

Operating System: Arch Linux 64-bit
GCC: 6.1.1
no CUDA or cuDNN used

Tensorflow installed from Git repo sources. I tried v0.9.0 and current master ().
### Steps to reproduce

**1.** Build  ( + )
**2.** Link produced  to a C++ project
**3.** Try to create a new TensorFlow session:



**4.** Compile succeeds, but running the code gives:



 seems to be working.
"
1062,26107,0,"Big memory consumption conv2d vs conv3d. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
VERSION=""16.04.5 LTS (Xenial Xerus)""
- **TensorFlow installed from**: 
Binary
- **TensorFlow version**:
1.12.0
- **Python version**:
Python 3.5.2
- **CUDA/cuDNN version**:
CUDA: 9.0, V9.0.176
cuDNN: 7.4.2
- **GPU model and memory**:
NVIDIA Tesla V100 SXM2, 32GB
Driver Version: 384.145

### Describe the problem
Conv2D layer consumes a lot of memory comparing to the same operation performed by Conv3D.
I have 2 independent graphs:
1)
Input (shape=[16, 224, 224, 4])
conv2d (padding = SAME, filter=[3, 3, 4, 16])
bias_add (shape=[16])
2)
Input (shape=[1,16, 224, 224, 4])
conv3d (padding = SAME, filter=[1, 3, 3, 4, 16])
bias_add (shape=[16])

TF profiler reports that conv2d operation in first graph consumes 2900MB whereas conv3d that I presume should perform the same operation (cause leading dimensions of filter is 1) consumes 269.2MB which is an order of magnitude less.

Also for the graph with conv2d I see 2 additional layers are injected:
Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer consumes 16.78MB
Conv2D-0-0-TransposeNCHWToNHWC-LayoutOptimizer consumes 67.11MB
these layers are disappeared if I remove bias_add operation but memory consumption still stays the same.

Am i missing something obvious or my expectations regarding conv2d vs conv3d doing the same in my case are wrong?

### Source code / logs

### Output
1)
Profile:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
Conv2D                      2900.02MB (100.00%, 97.19%),      2.70sec (100.00%, 99.83%),      14.11ms (100.00%, 97.40%),      2.68sec (100.00%, 99.84%)
Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer        16.78MB (2.81%, 0.56%),          4.23ms (0.17%, 0.16%),            48us (2.60%, 0.33%),          4.18ms (0.16%, 0.16%)
BiasAdd                               0B (0.00%, 0.00%),           207us (0.02%, 0.01%),           168us (2.27%, 1.16%),            39us (0.00%, 0.00%)
Conv2D-0-0-TransposeNCHWToNHWC-LayoutOptimizer        67.11MB (2.25%, 2.25%),           202us (0.01%, 0.01%),           160us (1.10%, 1.10%),            42us (0.00%, 0.00%)
VariableV2                        2.56KB (0.00%, 0.00%),            20us (0.00%, 0.00%),             0us (0.00%, 0.00%),            20us (0.00%, 0.00%)

2)
Profile:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
Conv3D                      269.20MB (100.00%, 100.00%),     137.06ms (100.00%, 99.85%),      64.03ms (100.00%, 99.74%),      73.03ms (100.00%, 99.94%)
BiasAdd                               0B (0.00%, 0.00%),           197us (0.15%, 0.14%),           168us (0.26%, 0.26%),            29us (0.06%, 0.04%)
VariableV2                        2.56KB (0.00%, 0.00%),            12us (0.01%, 0.01%),             0us (0.00%, 0.00%),            12us (0.02%, 0.02%)
"
1487,848,0,"tf use all gpus in one machine, and killed my other training job, how to fix this?. Hi, all,

i installed the gpu vision of tf which refer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#virtualenv-installation, and when i run the third line of:



then it will start all of my k80s in one machine, because there is som traning job of some gpus with almost full memeory, then when tf starts, it killed all of the other job. so how to fix this probem?that to say, when i start tf, it use the gpu 0 default?
thanks very much!
"
99,5847,1,"the embedding_lookup() returns zeros when the index exceed embedding matrix size?. NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

I wrote an issues a month ago about the same problem #5260 
I recently update the TF to latest master branch() without any custom modification and the commit hash is 55dbc54192a378c5e685c52595f42503f037320e

The tf.embedding_lookup() still do not raise error even though the index is exceeding the embedding matrix size. It automatically fills zeros as you can see in the below example. 

I tested same code with two different machines and the result was same.
In the previous issue(#5260), @strategist333 said it raise an  in binary release.
But when I tested TF installed from source, it didn't...
 



The output of above code is,

"
1044,5036,0,"Feature request: Inception v3 MetaGraph. Would it be possible for the TensorFlow developers to put a tar-ball online with the Inception v3 saved as a MetaGraph? I can't find it anywhere.

I'm currently using the following tar-ball with a frozen graph for Inception v3:

http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz

The problem is that I cannot continue training that graph because it is frozen, so all the variables have been converted to constants before it was saved. I can't find a way to convert the constants back to variables so I don't think that is possible. (There are also some deprecation warnings regarding BatchNormWithGlobalNormalization so it will presumably stop working at some point in the future).

After searching for a solution for days, I found that you have released a newer checkpoint-file for Inception v3:

http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz

I downloaded it but it's only the checkpoint-file, not the graph-definition. So in my Python code I would apparently have to create the Inception graph using this function first:

https://github.com/tensorflow/models/blob/master/inception/inception/slim/inception_model.py#L52

But this apparently requires building TensorFlow from source, as far as I could understand from the README. There's also several options for using the function and it apparently has to be wrapped in arg_scopes and what-not:

https://github.com/tensorflow/models/blob/master/inception/inception/inception_model.py#L76-87

Would it be possible to update the above tar-ball (dated 2016-03-01) so it also contains the MetaGraph-files, so I can load it more easily and use it in my own Python program? I have another data-set so I replace the softmax-layer of the Inception-graph, and I also want to continue optimizing the rest of the variables of the Inception-graph.

Please also consider including a small example program in the tar-ball (or a link to some python-code), as it would make it a lot easier for everyone who wants to use it. Or at least make a list of all the relevant tensor-names (input, output, etc.)

Thanks!
"
1043,28303,0,"ImportError: DLL load failed: The specified module could not be found.. Python 3.7.3
tensorflow-gpu                     2.0.0a0
NVIDIA CUDA runtime 10.1 with cudnn-10.1-windows10-x64-v7.5.1.10

Traceback (most recent call last):
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\hanna\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\hanna\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\Users\hanna\.vscode\extensions\ms-python.python-2019.4.12954\pythonFiles\ptvsd_launcher.py"", line 43, in <module>
    main(ptvsdArgs)
  File ""c:\Users\hanna\.vscode\extensions\ms-python.python-2019.4.12954\pythonFiles\lib\python\ptvsd\__main__.py"", line 410, in main
    run()
  File ""c:\Users\hanna\.vscode\extensions\ms-python.python-2019.4.12954\pythonFiles\lib\python\ptvsd\__main__.py"", line 291, in run_file
    runpy.run_path(target, run_name='__main__')
  File ""C:\Users\hanna\Anaconda3\lib\runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Users\hanna\Anaconda3\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\hanna\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""c:\Users\hanna\Google Drive\hannannussbaum\Data Science\TensorFlowTest.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\_api\v2\audio\__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\hanna\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\hanna\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors
"
717,7533,0,"Feature Request: Loading images from a URL. ### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I couldn't find a solution for this, so I made a stackoverflow answer:
https://stackoverflow.com/questions/42218771/how-do-i-load-images-from-urls-into-tensorflow?noredirect=1#comment71642082_42218771

### Environment info
Operating System:
macOS Sierra 10.12

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Here is a gist of me loading images via URL, saving to disk, and processing it. To reproduce, skip the saving to disk part and send the jpeg image directly into tensorflow's . It will fail since it is not the type it is expecting.

### What other attempted solutions have you tried?
I've tried downloading image, saving to disk, and using that. This however is very slow.


Is there a way to do something like this? If not, I would like this to be a feature request.
"
464,22382,0,"Creating pip package for TensorFlow with GPU support results in 0 byte simple_console_for_windows.zip. Here is the stackoverflow question: https://stackoverflow.com/questions/52394305/creating-pip-package-for-tensorflow-with-gpu-support-results-in-0-byte-simple-co

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.2/7.2.1
- **GPU model and memory**: Nvidia M1000M
- **Exact command to reproduce**: bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/tmp/tensorflow_pkg

### Describe the problem
After successfully building TensorFlow with GPU support, I'm trying to build the pip package and I'm getting an error saying it can't read the simple_console_for_windows.zip file.  

I've confirmed that the file is in C:\tensorflow\bazel-bin\tensorflow\tools\pip_package folder, but it is 0 bytes.

This is my pip build command:
    bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/tmp/tensorflow_pkg

My build command was:
    bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Source code / logs
This is the full error:

    Unzipping simple_console_for_windows.zip to create runfiles tree...
    [./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip]
      End-of-central-directory signature not found.  Either this file is not
      a zipfile, or it constitutes one disk of a multi-part archive.  In the
      latter case the central directory and zipfile comment will be found on
      the last disk(s) of this archive.
    unzip:  cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or
            ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.
"
1170,25151,0,"version 1.13.0rc0 build failed. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.0rc0
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?:  no
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source):  7.3.0
- CUDA/cuDNN version: 10.0/ 7.4
- GPU model and memory:  GTX1080Ti GDDR5X 11GB X 7



**Describe the problem**

bazel build failed

ERROR: /home/wmind/repo/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 72, in <module>
    from tensorflow.python.ops.standard_ops import *
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/standard_ops.py"", line 25, in <module>
    from tensorflow.python import autograph
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/__init__.py"", line 37, in <module>
    from tensorflow.python.autograph.core.converter import ConversionOptions
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/core/converter.py"", line 74, in <module>
    from tensorflow.python.autograph.pyct import cfg
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/cfg.py"", line 41, in <module>
    from tensorflow.python.autograph.pyct import compiler
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/compiler.py"", line 30, in <module>
    import astor
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/__init__.py"", line 14, in <module>
    from .code_gen import to_source  # NOQA
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/code_gen.py"", line 311
    def visit_FunctionDef(self, node, async=False):
                                          ^
SyntaxError: invalid syntax
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2248.641s, Critical Path: 409.07s
INFO: 15004 processes: 15004 local.
FAILED: Build did NOT complete successfully
"
1034,12202,0,"Using keras built-in models. #### Version
v1.2.0-rc2-21-g12f033d

#### Problem
The Keras built-in models in  cannot be used as a subgraph in TF.

#### Example
https://stackoverflow.com/questions/45585546/error-with-tf-contrib-keras-tf-placeholder

#### Cause
Calling  is supposed to load pre-trained weights only for related variables, but it initializes the entire TF graph.
"
546,35180,0,"Windows build failed - Internal compiler error Visual Studio 2017 - FAILED: Build did NOT complete successfully. **System information**
- OS Platform and Distribution: Windows 7 Professional SP1 64 bit
- TensorFlow installed from (source or binary): source
- TensorFlow version: tensorflow-master (downloaded 18/12/2019)
- Python version: 3.7
- Installed using virtualenv? pip? conda?: YES, conda
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source): MSVC 14.16.27023 (Visual Studio 2017)
- CUDA/cuDNN version: build for CPU
- GPU model and memory: (Quadro K200M)

**Describe the problem**
Followed this guide: https://www.tensorflow.org/install/source_windows and just build failed.
I tested different combinations (https://www.tensorflow.org/install/source_windows#cpu) and I was just able to compile just the **r1.14 successfully**, by the way.
**r2.0** failed with the same message as master.
But back to master release.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Donwloaded the  from github
- Installed Bazel version 1.1.0
- cmd to folder 
-  (conda)
- Set env var for bazel: 
- Check Bazel version

- Configure

- Launch the compilation: 
- After time elapsed the process ended with: 

**Any other info / logs**
Just attaching the last rows of the terminal messages

Message has Italian chunks of which I'm trying to provide a translation here:






[tensorflow-master-build-fail-log.txt](https://github.com/tensorflow/tensorflow/files/3971998/tensorflow-master-build-fail-log.txt)

"
178,13141,1,"label image run slower and slower?. Hello ,Everyone
Is there anybody who had ever run the code label_image.py in tensorflow/tensorflow/examples/label_image/label_image.py
I have modify it to run on a dataset and read and calssify image one by one,and as the number of images goes,the speed is slower and slower,at first,that's about ten images per second,and when the number of image goes to 1000,the time is about 7s,Incredibly!and  I find the problem is in the function  read_tensor_from_image_file in label_image.py and this part is read and preprocess images, so what's the matter?and I want to know how to speed up?and how to modify the code so as to making it run for batches ?"
269,28653,1,"Multi-GPU performance degradation in custom built TF. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): tensorflow/benchmarks
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 33141833284a81a5ab3300047d2845c7124f4a66
- Python version: Python 3.5.3
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 6.3
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: V100-PCIE-32GB

**Describe the current behavior**

Use the following command to compile (without running of ):



Use the following command for benchmarking:



| TF Version | 1-GPU | 2-GPU | 4-GPU | 8-GPU |
|:-:|:-:|:-:|:-:|:-:|
|1.13.1|786.63|1557.89|2982.52|5741.76|
|1.14.1.dev20190512|779.81|1547.23|2872.75|5671.7|
| 33141833284a81a5ab3300047d2845c7124f4a66|776.75|387.99|713.77|1249.80|

**Describe the expected behavior**

Related to #28628. I am currently not sure how to reproduce a custom build with the same GIT_VERSION as of nightly. I will try to reproduce this issue with the next nightly package."
297,12017,0,"stream_executor/platform/mutex.h doesn't compile under C++14. Please go to Stack Overflow for help and support:

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS X with clang/llvm 5.0

- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
0.4.5

### Describe the problem
There's an ifdef in mutex.h that uses shared_timed_mutex when compiled as C++14 and up. The file doesn't compile because C++14 requires using condition_variable_any rather than condition_variable with that kind of mutex.

### Source code / logs
"
279,24720,0,"ExtractImagePatches request. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.12


**Provide the text output from tflite_convert**



Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
303,22056,0,"Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED. TensorFlow: 1.10.1
cuda: 9.0
cudnn: cudnn7.1 for cuda9.0
python3
Ubuntu 18.04
nvidia-driver: 390

Try running cnns error got:


I think the cuda version are already the newest version, and my cudnn and cuda are OK. Why still got this problem? The error message throw from cuda_dnn.cc should be more specific what's going on."
183,34144,1,"Tensorflow 2.0 too slow when minimizing a custom cost function. **System information**
- OS : Windows 10
- TensorFlow version: v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7.4

I have a code that looks like the following, where I want to minimize a custom cost function with respect to parameters w. However, when running the code, it appears to me that it is very slow (like more than 30 times slower) compared to the same code implemented without tensorflow (by explicitly defining a function that gives the gradient of the cost).

I am not sure if it's a problem with tf or if I am doing something wrong and unnecessarily re-computing the tf graph each time. I posted this issue on stackoverflow and have been advised to open an issue here.

In the following code, I am using a simple dummy cost function just as an example to show the big difference in performance.

**Code with Tensorflow:**



**Code without Tensorflow (just numpy) :**

"
365,35116,0,"Cannot build raspberry pi wheel for python 3.7. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.1.0-rc1

 I'm building using the docker image for a raspberry pi 3 build.



**Describe the problem**

I'm able to correctly build  using:


I'm trying to do the same but with the python 3.7 docker images  () but it fails with:



**Provide the exact sequence of commands / steps that you executed before running into the problem**



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
168,25923,1,"[performance] CPU is idle even when there are operations ready to be executed. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.19.2- (@non-git)
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: No
- GPU model and memory: No


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I'm trying to feed tensors produces by a ParseExample OP when inference an exported graph to avoid 
serialization/deserialization tf record of input examples.
I have got the correct prediction results. But the performance is not consistent with my expectation.
There is several idle time in the timeline. And the feed tensors are not processing in parallel.

**Describe the expected behavior**
No CPU idle time in prediction. No ParseExample OP in inference should be faster.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
chrome tracing results:
with ParseExample, no cpu idle during inference
![image](https://user-images.githubusercontent.com/32092715/53081857-a7216300-3536-11e9-807c-791c1f47f4c2.png)
[with_parse_example.json.txt](https://github.com/tensorflow/tensorflow/files/2883952/with_parse_example.json.txt)
without ParseExample, cpu idle during inference with red marks
![image](https://user-images.githubusercontent.com/32092715/53082111-231bab00-3537-11e9-922f-3ab8c2f5f743.png)
[without_parse_example.json.txt](https://github.com/tensorflow/tensorflow/files/2883961/without_parse_example.json.txt)

Any advices will be appreciated, Thanks"
39,6224,1,"Extremely inefficient cuda event polling. I am running tensorflow with this application: https://github.com/openai/pixel-cnn. I am running on 4 NVIDIA K80 cards and CUDA 7.5. For my experiment, I modified the code to only do 100 training iterations.

During the 100 iterations, tensorflow destroys 2662 cuda events. But how many times it polls event status? 40669120 times. This means on average it polls ~15,000 times for 1 destroy. Isn't it extremely inefficient?

Per my understanding of tensorflow source code, it uses one dedicated thread to poll event status and destroy an event once it's complete. That's the only place where the event status is polled."
1392,17643,0,"Failed to import the TensorFlow module. I tried to install TensorFlow (CPU only) on windows 10 and python 3.6 (64 bit) and after write
import tensorflow as tf
{ hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello)) }
on visual studio 2017 I got this

Traceback (most recent call last):
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\PythonApplication3\PythonApplication3\PythonApplication3.py"", line 228, in <module>
    import tensorflow as tf
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
 
 ,I used this script 
https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c  to know the proplem
and I got this
ERROR: Failed to import the TensorFlow module.
=============================================
Since TensorFlow 1.4, the self-check has been integrated with TensorFlow itself,
and any missing DLLs will be reported when you execute the 
statement. The error messages printed below refer to TensorFlow 1.3 and earlier,
and are inaccurate for later versions of TensorFlow.

- Python version is 3.6.

- TensorFlow is installed at: C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow

- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow
  requires that this DLL be installed in a directory that is named in
  your %PATH% environment variable. Download and install CUDA 8.0 from
  this URL: https://developer.nvidia.com/cuda-toolkit

- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that
  this DLL be installed in a directory that is named in your %PATH%
  environment variable. Typically it is installed in 'C:\Windows\System32'.
  If it is not present, ensure that you have a CUDA-capable GPU with the
  correct driver installed.

- Could not find cuDNN 6.

  The GPU version of TensorFlow requires that the correct cuDNN DLL be installed
  in a directory that is named in your %PATH% environment variable. Note that
  installing cuDNN is a separate step from installing CUDA, and it is often
  found in a different directory from the CUDA DLLs. The correct version of
  cuDNN depends on your version of TensorFlow:

  * TensorFlow 1.2.1 or earlier requires cuDNN 5.1. ('cudnn64_5.dll')
  * TensorFlow 1.3 or later requires cuDNN 6. ('cudnn64_6.dll')

  You may install the necessary DLL by downloading cuDNN from this URL:
  https://developer.nvidia.com/cudnn
"
423,35130,0,"mnist data download failure. When I use 
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/mnist/"", one_hot = True)

It seems that I cannot download the gz files from the googleapis? Error as below
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)>

Then I manually visit the source site, also error xml replied as below:
<Error>
<Code>AccessDenied</Code>
<Message>Access denied.</Message>
<Details>
Anonymous caller does not have storage.objects.get access to cvdf-datasets/mnist.
</Details>
</Error>

So how can I fix the problem? Thanks a lot."
1426,18039,0,"Error building contrib/nccl on CentOS 6.5 on 1.7.0-rc1. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 6.5
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.7.0-rc1 release
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.11.1- (non-git)
- **GCC/Compiler version (if compiling from source)**: gcc (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)
- **CUDA/cuDNN version**: CUDA: 7.5 cuDNN: 6.0
- **GPU model and memory**: NVIDIA Tesla K20m
- **Exact command to reproduce**: 


### Describe the problem
Tensorflow fails to build from source release 1.7.0-rc1 with error from nccl_ops.cc
I took the following steps:



### Source code / logs
Build fails at this point:


So it looks like a parameter type problem here: [nccl_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/nccl/kernels/nccl_ops.cc#L207)"
575,20817,0,"Java SavedModelBundle.load do not support long path in windows. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 64bit.
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
### Describe the problem
Java SavedModelBundle.load do not support long path in windows.
It will throw something like:

I shorten the model path, everything works fine.
I enabled long path based on this [doc](https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file). It still has this issue.

I know it's a limit of Windows OS. I just wandering is there any way to let the C++ part bypass this limit?
"
362,18115,0,image restauration
56,34932,1,"Severe TPU/CPU behaviour discrepency. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):No
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0-dev20191203
- Python version: 3.5
- GPU model and memory: TPU (nightly-2.x)

**Describe the current behavior**
When training using a TPU backend, if a   code is defined before connecting to a TPU cluster. Calling the function (as ) results in a python kernel crash, without any errors or other info. 

This is especially severe (IMHO), since, as the training code base grows, more autograph functions are defined in modules instead of on the main program. As it's natural to import modules at the start of the program, if the TPU connection is initiated after the imports, using the @tf.fucntion code defined in the modules results in a kernel crash.
If the same code is, however, being run just on the CPU, it works as expected.
This leads to a somewhat frustrating experience of everything working on a CPU dev env, and then crashing inexplicably when connecting to a TPU.
The unintuitive solution is to run the TPU connection boilerplate before any imports.

**Describe the expected behavior**
1. connecting to a TPU shouldn't create an implicit scope, if a scope is required it should be with a  idiom
2. It should be better documented if all autograph function definitions should be defined after connecting to a TPU
3. If a code executed within a TPU scope depends on a code defined outside, it should fail gracefully and informatively (not crash the kernel)

**Code to reproduce the issue**

---
Fail case:

---
Working case:

---
The reproduction code doesn't use imports, but 

Would typically run as part of the import code, and not the main program.

**Other info / logs**
No error logs produced"
829,16313,0,"Bug of tf.data.TFRecordDataset? Couldn't use tf.reshape after the operations of tf.data.TFRecordDataset. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia GeForce GTX TITAN X 12GB
- **Exact command to reproduce**:


### Describe the problem
I want to use  function **tf.profiler.ProfileOptionBuilder.float_operation** to show the flops of the model. But it need a certain input shape while the the output shape of **tf.data.TFrecordDataset** is like (?, 32,32,3). When I want to use tf.reshape to reshape the output of **tf.data.TFrecordDataset**, it generates an error ""Input to reshape is a tensor with 64512 values, but the requested shape has 98304"".  

### Source code 

  def dataset_input(self, dataset_type):
    with tf.variable_scope(""batch_"" + dataset_type):
        def parser(record):
            features = tf.parse_single_example(
                record,
                features={
                    'image': tf.FixedLenFeature([], tf.string),
                    'label': tf.FixedLenFeature([], tf.int64)
                })
            image, label = features['image'], features['label']
            height, width, channels = self.input_size, self.input_size, self.input_dim
            image = tf.decode_raw(image, tf.uint8)
            image = tf.reshape(image, [height, width, channels])
            return image, label
        dataset = tf.data.TFRecordDataset([self.dataset_dir[dataset_type]])
        dataset = dataset.map(parser)
        dataset = dataset.shuffle(buffer_size=50000)
        dataset = dataset.batch(self.batch_size)
        dataset = dataset.repeat()
        iterator = dataset.make_one_shot_iterator()
        features, labels = iterator.get_next()
        features = tf.reshape(features, [self.batch_size, self.input_size, self.input_size, self.input_dim])
        return features, labels
  "
650,11328,0,"wide_n_deep AttributeError (master) and ValueError (r1.2). I'm using TensorFlow 1.2.1. When I run  from the [Linear Model tutorial](https://www.tensorflow.org/tutorials/wide), I get the following AttributeError:



I tried switching to r1.2, as suggested in #11256, but got this ValueError on r1.2:


"
561,29835,0,"GPU Support - Shared Object Exclusion . [UPDATE]:  This issue is about Maven artifact organization / dependency management and documentation.  

**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04.2 LTS
- TensorFlow installed from Maven, version 1.13.1
- TensorFlow version: 1.13.1
- Python version: 3.6.7
- Installed using virtualenv
- CUDA/cuDNN version:  NVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1
- GPU model and memory: 4xNvidia 1080Ti, 

**Describe the problem**

I have created a TensorFlow model in Python and saved it to the disk using the standard method:

    builder = tf.saved_model.builder.SavedModelBuilder(model_directory) 
    builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING]) 
    builder.save(False)

Next I am loading the model in Java:

    SavedModelBundle savedModelBundle = SavedModelBundle.loader(modelDir)
        .withTags(""serve"")
        .withConfigProto(
            ConfigProto.newBuilder()
                .setGpuOptions(
                    GPUOptions.newBuilder()
                        .setPerProcessGpuMemoryFraction(1.0).build())
                .setLogDevicePlacement(true)
                .build().toByteArray())
        .load();
    Session session = savedModelBundle.session();

In my pom.xml I have

    <dependency>
        <groupId>org.tensorflow</groupId>
        <artifactId>tensorflow</artifactId>
        <version>${tensorflow.version}</version>
    </dependency>
    <dependency>
        <groupId>org.tensorflow</groupId>
        <artifactId>proto</artifactId>
        <version>${tensorflow.version}</version>
    </dependency>
    <dependency>
        <groupId>org.tensorflow</groupId>
        <artifactId>libtensorflow_jni_gpu</artifactId>
        <version>${tensorflow.version}</version>
    </dependency>

However, this fails to use my GPU's on startup

    2019-06-15 23:48:38.130731: I tensorflow/compiler/xla/service/service.cc:150] 
        XLA service 0x7fbfc4656e10 executing computations on platform Host. Devices:
    2019-06-15 23:48:38.130768: I tensorflow/compiler/xla/service/service.cc:158]   
        StreamExecutor device (0): <undefined>, <undefined>
        Device mapping:
            /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
    2019-06-15 23:48:38.131168: I tensorflow/core/common_runtime/direct_session.cc:317]    
       Device mapping:
        /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device

Please advise on how to use GPU with SaveModelBundle.  "
704,12099,0,"Cannot import tensorflow 1.0.1 after compiled from source. 

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.0.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.3
- **CUDA/cuDNN version**: x
- **GPU model and memory**: x
- **Exact command to reproduce**: import tensorflow

### Describe the problem
I successfully built tensorflow 1.0.1 from source using optimization flags with the command . After that, I activated my virtualenv and issued a  to install in the virtual env this compiled from source version. When I try to import tensorflow in the python code, I receive a error (showing it with -v flag from python interpreter activated) showed below in the source code / logs session.

### Source code / logs

"
432,3632,0,"Not finding cuda library for jobs that do NOT require GPU. I am having this error when I use the slurm (http://slurm.schedmd.com/) workload manager. When I run some tensorflow python scripts, sometimes it results in an error (attached). It seems that it can't find cuda library installed but I am running scripts that do **not** require GPUs.  Therefore, I find it very confusing why cuda would be an issue at all. Why is cuda installation an issue if I don't need it?

---

Operating System:   CentOS Linux release 7.2.1511

Installed version of CUDA and cuDNN (calling ls -l /usr/lib/libcuda.so.352.63): 
-rwxr-xr-x 1 root root 14272428 Dec 15  2015 /usr/lib/libcuda.so.352.63

If installed from binary pip package, provide:
tensorflow (0.9.0)
1. The output from . running this seems to produce different results from the error I expected. what it outputs:



 the error I expected:


### Steps to reproduce
1. run the sbatch command in slurm and dispatch about 1000 jobs running an idential tensorflow script. Returns that error.
### What have you tried?
1. I've tried logging into the node directly throwing the error and run tensorflow jobs but it seems when I do that there are no errors.
### Logs or other output that would be helpful

(If logs are large, please upload as attachment).


"
981,109,0," C++ compilation of rule '//tensorflow/python:tf_session_helper' failed. I am trying to compile build_pip_package on Ubuntu 12.04 with gcc 4.8, but it gives an error during compilation:


"
859,4401,0,"Broken blog link on tensroflow.org dated 26 Aug 2016. https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html.html

Should be 

https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html
"
70,34451,1,"'accuracy' and tf.metrics.get('accuracy') produce different results. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSUSE
- TensorFlow installed from (source or binary): pip binary within pyenv
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7.5

**Describe the current behavior**
The same model behaves differently whether one uses  or  (see below).

**Describe the expected behavior**
They should behave identically.

**Code to reproduce the issue**

Example output:


**Other info / logs**
Closely related to #34088"
64,33412,1,"tf2.0 takes more than twice memory than 1.14. I have 2 environments. tf 1.14 installed for python 3.5 and tf 2 for python 3.7.

I run the same code and load the same keras model in both environments. in py3.5, after model loaded, the process takes 560MB memory. while in py3.7, it takes 1.2GB.

is there any way to reduce the memory for tf 2 ?"
581,22,0,"OpenCL support. I understand TensorFlow only supports CUDA. What would need to be done to add in OpenCL support?
"
662,9398,0,"Tf.gradients returning all zeroes when called on result of a tf.gradient call. I have the following code snippet:

interpolates = alpha*real_data + ((1-alpha)*fake_data)
disc_interpolates = Discriminator(interpolates)
gradients = tf.gradients(disc_interpolates, [interpolates])[0]
second_grad = tf.gradients(gradients[0], [interpolates])[0]

Where Discriminator corresponds to a neural network. The first call to tf.gradients is working correctly and I get back non-zero slope values in the gradients variable. However whenever I try to find the second derivative by applying tf.gradients to the gradients variable, my result is always a vector of zeroes. 

Is this expected behavior or should I be able to find the second derivatives of my neural net? "
434,947,0,"sparse_softmax_cross_entropy_with_logits fails when labels is a placeholder. Running the following results in :



Making the minibatch dimension explicit makes no difference: . 

When I run the unit test for this function there is no problem, but I note that the unit test passes NumPy ndarrays, which are converted to constant tensors. Playing around a bit it seems it fails whenever  is a placeholder.
"
242,3009,1,"FIFOQueue: dequeue many operation very slow?. When training a relatively simple model (1-layer LSTM, 256 units) my Titan X GPU keeps spiking from 0% to 30% GPU utilization. Conclusion: somewhere in the pipeline there is a bottleneck which limits the GPU to be processing the training batches continuously. I use a FIFOQueue to which examples are being fed in one or more separate threads:



For the training operation I use  to get examples from the queue. As you can see the batch size is 64 examples. So in the end the input tensor is  of type :



To find out why my code is running ""slow"" (i.e. spiking GPU allocation and no temperature increase) I use the  object ([see here](http://stackoverflow.com/questions/34293714/tensorflow-can-i-measure-the-execution-time-of-individual-operations)) to measure execution times of individual operations. The results displayed below show the measurements for one training iteration at which point the queue was filled with more than 1000 examples. I have included screenshots for both GPU and CPU-only runs (forced with . 

What strikes me from these results is that it takes a really long time to dequeue examples from the FIFOQueue. What is happening here...something wrong or is the dequeuing operation just very slow? Overall the dequeuing operation and sending the data to the GPU takes up half of the time of a training iteration. No wonder that the GPU utilization is spiking. Any help is welcome optimizing my training pipeline! As I understand correctly the examples are all queued in RAM, is there also a way to queue them ahead on GPU memory so when they are needed they do not have to be moved CPU => GPU?

This is tested on TensorFlow v9.0 build from sources about 1.5 week ago.

**GPU running on Titan X**
![gpu](http://i.imgur.com/eTZNPDM.png?1) 

**CPU running on Xeon CPU E5-2640**
![cpu](http://i.imgur.com/mQkLH8h.png?1)
"
950,28228,0,"Documentation for C APIs. Is there any detailed documentation for C APIs besides  [version example](https://www.tensorflow.org/install/lang_c#build) and [c_api.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h)? For example, creating a session and tensors, running queries, get tensor result, etc.


"
982,32766,0,"model_pruning: why does strip_pruning_vars always show 50% zeros?.  I'm trying tensorflow/contrib/model_pruning/examples/cifar10/cifar10_train. Tensorboard shows conv1 sparsity increased to 70%，conv2 sparsity increased to 90%，but  strip_pruning_vars alway shows 50% zeros。
   
![image](https://user-images.githubusercontent.com/11002654/65488056-cd4b1f00-deda-11e9-92fa-9d77d009103c.png)

Use 
I0924 14:51:55.225000 140698707568448 graph_util_impl.py:311] Froze 15 variables.
I0924 14:51:55.267543 140698707568448 graph_util_impl.py:364] Converted 15 variables to const ops.
I0924 14:51:55.336094 140698707568448 strip_pruning_vars_lib.py:69] conv1/weights/masked_weight has 4800 values, **50.00% zeros** 
I0924 14:51:55.345871 140698707568448 strip_pruning_vars_lib.py:69] conv2/weights/masked_weight has 102400 values, **50.00% zeros** 
I0924 14:51:55.447343 140698707568448 strip_pruning_vars_lib.py:69] local3/weights/masked_weight has 884736 values, **50.00% zeros** 
I0924 14:51:55.470997 140698707568448 strip_pruning_vars_lib.py:69] local4/weights/masked_weight has 73728 values, **50.00% zeros** 
I0924 14:51:55.495334 140698707568448 strip_pruning_vars_lib.py:69] softmax_linear/weights/masked_weight has 1920 values, **50.00% zeros** 
I0924 14:51:55.537974 140698707568448 strip_pruning_vars.py:73] 
Final graph written to /home/terse/code/programming/tensorflow/model_pruning/cifar_pruning_stripped.pb"
76,23710,1,"the performance of tensorflow distributed. **Describe the current behavior**
I have trained a speech recognition network using tensorflow both on multi-gpu single machine version(multi-gpu) and multi-gpu multi-machine(distributed) version. the training speed was ok in multi-GPU but the speed on distributed is slow and accuracy is not same as multi-GPU version at same steps. I watched the timeline and found RecvTensor is waste so long time. i‘m wondering how to improve the performance of distributed training.
![c2e456a6 17eda71d 1b93e65b](https://user-images.githubusercontent.com/16797858/48396541-c0f02480-e755-11e8-97fd-8dad532956fa.png)

**log info**
the start script for each worker (3 workers in total and 3 ps) is:

the training log:
multi-gpu:

distributed:



**System information**
- Have I written custom code: yes    (code is [here](https://github.com/dingevin/distributed-training/blob/master/distributed.py))
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.8
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): 1.10.1
- Python version: 2.7.13
- GPU model and memory: TITAN X 12G
- Network bandwidt: 125M/s

**What I have tried**
1. the **RecvTensor** is so long time , so i increase the number of ps, the speed more fast than before but accuracy don't improve. i wonder how to confirm the number of ps and worker？if i increate the numer of ps the speed can be faster?  my network is 1CNN+5LSTM+1Fully Connected Layer。
2. the accuracy was 10% in multi-gpu 4000 steps; so i set 2000 steps to each worer(2 ps 2 worker), but the accuracy is 5%， it looks don't sync updata parameter? Is there something wrong with my [code](https://github.com/dingevin/distributed-training/blob/master/distributed.py)?"
1181,3032,0,"Optimizer classes do not support complex variables. Using TF 0.8.0.

 ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-871aef3761c0> in <module>()
----> 1 train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
      2 init = tf.initialize_all_variables()

/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name)
    188         loss, var_list=var_list, gate_gradients=gate_gradients,
    189         aggregation_method=aggregation_method,
--> 190         colocate_gradients_with_ops=colocate_gradients_with_ops)
    191     return self.apply_gradients(grads_and_vars, global_step=global_step,
    192                                 name=name)

/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops)
    243       grads = control_flow_ops.tuple(grads)
    244     grads_and_vars = list(zip(grads, var_list))
--> 245     self._assert_valid_dtypes([v for g, v in grads_and_vars if g is not None])
    246     return grads_and_vars
    247 

/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in _assert_valid_dtypes(self, tensors)
    354         raise ValueError(
    355             ""Invalid type %r for %s, expected: %s."" % (
--> 356                 dtype, t.name, [v for v in valid_dtypes]))
    357 
    358   # --------------

ValueError: Invalid type tf.complex64 for complex_weight_variable:0, expected: [tf.float32].`
"
1077,12770,0,"GPU support on Mac OS X? compile by oneself?. I'm 纠结 to select ubuntu or Hackintosh as my PC.

Note: As of version 1.2, TensorFlow no longer provides GPU support on Mac OS X.

Could anyone tell the reason?

Could I compile tensorflow with gpu[cuda] support by myself? Why or Why not?


"
1436,4438,0,"tf.import_graph_def() requires a non-empty name. After updating version of tenserflow  there is now an issue when loading with my old models. 

./importer.py"", line 241, in import_graph_def
    raise ValueError('tf.import_graph_def() requires a non-empty  '
ValueError: tf.import_graph_def() requires a non-empty  if  is used.

The same is case when i try colorize model colorize-20160110

how to fix this. The code is given below
import tensorflow as tf
import skimage.transform
from skimage.io import imsave, imread

def load_image(path):
    img = imread(path)
    # crop image from center
    short_edge = min(img.shape[:2])
    yy = int((img.shape[0] - short_edge) / 2)
    xx = int((img.shape[1] - short_edge) / 2)
    crop_img = img[yy : yy + short_edge, xx : xx + short_edge]
    # resize to 224, 224
    img = skimage.transform.resize(crop_img, (224, 224))
    # desaturate image
    return (img[:,:,0] + img[:,:,1] + img[:,:,2]) / 3.0

shark_gray = load_image(""shark.jpg"").reshape(1, 224, 224, 1)

with open(""colorize.tfmodel"", mode='rb') as f:
    fileContent = f.read()

graph_def = tf.GraphDef()
graph_def.ParseFromString(fileContent)
grayscale = tf.placeholder(""float"", [1, 224, 224, 1])
tf.import_graph_def(graph_def, input_map={ ""grayscale"": grayscale }, name='')

with tf.Session() as sess:
    inferred_rgb = sess.graph.get_tensor_by_name(""inferred_rgb:0"")
    inferred_batch = sess.run(inferred_rgb, feed_dict={ grayscale: shark_gray })
    imsave(""shark-color.jpg"", inferred_batch[0])
    print (""saved shark-color.jpg"")
"
882,3180,0,"Execution order of ReLU and Max-Pooling. Hello Everyone,

I'm new to Deep Learning and TensorFlow. From studying tutorials / research papers / online lectures it appears that people always have the execution order: ReLU -> Pooling. But in case of e.g. 2x2 max-pooling it seems that we can save 75% of the ReLU operations by simply reversing the execution order to: Max-Pooling -> ReLU. This should calculate the exact same thing using only a quarter of the ReLU operations. This reversal of operations can be done in general for max-pooling and all non-decreasing activation functions (which I guess they all are?), but it won't work for average-pooling.

This is an optimization that TensorFlow could perform automatically when compiling the computation graph. I haven't quite figured out how to use TensorBoard yet, so I can't tell if this automatic reversal of ReLU and max-pooling is already being done in TensorFlow.

So I've done a few experiments instead, timing the optimization of a convolutional net on MNIST, but the results are inconclusive for the two execution orders. Perhaps this means that TensorFlow already does the reversal automatically, or it means that there's no consistent advantage because the saved ReLU operations are such a tiny fraction of the overall computational cost, or perhaps it takes much larger images than MNIST and much deeper convolutional networks for the performance difference to become apparent.

Any thoughts?
"
462,15252,0,"vm compiled tensorflow - libmklml_intel.so ImportError. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04 VM
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
Branch  and branch 
- **Python version**: 
3.6.3
- **Bazel version (if compiling from source)**:
Build label: 0.8.1 (Install using apt repository)
- **GCC/Compiler version (if compiling from source)**:
gcc-6 (Ubuntu 6.4.0-10ubuntu1~16.04.york0) 6.4.0 20171112

- **Exact command to reproduce**:
python -c ""import tensorflow as tf;""

### Describe the problem
After compiling Tensorflow from source with tutorial:  and install the compiled pip package, I import tensorflow on python console and get those errors:

~~~
python -c ""import tensorflow as tf;""
Traceback (most recent call last):
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 73, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/potatoman/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
~~~

My machine is a headless KVM virtual machine running on a ubuntu 17.10, btw.

I figure out where the problem is somehow, which is the pip packaging script don't include those mkl so files, and in my case, replace  in script  at line 185 with  solves the issue.

"
559,9918,0,"preserving specific checkpoints. Savers automatically clean up checkpoints and that's lovely.  But there are special points during training that I want to be sure to save (e.g. transitioning from a pre-training phrase to full training), which I can't ensure with the current options (unless I just keep everything, by making  &  huge).

Two possible approaches:

1)  
Never clean up this checkpoint.

2)   is defined **per** 
i.e. Whenever I change the  argument to  in the middle of the session, don't clean up the checkpoints with the previous .

This is related to #8658 (with a little book-keeping on the client, you could do #8658 yourself)."
828,7959,0,"ImportError: cannot import name model_fn. I tried run cnn_mnist.py and I got the following error.

Traceback (most recent call last):
  File "" cnn_mnist.py"", line 13, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib
ImportError: cannot import name model_fn

cuda veriosn
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sun_Sep__4_22:14:01_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44

tensorflow version
tensorflow (0.10.0)

python version
Python 2.7.12
"
285,23048,0,"Bug in function: MutableGraphView::ReplaceInput(..) ?. ------------------------
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.11
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.16
- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:


### Describe the problem
I am modifying the Tensorflow source to add a new  pass, and I am having a hard time of using the fuction: . I think there is a bug inside this function, (the master branch version is the same as the r1.11 version), in that the new input is not specified with the :

### Source code / logs
The description of the function seems pretty clear in the header file:
output_port_id

Thanks a lot!

"
289,20421,0,"Guidelines to have a working C API (libtensorflow.so) on FreeBSD 11.1. ### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: FreeBSD-11.1
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: github branch r1.9
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: clang (llvm40)
- **Exact command to reproduce**: bazel build //tensorflow:libtensorflow.so

### Describe the problem
Tensorflow does not compile out of the box on FreeBSD-11.1 systems. I provide here the workarounds and adjustements i've done for compiling it. This is not perfect but I can run TensorFlow C programs on FreeBSD without problem. 

I hope it can be useful for someone.

### Source code / logs

The following  generates an error at compile time in tensorflow/core/platform/env.cc, line 350: 

static_cast from 'pthread_t' (aka 'pthread *') to 'int64' (aka 'long long') is not allowed

As a workaround I used:
 

A non-existing header file on FreeBSD is included in tensorflow/core/platform/posix/posix_file_system.cc, line 22:

I replaced this code with:


At link time there are undefined references to backtrace() and backtrace_symbols_fd() functions in tensorflow/core/platform/*. This is because the **LD flag  is missing in linker options**

At last, under FreeBSD, the ""libdl"" library does not exists. So the **LD flag  should be removed from linker options**.

"
714,33563,0,"tf.io.GFIle not working correctly with UTF-8 files and Python3. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu
- TensorFlow installed from (source or binary): source internal Google
- TensorFlow version (use command below): 1.5.0, internal Google
- Python version: 3.6.7

**Describe the current behavior**
Calling 'read(X)' on the text files opened with GFile in python3 doesn't work properly (it fetches X bytes rather than X characters). This often results with the UnicodeDecodeError (as the read can happen in the middle of the unicode character).

**Describe the expected behavior**
It should behave like python3: reading the X characters.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The error will be:

"
1451,30609,0,"(more) spurious deprecation warnings. Similar to #27897

**System information**
- OS Platform and Distribution
 Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic
- TensorFlow installed from:
pip install tensorflow==2.0.0-beta1
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
- Python version: 3.6.8

(This is happening in colab.sandbox.google.com)

**Describe the current behavior**

When using new APIs that replaced old APIs, you deprecation warnings as if you were still using the old API.

**Describe the expected behavior**

If I use the new APIs, I should not get deprecation warnings.

**Code to reproduce the issue**


tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)tf.data.Options.experimental_determinstic
"
1116,21274,0,"Using a tuple of Numpy arrays as validation_data fails when fitting a tf.keras model with a tf.data.Dataset. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6.6
- **CUDA/cuDNN version**:9.2
- **GPU model and memory**: NA
- **Exact command to reproduce**:

### Describe the problem
Usecase: train a tf.keras.Model using the  method. I for training data I have a  but for   I'd like to have a prefetched tuple  of Numpy arrays. At the end of the epoch it fails with the error 

It seems it's trying to calculate  but it fails. Passing  will avoid the exception but it's unnecessary and i'm not sure what the  model does with it.

### Source code / logs



as a smoke test, I used  and  as training data ( and ) and it runs fine. So the problem seems to be when using a  and . Using a  as  (with ) also works fine.



error log at the end of epoch:
"
493,24138,0,"How can I convert custom pb file to tflite?. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:-
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):1.15.0
- Python version:2.7.12
- Bazel version (if compiling from source):0.19.2
- GCC/Compiler version (if compiling from source):-
- CUDA/cuDNN version:-
- GPU model and memory: no GPU / 8GB memory(VM)

I tried to convert frozen & custom .pb file(Specifically, SSD) to .tflite with toco tools.
but I get some error & core dump.(below)

*convert command & results is below:******************************************************************
$   (path)/.local/bin/toco 
--input_file=(path)/ssd1.pb   
--output_file=(path)/ssd2.tflite 
--input_format=TENSORFLOW_GRAPHDEF  
 --output_format=TFLITE 
--input_shape=1,300,300,3 
--input_array=input_13   
--output_array=predictions/concat 
--input_data_type=FLOAT   
--inference_type=FLOAT 
--allow_custom_ops
2018-12-04 11:39:52.856924: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857097: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857156: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857214: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857271: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857389: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857474: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857533: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857586: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857638: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Prod
2018-12-04 11:39:52.857720: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.857784: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.857878: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.857976: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858055: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858156: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858226: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858287: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858337: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: StridedSlice
2018-12-04 11:39:52.858382: I   tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting   unsupported operation: Exp
2018-12-04 11:39:52.862297: I   tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39]   Before general graph transformations: 309 operators, 499 arrays (0 quantized)
2018-12-04 11:39:53.124941: F   tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:785]   Check failed: crops_data[1] == 0 (5 vs. 0)
Coredump
*************************************************************************************************************

*model summarize is below:(trained environmental is keras. and I converted .json & .h5 to .pb)**********
$   bazel-bin/tensorflow/tools/graph_transforms/summarize_graph   
 --in_graph=(path)/ssd1.pb

Found 1 possible inputs: (name=input_13,   type=float(1), shape=[?,300,300,3])
No variables spotted.
Found 1 possible outputs:   (name=predictions/concat, op=ConcatV2)
Found 25824093 (25.82M) const   parameters, 0 (0) variable parameters, and 0 control_edges
Op types used: 189 Const, 73 Identity,   33 BiasAdd, 31 Conv2D, 21 Relu, 19 Pack, 19 StridedSlice, 19 Shape, 13   Reshape, 10 Prod, 6 Tile, 6 ExpandDims, 5 MaxPool, 4 ConcatV2, 2 Sum, 2 Mul,   2 MatMul, 1 RealDiv, 1 Sub, 1 Square, 1 SpaceToBatchND, 1 Exp, 1 Rsqrt, 1   Placeholder, 1 Pad, 1 BatchToSpaceND, 1 Mean, 1 Maximum, 1 Max
To use with   tensorflow/tools/benchmark:benchmark_model try these arguments:
bazel run   tensorflow/tools/benchmark:benchmark_model --   
--graph=(path)/ssd1.pb --show_flops   --input_layer=input_13 
--input_layer_type=float   --input_layer_shape=-1,300,300,3 --output_layer=predictions/concat
**************************************************************************************************************

How can I convert pb file to tflite?
or Is Tensorflow lite unsupported this problem still now?
or process of convert pb file has something wrong?
    (because I feel that the number of output_layer is small...)"
523,27909,0,"ModelCheckpoint callback error. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 7.3.4
- GPU model and memory: GTX 2080 TI

**Describe the current behavior**
Using tf.keras, when I fit the model with train dataset and validation dataset created from the tf.dataset, and use ModelCheckpoint with default policy, an error showed up. The error seemed to happen because the callbacks tried to save the model with the same name twice, one at the end of each training epoch, one at the end of each validation epoch twice(using validation dataset). This should not happen because I think the source code already set overwrite = true, but it still happened.

The error information:



**Describe the expected behavior**
At least the error should not happen when saving model with the same name twice. Better if we can explicitly control when a model is saved.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
58,33911,1,"tf.keras (tf 2) much slower than tf 1.14. I try to train a simple dense feedforward network consisting of:

- input layer of size 8
- 3 hidden layers, each with 40 neurons and ReLU activations
- output layer of size 9

for a regression.

The performance of keras using tensorflow 1.14 is about 3 times faster than using a freshly installed tensorflow 2.0.0... Is this to be expected, or might this be a bug?
I am training on a CPU.
"
333,11332,0,"[feature request] Inconsistency of new Decoder api and Dense layer. TF 1.1 has released new seq2seq API ( etc.) and Decoders have  parameter, which must be subclass of .

If one wants to use dense projection, it has to do  which seems a bit inconvenient. 

Maybe the better way is to include  and  classes into ? After that, one could write just  or ."
549,15447,0,"[cmake] CPU only build error in tf_stream_executor.cmake. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: N/A, building with cmake
- **Bazel version (if compiling from source)**: N/A, building with cmake
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
cd tensorflow/contrib/cmake
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=""../bin"" ..

### Describe the problem
cmake build fails when 
option(tensorflow_ENABLE_GPU ""Enable GPU support"" OFF)
with following error:
.../tensorflow-master/tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory
compilation terminated.

### Source code / logs
Pretty sure that the issue lies in this commit:
https://github.com/tensorflow/tensorflow/commit/f1582cf82f06810900ee99870f5d5d3a7478d044#diff-1d799fa350437420218e5e5aa680c481

in CMakeLists.txt the line
""  include_directories(${tensorflow_source_dir}/third_party/gpus)""
is still under tensorflow_ENABLE_GPU

Which is why dso_loader cannot find cuda_config.h

On the other hand, I suppose it should not use this include at all in the CPU mode."
126,21761,1,"[XLA] code generation for ARM NEON produce very slow code with tensorflow 1.9.0 and 1.10.0. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Cortex A7
- **TensorFlow installed from (source or binary)**: from source
- **TensorFlow version (use command below)**: v1.9.0 and superior
- **Python version**: 3.4
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 4.9.4
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
tfcompile --graph=network.pb --config=network.config.pbtxt  --cpp_class=Network --out_header=network.h --out_function_object=network.o --xla_enable_fast_math=true --target_triple=armv7a-none-android --target_features=+neon --entry_point=run_network

arm-linux-androideabi-g++ -shared -o test.so network.o

(details of network.pb and network.config.pbtxt are not relevant, The only thing I can say is that it contains 2D Convolution and Dense layer)

### Describe the problem
I use XLA AOT (tfcompile) to compile network down to shared library with tfcompile and it was working great until v1.9.0.
Starting from v1.9.0, I see huge speed penalty (more than 15x slowdown) when executing the network on ARM v7 + NEON. After some investigation, **I think it is pretty clear that v1.9.0 and v1.10.0 do not use specialized neon runtime for matmul and conv** (even when +neon is specified in target_features) as shown in the following objdumps:

- ** With tensorflow v1.8.0 and previous versions
objdump -CT test.so               

test.so:     file format elf32-little

DYNAMIC SYMBOL TABLE:
00000000      DF *UND*	00000000  LIBC        __cxa_finalize
00000000      DF *UND*	00000000  LIBC        __cxa_atexit
00000000      D  *UND*	00000000              **__xla_cpu_runtime_EigenConvF32**
00000000      D  *UND*	00000000              **__xla_cpu_runtime_EigenMatMulF32**
000003c0 g    DF .text	0000abc0  Base        run_network
00672004 g    D  *ABS*	00000000  Base        __bss_start
00672004 g    D  *ABS*	00000000  Base        _end
00672004 g    D  *ABS*	00000000  Base        _edata

- ** With tensorflow v1.9.0 and superior:

objdump -CT test.so 

test.so:     file format elf32-little

DYNAMIC SYMBOL TABLE:
00000000      DF *UND*	00000000  LIBC        __cxa_finalize
00000000      DF *UND*	00000000  LIBC        __cxa_atexit
00000320 g    DF .text	0000cb40  Base        run_network
00674004 g    D  *ABS*	00000000  Base        __bss_start
00674004 g    D  *ABS*	00000000  Base        _end
00674004 g    D  *ABS*	00000000  Base        _edata

I tried to follow the code generation process but got lost in LLVM code generation.
Could you have a look at the problem, please? Any help will be helpful.

### Source code / logs
Running code just call run_network function in the shared library with the correct parameters."
850,16540,0,"//tensorflow/python:nn_test fails with AssertionError. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 s390x
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel test -c opt //tensorflow/python:nn_test 

### Describe the problem
While running , 2 sub tests (,), fail on s390x with : 


The test passes if the [tolerance](https://github.com/tensorflow/tensorflow/blob/v1.4.1/tensorflow/python/ops/nn_test.py#L865) is slightly increased:


Is it ok to create a PR with this change? Could you please share your thoughts on this.

### Source code / logs
======================================================================

"
824,4107,0," While training first TensorFlow neural net model. I build tensorflow for Mac OSX from sources following the Download and Setup guide from tensorflow.org. 



Created the pip package and installed it with NO GPU support. 



When attempting to Train first neural net model from the root of the source tree:


"
1389,8871,0,"text_classification.py doesn't works .  ....

  File ""C:\Users\danie\Anaconda3\lib\site-packages\tensorflow\contrib\layers\python\ops\sparse_feature_cross_op.py"", line 21, in <module>
    from tensorflow.contrib.framework import deprecated_arg_values
    ImportError: cannot import name 'deprecated_arg_values'

I'm using Spider.



  "
1233,6449,0,"Using OpenCL on macOS.  Thank you very much for developing nice framework. I have a question about OpenCL support for macOS. Is there no way to run tensorflow with GPU on recent MacBookPro?  Tensorflow support OpenCL. But I think this is for Linux, not for macOS. Tensorflow + OpenCL need ComputeCpp. But ComputeCpp compiler do not support macOS(I couldn't find ComputeCpp binary for mac). Do you have a plot to support recent MacBook pro's GPU? "
1190,8033,0,"Direct access to Tensor Buffers in C++ interface. I would like to request direct access to the tensorflow buffers through the C++ interface.  I have commented previously about this on [StackOverflow](http://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying), and have gotten it to work by exposing the  class.  This required some fiddling around with the source code.

My specific use case is using tensorflow from a [ROS](http://www.ros.org/) (robot operating system) node.  ROS has its own build system, so I have to link to the tensorflow library (libtensorflow.so) externally.  The C++ interface is much more convenient than the C interface, as I only load and do forward inference on static graphs.

Because I get an OpenCV array as input, my only other option is to iterate the entire array and copy it to a newly allocated tensor buffer as suggested in [this post](http://stackoverflow.com/questions/36044197/how-do-i-pass-an-opencv-mat-into-a-c-tensorflow-graph).  In my case, copying the buffer by iterating it can take 25-75ms, whereas simply pointing to the memory already allocated by OpenCV incurs almost no overhead.  I am doing all of this in real time as part of a tight control loop, so this extra time is absolutely critical.

I realize this patch is probably not the right way to expose the interface, and my example code provides no memory checking or safeguards, but it is an example (diff against the r1.0 tensorflow tag):



and a small snippet of code to use it:
"
811,8617,0,"Error when using lookup table from tf.contrib.keras. I'm trying to build a simple model that includes string input and lookup in a lookup table, and embedding.

I get an error that appears to caused by using non keras layers in a keras model

the code:

the error:


using tensorflow on macos, built from source revision: 88998663605ca3b728d6eec298717c8a8558639f"
1396,16606,0,"tf.contrib.data.rejection_resample not balancing class/freq on random crops. randomly sampling / cropping data seems to break rejection_resample ==> meaning it won't do any re balancing of the class probability -- see these two simple feeders as example:

The first randomly samples data with tf.random_uniform and breaks rejection_resample -- the second with static random data having  the same distribution is correctly resampled (output has p(class=0)=0)

This creates issues when trying to real-time sample/crop/data-augment and at the same time rebalance classes with on the same pipeline

`python
def get_data_breaks(self, batch_size, iihook, this_set='train'):
    def sample(data, label): # sample detection window inside chunk
        xx = tf.cast(tf.random_uniform([1])*self.class_num, tf.int32)[0]
        with tf.control_dependencies([xx]): tf.Print(xx , [xx], 'xx>>')
        return xx, xx

    initial_dist=[1.0/self.class_num for cc in range(self.class_num)]
    classes = np.random.choice(self.class_num,20000,p=initial_dist)

    data_ph = tf.placeholder(classes.dtype, classes.shape)
    labels_ph = tf.placeholder(classes.dtype, classes.shape)
    dataset = tf.data.Dataset.from_tensor_slices((data_ph, labels_ph))

    dataset = dataset.map(sample, num_parallel_calls=1)

    target_dist=[1.0/self.class_num for cc in range(self.class_num)]
    target_dist[1]+=target_dist[0] ; target_dist[0]=0
    print('target-dist>>', target_dist)
    initial_dist = None

    dataset = dataset.apply(tf.contrib.data.rejection_resample(
                class_func=lambda c, _: c,
                target_dist=target_dist,
                initial_dist=initial_dist,
                seed=42)).map(lambda a,b: b)

    dataset = dataset.repeat(None)
    iterator = dataset.make_initializable_iterator()
    iihook.iterator_initializer_func = lambda sess: sess.run(iterator.initializer,
                    feed_dict={data_ph: classes, labels_ph: classes})

    return iterator.get_next()


def get_data_works(self, batch_size, iihook, this_set='train'):

    initial_dist=[1.0/self.class_num for cc in range(self.class_num)]
    classes = np.random.choice(self.class_num,20000,p=initial_dist)

    data_ph = tf.placeholder(classes.dtype, classes.shape)
    labels_ph = tf.placeholder(classes.dtype, classes.shape)
    dataset = tf.data.Dataset.from_tensor_slices((data_ph, labels_ph))

    target_dist=[1.0/self.class_num for cc in range(self.class_num)]
    target_dist[1]+=target_dist[0] ; target_dist[0]=0
    print('target-dist>>', target_dist)
    initial_dist = None

    dataset = dataset.apply(tf.contrib.data.rejection_resample(
                class_func=lambda c, _: c,
                target_dist=target_dist,
                initial_dist=initial_dist,
                seed=42)).map(lambda a,b: b)

    dataset = dataset.repeat(None)
    iterator = dataset.make_initializable_iterator()
    iihook.iterator_initializer_func = lambda sess: sess.run(iterator.initializer,
                    feed_dict={data_ph: classes, labels_ph: classes})

    return iterator.get_next()
"
622,24205,0,"Hard arbitrary limit on Saved model size. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Both
- TensorFlow version (use command below): 1.10+
- Python version: N/A
- Bazel version (if compiling from source): 17.2
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
I am trying to load a 1.4GiB binary saved model and am getting an error saying that the protobuf message is too big i.e. over 1GiB

**Describe the expected behavior**
Current default limits for protobuf messages loaded by a CodedInputStream are 2GiB so my model should load without error.

**Code to reproduce the issue**

Comment out line 503 in env.cc:
//coded_stream.SetTotalBytesLimit(1024LL << 20, 512LL << 20);
rebuild and everything works as expected. 

**Other info / logs**

Also the second parameter to SetTotalBytesLimit has been deprecated and is no longer enforced. 
"
5,26635,1,"Transformer step/sec decrease over time to 0. **System information**
**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**
Yes
**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**
Ubuntu 18.04
**- TensorFlow installed from (source or binary):**
Binary / pip install
**- TensorFlow version**
1.13.1
**- Python version:**
3.6.7
**- CUDA/cuDNN version:**
CUDA = 10
CUDNN_VERSION 7.5
**- GPU model and memory:**
8X V100 16 GB

(We observed same behavior with CUDA 9.2 & TF 1.12 compiled for CUDA 9.2.)
[Docker Image](https://gitlab.com/nvidia/cuda/blob/ubuntu18.04/9.2/devel/cudnn7/Dockerfile)

**Problem:** 
Training steps/s while using a Transformer model repeatedly drops from 20 steps/s to <1 step/s.  
This is internally reproducible. 
GPU usage plummets to ~0% during the periods at <1 step/s

**Context:**
We train with a [Transformer model](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py#L175).

Training behaves poorly with [Transformer](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py#L175), but works well with [Universal Transformer](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py#L41).

In both scenarios, we use 4x P100, subword tokens (https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py#L448), and [MirroredStrategy Enabled](https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/MirroredStrategy) distribution strategy. 

(We observed the same behavior with 8x V100, as well.)

Ablation tests on 1) P100 versus V100, 2) MirroredStrategy versus t2t’s built-in multi-GPU, and 3) Universal Transformer versus Transformer reveal that #3 is the driving variable.

We’re using tensor2tensor’s transformer / universal_transformer implementations. 
The hparams used are transformer_tiny and universal_transformer_tiny. One noteworthy deviation from common usage is that our hparams.max_length value is large (2500) and our batch size is often small, since our median sequence length is 750 tokens.

**Describe the current behaviour**
With Transformer, training run’s step/sec alternates between roughly 20 step/sec and 0.5 step/sec.

For the first 3-4 hours it is biased towards 20 steps/sec. For the next is ~2 it hours it  begins dropping to 0.5 step/sec more frequently, before finally dropping almost exclusively to 0.5 step/sec.

If we restart our training process from a checkpoint that was made when the model ran slowly, the behaviour repeats itself, starting at 20 step/sec before dropping back down to 0.5. 

Below are two graphs The first is a graph of our model’s step/sec degradation over a training run. The median value early on is ~20 (with some occasional drops to below 5 step/sec) but as the training continues our performance drops to almost 0 step/sec.
Note the vertical axis is logarithmic
![image](https://user-images.githubusercontent.com/43351375/54249496-c64c5880-4516-11e9-86ad-3a6d6f1c940b.png)

 The second graph shows 3.25 consecutive runs of our model, where each restart picks up a checkpoint the previous run generated. These restarts were not caused an error, but are due to our system automatically preempting gpu intensive jobs after 24 hours. Note the consistent degradation in performance after every restart.
![image](https://user-images.githubusercontent.com/43351375/54249582-0b708a80-4517-11e9-9287-663c54799d00.png)



**Describe the expected behavior**

Our runs with universal transformer exhibit a completely flat step/sec curve. The figure below shows the expected behaviour in red in terms of step/sec variance. Note that the model’s step/sec exhibit almost no variation except for the sharp drops attributed to evaluation steps.
![Transformer vs Universal Transformer Step_Sec Decrease (2)](https://user-images.githubusercontent.com/43351375/54287868-dc8cff80-457c-11e9-8c64-52dbfa98c833.png)

**Other info / logs**
Our GPU utilization (as measured by nvidia-smi) is tightly coupled with the above graph. Where our step/sec is high, our gpu utilization is nearly always at 50%, occasionally dropping to 0 for a second or two before shooting back up. When our step/sec consistently drops to below one, our gpu utilization is mostly at 0%. Every few minutes it will briefly shoot up to 50% and then drop to 0% a second later.

In terms of auc performance, our transformer model continues to improve even as the step/sec decay. 

Note this is a sibling issue to: https://github.com/tensorflow/tensor2tensor/issues/1484"
897,20722,0,"TFSA-2018-001 commit link is invalid. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2018-001.md

Links to:
https://github.com/tensorflow/tensorflow/commit/49f73c55d56edffebde4bca4a407ad69c1cae4333c55

Which is 404. Can you please update this with the correct commit? Thanks!"
901,18555,0,"[Feature Request] Inverse Functions: Auto-Solve similar to Auto-Grad?. Would it be possible to implement some type of automatic equation solving?
E.g. f(x, y) = z => tf.solve(y) = f'(x, z)

Functions like tf.sigmoid have known inverse functions which could be used to solve functions if all other parameters are known.

I'm thinking of something like simplified SymPy solvers:
http://docs.sympy.org/latest/modules/solvers/solvers.html

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: None
- **TensorFlow installed from (source or binary)**: None
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

"
51,23136,1,"Estimators Siamese Model Peformance Issue . tf.esitmators producing poor performance vs TF & Keras.  

* System information
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
    TensorFlow installed from (source or binary):
    pip3 install --upgrade tensorflow-gpu==1.12.0-rc1
    TensorFlow version: 1.12.0-rc1
    Python version: Python 3.6.5 
    Installed using virtualenv? pip? conda?: pip3
    CUDA/cuDNN version: 9.0 / 7.0.5
    GPU model and memory: nvidia gtx 1050 (Lenovo Laptop)

* Code to reproduce performance issue:



With Keras model.fit(), this yields:

With TF this yields:

With tf.estimators this yields:


The code for the benchmark code for Keras and TF can be found here:
+ Keras:
https://gist.github.com/imranparuk/25963eb3b2db1540ead684271de6f5a8
+ TF:
https://gist.github.com/imranparuk/4fe48323a006ff030bf2037136db7868

"
1232,24808,0,"InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]          [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]. Wenn  I ran following the bashscript, I got the following the errors. I want to train my dataset using the transferlearning of Inception-v3.
How do I go about correcting this? Thank you very much.

Bashcript:
python3 train_image_classifier.py \
 --train_dir=satellite/train_dir \
 --dataset_name=satellite \
 --dataset_split_name=train \
 --dataset_dir=satellite/data \
 --model_name=inception_v3 \
 --checkpoint_path=satellite/pretrained/inception_v3.ckpt \
 --checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \
 --trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \
 --max_number_of_steps=100000 \
 --batch_size =32 \
--learning_rate=0.001 \
--learning_rate_decay_type=fixed \
--save_summaries_secs=2 \
-log_every_n_steps=10 \
--optimizer=rmsprop \
--weight_decay=0.00004

Errors:
Caused by op 'save/Assign_31', defined at:
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 593, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 583, in main
    init_fn=_get_init_fn(),
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 377, in _get_init_fn
    ignore_missing_vars=FLAGS.ignore_missing_vars)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\variables.py"", line 695, in assign_from_checkpoint_fn
    write_version=saver_pb2.SaverDef.V1)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1284, in __init__
    self.build()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 781, in _build_internal
    restore_sequentially, reshape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 422, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 113, in restore
    self.op.get_shape().is_fully_defined())
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\state_ops.py"", line 219, in assign
    validate_shape=validate_shape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_state_ops.py"", line 63, in assign
    use_locking=use_locking, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]
         [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]
         [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 593, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 589, in main
    sync_optimizer=optimizer if FLAGS.sync_replicas else None)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 748, in train
    master, start_standard_services=False, config=session_config) as sess:
  File ""C:\ProgramData\Anaconda3\lib\contextlib.py"", line 112, in __enter__
    return next(self.gen)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 1005, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 833, in stop
    ignore_live_threads=ignore_live_threads)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 994, in managed_session
    start_standard_services=start_standard_services)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 731, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\session_manager.py"", line 289, in prepare_session
    init_fn(sess)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\variables.py"", line 697, in callback
    saver.restore(session, model_path)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1752, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]
         [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]

Caused by op 'save/Assign_31', defined at:
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 593, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 583, in main
    init_fn=_get_init_fn(),
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 377, in _get_init_fn
    ignore_missing_vars=FLAGS.ignore_missing_vars)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\variables.py"", line 695, in assign_from_checkpoint_fn
    write_version=saver_pb2.SaverDef.V1)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1284, in __init__
    self.build()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 781, in _build_internal
    restore_sequentially, reshape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 422, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 113, in restore
    self.op.get_shape().is_fully_defined())
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\state_ops.py"", line 219, in assign
    validate_shape=validate_shape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_state_ops.py"", line 63, in assign
    use_locking=use_locking, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]
         [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]

"
1478,30798,0,"slim. Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
405,17334,0,"Still Seeing AVX Warnings on TF 1.6 Docker Image. ### System information
floatnp.floatingnp.float64 == np.dtype(float).typefloatnp.floatingnp.float64 == np.dtype(float).type
Image: 

### Describe the problem
I am seeing this warning message even though I thought that AVX instructions should be used in 1.6:

This is the same warning I saw on 1.5."
1292,8054,0,"summarize_graph doesn't recognize VariableV2 . NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None
### Environment info
Operating System:
Windows 10

Installed version of CUDA and cuDNN: 
(please attach the output of ):
None

If installed from source, provide 

1. The commit hash ()
8cac382a5425d64f3083cb5adec525baa163e18e
2. The output of 
compiled by cmake

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?

"
152,29996,1,"TF2 - apparent memory leak when running dataset ops eagerly. **System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX
- TensorFlow installed from (source or binary): 2.0.0beta
- TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0
- Python version: 3.6.8

**Describe the current behavior**
When using the function , I see a memory leak which I don't see if I use the annotation 

**Describe the expected behavior**
There should not be a memory leak.

**Code to reproduce the issue**

"
751,19721,0,"UnicodeDecodeError while loading trained model through import_meta_graph function.. 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Windows 10 Home, but using online Jupyter Notebook environment for coding purposes

- **TensorFlow version (use command below)**:

Version : 1.8.0
- **Python version**: 

3.6.3
- **Bazel version (if compiling from source)**:

N/A
- **GCC/Compiler version (if compiling from source)**:

N/A
- **CUDA/cuDNN version**:

N/A
- **GPU model and memory**:

Tesla K80, 11.5 GB RAM
- **Exact command to reproduce**:

new_saver = tf.train.import_meta_graph('model-21-epochs.meta')


### Describe the problem

I trained a text classification model on an online Jupyter Notebook, and saved models at each epoch to evaluate the best performing one. On the notebook itself, after the training session is over, running the tf.train.import_meta_graph() function seemed to work fine. 

**I then downloaded all 3 files : the .meta file, the .data-00000-of-00001 file and the .index file** and on running it locally on my machine, I get a UnicodeDecodeError like this : 


The online notebook only stores files on it for a certain amount of time, and after that they are deleted. So I uploaded the meta file on the notebook again, to see if the problem persisted on a new session ( session on the notebook, not used in the tensorflow 'session' lingo ) in the notebook. And it gave me the same error then too. 

An issue almost identical to this one has been raised fairly recently in #19573 but there hasn't been an update at all. And this was asked on StackOverflow too, to which the response was that it might be a bug  and github was the place for it, as can be seen in #19573. 

For reference, this is the entire inference portion of the code:

  

So is it the case that saved models and meta graphs only be restored from the directory it was saved to during the training phase? Is it impossible to run inference on a pre-trained model downloaded from an online source?"
837,17809,0,"Wrong object and switch . Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
889,8723,0,"Disturbing Grammatical error.... Grammatical error with this sentence...
""In this lab, we will be using transfer learning, which means we are starting with a model that has been already trained on another problem""
Correction: ""has already been trained on another problem""


"
1446,28429,0,"Profile tab in tensorboard keeps saying ""Processing datasets"" . <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1, 7.5
- GPU model and memory: 3x GTX 1080Ti, 11Gb

**Describe the current behavior**
I was training a tf.keras and had a tensorboard callback, but i don't seem to get the new profiling feature to work.



**Code to reproduce the issue**

**Other info / logs**
There are the logs from tensorboard

"
1242,31200,0,"illegal instruction. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
553,15771,0,"Flexible input shape for map method in class RandomFourierFeatureMapper. This is a feature request. In tensorflow r1.4, Class RandomFourierFeatureMapper, map method,
The shape of the input must be, [batch_size, self._output_dim].   

There can be scenarios where in each batch there are multiple training point, essentially i'm proposing a scenario where the input shape  must be  [batch_size, x ,self._output_dim].   

Which is not possible in current API.        
In the matmul we can see that it is mentioned, ""The inputs must, following any transpositions, be tensors of rank >= 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match."" which is in-fact very flexible.     

"
72,5785,1,"tensorflow label_image recognize so slow. I already asked [this question](http://stackoverflow.com/questions/40705203/tensorflow-label-image-recognize-so-slow?noredirect=1#comment68687966_40705203) on stackoverflow but It seems no one knows.

would you mind giving advices to make to code run faster.
Cause It took 5 second on laptop (30 seconds on pi 2) to recognize one picture of letter  



"
143,21788,1,"tf.keras.utils.multi_gpu_model does use only one GPU. Tensorflow 1.10
Docker container 1.10.0-gpu-py3 from https://hub.docker.com/r/tensorflow/tensorflow/
I think this is a bug. I expect tf.keras.utils.multi_gpu_model to use multiple GPUs. However with the following example it uses only 1 (checking with nvidia-smi):







See also https://stackoverflow.com/questions/51962659/tf-keras-utils-multi-gpu-model-does-use-only-one-gpu
"
1085,34559,0,"Gradient doesn't exist/ becomes None for conv_transpose layers. **System information**
- Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I get a warning 
 
when trying to backprop loss for a specific layer in my model. The gradients keep turning out to be None regardless of optimizer.


**Describe the expected behavior**
The gradients should exist for these layers.


**Code to reproduce the issue**
This is the model in question:


And then running this code is what results in the error:

Changing SGD() to Adam() yields the same warning. Is it a problem with how the model is defined? Repeatedly running the above code yields the same warning (with/without reinitialising the optimizer).

**Other info / logs**
None at the moment.
"
774,27861,0,"Tensorflow 2.0 keras load_model does not restore step / epoch counters. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see example below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0 alpha 0 - gpu
- Python version: 3.5.3
- CUDA/cuDNN version: Cuda 10
- GPU model and memory: GeForce 960M 

**Describe the current behavior**
I train a keras model and let it be saved by the . After the training is finished, I want to continue it from the saved checkpoint. Therefore, I load the model with  and run  again. Unfortunately, it seems that the steps/epochs counters are not restored. 

**Describe the expected behavior**
The steps /epochs counters should be restored to allow continuation of training. If they are not restored, e.g. tensorboard cannot be used properly, since the new training will write its values to the same steps as the first training, instead of appending them. The result looks like this: 

![Unbenannt](https://user-images.githubusercontent.com/9267365/56139393-12b50900-5f99-11e9-9c4e-b553ab2c1589.PNG)


**Code to reproduce the issue**
Run the following code multiple times and have a look at the tensorboard results by running .


"
909,76,0,"cuDNN v2 (6.5) not available anymore. I don't see how I can run TensorFlow with cuDNN: cuDNN v2 (6.5) doesn't seem to be available for download from nVidia anymore, but it's the only version actually supported by TensorFlow.
"
137,3862,1,"tf.cumprod's gradient produces nans given zeros. Example:



@ibab Do you know the right way to fix this?
"
345,15914,0,"How to know my loss function does not have numerical problems?. I wrote the following loss function that I post below. How do I know that I don't have any kind of numerical issues with it? (because something tells me that I do)

"
921,33174,0,"TF2.0 OOM error training imagenet with vgg, fine when eager execution off. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0
- Python version: 3.7
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: V10.0.130
- GPU model and memory: GeForce RTX 2080ti, 10G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
Getting OOM error when training vgg16 on imagenet with batch_size=256. Was able to get away with OOM error with batch_size=8 or turning off eager execution.
**Describe the expected behavior**
The exact same code was running fine with tf1.14 with no memory problem
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
579,23859,0,"Android Bazel Build Error . **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): source
- TensorFlow version: master (5c60fb7e9b90d6641c2b5848773ef49956ed54e3)
- Python version: Python 2.7.15rc1
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): gcc 7.3.0
- CUDA/cuDNN version:
- GPU model and memory:

I am trying to build  so I can use some new ops (cos specifically) in my graph, but am getting build errors:

NDK: r17c
SDK: 28


**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. 
1. 
1. 
1. 

**Any other info / logs**
I have tried with a variety of NDK versions (18b, 14b) and get different build errors each time. I also tried with  and got build errors. I am unsure which cpu architecture to target for the Pixel.
"
526,6287,0,"Windows cuptiActivityRegisterCallbacks not found. ### Environment info
Operating System:
Windows 10 64bit
Python 3.5.2
nVidia GTX 760 (compute capability 3.0)

Installed version of CUDA and cuDNN: 
CUDA 8.0
cuDNN 5.1

A link to the pip package you installed:
Latest nightly build (at least latest I could find)

http://ci.tensorflow.org/job/nightly-win/DEVICE=gpu,OS=windows/lastSuccessfulBuild/artifact/*zip*/archive.zip

2. The output from .
0.12.head

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

This is the complete code:



### What other attempted solutions have you tried?
Works OK:
tf.RunOptions.SOFTWARE_TRACE

Don't work:
tf.RunOptions.HARDWARE_TRACE
tf.RunOptions.FULL_TRACE

### Logs or other output that would be helpful



I suppressed ""I"" messages with ""TF_CPP_MIN_LOG_LEVEL = 1"". I don't know if I'm missing something but it seems like it's trying to load libcupti.so from ""c:\tf_jenkins\..."" path witch doesn't exist. I do have:
""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin""
""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\libnvvp""
in path environment variable. Program works OK. It just gives me an error when I try to trace execution.

I don't know if I messed up something or is this a bug."
773,28365,0,"Retrain image detection with MobileNet for use in TFjs, failure in tensorflowjs_converter. It is unclear whether this is a documentation or a programming issue. If misfiled, please say so and suggest whether a new issue should be raised, or what else can be done.

**System information**
- TensorFlow version:
 ('v1.13.1-0-g6612da8951', '1.13.1') (see #27538)
('v1.12.0-9492-g2c319fb415', '2.0.0-alpha0') (see #27539) and
tf-nightly-2.0-preview (see https://stackoverflow.com/questions/55682557/)

- Doc Link:
https://www.tensorflow.org/hub/tutorials/image_retraining and
https://www.tensorflow.org/tutorials/images/hub_with_keras

**Describe the documentation issue**
Both documents mention but do not describe conversion to mobile models. Yet, conversion using  failed in every attempt. See https://stackoverflow.com/questions/55849309/retrain-image-detection-with-mobilenet, which mentions two attempts by me as well as questions by others with the same problem.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

As soon as it is working, I would gladly fix and/or augment the docs.  I have not gotten **** to **work on a retrained MobileNet model**, despite the docs stating that it should work, [e.g.](https://www.tensorflow.org/tutorials/images/hub_with_keras#export_your_model):

> This saved model can loaded for inference later, or converted to TFLite or TFjs."
613,34290,0,"Brackets in directory for tf.train.CheckpointManager in TF2.0. 
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian (GCP notebook)**
- TensorFlow installed from (source or binary): **preinstalled on GCP notebook**
- TensorFlow version (use command below): **2.0.0**
- Python version: **3.5.3**
- GPU model and memory: **None**

**Describe the current behavior**
This problem concerns tf.train.CheckpointManager and tf.train.Checkpoint in TF 2.0. If the checkpoint directory contains square brackets ( or ), then loading checkpoints with tf.train.Checkpoint.restore fails. CheckpointManager will save and track checkpoints as expected but does not remove them according to .

I see that square brackets are [not recommended](https://cloud.google.com/storage/docs/naming) for Google Cloud Storage blobs, however this happens for both checkpoints stored in Google Cloud Storage and checkpoints stored locally.

As an example:
Valid checkpoints exist in  Calling

Gives the error:


**Describe the expected behavior**

- Allow square brackets in the checkpoint path
or
- fail at CheckpointManager creation if  parameter contains disallowed characters 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**[https://gist.github.com/klanderson/be46cc3e3f7e6575bd2a45450c0ac102](https://gist.github.com/klanderson/be46cc3e3f7e6575bd2a45450c0ac102)
Code works as-is. Add a bracket somewhere in the path in  to see error**



"
980,29260,0,"[TF 2.0 API Docs] tf.greater_equal. ## URL(s) with the issue:

https://www.tensorflow.org/versions/master/api_docs/python/tf/math/greater_equal

## Description of issue (what needs changing):

Correct link is not provided in the sense that it is only a text and not an actual link to the file.
No usage example is given in the documentation.
Raises are also not listed

### Correct links

Correct link is not provided in the sense that it is only a text and not an actual link to the file.

### Raises listed and defined

Raises are also not listed

### Usage example

No usage example is given in the documentation."
985,1636,0,"dynamic_rnn got  an error. I want to test dynamic_rnn based on ptb_word_lm.py.

when I use rnn,it's ok!



But,when I use dynamic_rnn,like this:



It's got an error:


"
136,4885,1,"After some iterations my accuracy has decrease to 0, but the loss and cross entropy are not nan.. Hello
I can't understand this question:
I can trained my net in a correct result in beginning but after about 4K iterations ,the accuracy suddenly decrease to zero, and the loss and cross entropy suddenly increased to a high value, it is like this curves:
train accuracy:
![image](https://cloud.githubusercontent.com/assets/5306116/19257677/2f14d1ec-8fa4-11e6-9f31-50255f2cef0e.png)
val accuracy:
![image](https://cloud.githubusercontent.com/assets/5306116/19257685/3c3b26f0-8fa4-11e6-854f-3d5b459158f6.png)
cross entropy:
![image](https://cloud.githubusercontent.com/assets/5306116/19257709/54f385de-8fa4-11e6-88c7-d5e90af88f94.png)
regular loss:
![image](https://cloud.githubusercontent.com/assets/5306116/19257716/676f5f1c-8fa4-11e6-8b87-79d92f35202d.png)

I think you can reproduce it in this link(3D Convolution, UCF101):
https://github.com/hx173149/C3D-tensorflow

thanks~
"
262,34277,1,"C++ API producing incorrect model metaparams. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.0 and 1.15
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

An autoencoder model consisting only of standard keras Dense layers is converted into a tflite model. This model can be loaded and inspected with the Python API. The output there is consistent with the output from the visualize.py script.

When loading the very same model with the C++ API, I get ridicolous large results for the number of inputs/outputs/nodes. 

The C++ functions that were used to inspect the model are:

This produces the following output:


The code to create the tflite model to inspect can be found on my repo (https://github.com/DocDriven/tflite-cpp-api-tests). All relevant files are named . 

I suspect the C++ API to be broken at some point, as the models seem to be fine. Same results for TF 1.x and TF2.0. Trying different models yields the exact same ridicolous values, independently from their size.
"
484,22510,0,"Tensorflow lite aborts due to ""pure virtual method called"" on Raspberry Pi. Hello.
I compiled the library as described in
https://www.tensorflow.org/lite/rpi

When running

or



it crashes with the following output:


I tried to debug the problem myself: it occurs in a secondary thread.
Some function that is called from 
 
gets the this parameter equal to NULL.

The host is a raspberry pi 3B+, with raspbian.
I tried both to cross-compile the code and to compile natively.
I tried more versions: one is r1.11
Gcc version: 4.9.2

Does anyone else have the same problem or is it just me?
Thank you.
"
120,3320,1,"CPU vs GPU Performance. I am working on a reinforcement learning model problem.  I have been working to get the model creation running faster and bumped into a strange issue I cannot explain.  It runs much faster ~50% or so on the CPU vs the GPU.  This was unexpected and I have disabled the GPU using **""export CUDA_VISIBLE_DEVICES=-1""** so the learning runs faster.  I have been looking at upgrading my GTX 950, but not sure it makes sense if I don't get a speed improvement.

I ran a profile based upon #1824 and got the following trace files for a single "".run()"" iteration. I am not sure how to read this, but the GPU iteration took over 10ms where the CPU alone is <10ms. I am running the HEAD of TensorFlow (reports 0.9.0) on Ubuntu 15.10 with CUDA 7.5 and cuDNN 4 & 5 (tried both). The CPU is a dual XEON 6 core, 2.66 GHz  processors (24 threads total) with 72 GB or RAM (DDR3).

I have a GTX 950 GPU.  I can't tell if this performance difference is related to the structure of the graph or simply the data set isn't big enough to get a benefit from the GPU given the IO overhead?  I have tested the GPU with TF on a basic ""matmut()"" of a 7000x7000 matrix and it beats the CPU hands down by orders of magnitude.  So I known it is installed correctly.

Then networks runs in this case a Batch of 200 x 189 into 5 layers with Dropout() between each layer. The layers are 140, 120, 100, 80, and 3 as the output.  Any advice or things to try would be much appreciated.

**CPU Timeline:**
![cpu timeline](https://cloud.githubusercontent.com/assets/18412448/16854147/ad4c8760-49dd-11e6-8919-2c0940322b53.png)

**GPU Timeline:**
![gpu timeline](https://cloud.githubusercontent.com/assets/18412448/16854181/cfa25844-49dd-11e6-9f20-e2268bdd4299.png)

NOTE: This issue was posted to #2444 but may have got lost.  It might be related to RL performance but feel it is a separate issue.

I have enclosed the TIMELINE trace files for more detail.  If it is of value, I can create a ""tensorboard"" log file so you can review the model in detail.
[GPU Slowdown.zip](https://github.com/tensorflow/tensorflow/files/364706/GPU.Slowdown.zip)
"
1122,20825,0,"TFLite --- TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(""MobilenetV1/Conv2d_0/BatchNorm/beta:0"", shape=(32,), dtype=float32). we can get mobilenet tflite&pb(Mobilenet_1.0_224) from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md

when I use the command:
bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/mobilenet_v1_1.0_224_frozen.pb --input_checkpoint=/path/to/mobilenet_v1_1.0_224.ckpt --output_graph=/path/to/freezed_mobilenet.pb --output_node_names=MobilenetV1/Predictions/Reshape_1 --input_binary=true

error occured:
/home/leve/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from  to  is deprecated. In future, it will be treated as .
  from ._conv import register_converters as _register_converters
2018-07-16 11:04:48.613520: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-07-16 11:04:48.728926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-16 11:04:48.729264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.31GiB
2018-07-16 11:04:48.729276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-07-16 11:04:48.873004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-16 11:04:48.873030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-07-16 11:04:48.873035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-07-16 11:04:48.873192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5086 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 382, in <module>
    run_main()
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 379, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 378, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 272, in main
    flags.saved_model_tags, checkpoint_version)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 254, in freeze_graph
    checkpoint_version=checkpoint_version)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 128, in freeze_graph_with_def_protos
    var_list=var_list, write_version=checkpoint_version)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 813, in _build_internal
    saveables = self._ValidateAndSliceInputs(names_to_saveables)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 714, in _ValidateAndSliceInputs
    variable)
**TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(""MobilenetV1/Conv2d_0/BatchNorm/beta:0"", shape=(32,), dtype=float32)**

### System information
- **Linux Ubuntu 16.04**:
- **TensorFlow installed from source**:
- **TensorFlow version 1.8.0**:
- **Python version:3.6**: 
- **Bazel version 0.11.1**:
- **GCC/Compiler version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) **:
- **CUDA/cuDNN 9.0/7.0.5**:
- **GPU  1060-6G**:
- **Exact command to reproduce**:"
205,3974,1,"Regression in Performance between r0.9.0 and r0.10.0rc0.. ### Environment info

Operating System:


### Observations

I am testing on a Tesla K80 (details below) using the following lines:
- 
- 

On r0.0.9, I get: ~.
On r0.10.0rc0 I get ~.

Here are the device stats:



I have the: Nvidia Driver Version: 367.35
"
69,31377,1,"Quantized SSD TFLite runs much slower in GPU (GTX 1080) when compared to its original model. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): NO
- TensorFlow version (use command below): 1.14
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/7.5
- GPU model and memory: GTX 1080 TI

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
I converted the SSD into the TFLite model with quantization and ran a speed comparison between the quantized SSD and the original SSD. The time used for each image: 
quantized SSD - 110 ms 
original SSD - 12 ms
the quantized SSD is almost ten times slower than the original one in GPU. 
Some additional info:
SSD is based on the mobilenet_v1, no other costumed operations. 
Codes used to do the conversion:
tflite_convert \
  --output_file=./ssd.tflite \
  --graph_def_file=./tflite_graph.pb \
  --inference_type=FLOAT \
  --input_arrays=normalized_input_image_tensor \
  --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
  --std_dev_values=128 --mean_values=128 \
  --default_ranges_min=-6 --default_ranges_max=6 \
  --input_shapes=1,300,300,3 \
  --allow_custom_ops

The two models are in the link:
https://drive.google.com/drive/folders/1Q5aOZRlxhDsMdiSYYSuWlUyQP82FJTdC?usp=sharing

Anyone can give me some suggestion?  Thank you!

"
1430,19104,0,"Illegal instruction (core dumped). I have install Tensorflow via using this commands:
sudo apt-get install python3-pip python3-dev  :->  this for the 'pip' installation
pip3 install tensorflow   :-> this for a tensorflow installation

I am using ubantu 16.04 64 bit version Configuration as below :-
![screenshot from 2018-05-05 10-30-59](https://user-images.githubusercontent.com/30696388/39659867-076002a2-5050-11e8-8ee3-361881b92e3b.png)


still when as importing the tensorflow in python3 it gives ""Illegal instruction (core dumped)"" error
as :-1: 
![screenshot from 2018-05-05 10-37-31](https://user-images.githubusercontent.com/30696388/39659879-55f635da-5050-11e8-8fb4-03b0fce0571b.png)

"
1047,13331,0,"Passing CPU value along with GPU tensor. Hello, i'm trying to pass a CPU value or a block of values from one custom op into another along with a GPU tensor but the framework seems to be converting everything to GPU tensor. There doesn't seem to be a mechanism for passing mixed GPU+CPU op results right now."
101,28890,1,"XLA_GPU_JIT Slows Down GNMT (when large clusters are formed). **System information**
- Have I written custom code: Use models from NVIDIA [OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq)
- OS Platform and Distribution:  Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): upstream-base-779-g83909d2 1.13.1
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 5.4.0 20160609
- CUDA/cuDNN version: 10.0
- GPU model and memory: NVIDIA GV100

**Code to reproduce the issue**
Note that let's assume a context where the deadness analysis is disabled for forming large clusters. In such scenarios, XLA_GPU_JIT slows down this reported GNMT implementation by around 30%.

Reproduction steps:

1. Download the training dataset from the following link. Assuming putting it under /wmt16/
https://drive.google.com/open?id=1ooQiWhmzmYsk2qMOfaunjTlx_z6lcUyO
2. 
3. 
4.  ([file here](https://github.com/tensorflow/tensorflow/files/3200298/gnmt_config.txt)). Assuming the dataset is placed under /wmt16/.
5. Run command:

6. Remove  to run with native Tensorflow.

**Observed slowdown**
The performance measured on my GV100 (1 GPU):
With XLA_JIT:
    Avg time per step: 1.000s
With native Tensorflow:
    Avg time per step: 0.760s

This is around 30% slowdown, comparing XLA_JIT to native TF.

**A Theory of why such a slowdown**
My theory of why XLA slows down is as follows. Ideally, the TF runtime (host) should be able to run ahead of the device, so that the host overhead can be hidden. To achieve that, the TF runtime usually executes the dataflow ops in parallel on multiple CPUs to feed computation to one GPU stream. For example, when ops related to an LSTM cell are executed, the tf.while ops (e.g., Merge/Switch, etc.) could be executed in parallel, so that the loop related latencies are hidden. This, however, is not the case observed in XLA_JIT on the forward path of the GNMT.

A possible explanation of why the host does not run ahead is a result of interaction between host-device synchronization needed by tf.while and long-latency _XlaRun ops. The tf.while ops involve synchronization between host and device as they need to copy some computation results back to the host to make decisions about when to exit the loop. For example, a typical op sequence of tf.while is Add, Less, LoopCond, and Switch; the result of Add is on device but TF computes Less, LoopCond, and Switch on host. These synchronization latencies are better hidden in pure TF executions, as there are more parallel ops and each op is of lower latency. In contrast, scheduling a long-latency op such as _XlaRun along with the op sequence of tf.while on the same GPU stream can introduce extra dependency (established through the runtime scheduling order on the single GPU stream) and force the host to wait for the completion of this long-latency op even if they have no dependencies in the dataflow graph. Often, this is the case observed.

**Other info**
Further reference for script options:
https://nvidia.github.io/OpenSeq2Seq/html/machine-translation.html


"
89,21698,1,"INT TFLITE very much slower than FLOAT TFLITE. Hi Guys. I am using Mobilenet 0.25,128. I used the pretrained models provided in the repo for obtaining the int tflite and float tflite models for the same [found here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). I am trying to infer some images. Using the imagenet images and some other test images as well, the FLOAT TFLITE is faster than the INT TFLITE (FLOAT TFLITE takes roughly 3-4 milliseconds while INT one takes 8-9 ms). Any suggestions as to why this is happening ?I am running the inferences using the tflite interpreter following the documentation [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#interpreter). This issue happens with the tf-nightly builds as well as the normal tensorflow. Tried on both, and also on both python 2 as well as 3.
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:Binary
- **TensorFlow version (use command below)**:1.10 (tf-nightly) 
- **Python version**:2
- **Bazel version (if compiling from source)**
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: No
- **GPU model and memory**: No



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs

# ####INT TFLITE CODE####


# ####FLOAT TFLITE CODE####
"
336,5551,0,"Inconsistent behavior for variables_collections and outputs_collections parameters (contrib/layers). Using layers from contrib,
- The outputs_collections parameter accepts either a string or list of strings.
- The variables_collections parameter requires a list of strings.

Granted, the documentation does specify this

But I don't think it's very obvious.  Furthermore, is it really necessary to have these two parameters behave differently?

The variables_collections parameter is passed to the model_variable function()
Currently, the model_variable() function performs this before creating the variable:

So, if



Whereas, for outputs_collections it uses the utils.collect_named_outputs() function, wherein


Perhaps the behavior of these two parameters should be unified?"
344,33611,0,"An error occurred in build  quantized model to lite . tensorflow 1.14.0

when I build quantized SSD MobileNetV2 model, haapen error.  reference documentation: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md





But I used  --default_ranges_min= and --default_ranges_max= ,   model canit detection in predict.
"
1363,14116,0,"py_func cannot handle Chinese string correctly. tensorflow: 1.3.0

I write a simple code to split and concatenate utf8 string. However, I found that only English string works well on python 2.7. 

### script:


### logs

#### python: 2.7

success for English, failed for Chinese.



#### python 3.5.2

both failed.

"
1090,6650,0,"`histogram_fixed_width` type errors and GPU issues. With the following script:



Gives the following error:



I believe the issue here is this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/histogram_ops.py#L75

It should say  instead of . However, even if you fix this, you then get this error: 



I ran this both with the TF version from  as well as when building from source with  and got this error. However, the source makes it seem like this should work: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/segment_reduction_ops.cc#L323
"
2,27143,1,"[TF 2.0] tf 2 around 40 times slower than tf 1 for unrolled taylor series. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13.1
- Python version: Python 3.6.7
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: GeForce GTX 1060, Compute Capability 6.1, 6GB RAM

This issue follows on from https://github.com/tensorflow/tensorflow/issues/26807

**Describe the current behavior**

The code linked at the bottom calculates a matrix exponential () using a [taylor series](https://en.wikipedia.org/wiki/Taylor_series#Exponential_function). It outputs the following when running with the latest TF1 and TF2 installations from pip:



I've included some profiling information for the function of interest ( in TF 2) which shows that it spends all the time in .

**Describe the expected behavior**

I expect TF 1  (which uses ) and TF 2  (which uses ) to have similar performance.

**Code to reproduce the issue**


"
625,33863,0,"Build breaks: The value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name).. TF 2.0.0 fails to be built by bazel-1.1.0:



Both bazel and TF are of the latest versions. What's going on?"
335,27769,0,"[TF 2.0 keras] Unable save and load weights for double nested models. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7

**Describe the current behavior**
load_weights throw exception on a doubly nested model

**Describe the expected behavior**
load_weights should work

This problem only happens on two+ layers of nested model with non-trainable weights.
The reason is save_weights and load_weights handles nested model differently
save_weights -> call layer.weights for each layer
load_weights -> recursively call model.weights if layer is a nested Model

**Code to reproduce the issue**


**Other info / logs**
This bug is also reported on upstream keras https://github.com/keras-team/keras/pull/11847
Here is a detailed analysis on why this is happening https://github.com/keras-team/keras/pull/11847#issuecomment-482438283

Full Exception
"
395,11544,0,"PyImport_Import crash while using bazel to build the project. I recently ran into a problem, while using bazel to build a project.  This project is compiled as a dynamic linking library, using PyImport_Import to import python module. when there is ""import tensorflow as tf"" in the python file , application who calls the dynamic linking library crashed everytime, but when it‘s not there ,everything works just fine. where is the problem?
my tensorflow version is 1.0.0,python 2.7.0,bazel 0.4.3


here is the console information when the application crashes:

> F tensorflow/core/framework/function.cc:1015] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for Softmax


here is the test python file looks like: 


here is the source code of .so file:


and here is the dynamic linking library part of my BUILD file



"
1482,16109,0,"R1.4 restore a model from r0.8 encounter NotFoundError (see above for traceback): Tensor name . When using tf1.4 to restore a model from tf0.8, I met a NotFoundError, the related code as flow:

ema = tf.train.ExponentialMovingAverage(1.0)
saver = tf.train.Saver(ema.variables_to_restore())
model_checkpoint_path='./model_check_point/model-20160506.ckpt-500000'
saver.restore(sess, model_checkpoint_path)

The error as flow:
NotFoundError (see above for traceback): Tensor name ""incept3a/in3_conv5x5_8/batch_norm/moments/Squeeze/ExponentialMovingAverage"" not found in checkpoint files ./model_check_point/model-20160506.ckpt-500000
	 [[Node: save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_46/tensor_names, save/RestoreV2_46/shape_and_slices)]]
	 [[Node: save/RestoreV2_315/_35 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_702_save/RestoreV2_315"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

How can I solve this problem?"
385,35108,0,"tf.py_function is unusable in map function. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7
- CUDA/cuDNN version: 10.0/7
- GPU model and memory: Tesla T4, 16gb


**Describe the current behavior**
Attempting to use  inside a . Failure occurs because the  tensor returned from py_function is accessed during build time of the graph, but function is expected to be evaluated during runtime of the graph. 

**Attempt 1**

Error: 


**Attempt 2**

Error

**Attempt 3**

Error
tf.Tensortf.Tensortf.Tensortf.Tensor"
1036,5631,0,"tf.Print is not an identity op w/ tuples or lists of Tensors; clarify in docs or add warning msg?. **Documentation issue with bidirectional_dynamic_rnn:**

For ""outputs"":

""...It returns a tuple instead of a single concatenated Tensor, unlike in the bidirectional_rnn. If the concatenated one is preferred, the forward and backward outputs can be concatenated as tf.concat(2, outputs).""

This doesn't appear to work.  As a fix:

output, _ = tf.nn.bidirectional_dynamic_rnn(...)
output=tf.concat(2, tf.unpack(output))

...this appears to be necessary because tf.concat does not view the returned tuple as equivalent to a list of Tensors, but rather appears to treat the tuple (effectively) as a Tensor.

**Possible bug in tf.concat:**

Unclear whether tf.concat(2, output) actually should, in fact, return as the initial documentation for bidirectional_dynamic_rnn suggested.
"
419,32074,0,"TFLite build for rpi broken. **System information**
- OS Platform and Distribution (e.g., Linux Debian):
- TensorFlow installed from (source or binary): github
- TensorFlow version: head
- Installed using virtualenv? pip? conda?: docker tensorflow/tensorflow:nightly-devel
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 7.3.0

Followed https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi



Eventually this fails with:

tflite::benchmark::BenchmarkTfLiteModel::LogParams()':                                      
benchmark_tflite_model.cc:(.text+0x1164): undefined reference to 

"
787,24987,0,"Running in google colab and got the errror:  'tensorflow.python.framework.ops.EagerTensor' has no len(). <em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: '1.12.0'
- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/eager/custom_training_walkthrough.ipynb

**Describe the documentation issue**
By running on google CoLab this notebook ( https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/eager/custom_training_walkthrough.ipynb#scrollTo=iDuG94H-C122 ), I have received the following error

![image](https://user-images.githubusercontent.com/19335547/51309762-17445100-1a80-11e9-8199-195b3d460e2f.png)

If one converts the tensors to numpy, it works normally.



**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
74,35483,1,"High percentage of CPU usage in ssdlite_mobilenet model . Hi all,
I trained a model for license plate detection with ssdlite_mobilenet_v2_coco (training images:20000,steps:200000) and this model is going to be used in a traffic control app.

The accuracy of this model on frames of video is very good but the percentage of CPU usage on my system is between 70-80% (CPU info : Intel Core i7-6500 @ 2.50GHz.) and given that traffic control app should be constantly running on client system, this percentage is quite high.

So what are the best ways to optimize CPU usage to analyze frames of video and detect license plate? (reduce to ~30-40%)
Do I need to make any changes to the config file or use another pre-trained model?
Please guide me :)"
291,17203,0,"Modify distributed TF examples to take kubeflow's TF_CONFIG as well as command line arguments.. Currently most of the examples under  require command line arguments to be passed in to construct the cluster spec.

The tensorflow operator is standardizing under a representation of the cluster spec within a variable called , which looks like this:



It would be ideal of the examples were modified to take this variable so we can reduce the shimming required in Kubeflow related efforts, without breaking backward compatibility and keeping the command line args."
867,19063,0,"slim.conv2d_transpose  has no output_shape. slim.conv2d_transpose  has no output_shape param, So I can't control the output_shape"
754,14387,0,"tensorflow-gpu looks for the wrong driver version. ### System information
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.4
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**: GTX 1060 
- **GPU driver version**: 387.12
- **Exact command to reproduce**: 



### Problem
It looks for the wrong version of libnvidia-fatbinaryloader.so.xxx.xx. 


"
610,14221,0,"bayeslfow.hmc - provide option for skipping the MH step. Currently  returns directly the update x and value of the potential after the Metropolis-Hasting steps. It would be useful to have an option to omitting the MH step. This, for instance, is required for implementing HVI [1], where we want to propagate gradients through the HMC step and not reject any samples:

[1] https://arxiv.org/pdf/1410.6460.pdf
"
1274,33788,0,"TFLiteConverter from_keras_model TypeError: call() got an unexpected keyword argument 'training'. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Custom code
- Arch Linux Kernel 5.3.7
- TensorFlow installed via pip
- TensorFlow version v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7.4

**Describe the current behavior**
When trying to convert a Keras CNN to TFlite file I get an error at the following line: 

**Describe the expected behavior**
A TFlite file is expected to be created and written to the local directory.

**Code to reproduce the issue**


**Other info / logs**

> Traceback (most recent call last):
  File ""/home/user/code/classifier.py"", line 98, in <module>
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 383, in from_keras_model
    concrete_func = func.get_concrete_function()
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 776, in get_concrete_function
    self._initialize(args, kwargs, add_initializers_to=initializer_map)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py"", line 143, in _wrapped_model
    outputs_list = nest.flatten(model(inputs=inputs, training=False))
  File ""/usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py"", line 75, in symbolic_fn_wrapper
    return func(*args, **kwargs)
  File ""/usr/lib/python3.7/site-packages/keras/engine/base_layer.py"", line 489, in __call__
    output = self.call(inputs, **kwargs)
TypeError: call() got an unexpected keyword argument 'training'
[Finished in 666.7s with exit code 1]"
1210,33077,0,"TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution ( Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.14
- Python version:2.7
- Bazel version (if compiling from source):0.19
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0  7
- GPU model and memory: V100 32G

You can collect some of this information using our environment capture

NGC images 1904

**Describe the current behavior**
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
**Describe the expected behavior**

**Code to reproduce the issue**

def _train(total_loss, global_step):
  """"""Train CIFAR-10 model.

  Create an optimizer and apply to all trainable variables. Add moving
  average for all trainable variables.

  Args:
    total_loss: Total loss from loss().
    global_step: Integer Variable counting the number of training steps
      processed.
  Returns:
    train_op: op for training.
  """"""
  # Variables that affect learning rate.
  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size
  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)

  # Decay the learning rate exponentially based on the number of steps.
  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,
                                  global_step,
                                  decay_steps,
                                  LEARNING_RATE_DECAY_FACTOR,
                                  staircase=True)
  tf.summary.scalar('learning_rate', lr)

  # Generate moving averages of all losses and associated summaries.
  #loss_averages_op = _add_loss_summaries(total_loss)
  loss_averages_op = total_loss

  # Compute gradients.
  with tf.control_dependencies([loss_averages_op]):
    opt = tf.train.GradientDescentOptimizer(lr)
    opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)
    grads = opt.compute_gradients(total_loss)

  # Apply gradients.
  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)
  #apply_gradient_op=opt.minimize(total_loss)
  #apply_gradient_op = tf.train.GradientDescentOptimizer(lr).minimize(total_loss,global_step=global_step)

  # Add histograms for trainable variables.
  for var in tf.trainable_variables():
    tf.summary.histogram(var.op.name, var)

  # Add histograms for gradients.
  #for grad, var in grads:
  #  if grad is not None:
  #    tf.summary.histogram(var.op.name + '/gradients', grad)

  # Track the moving averages of all trainable variables.
  variable_averages = tf.train.ExponentialMovingAverage(
      MOVING_AVERAGE_DECAY, global_step)
  with tf.control_dependencies([apply_gradient_op]):
    variables_averages_op = variable_averages.apply(tf.trainable_variables())

  return variables_averages_op
**Other info / logs**

Traceback (most recent call last):
  File ""cifar_for_common_cnn.py"", line 566, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""cifar_for_common_cnn.py"", line 562, in main
    train()
  File ""cifar_for_common_cnn.py"", line 495, in train
    train_op = _train(loss, global_step)
  File ""cifar_for_common_cnn.py"", line 421, in _train
    grads = opt.compute_gradients(total_loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/experimental/loss_scale_optimizer.py"", line 116, in compute_gradients
    loss = self._scale_loss(loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/experimental/loss_scale_optimizer.py"", line 134, in _scale_loss
    return loss * loss_scale
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 897, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 1180, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 6490, in mul
    ""Mul"", x=x, y=y, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 563, in _apply_op_helper
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
"
1340,33851,0,"TF lite Gpu delegate  E/libEGL: call to OpenGL ES API with no current context (logged once per thread). **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Fllow this document 
[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md)
and using this project in android studio.
https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) : virtual device Pixel 2 in Android Studio
- TensorFlow installed from (source or binary):java package
- TensorFlow version (use command below):   
org.tensorflow:tensorflow-lite:0.0.0-nightly
org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly
- Python version:---
- Bazel version (if compiling from source):---
- GCC/Compiler version (if compiling from source):---
- CUDA/cuDNN version:CUDA 10.0
- GPU model and memory: gpu 1080


**Describe the current behavior**
My retrain ssd_mobilenet_v2 model with my  own datasets--called detect.tflite 

Input shape


Output shape



It could run object detect app using   projects, Just modify path to tflite and labelmap.

However we want to Using Gpu delegate like  https://www.tensorflow.org/lite/performance/gpu.

so I just using mobile_ssd_v2_float_coco.tflite which download from https://www.tensorflow.org/lite/performance/gpu .




But report this error in Android Studio when using virtual devices pixel 2 to Debug.
 

If I use my detect.tflite, error is 



And I modify TFLiteObjectDetectionAPIModel.java to use Gpu Delegate:




**Describe the expected behavior**

1.Why the mobile_ssd_v2_float_coco.tflite input shape and output shape is different from the model  retrained using object detection api ? 
2.The code  I modifyied  TFLiteObjectDetectionAPIModel.java to using Gpu delegate  is right?


"
487,7680,0,"CUDA_ERROR_OUT_OF_MEMORY with tf.contrib.learn (basic linear regression model). ### Environment info
Operating System: Ubuntu 16.04,  GeForce GTX TITAN X

Installed version of CUDA and cuDNN: 
(please attach the output of ):
ls /usr/local/cuda-8.0/lib64/libcud*
/usr/local/cuda-8.0/lib64/libcudadevrt.a
/usr/local/cuda-8.0/lib64/libcudart.so
/usr/local/cuda-8.0/lib64/libcudart.so.8.0
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudnn.so
/usr/local/cuda-8.0/lib64/libcudnn.so.5
/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10
/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
/usr/local/cuda-8.0/lib64/libcudnn_static.a

### If installed from binary pip package, provide:

1) Installed with pip tensorflow-gpu
2) ~ python -c ""import tensorflow; print(tensorflow.__version__)
1.0.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Using the provided tf.contrib.learn model in get_started/get_started documentation


The Core model works fine.


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

"
514,47200,0,"How can I get min/max information from TFlite Intepreter?. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.4.1
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
I can see the min/max information by using netron open .tflite file. I need those two information to calculate radix. However, I cannot get min/max from any API provided in TFLite Intepreter. I guess to use get_tensor_details(). However I can only get the scale and zero point. Is there any way I can access min/max?

**Will this change the current api? How?**
Yes, adding one or two items in get_tensor_details()
**Who will benefit with this feature?**
Anyone that wants to calculate radix from quantized models in TFLite
**Any Other info.**
"
1279,29155,0,"when i use conv algorithm. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Debian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13.1,1.14.0，2.0.0a0，1.9.0
- Python version:3.7,3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:Cuda 10/7.5.0,7.4.2,7.4.1,7.4.0,Cuda 9 can't support 2060
- GPU model and memory:rtx 2060 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
conda activate base
2.0.0-alpha0
2019-05-29 19:58:31.543654: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-05-29 19:58:31.557759: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-05-29 19:58:31.670769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1009] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-05-29 19:58:31.672348: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55c9b57a0a60 executing computations on platform CUDA. Devices:
2019-05-29 19:58:31.672364: I tensorflow/compiler/xla/service/service.cc:169] StreamExecutor device (0): Graphics Device, Compute Capability 7.5
2019-05-29 19:58:31.693201: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-05-29 19:58:31.693619: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55c9b580db30 executing computations on platform Host. Devices:
2019-05-29 19:58:31.693637: I tensorflow/compiler/xla/service/service.cc:169] StreamExecutor device (0): , 
2019-05-29 19:58:31.693791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:
name: Graphics Device major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:01:00.0
totalMemory: 5.76GiB freeMemory: 5.17GiB
2019-05-29 19:58:31.693804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-05-29 19:58:31.693844: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-05-29 19:58:31.694402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-29 19:58:31.694413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021] 0
2019-05-29 19:58:31.694419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0: N
2019-05-29 19:58:31.694516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4990 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:01:00.0, compute capability: 7.5)
Number of training examples: 60000
Number of test examples: 10000
Epoch 1/5
2019-05-29 19:58:32.786674: W ./tensorflow/core/framework/model.h:202] Encountered a stop event that was not preceded by a start event.
2019-05-29 19:58:36.114172: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-05-29 19:58:36.288348: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-05-29 19:58:36.934479: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-05-29 19:58:36.945020: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-05-29 19:58:36.945127: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
[[{{node conv2d/Conv2D}}]]

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
112,22763,1,"AdamWOptimizer and learning rate decay. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**1.11:
- **Python version**:3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I think I have found that the contrib.opt.AdamWOptimizer and associated decoupled weight decay optimizers do not function correctly when using learning rate decay without explicitly applying the decay to the weight_decay parameter. 

In the original paper, as seen in algorithm 2, the schedule multiplier is factored out and applied to the whole expression, the current interface means you have to do  to achieve parity with the paper, this is not in its self an issue, but I think the documentation should reflect this difference. Alternatively the API could give a schedule multiplier parameter and then fixed lr and wd parameters used. 

Happy to submit a PR if someone can advise whether a documentation or interface update is the desired approach.

"
1395,30808,0,"Serialization of keras object fails if called with different input sizes. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.1-6461-gc6352706d6 1.14.0
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: Nvidia Geforce GTX 1080 Ti, 11 GB

**Describe the current behavior**
When I try to save a function of tf.Module as saved_model that calls another function with different input shapes, it fails with the following error:



**Describe the expected behavior**
The model can be saved successfully.

**Code to reproduce the issue**
The following testcase allows to reproduce the issue:

"
71,34456,1,"AsyncResult hangs in unexpected cases in fit_generator. **System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: pip
- TensorFlow version: 2.0.0b1
- Python version: 3.6.8
- CUDA/cuDNN version: V10.0.130
- GPU model and memory: Quadro P5000 (16GB)

**Describe the current behavior**
I have a very complicated model solving an image-to-image problem. I also use a custom callback which at some point generates some noise using .
When I use  on this model, it manages to do the first epoch, then on the second, third or fourth it hangs at the beginning of the epoch. I managed to see where the problem was happening, and it happens here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/data_utils.py#L875
Basically, if I put a timeout on the second  it times out after a few successful epochs (sometimes just one). There is no error thrown out so I don't know why it hangs. Furthermore, if I debug at that point in code, I can just execute the function synchronously and everything will work just fine.

**Describe the expected behavior**
I would like  to complete even when I use my custom callback.

**Code to reproduce the issue**
I didn't manage to get a minimal example using  (basically it relies too much on me using my model which is complex). However, I have a minimal example which reproduces the bug when I mimic the [](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_generator.py#L41) function.
You need to install the following to make it work: 


If you leave it as such, it will hang after about 1, 2, 3, or 4 iterations.
You can comment out the  line and see that it doesn't hang.

You can also modify tensorflow's source code and timeout [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/data_utils.py#L875) in the second  so you can debug.

Note also that if the sleeping time is not high enough, hanging doesn't appear.

I am still working on a minimal example involving  directly, but to me this example is enough to understand what's happening.
"
150,5417,1,"1% GPU usage and slow training times after unknown DSO update?. Tensorflow with CUDA was working fine. Cross-entropy loss was going to zero (CIFAR-10 and a very simple CNN), but training error stayed around the same as random chance. I mention this in case it may be related to my issue.

In between runs, not 5 minutes after it was just working, I mysteriously got this error message about my video driver: 



I never updated anything, so that's strange. I update my video driver using  and the Ubuntu ppa repo, then restart. I didn't check what version the video driver was before the update but I assume it was 361.42?

The error message is gone, but **training is now an order of magnitude slower with ~1% GPU usage and ~5% CPU usage**.  indicates 90% memory usage, which means the model is in memory, but what before took ~0.4 seconds per batch is now taking 4+ seconds per batch.

I tried re-installing CUDA and cuDNN from official sources but no change.
CUDA bandwidth test and deviceQuery results normal.

Ubuntu 16.04
Tensorflow 0.10.0 (built from source w/ CC 3.5 and 5.0)
nvidia Quadro K2200
CUDA 8.0
cuDNN 5.1.5
Python 2.7"
1167,20398,0,"Why dense layer cannot be speed up in tf.contrib.trt ?. ------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  2.7.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

When using tf.contrib.trt.create_inference_graph, I meet the following error, it seems that dense layer is not support because the input tensor is not rank 4 ? Why has this request on dense layer ?

> subgraph conversion error for subgraph_index:9 due to: ""Unimplemented: Require 4 dimensional input. Got 2 dense/MatMul"" SKIPPING

### Source code / logs

Need not to code, clear above ...

"
868,4516,0,"Interface improvement suggestions for tf.tranpose and tf.slice. I would like to suggest the following improvements for the interfaces of  and .
1.  currently accepts a boolean mask that selects the dimensions to be reversed. AFAIK this is the only op in Tensorflow that uses a mask for this purpose, whereas other, e.g. , take an axis number or a list of axis numbers. I think  could also be changed accordingly.
2.  signature is , and it is very unusual for Python that the slices are defined by their size, not by their end. Also, there is a  object in Python, which would be a very natural argument for . 
"
944,13761,0,"tf.reduce_sum gives value error when given int64 as input.. Passing a tensor of dtype=int64 into tf.reduce sum I receive the following error:

Tensor(""loss/diff:0"", shape=(50,), dtype=int64)
ValueError: Invalid type tf.int64 for loss/Sum:0, expected: [tf.float32, tf.float64, tf.float16].

According to the documents from https://www.tensorflow.org/api_docs/python/tf/reduce_sum:

input_tensor: The tensor to reduce. Should have numeric type.

As int64 is a numeric type I am not sure what's wrong. 

To create the diff tensor I do:



Where predictions is of type int64 and my targets placeholder is also of type int64.

Is this a tensorflow error or an error on my end?
"
588,19679,0,"Build fails if Nvidia nccl doc files (NCCL-SLA.txt) are relocated. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Slackware 14.2+ (-current)
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:

- **Python version**: 
2.7.15
- **Bazel version (if compiling from source)**:
0.13.1- (@non-git)
- **GCC/Compiler version (if compiling from source)**:
gcc (GCC) 7.3.0
- **CUDA/cuDNN version**:
CUDA 9.2/cuDNN 7.1
- **GPU model and memory**:
Titan X Pascal 16 GB
- **Exact command to reproduce**:
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Tensorflow build process with nccl enabled is too ""picky"" about location of Nvidia NCLL doc file(s) - for ex. NCCL-SLA.txt.
It expects to find the text file(s) in the root of the nccl install dir (in my case ) and the build fails if I relocate the txt file(s) (to for ex. a  dir in the nccl install dir (for ex. )

Would be great if the build process would also look for the file(s) in subdirs of the  install directory. This would also make it possible to install nccl in a prefix such as  and put the docs in . Not a big deal though, considering there are always more important issues to worry about.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


"
334,4255,0,"iOS memory warnings. I tried posting this to SO first and it's been there a week with no activity: https://stackoverflow.com/questions/39255211/tensorflow-ios-memory-warnings

We are building an iOS app to perform image classification using the TensorFlow library.

Using our machine learning model (91MB, 400 classes) and the TensorFlow 'simple' example, we get memory warnings on any iOS device with 1GB of RAM. 2GB models do not experience any warnings, while < 1GB models completely run out of memory and crash the app.

We are using the latest TensorFlow code from the master branch that includes [this iOS memory performance commit](https://github.com/tensorflow/tensorflow/commit/459c2fed498530b794c4871892fd68d1e6834ac6), which we thought might help but didn't.

We have also tried setting various GPU options on our TF session object, including  and .

Our only changes to the TF 'simple' example code is a  and  of 299, and an  and  of 128.

Any thoughts? Is our model simply too big?
"
294,20375,0,"Feature Request: CheckpointSaverHook should allow writing graph in binary mode. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
First, I found that the **MonitoredTrainingSession** is slow to start due to saving the  file when the graph is huge. I could see that if it was saved in a binary format (), it would be much faster (and also smaller).

Now, I see that **MonitoredTrainingSession** utilizes the **CheckpointSaverHook** to save the graph. However, I see no way to make **CheckpointSaverHook** save in binary format. 

See: https://github.com/tensorflow/tensorflow/blob/51ddb66cdf1444998827e03c4b5f592841ee6255/tensorflow/python/training/basic_session_run_hooks.py#L433

Should **CheckpointSaverHook** has an option to save in binary format?

PS. I also doubt the usefulness of  file where the  file is also present.
"
952,32907,0,"tensorflow c sdk crash when called by a java program. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Red Hat 4.8.5-4**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **tensorflow-gpu-linux-x86_64-1.13.1**
- Python version: **No**
- CUDA/cuDNN version: **cuda 10.0**
- GPU model and memory:  **NVIDIA Tesla v100 GPU 16GB**
- JAVA version: 


**Describe the current behavior**

I write a dynamic library with the tensorflow c sdk which can be called normally by a c++ program but when I wrap it as a jni and call it from a java program, it will crash:


What's the reason for that? Thanks.

**Code to reproduce the issue**

A sample class like [here](https://gist.github.com/knsong/4f39a467cb43d55e6176ffefed27863c), which was instantiated globally.  When called by the java program, the code will crash at this [line](https://gist.github.com/knsong/4f39a467cb43d55e6176ffefed27863c#file-sample-code-L142)
"
182,23903,1,"Poor performance when scaling up data pipeline in multi CPU environments. **System information**
AWS Deep Learning AMI (ubuntu 16.04, tensorflow 12) on AWS p3.8 and p3.16 instances.

**Describe the current behavior**
I am trying to optimize the speed of my code when training on Imagenet and started by looking at the data pipeline. I am using TFRecordDataset(), and use both prefetch() and num_parallel_calls for mappings. Here I just load in the batches without performing any action on them.

I seem to get reasonably good results on AWS p3.8 (32 vCPU, 4 Nvidia Tesla V100 GPUs), although it's still a bit below the ~3000 images/second that I've seen mentioned elsewhere. However, if I run exactly the same code on AWS p3.16 (64 vCPU, 8 GPUs), throughput *decreases* significantly, even while CPU use is higher. 

*The two systems perform as expected on other tasks.*  I did some very elementary benchmarking of both systems:
- calculating the number of primes, spawning a number of threads using multiprocessing.Process(). As expected, performance between p3.8 and p3.16 appears to be identical when n_threads < 32, and 3.16 performs better when n_threads = 128.  
- reading a file with 1,000,000 lines (and repeat 100 times): identical performance

This suggests to me the problem is with the TFRecordDataset, but if there is any other benchmarking that makes sense, please let me know.

Both systems allow two threads for each CPU. I launched another p3.16 instance with a single thread per CPU, so that the number of virtual CPUs is identical to the p3.8 instance. This did not appear to make any difference.

**Describe the expected behavior**
Performance should improve, or at least remain the same, on p3.16 as compared to p3.8, as there are more CPUs available.

**Code to reproduce the issue**
Taking only the data pipeline part and skipping all shuffling, preprocessing, etcetera I am left with this as a minimal working example.

(Note that I use multiple prefetch statements; this significantly improves performance on both systems for me, but removing them does not change their relative performance. Similarly, using 128 parallel calls in the mapping function gives better results than 32 or 64, but changing that or making it dependent on the actual number of vCPUs doesn't solve the observed behaviour.)

    import tensorflow as tf
    import os
    import time

    DATA_FOLDER = '/home/ubuntu/datasets/imagenet-data'
    BATCH_SIZE = 256
    N_TRIALS = 10
    STEPS_PER_TRIAL = 100

    def decode_imagenet(serialized_example):
        feature_map = {
            'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),
            'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1)
        }
        features = tf.parse_single_example(serialized_example, feature_map)

        image = features['image/encoded']
        image = tf.image.decode_jpeg(image, channels=3)
        image = tf.image.convert_image_dtype(image, tf.float32)
        image = tf.image.resize_images(image, [224, 224])

        label = tf.squeeze(tf.cast(features['image/class/label'], tf.int32))
        
        return image, label

    training_files = [os.path.join(DATA_FOLDER, fn) for fn in os.listdir(DATA_FOLDER) if 'train' in fn]

    dataset = tf.data.TFRecordDataset(training_files)
    dataset = dataset.repeat()
    dataset = dataset.prefetch(BATCH_SIZE*16)
    dataset = dataset.map(decode_imagenet, num_parallel_calls=128)
    dataset = dataset.prefetch(BATCH_SIZE*4)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(1)


    iterator = dataset.make_initializable_iterator()

    batch_x, batch_y = iterator.get_next()

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(iterator.initializer)

        res = []
        for trial in range(N_TRIALS):
            t0 = time.time()
            for _ in range(STEPS_PER_TRIAL):
                sess.run([batch_x, batch_y])
            t1 = time.time()
            images_per_second = BATCH_SIZE*STEPS_PER_TRIAL/(t1-t0)
            res.append(images_per_second)
            print('Trial %i: %f ips' % (trial, images_per_second))
        print('Overall average: %f ips' % (sum(res)/N_TRIALS))



**Other info / logs**

Output of above code on p3.8:

    Trial 0: 2687.172262 ips
    Trial 1: 2861.731855 ips
    Trial 2: 2832.192681 ips
    Trial 3: 2826.211587 ips
    Trial 4: 2851.585431 ips
    Trial 5: 2809.069081 ips
    Trial 6: 2869.325024 ips
    Trial 7: 2858.402548 ips
    Trial 8: 2833.678433 ips
    Trial 9: 2859.654147 ips
    Overall average: 2828.902305 ips

Meanwhile, %CPU (observed using the top command) is 2300-2400.

On p3.16:

    Trial 0: 1585.289254 ips
    Trial 1: 2077.237178 ips
    Trial 2: 2165.624391 ips
    Trial 3: 2183.211334 ips
    Trial 4: 2226.638648 ips
    Trial 5: 2217.900892 ips
    Trial 6: 2217.662143 ips
    Trial 7: 2232.781854 ips
    Trial 8: 2178.928816 ips
    Trial 9: 2222.856626 ips
    Overall average: 2130.813114 ips

%CPU is between 3700-4000.

"
1419,2150,0,"rnn_cell.py improvements. ## Motivation

Current implementation of [rnn_cell.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py) does not support custom initialization on a gate and input-to-hidden/hidden-to-hidden level (like setting forgetgate bias to 0 while leaving updategate bias at 1, etc.). Further when debugging the gates in TensorBoard, the matrix is represented as one large matrix, which makes it difficult to see whats happening inside a specific matrix (e.g. hidden-to-hidden) of a specific gate (e.g. forgetgate).

In recurrent neural networks (RNNs) GRU and LSTM uses various gates with separate weights, for both hidden-to-hidden and input-to-hidden, to computing steps in a recurrent sequence.
To optimize computational speed these gates, and their separate weights, are often stacked and computed simultaneous at every step.
In TensorFlows rnn_cell.py the [GRUCell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L123) and [BasicLSTMCell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L153) are implemented using [linear](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L677) to handle weigths and computation hereof.
However, the implementation of _linear_ does not initialize separate matrices for each gate, but initializes the gates, and their input-to-hidden/hidden-to-hidden matrices, as one big matrix for [weights](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L712) and [bias](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L719).
## Proposal

Implementing separate initialization of gates, and their input-to-hidden/hidden-to-hidden matrices, and concatenating these gates. This allows custom initializatio, TensorBoard information on hid_in/hid_hid/bias for every gate and still retains the advantage of weights in a large matrix.
## Implementation

With minimal rewriting of the current structure in [rnn_cell.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py) I implemented a [lasagne-gate](https://lasagne.readthedocs.org/en/latest/modules/layers/recurrent.html#lasagne.layers.Gate) like structure, made a new _linear_ function and made some minor changes to _GRUCell_.
All of these changes should, with minor modifications, work for _BasicLSTM_ as well.

Do notice that code below is for my own purpose, it is not rigorously tested yet.

First, the gate to hold initialization for every weight matrix/bias in a gate (notice it also handles LSTM's by having W_cell)



A modified GRU cell to handle weigths (took only minimal modification to **init** and **call**)



I found _linear_ required the largest amount of rewriting, however I have tried to keep the original structure and functionality intact.

args
## Questions
- Would this be of interest for a PR to _rnn_cell.py_? (given further development of code and _BasicLSTMCell_ implementation)
- General comments/thoughts would be much appreciated
"
564,21644,0,"Not found: FeedInputs: unable to find feed output phase_train. I try to use  Facenet  by   tensorflow C++ API (VS2015),it can load graph,but it doesn't work.
Not found: FeedInputs: unable to find feed output phase_train
code like this:
tensorflow::Tensor input_tensor(DT_FLOAT, TensorShape({ 2 , iHeight, iWidth, depth }));
auto input_tensor_mapped = input_tensor.tensor<float, 4>(); 
...................................................
tensorflow::Tensor phase(DT_BOOL, TensorShape());
	 phase.scalar<int>()() = FALSE;

	input_tensor.shape();
	std::vector<std::pair<std::string, tensorflow::Tensor>> inputs = {
		{ ""input"", input_tensor },                                    //it's OK
		{"" phase_train"",phase }      // it' bad    
	};
 please help me,thansk"
1355,30253,0,"Problem Passing Tensor Attr to Custom Op in Eager Execution Mode. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Windows 10:
- TensorFlow installed from binary:
- TensorFlow version 1.14:
- Python version 3.7:
- CUDA/cuDNN version 10:

I am defining a new custom Op in C++, which takes in a single attribute of type tensor and a single input tensor variable. A stripped version of the Op code is below:

    #include ""tensorflow/core/framework/op.h""
    #include ""tensorflow/core/framework/op_kernel.h""
    
    using namespace tensorflow;
    
    REGISTER_OP(""DoStuff"")
        .Attr(""attr: tensor = { dtype: DT_FLOAT }"")
        .Input(""in: float"")
        .Output(""out: float"");
    
    class DoStuffOp : public OpKernel {
    public:
        explicit DoStuffOp(OpKernelConstruction *context) : OpKernel(context) {
            OP_REQUIRES_OK(context, context->GetAttr(""attr"", &attr_));
            // ...
        }
    
        void Compute(OpKernelContext *context) override {
            // ...
        }
    
    private:
        Tensor attr_;
    };
    
    REGISTER_KERNEL_BUILDER(Name(""DoStuff"").Device(DEVICE_CPU), DoStuffOp);

I can compile the Op into a .so file fine. Now, the following code runs.

    import tensorflow as tf
    dostufflib = tf.load_op_library('build/do_stuff.so')
    sess = tf.InteractiveSession() 

    sample_in = np.random.rand(3,3)
    sample_in_t = tf.convert_to_tensor(sample_in, dtype=np.float32)
    sample_atrr = np.zeros([3,3], dtype=np.float32)
    sample_attr_t = tf.contrib.util.make_tensor_proto(sample_atrr)

    Y = dostufflib.do_stuff(in=sample_in_t, attr=sample_attr_t)

However, if I try to use eager execution mode i.e.

    import tensorflow as tf
    tf.compat.v1.enable_eager_execution()
    dostufflib = tf.load_op_library('build/do_stuff.so')
    
    sample_in = np.random.rand(3,3)
    sample_in_t = tf.convert_to_tensor(sample_in, dtype=np.float32)
    sample_atrr = np.zeros([3,3], dtype=np.float32)
    sample_attr_t = tf.contrib.util.make_tensor_proto(sample_atrr)
    
    Y = dostufflib.do_stuff(in=sample_in_t, attr=sample_attr_t)

I get the following error,

    tensorflow.python.framework.errors_impl.UnimplementedError: Attr sample_locs has unhandled type 6
"
1417,31343,0,"Behavior of tf.data.Dataset when `steps_per_epoch` is set. Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
[tf.data.dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)
[model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)

## Description of issue (what needs changing):
My  does not have  set which means it should go forever. At the end of , does the  shuffle itself? Or does it pick up from where it left off? Or does it reset? 

I couldn't find a clear explanation online from the googling I did. My dataset is about 14 million examples, and the loss seems to be decreasing between epochs (with  set). I'm just worried that it's fitting on the same X samples again and again

It's not entirely clear to me what is happening in the background with "
670,27127,0,"Issue while importing sugartensor --- tensorflow has no attribute core - SOLVED. I am using versions:
Tensorflow 1.13.1
Python 3.6
Sugartensor (0.0.2.4)



"
1404,33721,0,"CancelledError:  [_Derived_]RecvAsync is cancelled. 	 [[{{node Reshape_17/_52}}]] 	 [[GroupCrossDeviceControlEdges_0/RMSprop/RMSprop/Const/_57]] [Op:__inference_distributed_function_24912]. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NaN
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: v10.1
- GPU model and memory: GTX 1060 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
During fitting the data, it gives Cancelled Error in the very first batch
**Describe the expected behavior**
To ""Fit"" the model without error
**Code to reproduce the issue**






 
 

 

 
  

 

 











Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Train on 314860 samples, validate on 78716 samples
Epoch 1/50
   256/314860 [..............................] - ETA: 5:22WARNING:tensorflow:Early stopping conditioned on metric  which is not available. Available metrics are: 

---------------------------------------------------------------------------
CancelledError                            Traceback (most recent call last)
<ipython-input-30-8fb3a6c938b7> in <module>
      1 Model_training=model.fit([X_train,Y_train[:,:-1]], Y_train.reshape(Y_train.shape[0],Y_train.shape[1], 1)[:,1:] 
      2                          ,epochs=50,callbacks=[es],batch_size=256, validation_data=([X_test,Y_test[:,:-1]], 
----> 3                          Y_test.reshape(Y_test.shape[0],Y_test.shape[1], 1)[:,1:]))

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in execution_function(input_fn)
     84     #  translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    485       # In this case we have created variables on the first call, so we run the
    486       # defunned version which is guaranteed to never create variables.
--> 487       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    488     elif self._stateful_fn is not None:
    489       # Release the lock early so that multiple threads can perform the call

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-> 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

C:\ProgramData\Anaconda3\lib\site-packages\six.py in raise_from(value, from_value)

CancelledError:  [_Derived_]RecvAsync is cancelled.
	 [[{{node Reshape_17/_52}}]]
	 [[GroupCrossDeviceControlEdges_0/RMSprop/RMSprop/Const/_57]] [Op:__inference_distributed_function_24912]

Function call stack:
distributed_function"
1102,9113,0,"Asynchronous Training Issue for Distributed Tensorflow. I guess this issue may not be supposed to show up here, and I apologize for opening the ticket here, but really want to be clear about asynchronous training with Distributed Tensorflow. I posted my question on StackOverflow: http://stackoverflow.com/questions/43147435/how-does-asynchronous-training-work-in-distributed-tensorflow, but I got two opposite answers and didn't know which one is the correct. I read the TF docs and example code multiple times, but they're still confused me. So I really appreciate if I could get some official interpretations on asynchronous training."
372,17629,0,"ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. OS Platform and Distribution:
Linux Ubuntu 17.10

TensorFlow installed using pip
TensorFlow version: 1.6, with GPU support
Python Version: 3.6.4
CUDA version: 9.1
GPU model and memory: NVidia GEForce 940MX 2GB
command to reproduce:
~$ python3
>>> import tensorflow as tf
(basically run any tensorflow program to reproduce)

Problem:
Whenever you run a tensorflow program, you get a huge error log, but the main problem is this:

So, the reason this is happening is because TensorFlow wants Cuda 9.0, but I have Cuda 9.1. This problem can be fixed by installing Cuda 9.0, but I have a few requests. Seeing that a couple of people have this problem (see https://github.com/tensorflow/tensorflow/issues/15604, https://github.com/tensorflow/tensorflow/issues/15817, https://github.com/tensorflow/tensorflow/issues/15817), I think that TensorFlow could be updated so that it works with Cuda 9.1 (but I think this issue is only with Ubuntu), or the following could be done:
Update the TensorFlow documentation, saying that you specifically need Cuda 9.0 for TensorFlow 1.6, and Cuda 8.0 for TensorFlow 1.4, and so on
And also, include this in the errors list at https://www.tensorflow.org/install/install_linux#common_installation_problems.

Edit: If a Pull Request is required to update the documentation, I am fine with doing that."
22,4132,1,"Unexpected performance changes as a function of batch size. I am observing unexpected performance from tensorflow as I change the batch size that I feed to the session.

![image](https://cloud.githubusercontent.com/assets/966348/18139518/cfa80cc0-6fa9-11e6-9b7f-20bbfb30ddc9.png)

I have created a [small jupyter](https://gist.github.com/tillahoffmann/63cbe9fce331df75a6b57420c48b7c36) notebook to demonstrate the issue. Errors bars correspond to the standard deviation of the mean over multiple runs.

In some of our more complex models, the jump in runtime occurs at small batch sizes (around 200 images of 40 by 80 pixels).
"
667,22354,0,"ig -v. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
1463,17542,0,"Feature map in tensorflow source code. OS Platform and Distribution : ubuntu16.04
TensorFlow installed from : source
Bazel version : 0.9.0
CUDA/cuDNN version : 8 / 6
GPU model and memory: nvidia 1080ti
Exact command to reproduce ; n/a

I try to learn tensorflow source code. When doing forward propagation, each layer will produce a feature map. I want to know after each layer is completed.Where is the feature map will be stored ? 
How can find this part? I hope i can know it.

I find the this part (conv2d):

    OP_REQUIRES_OK(context,
                   GetWindowedOutputSize(input_rows, filter_rows, stride_rows,
                                         padding_, &out_rows, &pad_rows));
    OP_REQUIRES_OK(context,
                   GetWindowedOutputSize(input_cols, filter_cols, stride_cols,
                                         padding_, &out_cols, &pad_cols));
    TensorShape out_shape =
        ShapeFromFormat(data_format_, batch, out_rows, out_cols, out_depth);

    // Output tensor is of the following dimensions:
    // [ in_batch, out_rows, out_cols, out_depth ]
    Tensor* output = nullptr;
    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));

Is the ""context"" saving the feature map after forward propagation ?
Thanks."
119,28407,1,"Performance issue with the C API. I'm currently working on a project that requires deep learning inference with Tensorflow's C API. I have a trained neural net (format: frozen graph) to do this. We use the inference for Computational Fluid Dynamics, which makes performance a key aspect for me. For example, one single simulation includes thousands of timesteps. In each timestep, the inference must be carried out for thousands of sets of input data. In my current case, I have a computational domain including 33400 cells and 880 boundary patches. That means, for each single of these thousands of timesteps I have to do the inference 34280 times. We use 3 input and 15 output values.

The whole inference process (from providing the input values to receiving the output values) requires a total of 91 milliseconds on my GPU. The actual inference step: TF_SessionRun(...) makes up for 98% of the computation time.



The problem now is that I need to do the inference 34280 times in every timestep, which then takes approximately 52 minutes. That means for thousands of timesteps, the computation time is extensive.

Surprisingly, if I convert the frozen graph to a uff-model and do the inference using TensorRT, it only takes me 90 milliseconds for all 34280 input sets. That means the speed-up of TensorRT vs. the C API would be about 35000. As we want to do the inference on a CPU-only architecture, later on, TensorRT is no option for me.

My question: do you know a way to use Tensorflow's C API in a way, that drastically reduces the computation time for multiple inferences? The bottleneck definitely is the TF_SessionRun(...) command, but I can not see a way to run 34280 inferences by only calling the command once. Moreover, the command provides several options (run options, run metadata, target operations, number of targets - see code above) that aren't used in a single example of those I found on the internet. Maybe these can be used to improve the performance?"
1259,1344,0,"op.device no longer showing which device an op is actually placed on. Using the latest builds, it appears that  now returns the device that an op is supposed to be placed on, i.e. what the user specifies, as opposed to what it is actually placed on. I believe this is true because when I set  to  when starting a , some ops, for example , cannot be placed on the GPU. But if I assign them to the GPU and then allow soft placement,  returns  for ops that were previously not placeable on the GPU.

This is a break from previous behavior, which showed the actual device an op is placed on. The new behavior makes it very difficult to debug device placement. Is there are any way to probe the actual device that an op is placed on?

P.S. I'm using a device function for my placement logic. Something like:


"
1305,19943,0,"i get the error Could not import tensorflow. Do not import tensorflow from its source directory; change directory to outside the TensorFlow source tree, and relaunch your Python interpreter from there.. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
959,27937,0,"Restoring from checkpoints are broken in TF 1.13.1. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary, pip3 install tensorflow-gpu
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: V10.0.130
- GPU model and memory: GTX 1060m, 6GB

**Describe the current behavior**
I am unable to restore the weights of any of my tf.keras models ONLY when restoring from a new initialization of the model. If I change the weights then restore without reinitializing the model, it will properly restore. Furthermore, a SILENT error is being thrown when this happens, requiring me to print the status of the restore to see it.

**Describe the expected behavior**
The weights should restore and not run into an error. And if an error would occur, it should be logged without me having to print it myself.

**Code to reproduce the issue**


**Other info / logs**
"
599,35333,0,"Optimizer within Estimator computes incorrectly small gradient when used with MirroredStrategy. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: Anaconda, clean install: 
- TensorFlow version: 2.0.0
- Python version: 3.7.5
- CUDA/cuDNN version: 10.0.130, 7.6.4
- GPU model and memory: Tesla V100, 16GB

**Describe the current behavior**
A gradient is reported that is only half of the expected gradient when using distributed training with Estimator. See example code below.
**Describe the expected behavior**
The gradient should be twice as large. If somehow this behavior is actually intended, then this needs to be much more loudly documented since it is quite unexpected - right now I cannot find any place where it is documented at all. For example, https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_estimator_limited_support and https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer#compute_gradients both have no mention of it.

**Code to reproduce the issue**


**Other info / logs**
The relevant portion of the output of the above test case is:

The model is attempting to fit two weights to be equal to 1 and 2, respectively, penalizing them by the squared error. The weights are initialized to 0.
* Single GPU case: We can see it reads one data element from the dataset, and correctly computes the loss as as ""1"" and ""4"", which are the squared differences. The gradients are correctly computed as ""-2"" and ""-4"", which indeed the mathematical derivatives of (x-1)^2 and (x-2)^2 at x = 0, respectively. So this is all correct.
* Multi GPU case: We can see each GPU independently and in parallel reads an element from the dataset, reading two elements in total. Each one correctly computes the loss as ""1"" and ""4"" as before on its own element. However, each one separately only computes ""-1"" and ""-2"" as the gradient. This is wrong, each one is a factor of 2 too small.

(edit: corrected linux version, some grammar edits)"
772,32807,0,"AttributeError: 'Tensor' object has no attribute '_lazy_read'. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.5
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: no
- GPU model and memory: no

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
I run this following code

It print the good result in eager mode, but it return AttributeError: 'Tensor' object has no attribute '_lazy_read' when I close the eager mode. I think I do feed a tf.Variable to scatter_update, but it still return the error message.
**Describe the expected behavior**
Return the value like eager mode.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
1286,31944,0,"failed to bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windiws 2012 r2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.13.1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): tried so many version
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: none 
- GPU model and memory: none

**Describe the problem**
Hi, I followed the instruction on [Tensorflow](https://www.tensorflow.org/install/source_windows)
I had a hard time when doing this command

(I also found others said: change to -c opt, but not working either)
most of the error said 
**FAILED: Build did NOT complete successfully**

I have tried many different versions of Bazel, from 0.18.0 to 0.28.1

**Provide the exact sequence of commands / steps that you executed before running into the problem**
follow the instruction on the website

1. install python and tensorflow: which I already have
2. install bazel and add to %path%
3. install msys2 and add to %path%
4. install visual c++ build tools
5. python ./configure.py
6. bazel build error

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

thanks a lot!!"
1072,27054,0,"tensorflow 2.0 transfer_learning tutorial: tensorflow_datasets error on local jupyter notebook Anaconda Win10. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Anaconda on Windows 10
- TensorFlow installed from (source or binary): install by pip in anaconda environment
- TensorFlow version (use command below): tensorflow-gpu 2.0 alpha
- Python version: 3.6.8
- CUDA/cuDNN version: Cuda toolkit 10.0; cuDNN 7.5
- GPU model and memory: GTX 980 Ti 6Gb


**Describe the current behavior**
I run the [transfer_learning.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/images/transfer_learning.ipynb) tutorial on/r2 for Tensroflow 2.0 on Jupyter Notebook in Anaconda Windows 10 and I get the error on step:


**Error**: 
Downloading / extracting dataset cats_vs_dogs (786.68 MiB) to C:\Users\Khoa\tensorflow_datasets\cats_vs_dogs\2.0.0...

Dl Completed...: 0 url [00:00, ? url/s]

Dl Size...: 0 MiB [00:00, ? MiB/s]



0 examples [00:00, ? examples/s]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-7-2bc776459ab0> in <module>
      4 (raw_train, raw_validation, raw_test), metadata = tfds.load(
      5     'cats_vs_dogs', split=list(splits),
----> 6     with_info=True, as_supervised=True)

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\registered.py in load(name, split, data_dir, batch_size, download, as_supervised, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs)
    251   if download:
    252     download_and_prepare_kwargs = download_and_prepare_kwargs or {}
--> 253     dbuilder.download_and_prepare(**download_and_prepare_kwargs)
    254 
    255   if as_dataset_kwargs is None:

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in download_and_prepare(self, download_dir, download_config)
    217         self._download_and_prepare(
    218             dl_manager=dl_manager,
--> 219             max_examples_per_split=download_config.max_examples_per_split)
    220 
    221         # NOTE: If modifying the lines below to put additional information in

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in _download_and_prepare(self, dl_manager, max_examples_per_split)
    666       self._file_format_adapter.write_from_generator(
    667           make_generator_fn(**split_generator.gen_kwargs),
--> 668           output_files,
    669       )
    670 

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in write_from_generator(self, generator_fn, output_files)
    105     wrapped = (
    106         _dict_to_tf_example(d).SerializeToString() for d in generator_fn())
--> 107     _write_tfrecords_from_generator(wrapped, output_files, shuffle=True)
    108 
    109   def dataset_from_filename(self, filename):

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in _write_tfrecords_from_generator(generator, output_files, shuffle)
    270     with _close_on_exit(writers) as writers:
    271       logging.info(""Writing TFRecords"")
--> 272       _round_robin_write(writers, generator)
    273     # Shuffle each shard
    274     if shuffle:

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in _round_robin_write(writers, generator)
    283 def _round_robin_write(writers, generator):
    284   """"""Write records from generator round-robin across writers.""""""
--> 285   for i, example in enumerate(tqdm.tqdm(generator, unit="" examples"")):
    286     writers[i % len(writers)].write(example)
    287 

D:\Anaconda\envs\tf2\lib\site-packages\tqdm\_tqdm.py in __iter__(self)
   1020                 """"""), fp_write=getattr(self.fp, 'write', sys.stderr.write))
   1021 
-> 1022             for obj in iterable:
   1023                 yield obj
   1024                 # Update and possibly print the progressbar.

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py in <genexpr>(.0)
    104   def write_from_generator(self, generator_fn, output_files):
    105     wrapped = (
--> 106         _dict_to_tf_example(d).SerializeToString() for d in generator_fn())
    107     _write_tfrecords_from_generator(wrapped, output_files, shuffle=True)
    108 

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in generator_fn()
    636 
    637       def generator_fn():
--> 638         for i, ex in enumerate(self._generate_examples(**kwargs)):
    639           # Use the DatasetInfo FeaturesDict to encode the example. This allows
    640           # the user's function to simply yield raw examples from the source

D:\Anaconda\envs\tf2\lib\site-packages\tensorflow_datasets\image\cats_vs_dogs.py in _generate_examples(self, archive)
    104     if num_skipped != _NUM_CORRUPT_IMAGES:
    105       raise ValueError(""Expected % corrupt images, but found %d"" % (
--> 106           _NUM_CORRUPT_IMAGES, num_skipped))
    107     logging.warning(""%d images were corrupted and were skipped"", num_skipped)

ValueError: Expected ۊorrupt images, but found 0


**Describe the expected behavior**
The code run perfectly on my Jupyter Notebook - Anaconda server Ubuntu 16.04 and also on Colab.
My Jupyter notebook is 5.7.4 on both windows and ubuntu.

"
1038,9186,0,"TFRecordReader num_records_produced() and num_work_units_completed() do not work. Hey guys, since I've been dealing a lot with TFRecord files lately, I stumbled upon the following bug:






And when I call the two aforementioned functions they always return 0. I am sure I have used the records since I have seen the end results dumped to an excel spreadsheet.

Cheers, Kris

"
716,29083,0,"ImportError: DLL load failed: The specified module could not be found.. <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 7 Enterprise 64-bit (6.1, Build 7601)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): not sure
- TensorFlow version:  1.13.1
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:  cudatoolkit: 10.0.130/ cudnn: 7.3.1
- GPU model and memory: Intel(R) HD Graphics 520, 1888 MB



**Describe the problem**
I installed tensorflow from anaconda and first installed everything in base (root), upon running  I encountered this issue.
Then per suggestions from #22794 , I followed the instructions from [link](https://www.pugetsystems.com/labs/hpc/The-Best-Way-to-Install-TensorFlow-with-GPU-Support-on-Windows-10-Without-Installing-CUDA-1187/) which is detailed below. But the same issue persisted.


**Provide the exact sequence of commands / steps that you executed before running into the problem**




**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

> ImportError: Traceback (most recent call last):
>   File ""C:\Users\JOLOU\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\Users\JOLOU\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\Users\JOLOU\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""C:\Users\JOLOU\AppData\Local\Continuum\anaconda3\lib\imp.py"", line 242, in load_module
>     return load_dynamic(name, filename, file)
>   File ""C:\Users\JOLOU\AppData\Local\Continuum\anaconda3\lib\imp.py"", line 342, in load_dynamic
>     return _load(spec)
> ImportError: DLL load failed: The specified module could not be found.
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://www.tensorflow.org/install/errors
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help.
> 
> "
1076,24949,0,"Tensorflow random categorical. Hi,

I'm running 
https://www.tensorflow.org/tutorials/sequences/text_generation

When I arrive at the line the following error is produced. Is there an import missing?
sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()

AttributeError                            Traceback (most recent call last)
<ipython-input-23-60a341594c52> in <module>
----> 1 sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
      2 sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()

AttributeError: module 'tensorflow._api.v1.random' has no attribute 'categorical'

**System information**
- TensorFlow version: 1.12 Jupyter NoteBooks on Ubuntu
- Doc Link:


**Describe the documentation issue**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
302,6823,0,"Upgrade HighwayHash. The HighwayHash module which is downloaded as an external dependency in TensorFlow produces different hash results on big endian and little endian architectures. This causes the test  from  to fail on big endian. After raising an [issue](https://github.com/google/highwayhash/issues/35) with HighwayHash community, they have added a change to make hash values consistent across architectures through [commit](https://github.com/google/highwayhash/commit/cdde139127319cb5eb3917b635c4d2b182533cb4).

Will it be possible to pick this or higher commit of HighwayHash in TensorFlow?"
543,28105,0,"libtensorflowlite.so usage is not documented. **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X API 28 (android studio)
- TensorFlow installed from (source or binary): source
- TensorFlow version: master, r1.14
- Python version: 3.7
- Installed using virtualenv? pip? conda?: virtualenv pip
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): MSVC 14.1
- CUDA/cuDNN version: 10.0 / 7.4
- GPU model and memory: RTX 2080 / 8GB



**Describe the problem**
I am in windows, but  build succeeded in windows for me.
But I cannot build libtensorflowlite.so, it says not all outputs were created or valid.
I don't know how to look into further build error details with bazel.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python configure.py
- no XLA JIT
- no ROCm
- yes CUDA (or no CUDA, both fails)
- yes Eigen Strong Inline




**Any other info / logs**
I'm just curious, but is it possible to run tensorflow lite source codes (C++) in Windows 10 x86-64, if I successfully build this libtensorflowlite.so and include some libraries? I already succeeded in running tensorflow C++ codes with libtensorflow_cc.so in Windows.
I have read the docs and it seems like you guys offer a C++ api, but only in mobile environments?? Is tensorflow lite impossible to run in normal desktops?


**Additionally what I tried later (also didn't work)**
Ok, so I just gave up on native Windows tf lite, and tried to build android c++ shared lib with bazel. I used  in linux, because it seemed like building on windows bazel fails. I successfully built  and tried these instructions : https://stackoverflow.com/questions/49834875/problems-with-using-tensorflow-lite-c-api-in-android-studio-project
So I built shared lib on linux, and tried to use it in windows android studio. I thought that it's eventually for android, so it would be ok to do that. (is it?)
But I got an error : incompatible target.


why is it incompatible? am I even right in using this C++ api this way? why is there no guide in tensorflow?"
945,23853,0,"Graph optimized using tf.contrib.tensorrt is not loadable with TF_GraphImportGraphDef. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source.
- TensorFlow version (use command below): v1.12
- Python version: 2.7.12

- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 9.0/7.0.5
- GPU model and memory: 1080 Ti

**Describe the current behavior**

I optimize a TensorFlow graph with


, save the resulting graph, and try to load it in a C++ program using C API.

First, I call



and call  with the optimized graph.

I get the following error:



**Describe the expected behavior**

The call to  must succeed.

**Code to reproduce the issue**

It seems that the issue, although not in the bug tracker, should be already known to the authors: https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/contrib/tensorrt/ops/trt_engine_op.cc#L46
However, I can make a minimal example to reproduce the problem on demand.

**Other info / logs**

It is a pain that a TRT-optimized graph cannot be used outside of python now.
I would be happy to know about a workaround, in case one exists.
"
639,23979,0,"Save/Load problem with keras.layers.ReLU. Running this example

returns trackback
Sequential.from_config(config)clstensorflow/python/keras/layers/advanced_activations.pyget_configdeserialize_layernegative_slopenumpy.ndarray` "
447,16608,0,"Can't find Stochastic Tensors class. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **Ubuntu 16.04**:
- **source**:
- **TensorFlow version 1.5**:
- **3.6**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Apparently,  is not present in  which doesn't reflect here in the [api_guides](https://www.tensorflow.org/api_guides/python/contrib.bayesflow.stochastic_tensor). I have not mentioned my complete system information as I think it might not be needed, please let me know if it is required to further investigate the issue. Any help/suggestions would be highly appreciated.

### Source code / logs
"
16,9754,1,"TensorFlow weight initialization taking 99% of total run time. ### System information
- **Have I written custom code**: No (but custom data)
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: tensorflow-gpu (1.0.0 and 1.1) (Python 3.5)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GTX 1080
- **Exact command to reproduce**: 

### Describe the problem
I noticed that one of my networks was running slow and nvidia-smi was reporting only around ~10% GPU usage. After running the profiler, I saw that  process was taking the vast majority of running time (see photo).

[Profiler report](https://i.stack.imgur.com/Ymup2.png)

### Source code / logs
Weight declaration function (from MNIST tutorial):

Code in action:

Please note this question was also asked on StackOverflow. [Link](http://stackoverflow.com/q/43793066)"
78,9892,1,"tf.while_loop runs very slow. I'm writing a simple RNN implementation using tf.while_loop but it runs incredibly slow. Any insights would be incredibly helpful.

OS: Ubuntu/Linux (16.04)
TensorFlow: Compiled from source
TensorFlow Version: r1.1
Bazel Version: 0.4.5
CUDA/CuDNN Versions: 8.0/5.1
GPU Model/Memory: TitanX/12Gb

Here's the implementation:


It takes ~5 minutes to complete one epoch of the mnist dataset."
1387,31719,0,"keras.backend.gradients shows error as tf.gradients. **System information**
tensorflow 2.0b1
windows 10
anaconda python 3.7

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

**Describe the current behavior**
![20190817164347](https://user-images.githubusercontent.com/27112868/63209122-87e34700-c10f-11e9-8040-38dbdda1d351.png)


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.





**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

runfile('F:/2019/temp1.py', wdir='F:/2019')
WARNING: Logging before flag parsing goes to stderr.
W0817 16:46:12.144755  7996 ag_logging.py:145] Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, ) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>>: AssertionError: 
Reloaded modules: tmpd51tclm6
WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, ) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000016B87200DD8>>: AssertionError: 
Traceback (most recent call last):

  File ""<ipython-input-2-763868c416b6>"", line 1, in <module>
    runfile('F:/2019/temp1.py', wdir='F:/2019')

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 826, in runfile
    execfile(filename, namespace)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""F:/2019/temp1.py"", line 28, in <module>
    c = model( tf.constant(2.0, dtype=tf.float64))

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 712, in __call__
    outputs = self.call(inputs, *args, **kwargs)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 753, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 895, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 712, in __call__
    outputs = self.call(inputs, *args, **kwargs)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\layers\core.py"", line 785, in call
    return self.function(inputs, **arguments)

  File ""F:/2019/temp1.py"", line 12, in <lambda>
    V =  tf.keras.layers.Lambda( lambda z: K.gradients( z[ 0 ], z[ 1 ] ), output_shape = [1] )( [ y, x ] )

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 3568, in gradients
    loss, variables, colocate_gradients_with_ops=True)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 158, in gradients
    unconnected_gradients)

  File ""C:\Users\lenovo\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_util.py"", line 504, in _GradientsHelper
    raise RuntimeError(""tf.gradients is not supported when eager execution ""

RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.
"
1489,32911,0,"tf.losses.mean_squared_error returns a list in tensorflow '2.0.0-rc1'. Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 

TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (use command below): 'v2.0.0-rc0-101-gd2d2566'
Python version: 3.6.6 :: Anaconda, Inc.
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A



I just updated my TensorFlow to version:

using pip install.  I am using python version:



the bug is that   returns a list rather than a scaler. A simple code to replicate this:



returns:




you can find a way around it by using 



which returns:
"
521,15219,0,"tf.while_loop and tf.foldl do not support second order gradients. The example

illustrates that despite applying  to a static list, the internal implementation via while loops leads to a type error. The problem disappears if the fold operation is carried out manually using a for loop. While implementing  using the while loop clearly makes the operation  more widely applicable, it seems problematic if syntactic sugar can lead to code that has qualitative differences from a naive implementation using a static loop. I cannot help but wonder whether  could be more efficient in the static case as well, although that is more of a conjecture.

I think it would be nice if  (and other while loop derivatives) had a keyword that enabled or disabled the ""dynamic mode"" using while, or if, at the very least, the TypeError would occur at the operation so that the error is easier to trace.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Custom code.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**:
pip install.
- **TensorFlow version (use command below)**:
v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**:
3.5.4 
- **Bazel version (if compiling from source)**:
Not applicable.
- **GCC/Compiler version (if compiling from source)**:
Not applicable.
- **CUDA/cuDNN version**:
Did not use CUDA.
- **GPU model and memory**:
Did not use GPU.
- **Exact command to reproduce**:
"
968,32098,0,"Unable to create TF-TRT model using docker image tensorflow/tensorflow:nightly-gpu-py3. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes, my model is custom.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 Cuda Drivers
- GPU model and memory: T4, 12GB

**Describe the current behavior**

When I run the following code from the latest python3 gpu build, nothing happens.  When I run it from the nightly build, I'm able to get a model with 1 TRT Engine node.



Is there an issue with the latest gpu build?  

Thanks,

**Describe the expected behavior**

Should be able to create a TRT model using the latest docker build.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
371,19610,0,"tflite AllocateTensors error after ResizeInputTensor. OS Platform and Distribution: Android Native C, Android API 26
TensorFlow installed from: Compiled from the official github master branch
TensorFlow version: Master
Bazel versio: 0.13.0-homebrew
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A
Code below: 

AllocateTensors errors after resize the InputTensor, any suggestions? I am using mobilenet_ssd.tflite model."
454,12482,0,"Failure to build on OS/X. I have the following error when building on OS/X:



Earlier on I get the following warning:

"
1230,31212,0,"New name disambiguation for scope argment in tf.contrib.copy_graph.copy_op_to_graph. <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No):
yes
just add the following code in the place of the if else for new_name 
Code:



**Any Other info.**
 
Hi @anush-o , i just added an option to append the new name in the exemplary code, instead of only doing the substitution. 
This is done by  doing scope=name+'//' . This will allow to not override names without intention, but you could always add '//' if you need to define a new name block.

I think this could be implemented with a look-up table (dict) for readability.

Hope this helps. Meanwhile, i will be reading the governance guidelines for contribution to follow the right path.
"
1009,19697,0,"ppc64le: ///tensorflow/python/kernel_tests:matrix_solve_ls_op_test and svd_op_test core dump. Please assign this issue to me and add the tag: stat:community support

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, Have written a Dockerfile.gpu.ppc64le file and modified ci_parameterized_build.sh to allow for build/test runs on ppc64le. (Will submit as a PR)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master from may 30th
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.2.88, 7
- **GPU model and memory**: 2 P100 GPUs with 16 GB of memory each

- **Exact command to reproduce**:
bazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:svd_op_test
bazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:matrix_solve_ls_op_test



### Describe the problem


### Source code / logs
[svd_op_test.log](https://github.com/tensorflow/tensorflow/files/2063542/svd_op_test.log)
[matrix_solve_ls_op_test.log](https://github.com/tensorflow/tensorflow/files/2063544/matrix_solve_ls_op_test.log)
"
234,20915,1,"Heavily increased memory consumption for optimizing batch_norm in tf versions > 1.3.0 . - **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

    I attach a code example that is intended to profile memory consumption for different layers of a network under different tensorflow versions. This should in principle work out of the box.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

    VERSION=""16.04.3 LTS (Xenial Xerus)""
    VERSION_ID=""16.04""
    VERSION_CODENAME=xenial


- **TensorFlow installed from (source or binary)**:

    pip install tensorflow-gpu==1.3.0
    and
    pip install tensorflow-gpu==1.5.0
    and
    pip install tensorflow-gpu==1.8.0
    and
    pip install tensorflow-gpu==1.9.0

- **TensorFlow version (use command below)**:
The following tf versions were used to recreate the issue
    tf.GIT_VERSION = b'unknown' 
    tf.VERSION = 1.3.0
    and
    tf.GIT_VERSION = v1.5.0-0-g37aa430d84
    tf.VERSION = 1.5.0
    and
    tf.GIT_VERSION = v1.8.0-0-g93bc2e2072 
    tf.VERSION = 1.8.0
    and
    tf.GIT_VERSION = v1.9.0-0-g25c197e023
    tf.VERSION = 1.9.0
  
- **Python version**:
Python 3.5.5 :: Anaconda, Inc.
- **CUDA/cuDNN version**:
    For tf 1.3.0:
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2016 NVIDIA Corporation
    Built on Tue_Jan_10_13:22:03_CST_2017
    Cuda compilation tools, release 8.0, V8.0.61
    
    For tf 1.5.0, 1.8.0, 1.9.0
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2017 NVIDIA Corporation
    Built on Fri_Sep__1_21:08:03_CDT_2017
    Cuda compilation tools, release 9.0, V9.0.176
- **Bazel version (if compiling from source)**:
    N/A
- **GPU model and memory**:
    GeForce GTX 1080 Ti
    total memory shown as 10.91GiB
- **Exact command to reproduce**:
    python 'script shown below'

### Describe the problem
When trying to update from tensorflow 1.3.0 to a newer version, we noticed that memory consumption increased significantly for our networks.  
We tried to find out what was causing this issue and realized that there is a large discrepancy for memory consumption in our batch normalization layers when optimizing the network in training.
I attach the code necessary to reproduce the results. 
In the code the following happens:
   First, a rather useless network of 10 batch normalization layers is created. In the examples shown below, this network expects 1D input of width 500 with 32 channels and a batch size of 16. 
   Second, a GPU profiler is started, repeatedly calling nvidia-smi to check the current memory consumption. While the memory usage is measured, the network is evaluated 2500 times, under many different conditions. Lastly, the timings and the memory consumption over time are saved and plotted for comparison of the tf versions. 

### What is the problem? 

The results from running the code under 4 different conditions is shown in the plots below.
1. Fused batch norm is used if possible, *with gradient update*, version specific batch norm
2. Fused batch norm is used if possible, *without gradient update*, version specific batch norm
3. No fused batch norm, *without gradient update*, all use batch normalization as in 
https://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py
4. No fused batch norm,*with gradient update*, all use batch normalization as in 
https://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py

For 3 and 4, we still run the different tf versions, but we copied the mentioned file directly to our repo and call the batch_normalization from that file.

Furthermore, currently the layout optimizer in the session config is turned off, but we also tried this with the layout optimizer turned on and this has no effect on this issue.

#### 1.
[batch_norm_fused_optimizing_step.pdf](https://github.com/tensorflow/tensorflow/files/2205279/batch_norm_fused_optimizing_step.pdf)
#### 2.
[batch_norm_fused_without_optimizing_step.pdf](https://github.com/tensorflow/tensorflow/files/2205281/batch_norm_fused_without_optimizing_step.pdf)
#### 3.
[batch_norm_unfused_1.3_implementaion.pdf](https://github.com/tensorflow/tensorflow/files/2205284/batch_norm_unfused_1.3_implementaion.pdf)
#### 4.
[batch_norm_unfused_1.3_implementaion_with_update.pdf](https://github.com/tensorflow/tensorflow/files/2205285/batch_norm_unfused_1.3_implementaion_with_update.pdf)

From these plots, it seems that versions > 1.3.0 need a significant amount of extra memory when doing a gradient update step. For example, memory requirements for Batch norm in 1.9 increased by 50% compared to 1.3. Why is that?

### Request
Is there a way to circumvent the increased memory consumption? I assume some optimizing is happening in the background in newer versions, which leads to an increase in memory requirements. Can this be turned off?


### Source code / logs
Source code necessary to reproduce the problem is attached below.

In principle, however, I repeatedly run nvidia-smi for memory profiling and measure timing and memory for the following 

[code.zip](https://github.com/tensorflow/tensorflow/files/2205292/code.zip)
"
1281,554,0,"Does the released Inception-v3 model only support forward pass with batch_size=1?. Hi all,
I am new to TF. I am playing with Inception-v3 model and I found that I can only pass  to the model. Here is my test code:



If I change  with , then TF would complain about the size mismatch.

Am I doing something wrong? Any pointer would be of great help.
"
43,22098,1,"Memory issue when inter_op_parallelism_threads > 1 on Ubuntu 16.04. ### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
floatnp.floatingnp.float64 == np.dtype(float).type
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:


Save this file as :



I run  and memory increases from ~500MiB to ~1200MiB after 100 calls of , **running on CPU**. Here is truncated output:



### Describe the problem

In short, memory keeps increasing after each predict call. I tried the above script with other networks such as  and pure tensorflow convnets and I see the same issue.

This issue seems to be resolved when I set  in the  as such:



And re-appears when I start increasing  to 32 (I'm running on 32 CPU cores).

### Another issue:

As I mentioned previously, setting  stops the memory from increasing on each predict call. Here is a script that sets the config  and does not exhibit a memory leak:


However, I see memory increasing again if I add an extra call to  before calling the one with the :



It seems like the ConfigProto from the first  is overwriting the config from the next session I create.

---

Here is a graph of the script above for 1000 predict calls:

![tf_issue](https://user-images.githubusercontent.com/7320238/45115233-3cfa3a80-b11d-11e8-9b5e-781508fc8583.png)
"
689,32072,0,"iOS GestureClassification (Type 'Interpreter' has no member 'Options'). When I build this project, I got the error
Type 'Interpreter' has no member 'Options'
Maybe changed class name recently, but the change is not complete
The file name is ModelDataHandler.swift and the line is 99

This is error message
![螢幕快照 2019-08-29 下午1 53 17](https://user-images.githubusercontent.com/32124047/63915118-bf36e980-ca67-11e9-8c32-6562128c73f5.png)

And I try this to fix
![螢幕快照 2019-08-29 下午1 57 12](https://user-images.githubusercontent.com/32124047/63915166-dd044e80-ca67-11e9-9381-7360a7685c5a.png)

"
1337,5485,0,"Possible bug when instantiating multi RNN cell in PTB LSTM model. Note sure whether this is a bug, since TensorFlow API is unclear on this. Would like someone more familiar with TensorFlow to investigate.

The possible bug is in file tensorflow/models/rnn/ptb/ptb_word_lm.py on line 115. The line reads:

cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers, state_is_tuple=True)

According to the API reference, the constructor for MultiRNNCell is:

tf.nn.rnn_cell.MultiRNNCell.__init__(cells, state_is_tuple=True)

where 

cells: list of RNNCells that will be composed in this order.

The following notation:

[lstm_cell] * config.num_layers 

will create a list of size config.num_layers, where each element in the list is a reference to the same object lstm_cell. This potentially means that if we modify one layer of ""cell"", we will also be modifying all the other layers. Is this what we want?"
1240,17981,0,"AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'leaky_relu'. Using https://github.com/hizhangp/yolo_tensorflow I get the following error. How should I fix it?
"
1002,26309,0,"tf.Data Pipeline using interleave starts fast then becomes extremely slow . ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.13
- **Python version**: 3.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9
- **GPU model and memory**: GTX 980 4GB
- **Exact command to reproduce**:

### Describe the problem

I am building a pipeline to feed a DNNRegressor model and the issue is that the input pipeline becomes extremely slow after the first few 100's steps.

I have thousands of TFRecord files of varying size (from 10MB to 3GB in size). Each input has 1038 columns. 
I am trying to create batches of size 512 with 1 ""row"" from each TFRecord file (each file has correlated data so I need 1 line from each of the 512 files being interleaved to have a ""shuffled"" batch). To do this,
I am using the Data.interleave method which seems perfect for my case.

Problem is the input is super fast at the start but gets painfully slow after the first 100's of steps.
CPU usage goes quickly from 100% at the start to near 0% as the pipeline becomes slower. However the disk read speed remains always at  > 60 MBps. What is the issue ? Are the smaller files straggling the pipeline in someway ?

I have tried using parralel interleave but that was even slower

### Source code / logs




Logs:

"
938,18956,0,"Fail to build cmake with gpu for _lstm and _rnn ops in windows. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: github master (should be 1.8?)
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: CMake build
- **GCC/Compiler version (if compiling from source)**: MSVC2015
- **CUDA/cuDNN version**:  CUDA 9.0 with CUDNN 7.0.5
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**:

cmake \
    	-DCMAKE_INSTALL_PREFIX=../install/ \
    	-DCMAKE_BUILD_TYPE=Release \
        -Dtensorflow_BUILD_SHARED_LIB=ON \
    	-Dtensorflow_BUILD_ALL_KERNELS=ON \
    	-Dtensorflow_BUILD_CONTRIB_KERNELS=ON \
    	-Dtensorflow_BUILD_CC_EXAMPLE=ON \
    	-Dtensorflow_BUILD_PYTHON_BINDINGS=ON \
    	-Dtensorflow_ENABLE_GRPC_SUPPORT=ON \
    	-Dtensorflow_ENABLE_SSL_SUPPORT=OFF \
    	-Dtensorflow_BUILD_CC_TESTS=OFF \
    	-Dtensorflow_BUILD_PYTHON_TESTS=OFF \
        -Dtensorflow_ENABLE_GPU=ON ..

### Describe the problem
In the [contrib/rnn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/BUILD) build, the gpu resource requires  . I found this is missed in the cmake 
[relevent position](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake)

 I have built pass in older version of tensorflow release (1.5) but not in 1.8. The cuda object built in rnn should be linked with blas_gemm? How can I modified the cmake files to do this?

### Source code / logs
The cmake file that fails to build



Bazel BUILD file in contrib/rnn:

"
764,5435,0,"User defined op without input + control dependencies causes op execution error. 1. Define a user-defined op without input, with 1 output:
REGISTER_OP(""MyOp"")
  .Output(""output: T"")
  .Attr(.....)

2. in python code:
op1 = my_op(attrs)

with tf.control_dependencies([op1]):
  op2 = otherOp

This runs OK, both op1 and op2 execute once:
sess.run(op2)


This is WRONG:
sess.run([op1, op2])
op1 will execute 2 times.

If adding an input for MyOp, op1 will execute only once as expected for 
 sess.run([op1, op2])"
991,23038,0,"AssertAllCloseToAccordingToType()  Error. 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
no
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.08
- **Python version**:
3.0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
CPU

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Comparing two results from division (Array-a / Array-b), if I use self.AssertAllEqual(a,b), it works perfectly, however, if I use self.AssertAllCloseAccordingToType(a , b), it gives me an error, saying:
not close where =  (array([], dtype=int64),)
not close lhs =  []
not close rhs =  []
not close dif =  []
not close tol =  []

I figured that there is a NaN in the array, but AssertAllEqual handles the NaN perfectly. 
"
849,32460,0,"Executor error message in GradientTape.jacobian. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.141-1-MANJARO-x86_64-with-arch-Manjaro-Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0
- Python version: 3.7.4
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: /
- GPU model and memory: /

**Describe the current behavior**
I am calculating the Jacobian of a model with respect to its parameters using a gradient tape. The method  outputs an error message about a missing function library. The program does not abort however and the computed Jacobian is correct.

**Describe the expected behavior**
There should be no error message.

**Code to reproduce the issue**


**Other info / logs**
The full output of the program shown above is:

With additional layers, more error messages like the one quoted above appear, one per matrix multiplication.
The problem persists when the Jacobian calculation is moved into a  decorated function."
1459,35146,0,"Missing information when saving model in tf format. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary (pip)
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: Python 3.6.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA 10.0.130_411.31; cuDNN 10.0 v7.6.5.32
- GPU model and memory: NVIDIA Quadro P2000, 4 GB

**Describe the current behavior**
When the model is saved in the default tf format, warnings are logged when trying to serve the model.

Examplary warning logs:


When the model is saved in the hdf5 format, the warnings do not occur.

**Describe the expected behavior**
The save formats should be equivalent and behave in the same way.

**Code to reproduce the issue**
Execute the following scripts to create and serve model
1. Run the first script with  which saves the model in tf format, **restart the Python console**, serve the model with the second script which creates the aforementioned warnings.
1. When running the scripts with , the model is saved in hdf5 format and no warnings appear.

Model creation:


Model serving:


**Other info / logs**
The warnings occur only if more than two vGPUs are used."
1458,6064,0,"AttributeError: module 'tensorflow.contrib.slim' has no attribute 'nets'. ### Environment info
Operating System:

1. A link to the pip package you installed: 
2. The output from :  

### reproducible example

    import tensorflow as tf
    import tensorflow.contrib.slim as slim
    from tensorflow.examples.tutorials.mnist import input_data
    vgg = tf.contrib.slim.nets.vgg

    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
    x = tf.placeholder(""float"", shape=[None, 784])
    y_ = tf.placeholder(""float"", shape=[None, 10])
    pred = vgg.vgg16(x)

    cross_entropy = -tf.reduce_sum(y_ * tf.log(pred))



### Error Message:

    Traceback (most recent call last):
    File ""resnet.py"", line 4, in <module>
    vgg = tf.contrib.slim.nets.vgg
    AttributeError: module 'tensorflow.contrib.slim' has no attribute 'nets'
"
23,33315,1,"Training slows down with repeated calls to Model.fit(). **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Version 1903
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)]
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Calls to  run slower and slower the more there have been. The sample code below reports about 200 us/sample at the start but after running for 20 minutes report about 400 us per sample. This continues to increase with no apparent upper limit. It remains slow even when training subsequent models but resets to fast after restarting the program.

**Describe the expected behavior**
No long term upward trend in training time.

**Code to reproduce the issue**

    import numpy
    import tensorflow as tf
    input = tf.keras.Input(shape=(1600,))
    z = tf.keras.layers.Dense(units=200, activation='relu')(input)
    z = tf.keras.layers.Dense(units=1)(z)
    model = tf.keras.Model(inputs=input, outputs=z)
    model.compile(loss='mse')
    x = numpy.full((100,1600), fill_value=2.34, dtype=numpy.float32)
    y = numpy.full((100,1), fill_value=1.23, dtype=numpy.float32)
    while True:
      tf.keras.backend.clear_session()
      model.fit(x=x, y=y)


**Other info / logs**
"
283,29115,0,"Support RaggedTensors in sequence feature columns . **System information**
- TensorFlow version (you are using): 2.0.0-dev20190527
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Currently  columns support only sparse tensor inputs.
It would be great if they will support ragged tensors too. At least thru type checking and  method (but maybe there will be a better performing way).

**Will this change the current api? How?**
No, API won't changes.

**Who will benefit with this feature?**
Developers who work with NLP models where ragged tensors are more natural than Sparse ones.

**Any Other info.**
See  and especially comment  from https://github.com/tensorflow/tensorflow/issues/29113"
750,9220,0,"How to recompile eigen3 library. I changed some code in the //tensorflow/third_party/eigen3, and recompile the C++ sample (label_image). I looked at the code of the  file in label_image and it depends on the //tensorflow/core, and I think the latter depends on the eigen3. However when I run the recompiled label_image, the code I've changed in eigen3 doesn't work. I am not very familiar with bazel so I hope someone can help me on this. How can I recompile the eigen3 to be reflected in the example?"
999,15056,0,"tf.losses.mean_squared_error is actually sum of squares. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171124
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

To truly get mean squared error one has to explictly use . This indicates bad naming. Maybe  is a better name, also similar to other names in "
1096,28038,0,"how to use tf.contrib.framework.fuse_op to fuse graph. Hello,
I want to use tf.contrib.framework.fuse_op to fuse graph to fuse my graph for faster inference,but when i call the api 

I got an error


I cannot find the detail deacription of the parameters, the source code as followings:
 

what is the correct input of the api?"
84,34987,1,"The quantized model has low accuracy on Android. I use these code  train mobilenet V2 model. Then I freeze graph . The last run this command to get  tensorflow lite model.



**But the accuracy of the results is very low.**

The accuracy of the non-quantized model did not decrease."
299,12496,0,SSL certificate for tensorflow.org expired. The SSL certificate for https://tensorflow.org (*not* https://www.tensorflow.org) expired on June 29.
525,34426,0,"Error on distributed training "" Variable is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key."". I have this issue when I try to run distributed training with my own custom training loop. 

There is something going wrong when calling apply_gradient. 

**System information**
Test on Google Colab with GPU TF 2.0

**Code to reproduce the issue**



**Error Track**
tf.distribute.Strategyvarfn()"
644,21995,0,"[solved] Android Static Library in Visual Studio with Tensorflow C++ API. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Any Android device
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.7
- **NDK version**: 14b
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:


### Describe the problem

Hi,
currently, I have been trying to use Tensorflow C++ API to create an Android static library using Visual Studio. There are some Stack Overflow Questions related to this ([1](https://stackoverflow.com/questions/45066790/tensorflow-c-example-on-android), [2](https://stackoverflow.com/questions/48647592/compile-tensorflow-c-api-for-arm64-v8a), [3](https://stackoverflow.com/questions/50964234/using-tensorflow-c-in-android-with-cmake), [4](https://stackoverflow.com/questions/48181092/tensorflow-c-input-output-tensor-reshape)), but all with no answers. I was unable to find issues about it in Tensorflow's repo.

I built Tensorflow for Android using Bazel in a Linux Ubuntu 18.04.1 LTS machine following [this instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android). Thus, I have a **libtensorflow_inference.so** and a java binding (**libandroid_tensorflow_inference_java.jar**). For my Android static library project in Visual Studio, I'm using the _libtensorflow_inference.so_ as ""_Additional Dependency_"" and the following ""_Additional Include Directories_"":



However, I'm getting the following errors:



Therefore, my questions are:
- It is possible to use the __libtensorflow_inference.so__ for a C++ Android static library?
- Am I missing addiitional dependencies?
"
617,18096,0,"Feature Request: Support for configuring deterministic options of cudNN Conv routines. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.6
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 7.1
- **GPU model and memory**: GPU
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

http://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility
cudNN documentation indicates that there are several routine options for , , and  operations. They default to non-deterministic atomic operations, but have the option to run in a deterministic mode. To achieve determinism on TensorFlow GPU, I would like to be able to make this performance trade-off, but currently cannot find a way to enable these options in TensorFlow.

Can a user-facing option be added, perhaps in , to configurate these cudNN routines? This could be configured in a similar way as  and  are set to 1 to achieve determinism on CPU (https://stackoverflow.com/questions/41233635/meaning-of-inter-op-parallelism-threads-and-intra-op-parallelism-threads)

### Source code / logs

N/A"
1012,33968,0,"Can not compile the tensorflow lite example. When I use the command ""make -f tensorflow/lite/experimental/micro/tools/make/Makefile micro_speech"", it shows the error message below:
**./tensorflow/lite/experimental/micro/kernels/activation_utils.h:43:23: error: ‘signbit’ was not declared in this scope 
    return signbit(a);**
I am sure I had build this example successfully few days ago(I built it on Ubuntu 16.04 x64). Is there anything changed for this example this week? Thanks.
"
358,32902,0,"bug when supplying metadata files for embeddings in tensorboard callback. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.0.0-dev20190927
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
getting the error:
Embeddingkeras.callbacks.TensorBoardembeddings_metadata
when passing embeddings_metadata to tensorflow.keras.callback.TesnorBoard
**Describe the expected behavior**
use the metadata in tensorboard
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L1547


should be:

"
1139,22936,0,"Invalid loop strucure. I had converted a h5 file to pb file, then while loading the pb file I get the following error.

Traceback (most recent call last):
  File ""import_model.py"", line 244, in <module>
    detections = sess.run(detectionsT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid loop structure: Loop ""mrcnn_detection/map/while/while_context"" has more than one LoopCond node: ""mrcnn_detection/map/while/LoopCond_1"" and ""mrcnn_detection/map/while/LoopCond"". This is an internal bug, please file a bug report with instructions on how to reproduce the error.


Have I written custom code - No, forked from - https://github.com/GustavZ/Mobile_Mask_RCNN
OS Platform and Distribution- linux ubuntu 16.04
TensorFlow installed from - using pip
Bazel version- N/A
CUDA/cuDNN version- N/A
GPU model and memory- N/A
Exact command to reproduce - convert the h5 file to pb and then load the pb file
Mobile device - N/A"
485,4158,0,"Add support for NVIDIA nccl library. The nccl library supports fast GPU-GPU communications and tensorflow could leverage its use.

https://github.com/NVIDIA/nccl

I think I read somewhere that Google is implementing this internally, but would be good to know what is the plan.
"
702,34905,0,"`experimental_relax_shapes` argument of `tf.function` does not work on instance methods. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Stretch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0, 2.1.0rc0
- Python version: 3.5.3

**Describe the current behavior**
Consider the following source:


Then all calls of  cause retracing of A.f, even with .

Note that if

is used instead of , the  kicks in and the retracing stops after third call to .

**Describe the expected behavior**

The  should work also on instance method.

**Code to reproduce the issue**

Given above.

**Other info / logs**

The problem is that when creating the instance method wrapper in
  https://github.com/tensorflow/tensorflow/blob/746e018d181f52b04f77811b1fbdf9bccdbd1d83/tensorflow/python/eager/function.py#L3179
several parameters including experimental_relax_shapes are not copied, see
  https://github.com/tensorflow/tensorflow/blob/746e018d181f52b04f77811b1fbdf9bccdbd1d83/tensorflow/python/eager/function.py#L3221-L3225"
870,34176,0,"tf.Identity messes determinism (initialization order changed?). ### System information
- **OS Platform and Distribution** : Linux Ubuntu 16.04.6 
- **TensorFlow installed from** : PIP (binary)
- **TensorFlow version**: tensorflow-gpu==1.14.0
- **Python version**: 2.7.12
- **GPU model and memory**: No relevant : the issue occurs both on GPU and CPU
- **Exact command to reproduce**:  (test_script is provided in the source code section below)

### Describe the problem
I am expecting the function below to be deterministic no matter the value of the  parameter, though it is not :  


### Source code / logs
Here is the complete test_case (i.e ) : 


And here is the result log : 

"
327,24208,0,"Can Tensorflow (not lite) run on Android NDK C++. It looks like features like Dropout are not supported on Tensorflow Lite + Android NDK.

I've got a Keras model that uses GPU, Dropout, CuDnnLSTM layer and I want to convert it to a tflite file or run it on Tensorflow for Android.

Is this possible? Is there a guide that tells me what is and is not supported on tflite?"
318,17854,0,"tf.estimator.RunConfig return worker is not a valid task_type in the cluster_spec job. ### System information
- os/ubuntu1604/x86_64
- **Exact command to reproduce**: config = tf.estimator.RunConfig()
- docker image: tensorflow/tensorflow:1.4.0-gpu

### Describe the problem
tf.estimator.RunConfig return worker is not a valid task_type in the cluster_spec job

### Source code / logs
   os.environ['TF_CONFIG'] = json.dumps({
    ##'cluster': cluster,
    'cluster': {
        ""chief"" : chief_node,
        ""ps_hosts"": ps_hosts,
        ""worker_hosts"": worker_hosts
    },
    'task' : {
        'type' : FLAGS.job_name,
        'index': FLAGS.task_index,
    }
})
config = tf.estimator.RunConfig()

-------LOG---------
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/run_config.py"", line 464, in __init__
    self._init_distributed_setting_from_environment_var(tf_config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/run_config.py"", line 480, in _init_distributed_setting_from_environment_var
    self._cluster_spec, task_env, TaskType.CHIEF)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/run_config.py"", line 188, in _validate_task_type_and_task_id
    'variable.' % (task_type, cluster_spec))
ValueError: worker is not a valid task_type in the cluster_spec:
ClusterSpec({'chief': ['10.0.0.5:2223'], 'ps_hosts': ['10.0.0.5:2222'], 'worker_hosts': ['10.0.0.6:2222', '10.0.0.4:2222']})
"
1330,18044,0,"FailedPreconditionError (see above for traceback): Failed to rename: <file_name> to: <file_name> : The process cannot access the file because it is being used by another process. ; Broken pipe. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Pro 64-bit (10.0, Build 16299)
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.6.0
- **Python version**: 
b'unknown' 1.6.0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I try to run a model using tf.train.MonitoredTrainingSession(), but get an error when the CheckPointSaverHook() tries to save the model. Having loked at the target saving directory, the saver seems to create a temporary directory from shards (something I assume is done because of the model being quite big) for later use. When the saver later on tries to use the sharded files in this directory, it seems to get blocked by another process accessing those files, resulting in a broken pipe error.

I assume the problem has to do with the sharding mechanism, as this is the first time I've seen the saver having to save the checkpoints in shards. I am not sure though, so if you're sure something else is causing this error, you're probably right.

### Source code / logs
Below is the error message I get when running my code. 



The important part of this error message is, as I see it:


"
762,15211,0,"Deep MNIST - exit code 139 (interrupted by signal 11: SIGSEGV). ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have copied all the code lines from the tutorial Deep MNIST for Experts manually into a python file.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux, kernel: 4.13.12-1-ARCH
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})

### Describe the problem
I am trying to run the example code from the tutorial Deep MNIST for Experts by copying every line from the tutorial into a python file and running it. The first part of the script with the model with a single linear layer runs fine, but the second part with a multilayer convolutional network fails to run. When the last line of the script executes:  I get the following error: . 

I tried to evaluate the accuracy with a batch of size 50 from the testing data instead of supplying mnist.test.images and mnist.test.labels and that worked fine.
I was directed here from Stackoverflow because this was believed to be a bug [(Stackoverflow question).](https://stackoverflow.com/questions/47640511/tensorflow-deep-mnist-exit-code-139)

### Source code / logs
I am unfamiliar with gdb so if I have missed to provide any relevant debugging output I'll try to add it if you can tell me how to get it.

[Source code](https://hastebin.com/ebidihiwey.py)
[Stackframe](https://hastebin.com/jukuzejira.cpp)
[Backtrace](https://hastebin.com/baqecavora.vbs)
"
228,10672,1,"Strange performance: sparse tensor matmul in kernel_test. Sorry for the previous issue 
According to the document [HERE](https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_dense_matmul)

> tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks
A sparse [m, k] with % nonzero values between 1% and 80%
B dense [k, n]

When I run [sparse_tensor_dense_matmul_op_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/sparse_tensor_dense_matmul_op_test.py) with large n,m,k
I get:

It is strange that  are very different when m=1821 -> 1820
BTY, I set the iterations to 10 for time saving...


Here's some information:
OS: Ubuntu 14.04
tf-version: 1.1.0  installed from source
GPU: NVIDIA K80, 4 kernels (only '/gpu:0' is used)
CUDA: 8.0
cudnn: 5.1.5

You can run sparse_tensor_dense_matmul_op_test.py with my settings

"
983,32812,0,deleted
1173,8471,0,"InvalidArgumentError (traceback): BiasGrad requires tensor size <= int32 max in 3D CNN, Tensorflow. I am using 3D CNN model in tensorflow. My input image dimension is 3D i.e. (192 * 256 * 256) with one color channel image and i have used three Convolutional layers (with patches of 5 * 5 * 5) and 3 pooling layers (kernal size : 3 * 3 * 3 and stride : 2 * 2 * 2), one full-connection-layer with 128 nodes and output layer with two nodes. The number of samples are 120 for training. Batch size : 10

I am facing the below error on the cluster:

W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Traceback (most recent call last):
  File ""./mri_cnn.py"", line 362, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./mri_cnn.py"", line 332, in main
    sess.run(optimizer, feed_dict={train_data_node: batch_data,train_labels_node: batch_labels})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: BiasGrad requires tensor size <= int32 max
         [[Node: gradients/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/Relu_grad/ReluGrad)]]

Caused by op u'gradients/BiasAdd_grad/BiasAddGrad', defined at:
  File ""./mri_cnn.py"", line 362, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./mri_cnn.py"", line 290, in main
    optimizer = tf.train.AdamOptimizer(learning_rate=0.0).minimize(train_loss) # Adam Optimizer
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 288, in minimize
    grad_loss=grad_loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 354, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 482, in gradients
    in_grads = grad_fn(op, *out_grads)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py"", line 204, in _BiasAddGrad
    data_format=data_format))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 313, in bias_add_grad
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

...which was originally created as op u'BiasAdd', defined at:
  File ""./mri_cnn.py"", line 362, in <module>
    tf.app.run()
[elided 0 identical lines from previous traceback]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./mri_cnn.py"", line 272, in main
    train_prediction = model(train_data_node, True)
  File ""./mri_cnn.py"", line 181, in model
    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 1316, in bias_add
    return gen_nn_ops._bias_add(value, bias, data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 281, in _bias_add
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): BiasGrad requires tensor size <= int32 max
         [[Node: gradients/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/Relu_grad/ReluGrad)]]

Please suggest, how to fix it?"
1477,28298,0,"auto_mixed_precision slow due to ""Sum"" being on blacklist.. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 30d87b5299a393cd0608ec1181af9f4529065749
- Python version: 2.7
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 10/7
- GPU model and memory: 8 V100s

**Describe the current behavior**

[Sum is on the blacklist](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/auto_mixed_precision_lists.h#L168) for the auto_mixed_precision grappler pass. Unfortunately, Sum is used in [the gradient of tf.add](https://github.com/tensorflow/tensorflow/blob/63db602e38c0c82bed9dfcf96f7df2b2041ffb20/tensorflow/python/ops/math_grad.py#L1008), if non-static shapes are used. This greatly slows down [resnet50](https://github.com/tensorflow/models/tree/master/official/resnet), from 709 to 551 images/sec. The Sums are done in fp32, and relus and batchnorm gradients are fp32 as well.

Using static shapes fixes this issue, but many users use dynamic shapes with tf.add.

Sum was added to the blacklist in 180542ba39fdb4a61cbe7a92d7ef512526383dea. The justification is ""The dice loss function used in U-net uses Sums that overflow in fp16""

**Describe the expected behavior**

Sum should not be on the blacklist, and resnet50 should get 709 images/sec. Maybe we can find a way to do the dice loss function Sum in fp32, but other Sums in fp16?

**Code to reproduce the issue**
N/A

**Other info / logs**
N/A

/CC @azaks2 @tfboyd @nluehr @benbarsdell @MattConley 
"
11,32787,1,"Noisy loss in distributed training. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12 & 1.13
- Python version: 3.6.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I am working on distributed learning in tensorflow through estimators API using below simple code template:



My model is defined as a simple 2-layer LSTM model in keras, and read_dataset() functions return datasets that I use for training and validation purposes.  The training/validation data files  and model directory are set in a shared place available to all workers. The whole code is exactly the same for servers (ps, chief, and worker) except the task setting in TF_CONFIG.
When I train model in single-worker configuration, the loss graph I see in tensorboard is gradually downward and reasonable.

![single](https://user-images.githubusercontent.com/17579773/65541564-975a6a80-dedb-11e9-9a56-f77d2b5906a8.jpg)

When using two machines of one chief and one worker, the total run time is less (as expected) but the loss graph is very noisy and higher than single-server case.

![double](https://user-images.githubusercontent.com/17579773/65541692-d092da80-dedb-11e9-96bc-a3a3b0f0d92e.jpg)

I expected to see the same performance in both cases, but it seems that training in the second server ruins the situation. Is there any special provision/setting/additional code that I should include in my work?"
176,35152,1,"Memory leaks when using tf.strings.split in map_func for tf.data.Dataset.map with eager execution.. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): 2.1.0rc1
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A 
- GCC/Compiler version (if compiling from source): N/A 
- CUDA/cuDNN version: N/A 
- GPU model and memory: N/A 

**Describe the current behavior**
If we use  in map_func to process each element in tf.data.Dataset, the used memory grows when we iterate the dataset and the used memory is not freed after iteration. What's more, the used memory continues to grow greatly if we repeatedly create the same tf.data.Dataset instance. However, the used memory keeps stable if we use tf.py_function to implement the split logic.
![image](https://user-images.githubusercontent.com/18071380/70906668-debb4780-2041-11ea-945c-bf2a05cbb637.png)


**Describe the expected behavior**
The used memory when iterating the dataset should be freed and should grow the create the same tf.data.Dataset instance.

**Code to reproduce the issue**
#### Experiment
map_functf.py_functiontf.data.Dataset` in [ElasticDL](https://github.com/sql-machine-learning/elasticdl)
"
648,14178,0,"what is this error stands for.?. Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
776,35156,0,"Item assigment. I really want to know why tf can't support item assigment like numpy


"
947,33134,0,"[TF 2.0 API Docs] tf.nn.softmax. ## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/softmax

## Description of issue (what needs changing):

### Clear description

Yes

### Correct links

yes

### Parameters defined

No. 
Per the code, the first argument ""logits"" can be of any type that can be passed to ""convert_to_tensor()"", not just a tensor. Therefore the documentation can be modified to include ""Tensor objects, numpy arrays, Python lists, and Python scalars"".

### Returns defined
yes

### Raises listed and defined
yes

### Submit a pull request?
yes
"
217,21921,1," Tensorflow lite model gives very different accuracy value compared to python model. ### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**:3.6
- **Bazel version (if compiling from source)**: 0.16.0
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**: See Source Code

### Describe the problem
I am using tensorflow 1.10 Python 3.6

My code is based in the premade iris classification model provided by TensorFlow. This means, I am using a Tensorflow DNN premade classifier, with the difference following difference:

10 features instead 4.
5 classes instead 3.
The test and training files can be downloaded from the following link: https://www.dropbox.com/sh/nmu8i2i8xe6hvfq/AADQEOIHH8e-kUHQf8zmmDMDa?dl=0

I have made a code to export this classifier to a tflite format, however the accuracy in the python model is higher than 75% but when exported the accuracy decrease approximately to 45% this means approximately 30% Accuracy is lost (This is too much). I have tried the code with different set of data and in all of them the accuracy after exporting decrease a lot! This made me think that something is going wrong with the TocoConverter function or that maybe I am exporting to tflite incorrectly, missing a parameter or something like that.

I share the code in which I calculate also the accuracy of the .tflite file.

I hope some of you can identify the error, or give a possible solution

### Source code / logs

"
1439,27519,0,"TF 2.0 'Tensor' object has no attribute 'numpy' while using .numpy() although eager execution enabled by default. Although Eager_execution is enabled by default in TF 2.0, I am getting errors while using .numpy()

Please note that i am not using the code in compatibility mode to TF 1.0.

  expt = [[[  0,   0,   0],
            [  4,  71, 141],
            [  0,   0,  0]],

           [[ 83,  25,  85],
            [ 90, 190, 143],
            [  4, 141,  49]],

           [[  0,   0,   0],
            [  4,  71,  49],
            [  0,   0,   0]]]
expt = tf.convert_to_tensor(expt)

expected_values = expt.numpy()


AttributeError: 'Tensor' object has no attribute 'numpy'


CPU TEST VERSION OF TENSORFLOW 2.0. "
475,33149,0,"AutoGraph error with XLA, MKL-DNN, Eager Execution, and custom keras layer. **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary, using conda
- TensorFlow version (use command below): unknown 1.14.0, with intel MKL
- Python version: 3.7.4
- CUDA/cuDNN version: no gpu
- GPU model and memory: no gpu

python build version: ('default', 'Aug 13 2019 20:35:49')
python compiler version: GCC 7.3.0
python implementation: CPython
os kernel version: #1 SMP Mon Jul 29 17:46:05 UTC 2019
os release version: 3.10.0-957.27.2.el7.x86_64
os platform: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
linux distribution: ('CentOS Linux', '7.6.1810', 'Core')
linux os distribution: ('centos', '7.6.1810', 'Core')
architecture: ('64bit', '')
machine: x86_64
== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)
== check pips ===================================================
numpy                1.16.4   
protobuf             3.9.2    
tensorflow           1.14.0   
tensorflow-estimator 1.14.0   
gast                    0.3.2

**Describe the current behavior**
AutoGraph not working properly with custom Keras Layer and eager execution.
export AUTOGRAPH_VERBOSITY=10

**Other info / logs**
Logs are included above."
225,32172,1,"Mirror Strategy slow down by adding GPUs. I am using the custom estimator with TFrecord. I am training VGG16. By increasing the number of GPUs the training time increase. All GPUs are in a single machine. GPUs utilization is about 100%. So it seems the input function can feed the data to GPUs well. I am using tf.layers.conv2d, tf.contrib.layers.flatten, tf.layers.dense to create the VGG Model. However, I have a large number of groups_Deps nodes in the computation graph. since all GPUs are in a single machine I am wondering why increasing number of GPUs lead to decreasing the global_Steps/Sec and also increasing the training time. 

![image](https://user-images.githubusercontent.com/17527773/64166782-195cf380-ce48-11e9-91d8-15ceade1db54.png)

"
481,27853,0,"Tensorflow variables not casting to ref type [BUG][TF 2.0]. <em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0.0-alpha
- Python version: 3.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
**Describe the current behavior**
When passed to different Function, tf.Variable dtype doesn't cast to _ref type.
For example:

Output: 
**Describe the expected behavior** (according to tf r1.13)

Output: 

**Code to reproduce the issue**
Given Above

**Other info / logs**
For this reason, a lot Function Calls like  are failing. This Leads to Failing of Keras Backend Calls, like , even in Graph Mode.
Traceback created after Appending  to the issue https://github.com/tensorflow/tensorflow/issues/27739, shows that assign_sub is failing as Tensor Object have no assign_sub. Upon Further Investigation, this error was found here:
https://github.com/tensorflow/tensorflow/blob/0c464c70cef2369b6ef5c5e17dbd2cda2a6107fb/tensorflow/python/ops/state_ops.py#L159-L162"
869,23416,0,"Error in compile  tensorflowr1.8 on windows by vs2015 (Error: LNK1189 ：More than 65535 object restrictions). <em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Windows 10):
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.8
- Python version:3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0/7
- GPU model and memory:2



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
348,29970,0,"Tensorflow installe using GPU but it is using only CPU. I have installed tensorflow from source using bazel on ubuntu 16.xx

tensorflow version 1.11.0
bazel 0.18.0rc6
python 2.7.11
cuda 9.2 (j'utilse celle dans dossier /home/ahmed/tmp/cuda-9.2)
cudnn 7.1.4
nccl 2.4.2

the build sucessfull , the installation is also OK.
However, the tensorflow does'nt use GPU.

Whe I run : 

import tensorflow as tf
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

**2019-06-19 17:49:28.775658: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: UNKNOWN ERROR (-1)
2019-06-19 17:49:28.775751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: ubuntu-Precision-Tower-7910
2019-06-19 17:49:28.775770: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: ubuntu-Precision-Tower-7910
2019-06-19 17:49:28.775831: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: Invalid argument: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""
2019-06-19 17:49:28.775898: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.54.0
Device mapping: no known devices.
2019-06-19 17:49:28.777666: I tensorflow/core/common_runtime/direct_session.cc:291] Device mapping:**


Wed Jun 19 17:54:39 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.54                 Driver Version: 396.54                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:04:00.0  On |                  N/A |
| 28%   49C    P8    19W / 250W |    180MiB / 11177MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN Xp            Off  | 00000000:05:00.0 Off |                  N/A |
| 23%   33C    P8    16W / 250W |      2MiB / 12196MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1385      G   /usr/lib/xorg/Xorg                           137MiB |
|    0      2744      G   compiz                                        41MiB |
+-----------------------------------------------------------------------------+
"
1472,31351,0,"tf.gradients with ""ValueError: Cannot create a tensor proto whose content is larger than 2GB."". **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary(pip tensorflow-gpu)
- TensorFlow version (use command below): 1.14.0 (v1.14.0-rc1-22-gaf24dc91b5)
- Python version: 3.6.7
- Bazel version (if compiling from source): Noe
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: 10.1/7.6.1
- GPU model and memory: 1080ti (12GB)

**Describe the current behavior**
Hi, 
I tried to use tf.gradeints to calculate  which is one of the Explainability Methods. To do that, I have to treat ~30,000 x ~30,000 matrix(pred) like the following code. My task is Knowledge-Graph.



However, I encountered . Is this bug? or Are there some methods to avoid this issue?

Thank you in advance,

ref) https://github.com/ankurtaly/Integrated-Gradients

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
